Conformance test: not doing test setup.
I0316 09:44:16.960851    8588 e2e.go:126] Starting e2e run "e8c1f544-07ec-46eb-9446-bdafdb3fef6d" on Ginkgo node 1
Mar 16 09:44:16.975: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /go/src/k8s.io/kubernetes/platforms/linux/amd64
=====================================================================================
Random Seed: 1678959856 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 16 09:44:17.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 09:44:17.184: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 16 09:44:17.637: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Mar 16 09:44:18.031: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 16 09:44:18.031: INFO: expected 14 pod replicas in namespace 'kube-system', 14 are Running and Ready.
Mar 16 09:44:18.031: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.26.1' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Mar 16 09:44:18.127: INFO: e2e test version: v1.26.1
Mar 16 09:44:18.215: INFO: kube-apiserver version: v1.26.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 16 09:44:18.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 09:44:18.306: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [1.125 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 16 09:44:17.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 09:44:17.184: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar 16 09:44:17.637: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
    Mar 16 09:44:18.031: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 16 09:44:18.031: INFO: expected 14 pod replicas in namespace 'kube-system', 14 are Running and Ready.
    Mar 16 09:44:18.031: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.26.1' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-host' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'network-problem-detector-pod' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Mar 16 09:44:18.127: INFO: e2e test version: v1.26.1
    Mar 16 09:44:18.215: INFO: kube-apiserver version: v1.26.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 16 09:44:18.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 09:44:18.306: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:18.351
Mar 16 09:44:18.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:44:18.352
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:18.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:18.796
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Mar 16 09:44:18.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 create -f -'
Mar 16 09:44:20.607: INFO: stderr: ""
Mar 16 09:44:20.607: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 16 09:44:20.607: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 create -f -'
Mar 16 09:44:21.372: INFO: stderr: ""
Mar 16 09:44:21.372: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/16/23 09:44:21.373
Mar 16 09:44:22.463: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:44:22.463: INFO: Found 0 / 1
Mar 16 09:44:23.463: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:44:23.463: INFO: Found 0 / 1
Mar 16 09:44:24.463: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:44:24.463: INFO: Found 1 / 1
Mar 16 09:44:24.463: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 16 09:44:24.552: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:44:24.552: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 16 09:44:24.552: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe pod agnhost-primary-sjcs8'
Mar 16 09:44:25.133: INFO: stderr: ""
Mar 16 09:44:25.133: INFO: stdout: "Name:             agnhost-primary-sjcs8\nNamespace:        kubectl-9054\nPriority:         0\nService Account:  default\nNode:             ip-10-250-19-136.ec2.internal/10.250.19.136\nStart Time:       Thu, 16 Mar 2023 09:44:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bd11936d85fa88bc73af8438a857d4b134aba5f57e6958d3da5d5a49642e4db8\n                  cni.projectcalico.org/podIP: 100.64.1.15/32\n                  cni.projectcalico.org/podIPs: 100.64.1.15/32\nStatus:           Running\nIP:               100.64.1.15\nIPs:\n  IP:           100.64.1.15\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://bce89ee07d42ad6d37600a417eb5fb6e32f25b30463e27d5bce7e74e6869cc21\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 16 Mar 2023 09:44:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-khqbw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-khqbw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-9054/agnhost-primary-sjcs8 to ip-10-250-19-136.ec2.internal\n  Normal  Pulling    4s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.43\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" in 2.349115595s (2.349132109s including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar 16 09:44:25.133: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe rc agnhost-primary'
Mar 16 09:44:25.788: INFO: stderr: ""
Mar 16 09:44:25.788: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9054\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-sjcs8\n"
Mar 16 09:44:25.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe service agnhost-primary'
Mar 16 09:44:26.433: INFO: stderr: ""
Mar 16 09:44:26.433: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9054\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.111.48.66\nIPs:               100.111.48.66\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.15:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 16 09:44:26.610: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe node ip-10-250-19-136.ec2.internal'
Mar 16 09:44:27.558: INFO: stderr: ""
Mar 16 09:44:27.558: INFO: stdout: "Name:               ip-10-250-19-136.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-19-136.ec2.internal\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=m5.large\n                    node.kubernetes.io/role=node\n                    topology.ebs.csi.aws.com/zone=us-east-1c\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1c\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.26.1\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 91cebe296e5c2f974e82352bdc2bea673db33abf7f4459b35925f67ff4933376\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0a6edab8fca492e36\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.19.136/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 16 Mar 2023 09:33:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-250-19-136.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 16 Mar 2023 09:44:24 +0000\nConditions:\n  Type                          Status    LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------    -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False     Thu, 16 Mar 2023 09:42:57 +0000   Thu, 16 Mar 2023 09:36:49 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False     Thu, 16 Mar 2023 09:41:35 +0000   Thu, 16 Mar 2023 09:35:20 +0000   NoNetworkProblems               no host network problems\n  FrequentDockerRestart         Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentDockerRestart         error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentContainerdRestart     Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentContainerdRestart     error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  KernelDeadlock                False     Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False     Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentUnregisterNetDevice   Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentUnregisterNetDevice   error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentKubeletRestart        Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentKubeletRestart        error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  MemoryPressure                False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True      Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:34:31 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.250.19.136\n  InternalDNS:  ip-10-250-19-136.ec2.internal\n  Hostname:     ip-10-250-19-136.ec2.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  31423468Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7830712Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  30568749647\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6679736Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec277dcafd29836b7021b132db450854\n  System UUID:                ec277dca-fd29-836b-7021-b132db450854\n  Boot ID:                    35b24b23-29e1-410d-bdb4-6ff16859ba39\n  Kernel Version:             5.14.21-150400.24.33-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      100.64.1.0/24\nPodCIDRs:                     100.64.1.0/24\nProviderID:                   aws:///us-east-1c/i-0a6edab8fca492e36\nNon-terminated Pods:          (17 in total)\n  Namespace                   Name                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits     Age\n  ---------                   ----                                               ------------  ----------  ---------------  -------------     ---\n  kube-system                 addons-nginx-ingress-controller-dfbd87bb4-tpjwc    23m (1%)      0 (0%)      128Mi (1%)       5497558138 (80%)  3m36s\n  kube-system                 apiserver-proxy-lt25p                              40m (2%)      0 (0%)      40Mi (0%)        1114Mi (17%)      10m\n  kube-system                 blackbox-exporter-5778995784-bpbqd                 11m (0%)      0 (0%)      11500k (0%)      128Mi (1%)        96s\n  kube-system                 calico-node-z796q                                  260m (13%)    0 (0%)      150Mi (2%)       2800Mi (42%)      10m\n  kube-system                 calico-typha-deploy-7c5596cb97-brtbb               320m (16%)    0 (0%)      262144k (3%)     4194304k (61%)    9m27s\n  kube-system                 csi-driver-node-mgpvm                              33m (1%)      0 (0%)      106Mi (1%)       3372Mi (51%)      7m26s\n  kube-system                 egress-filter-applier-cj2st                        50m (2%)      0 (0%)      64Mi (0%)        256Mi (3%)        10m\n  kube-system                 kube-proxy-worker-1-v1.26.1-jp7hq                  46m (2%)      0 (0%)      35074998 (0%)    2Gi (31%)         3m36s\n  kube-system                 metrics-server-96cc55556-ltn74                     23m (1%)      0 (0%)      60Mi (0%)        1Gi (15%)         96s\n  kube-system                 network-problem-detector-host-9gcfg                10m (0%)      50m (2%)    32Mi (0%)        64Mi (0%)         10m\n  kube-system                 network-problem-detector-pod-tqj4w                 10m (0%)      50m (2%)    32Mi (0%)        64Mi (0%)         10m\n  kube-system                 node-exporter-pvzcs                                11m (0%)      0 (0%)      50Mi (0%)        250Mi (3%)        36s\n  kube-system                 node-local-dns-4xz8b                               11m (0%)      0 (0%)      23574998 (0%)    94299992 (1%)     7m27s\n  kube-system                 node-problem-detector-fv295                        11m (0%)      0 (0%)      23574998 (0%)    120Mi (1%)        96s\n  kube-system                 vpn-shoot-55c8b48df6-6fhfb                         30m (1%)      0 (0%)      32Mi (0%)        100Mi (1%)        7m27s\n  kubectl-9054                agnhost-primary-sjcs8                              0 (0%)        0 (0%)      0 (0%)           0 (0%)            7s\n  kubernetes-dashboard        kubernetes-dashboard-5dd889f4f-zm2nt               11m (0%)      0 (0%)      23574998 (0%)    256Mi (3%)        2m36s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                900m (46%)        100m (5%)\n  memory             1107155736 (16%)  21945449426 (320%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type     Reason                             Age                From                          Message\n  ----     ------                             ----               ----                          -------\n  Normal   Starting                           3m33s              kube-proxy                    \n  Normal   Starting                           10m                kube-proxy                    \n  Normal   Starting                           8m24s              kube-proxy                    \n  Normal   NodeHasSufficientMemory            10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   Starting                           10m                kubelet                       Starting kubelet.\n  Warning  InvalidDiskCapacity                10m                kubelet                       invalid capacity 0 on image filesystem\n  Warning  UnscheduledNodeCriticalDaemonSets  10m                gardener-node-controller      Node-critical DaemonSets found that were not scheduled to Node yet: kube-system/apiserver-proxy, kube-system/calico-node, kube-system/csi-driver-node, kube-system/kube-proxy-worker-1-v1.26.1, kube-system/node-local-dns\n  Normal   NodeHasNoDiskPressure              10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID               10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced            10m                kubelet                       Updated Node Allocatable limit across pods\n  Normal   RegisteredNode                     10m                node-controller               Node ip-10-250-19-136.ec2.internal event: Registered Node ip-10-250-19-136.ec2.internal in Controller\n  Normal   Synced                             10m                cloud-node-controller         Node synced successfully\n  Warning  UnreadyNodeCriticalPods            10m (x2 over 10m)  gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-lt25p, kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6, kube-system/kube-proxy-worker-1-v1.26.1-bzhtx, kube-system/node-local-dns-txht6\n  Warning  UnreadyNodeCriticalPods            10m                gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-lt25p, kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6, kube-system/kube-proxy-worker-1-v1.26.1-bzhtx\n  Normal   NodeReady                          9m56s              kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeReady\n  Warning  UnreadyNodeCriticalPods            9m56s              gardener-node-controller      Unready node-critical Pods found on Node: kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6\n  Normal   NodeCriticalComponentsReady        9m46s              gardener-node-controller      All node-critical components got ready, removing taint\n  Normal   NoFrequentContainerdRestart        9m35s              systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentUnregisterNetDevice      9m35s              kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            9m35s              systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           9m35s              systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Warning  FailedNetworkChecks                8m35s              network-problem-detector-pod  cluster network problems for jobID/destination combinations: (https-p2api-int,nslookup-p)/kubernetes.default.svc.cluster.local\n  Normal   NoFrequentUnregisterNetDevice      8m26s              kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           8m26s              systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            8m26s              systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentContainerdRestart        8m26s              systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentUnregisterNetDevice      94s                kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           94s                systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            94s                systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentContainerdRestart        94s                systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n"
Mar 16 09:44:27.558: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe namespace kubectl-9054'
Mar 16 09:44:28.206: INFO: stderr: ""
Mar 16 09:44:28.206: INFO: stdout: "Name:         kubectl-9054\nLabels:       e2e-framework=kubectl\n              e2e-run=e8c1f544-07ec-46eb-9446-bdafdb3fef6d\n              kubernetes.io/metadata.name=kubectl-9054\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:28.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9054" for this suite. 03/16/23 09:44:28.383
------------------------------
• [10.122 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:18.351
    Mar 16 09:44:18.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:44:18.352
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:18.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:18.796
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Mar 16 09:44:18.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 create -f -'
    Mar 16 09:44:20.607: INFO: stderr: ""
    Mar 16 09:44:20.607: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 16 09:44:20.607: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 create -f -'
    Mar 16 09:44:21.372: INFO: stderr: ""
    Mar 16 09:44:21.372: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/16/23 09:44:21.373
    Mar 16 09:44:22.463: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:44:22.463: INFO: Found 0 / 1
    Mar 16 09:44:23.463: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:44:23.463: INFO: Found 0 / 1
    Mar 16 09:44:24.463: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:44:24.463: INFO: Found 1 / 1
    Mar 16 09:44:24.463: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 16 09:44:24.552: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:44:24.552: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 16 09:44:24.552: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe pod agnhost-primary-sjcs8'
    Mar 16 09:44:25.133: INFO: stderr: ""
    Mar 16 09:44:25.133: INFO: stdout: "Name:             agnhost-primary-sjcs8\nNamespace:        kubectl-9054\nPriority:         0\nService Account:  default\nNode:             ip-10-250-19-136.ec2.internal/10.250.19.136\nStart Time:       Thu, 16 Mar 2023 09:44:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bd11936d85fa88bc73af8438a857d4b134aba5f57e6958d3da5d5a49642e4db8\n                  cni.projectcalico.org/podIP: 100.64.1.15/32\n                  cni.projectcalico.org/podIPs: 100.64.1.15/32\nStatus:           Running\nIP:               100.64.1.15\nIPs:\n  IP:           100.64.1.15\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://bce89ee07d42ad6d37600a417eb5fb6e32f25b30463e27d5bce7e74e6869cc21\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 16 Mar 2023 09:44:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-khqbw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-khqbw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-9054/agnhost-primary-sjcs8 to ip-10-250-19-136.ec2.internal\n  Normal  Pulling    4s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.43\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" in 2.349115595s (2.349132109s including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Mar 16 09:44:25.133: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe rc agnhost-primary'
    Mar 16 09:44:25.788: INFO: stderr: ""
    Mar 16 09:44:25.788: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9054\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-sjcs8\n"
    Mar 16 09:44:25.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe service agnhost-primary'
    Mar 16 09:44:26.433: INFO: stderr: ""
    Mar 16 09:44:26.433: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9054\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.111.48.66\nIPs:               100.111.48.66\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.15:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 16 09:44:26.610: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe node ip-10-250-19-136.ec2.internal'
    Mar 16 09:44:27.558: INFO: stderr: ""
    Mar 16 09:44:27.558: INFO: stdout: "Name:               ip-10-250-19-136.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-19-136.ec2.internal\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=true\n                    node.kubernetes.io/instance-type=m5.large\n                    node.kubernetes.io/role=node\n                    topology.ebs.csi.aws.com/zone=us-east-1c\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1c\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.26.1\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 91cebe296e5c2f974e82352bdc2bea673db33abf7f4459b35925f67ff4933376\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0a6edab8fca492e36\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"true\",\"no...\n                    projectcalico.org/IPv4Address: 10.250.19.136/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 16 Mar 2023 09:33:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-250-19-136.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 16 Mar 2023 09:44:24 +0000\nConditions:\n  Type                          Status    LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------    -----------------                 ------------------                ------                          -------\n  ClusterNetworkProblem         False     Thu, 16 Mar 2023 09:42:57 +0000   Thu, 16 Mar 2023 09:36:49 +0000   NoNetworkProblems               no cluster network problems\n  HostNetworkProblem            False     Thu, 16 Mar 2023 09:41:35 +0000   Thu, 16 Mar 2023 09:35:20 +0000   NoNetworkProblems               no host network problems\n  FrequentDockerRestart         Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentDockerRestart         error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentContainerdRestart     Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentContainerdRestart     error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  KernelDeadlock                False     Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False     Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentUnregisterNetDevice   Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentUnregisterNetDevice   error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentKubeletRestart        Unknown   Thu, 16 Mar 2023 09:42:54 +0000   Thu, 16 Mar 2023 09:42:53 +0000   NoFrequentKubeletRestart        error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  MemoryPressure                False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False     Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:33:51 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True      Thu, 16 Mar 2023 09:42:31 +0000   Thu, 16 Mar 2023 09:34:31 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.250.19.136\n  InternalDNS:  ip-10-250-19-136.ec2.internal\n  Hostname:     ip-10-250-19-136.ec2.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  31423468Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7830712Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  30568749647\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6679736Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec277dcafd29836b7021b132db450854\n  System UUID:                ec277dca-fd29-836b-7021-b132db450854\n  Boot ID:                    35b24b23-29e1-410d-bdb4-6ff16859ba39\n  Kernel Version:             5.14.21-150400.24.33-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      100.64.1.0/24\nPodCIDRs:                     100.64.1.0/24\nProviderID:                   aws:///us-east-1c/i-0a6edab8fca492e36\nNon-terminated Pods:          (17 in total)\n  Namespace                   Name                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits     Age\n  ---------                   ----                                               ------------  ----------  ---------------  -------------     ---\n  kube-system                 addons-nginx-ingress-controller-dfbd87bb4-tpjwc    23m (1%)      0 (0%)      128Mi (1%)       5497558138 (80%)  3m36s\n  kube-system                 apiserver-proxy-lt25p                              40m (2%)      0 (0%)      40Mi (0%)        1114Mi (17%)      10m\n  kube-system                 blackbox-exporter-5778995784-bpbqd                 11m (0%)      0 (0%)      11500k (0%)      128Mi (1%)        96s\n  kube-system                 calico-node-z796q                                  260m (13%)    0 (0%)      150Mi (2%)       2800Mi (42%)      10m\n  kube-system                 calico-typha-deploy-7c5596cb97-brtbb               320m (16%)    0 (0%)      262144k (3%)     4194304k (61%)    9m27s\n  kube-system                 csi-driver-node-mgpvm                              33m (1%)      0 (0%)      106Mi (1%)       3372Mi (51%)      7m26s\n  kube-system                 egress-filter-applier-cj2st                        50m (2%)      0 (0%)      64Mi (0%)        256Mi (3%)        10m\n  kube-system                 kube-proxy-worker-1-v1.26.1-jp7hq                  46m (2%)      0 (0%)      35074998 (0%)    2Gi (31%)         3m36s\n  kube-system                 metrics-server-96cc55556-ltn74                     23m (1%)      0 (0%)      60Mi (0%)        1Gi (15%)         96s\n  kube-system                 network-problem-detector-host-9gcfg                10m (0%)      50m (2%)    32Mi (0%)        64Mi (0%)         10m\n  kube-system                 network-problem-detector-pod-tqj4w                 10m (0%)      50m (2%)    32Mi (0%)        64Mi (0%)         10m\n  kube-system                 node-exporter-pvzcs                                11m (0%)      0 (0%)      50Mi (0%)        250Mi (3%)        36s\n  kube-system                 node-local-dns-4xz8b                               11m (0%)      0 (0%)      23574998 (0%)    94299992 (1%)     7m27s\n  kube-system                 node-problem-detector-fv295                        11m (0%)      0 (0%)      23574998 (0%)    120Mi (1%)        96s\n  kube-system                 vpn-shoot-55c8b48df6-6fhfb                         30m (1%)      0 (0%)      32Mi (0%)        100Mi (1%)        7m27s\n  kubectl-9054                agnhost-primary-sjcs8                              0 (0%)        0 (0%)      0 (0%)           0 (0%)            7s\n  kubernetes-dashboard        kubernetes-dashboard-5dd889f4f-zm2nt               11m (0%)      0 (0%)      23574998 (0%)    256Mi (3%)        2m36s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                900m (46%)        100m (5%)\n  memory             1107155736 (16%)  21945449426 (320%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type     Reason                             Age                From                          Message\n  ----     ------                             ----               ----                          -------\n  Normal   Starting                           3m33s              kube-proxy                    \n  Normal   Starting                           10m                kube-proxy                    \n  Normal   Starting                           8m24s              kube-proxy                    \n  Normal   NodeHasSufficientMemory            10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   Starting                           10m                kubelet                       Starting kubelet.\n  Warning  InvalidDiskCapacity                10m                kubelet                       invalid capacity 0 on image filesystem\n  Warning  UnscheduledNodeCriticalDaemonSets  10m                gardener-node-controller      Node-critical DaemonSets found that were not scheduled to Node yet: kube-system/apiserver-proxy, kube-system/calico-node, kube-system/csi-driver-node, kube-system/kube-proxy-worker-1-v1.26.1, kube-system/node-local-dns\n  Normal   NodeHasNoDiskPressure              10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID               10m (x2 over 10m)  kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced            10m                kubelet                       Updated Node Allocatable limit across pods\n  Normal   RegisteredNode                     10m                node-controller               Node ip-10-250-19-136.ec2.internal event: Registered Node ip-10-250-19-136.ec2.internal in Controller\n  Normal   Synced                             10m                cloud-node-controller         Node synced successfully\n  Warning  UnreadyNodeCriticalPods            10m (x2 over 10m)  gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-lt25p, kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6, kube-system/kube-proxy-worker-1-v1.26.1-bzhtx, kube-system/node-local-dns-txht6\n  Warning  UnreadyNodeCriticalPods            10m                gardener-node-controller      Unready node-critical Pods found on Node: kube-system/apiserver-proxy-lt25p, kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6, kube-system/kube-proxy-worker-1-v1.26.1-bzhtx\n  Normal   NodeReady                          9m56s              kubelet                       Node ip-10-250-19-136.ec2.internal status is now: NodeReady\n  Warning  UnreadyNodeCriticalPods            9m56s              gardener-node-controller      Unready node-critical Pods found on Node: kube-system/calico-node-z796q, kube-system/csi-driver-node-c7zk6\n  Normal   NodeCriticalComponentsReady        9m46s              gardener-node-controller      All node-critical components got ready, removing taint\n  Normal   NoFrequentContainerdRestart        9m35s              systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentUnregisterNetDevice      9m35s              kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            9m35s              systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           9m35s              systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Warning  FailedNetworkChecks                8m35s              network-problem-detector-pod  cluster network problems for jobID/destination combinations: (https-p2api-int,nslookup-p)/kubernetes.default.svc.cluster.local\n  Normal   NoFrequentUnregisterNetDevice      8m26s              kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           8m26s              systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            8m26s              systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentContainerdRestart        8m26s              systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentUnregisterNetDevice      94s                kernel-monitor                Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentKubeletRestart           94s                systemd-monitor               Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentDockerRestart            94s                systemd-monitor               Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n  Normal   NoFrequentContainerdRestart        94s                systemd-monitor               Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart, message: \"error watching journald: failed to stat the log path \\\"/var/log/journal\\\": stat /v\"\n"
    Mar 16 09:44:27.558: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9054 describe namespace kubectl-9054'
    Mar 16 09:44:28.206: INFO: stderr: ""
    Mar 16 09:44:28.206: INFO: stdout: "Name:         kubectl-9054\nLabels:       e2e-framework=kubectl\n              e2e-run=e8c1f544-07ec-46eb-9446-bdafdb3fef6d\n              kubernetes.io/metadata.name=kubectl-9054\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:28.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9054" for this suite. 03/16/23 09:44:28.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:28.475
Mar 16 09:44:28.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 09:44:28.477
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:28.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:28.92
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/16/23 09:44:29.187
STEP: Verify that the required pods have come up. 03/16/23 09:44:29.277
Mar 16 09:44:29.367: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 09:44:29.367
Mar 16 09:44:29.367: INFO: Waiting up to 5m0s for pod "test-rs-fphgs" in namespace "replicaset-9637" to be "running"
Mar 16 09:44:29.456: INFO: Pod "test-rs-fphgs": Phase="Pending", Reason="", readiness=false. Elapsed: 89.103115ms
Mar 16 09:44:31.546: INFO: Pod "test-rs-fphgs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178663705s
Mar 16 09:44:33.546: INFO: Pod "test-rs-fphgs": Phase="Running", Reason="", readiness=true. Elapsed: 4.178662265s
Mar 16 09:44:33.546: INFO: Pod "test-rs-fphgs" satisfied condition "running"
STEP: Getting /status 03/16/23 09:44:33.546
Mar 16 09:44:33.638: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/16/23 09:44:33.638
Mar 16 09:44:33.817: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/16/23 09:44:33.817
Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: ADDED
Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:33.906: INFO: Found replicaset test-rs in namespace replicaset-9637 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 16 09:44:33.906: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/16/23 09:44:33.906
Mar 16 09:44:33.907: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 16 09:44:33.998: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/16/23 09:44:33.998
Mar 16 09:44:34.086: INFO: Observed &ReplicaSet event: ADDED
Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:34.087: INFO: Observed replicaset test-rs in namespace replicaset-9637 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
Mar 16 09:44:34.087: INFO: Found replicaset test-rs in namespace replicaset-9637 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 16 09:44:34.087: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:34.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9637" for this suite. 03/16/23 09:44:34.264
------------------------------
• [5.880 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:28.475
    Mar 16 09:44:28.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 09:44:28.477
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:28.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:28.92
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/16/23 09:44:29.187
    STEP: Verify that the required pods have come up. 03/16/23 09:44:29.277
    Mar 16 09:44:29.367: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 09:44:29.367
    Mar 16 09:44:29.367: INFO: Waiting up to 5m0s for pod "test-rs-fphgs" in namespace "replicaset-9637" to be "running"
    Mar 16 09:44:29.456: INFO: Pod "test-rs-fphgs": Phase="Pending", Reason="", readiness=false. Elapsed: 89.103115ms
    Mar 16 09:44:31.546: INFO: Pod "test-rs-fphgs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178663705s
    Mar 16 09:44:33.546: INFO: Pod "test-rs-fphgs": Phase="Running", Reason="", readiness=true. Elapsed: 4.178662265s
    Mar 16 09:44:33.546: INFO: Pod "test-rs-fphgs" satisfied condition "running"
    STEP: Getting /status 03/16/23 09:44:33.546
    Mar 16 09:44:33.638: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/16/23 09:44:33.638
    Mar 16 09:44:33.817: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/16/23 09:44:33.817
    Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: ADDED
    Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:33.906: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:33.906: INFO: Found replicaset test-rs in namespace replicaset-9637 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 16 09:44:33.906: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/16/23 09:44:33.906
    Mar 16 09:44:33.907: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 16 09:44:33.998: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/16/23 09:44:33.998
    Mar 16 09:44:34.086: INFO: Observed &ReplicaSet event: ADDED
    Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:34.087: INFO: Observed replicaset test-rs in namespace replicaset-9637 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 16 09:44:34.087: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 16 09:44:34.087: INFO: Found replicaset test-rs in namespace replicaset-9637 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 16 09:44:34.087: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:34.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9637" for this suite. 03/16/23 09:44:34.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:34.356
Mar 16 09:44:34.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:44:34.357
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:34.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:34.801
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 09:44:34.978
Mar 16 09:44:34.978: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 16 09:44:35.353: INFO: stderr: ""
Mar 16 09:44:35.353: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/16/23 09:44:35.353
STEP: verifying the pod e2e-test-httpd-pod was created 03/16/23 09:44:40.457
Mar 16 09:44:40.457: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 get pod e2e-test-httpd-pod -o json'
Mar 16 09:44:40.815: INFO: stderr: ""
Mar 16 09:44:40.815: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"51a7e897d3ea3efee2b8a0d863bb4cae4837c810656c0023aac16ad7cbbfd5cc\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.17/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.17/32\"\n        },\n        \"creationTimestamp\": \"2023-03-16T09:44:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-419\",\n        \"resourceVersion\": \"5947\",\n        \"uid\": \"5a3d6c77-605b-43b7-88d8-122a824d386e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-hsp9h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-19-136.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-hsp9h\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://52e280f92237ed015ea683ea6d801e000420d3110537d01ec74fa41f7fd216b2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-16T09:44:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.19.136\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.17\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.17\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-16T09:44:35Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/16/23 09:44:40.815
Mar 16 09:44:40.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 replace -f -'
Mar 16 09:44:42.146: INFO: stderr: ""
Mar 16 09:44:42.146: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/16/23 09:44:42.146
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Mar 16 09:44:42.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 delete pods e2e-test-httpd-pod'
Mar 16 09:44:44.166: INFO: stderr: ""
Mar 16 09:44:44.166: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-419" for this suite. 03/16/23 09:44:44.342
------------------------------
• [10.076 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:34.356
    Mar 16 09:44:34.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:44:34.357
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:34.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:34.801
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 09:44:34.978
    Mar 16 09:44:34.978: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 16 09:44:35.353: INFO: stderr: ""
    Mar 16 09:44:35.353: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/16/23 09:44:35.353
    STEP: verifying the pod e2e-test-httpd-pod was created 03/16/23 09:44:40.457
    Mar 16 09:44:40.457: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 get pod e2e-test-httpd-pod -o json'
    Mar 16 09:44:40.815: INFO: stderr: ""
    Mar 16 09:44:40.815: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"51a7e897d3ea3efee2b8a0d863bb4cae4837c810656c0023aac16ad7cbbfd5cc\",\n            \"cni.projectcalico.org/podIP\": \"100.64.1.17/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.64.1.17/32\"\n        },\n        \"creationTimestamp\": \"2023-03-16T09:44:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-419\",\n        \"resourceVersion\": \"5947\",\n        \"uid\": \"5a3d6c77-605b-43b7-88d8-122a824d386e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-hsp9h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-19-136.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-hsp9h\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-16T09:44:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://52e280f92237ed015ea683ea6d801e000420d3110537d01ec74fa41f7fd216b2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-16T09:44:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.19.136\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.17\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.17\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-16T09:44:35Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/16/23 09:44:40.815
    Mar 16 09:44:40.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 replace -f -'
    Mar 16 09:44:42.146: INFO: stderr: ""
    Mar 16 09:44:42.146: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/16/23 09:44:42.146
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Mar 16 09:44:42.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-419 delete pods e2e-test-httpd-pod'
    Mar 16 09:44:44.166: INFO: stderr: ""
    Mar 16 09:44:44.166: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-419" for this suite. 03/16/23 09:44:44.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:44.433
Mar 16 09:44:44.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 09:44:44.435
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:44.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:44.879
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 03/16/23 09:44:45.056
Mar 16 09:44:45.151: INFO: Waiting up to 5m0s for pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7" in namespace "downward-api-5922" to be "Succeeded or Failed"
Mar 16 09:44:45.240: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.083621ms
Mar 16 09:44:47.329: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178539406s
Mar 16 09:44:49.330: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179302271s
STEP: Saw pod success 03/16/23 09:44:49.33
Mar 16 09:44:49.330: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7" satisfied condition "Succeeded or Failed"
Mar 16 09:44:49.425: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 container dapi-container: <nil>
STEP: delete the pod 03/16/23 09:44:49.537
Mar 16 09:44:49.631: INFO: Waiting for pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 to disappear
Mar 16 09:44:49.719: INFO: Pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:49.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5922" for this suite. 03/16/23 09:44:49.896
------------------------------
• [5.553 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:44.433
    Mar 16 09:44:44.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 09:44:44.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:44.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:44.879
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 03/16/23 09:44:45.056
    Mar 16 09:44:45.151: INFO: Waiting up to 5m0s for pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7" in namespace "downward-api-5922" to be "Succeeded or Failed"
    Mar 16 09:44:45.240: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.083621ms
    Mar 16 09:44:47.329: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178539406s
    Mar 16 09:44:49.330: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179302271s
    STEP: Saw pod success 03/16/23 09:44:49.33
    Mar 16 09:44:49.330: INFO: Pod "downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7" satisfied condition "Succeeded or Failed"
    Mar 16 09:44:49.425: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 container dapi-container: <nil>
    STEP: delete the pod 03/16/23 09:44:49.537
    Mar 16 09:44:49.631: INFO: Waiting for pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 to disappear
    Mar 16 09:44:49.719: INFO: Pod downward-api-4b2a509f-07fe-48a8-a76b-fbe90dc765f7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:49.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5922" for this suite. 03/16/23 09:44:49.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:49.987
Mar 16 09:44:49.987: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 09:44:49.988
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:50.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:50.432
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-n6xfl" 03/16/23 09:44:50.609
Mar 16 09:44:50.700: INFO: Get Replication Controller "e2e-rc-n6xfl" to confirm replicas
Mar 16 09:44:50.788: INFO: Found 1 replicas for "e2e-rc-n6xfl" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-n6xfl" 03/16/23 09:44:50.788
STEP: Updating a scale subresource 03/16/23 09:44:50.878
STEP: Verifying replicas where modified for replication controller "e2e-rc-n6xfl" 03/16/23 09:44:50.968
Mar 16 09:44:50.968: INFO: Get Replication Controller "e2e-rc-n6xfl" to confirm replicas
Mar 16 09:44:51.057: INFO: Found 2 replicas for "e2e-rc-n6xfl" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3087" for this suite. 03/16/23 09:44:51.147
------------------------------
• [1.250 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:49.987
    Mar 16 09:44:49.987: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 09:44:49.988
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:50.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:50.432
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-n6xfl" 03/16/23 09:44:50.609
    Mar 16 09:44:50.700: INFO: Get Replication Controller "e2e-rc-n6xfl" to confirm replicas
    Mar 16 09:44:50.788: INFO: Found 1 replicas for "e2e-rc-n6xfl" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-n6xfl" 03/16/23 09:44:50.788
    STEP: Updating a scale subresource 03/16/23 09:44:50.878
    STEP: Verifying replicas where modified for replication controller "e2e-rc-n6xfl" 03/16/23 09:44:50.968
    Mar 16 09:44:50.968: INFO: Get Replication Controller "e2e-rc-n6xfl" to confirm replicas
    Mar 16 09:44:51.057: INFO: Found 2 replicas for "e2e-rc-n6xfl" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3087" for this suite. 03/16/23 09:44:51.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:51.238
Mar 16 09:44:51.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 09:44:51.24
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:51.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:51.686
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 03/16/23 09:44:51.863
STEP: Getting a ResourceQuota 03/16/23 09:44:51.953
STEP: Listing all ResourceQuotas with LabelSelector 03/16/23 09:44:52.042
STEP: Patching the ResourceQuota 03/16/23 09:44:52.131
STEP: Deleting a Collection of ResourceQuotas 03/16/23 09:44:52.221
STEP: Verifying the deleted ResourceQuota 03/16/23 09:44:52.312
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 09:44:52.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4740" for this suite. 03/16/23 09:44:52.504
------------------------------
• [1.375 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:51.238
    Mar 16 09:44:51.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 09:44:51.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:51.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:51.686
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 03/16/23 09:44:51.863
    STEP: Getting a ResourceQuota 03/16/23 09:44:51.953
    STEP: Listing all ResourceQuotas with LabelSelector 03/16/23 09:44:52.042
    STEP: Patching the ResourceQuota 03/16/23 09:44:52.131
    STEP: Deleting a Collection of ResourceQuotas 03/16/23 09:44:52.221
    STEP: Verifying the deleted ResourceQuota 03/16/23 09:44:52.312
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:44:52.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4740" for this suite. 03/16/23 09:44:52.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:44:52.614
Mar 16 09:44:52.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 09:44:52.616
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:52.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:53.06
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/16/23 09:44:53.327
Mar 16 09:44:53.422: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2364" to be "running and ready"
Mar 16 09:44:53.511: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.276585ms
Mar 16 09:44:53.511: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:44:55.602: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179560169s
Mar 16 09:44:55.602: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:44:57.601: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.178981311s
Mar 16 09:44:57.601: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 16 09:44:57.601: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 03/16/23 09:44:57.69
Mar 16 09:44:57.783: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2364" to be "running and ready"
Mar 16 09:44:57.872: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 88.999955ms
Mar 16 09:44:57.872: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:44:59.962: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.179242855s
Mar 16 09:44:59.962: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 16 09:44:59.962: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/16/23 09:45:00.051
STEP: delete the pod with lifecycle hook 03/16/23 09:45:00.186
Mar 16 09:45:00.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 16 09:45:00.365: INFO: Pod pod-with-poststart-http-hook still exists
Mar 16 09:45:02.367: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 16 09:45:02.456: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:02.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2364" for this suite. 03/16/23 09:45:02.633
------------------------------
• [10.109 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:44:52.614
    Mar 16 09:44:52.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 09:44:52.616
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:44:52.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:44:53.06
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/16/23 09:44:53.327
    Mar 16 09:44:53.422: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2364" to be "running and ready"
    Mar 16 09:44:53.511: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.276585ms
    Mar 16 09:44:53.511: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:44:55.602: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179560169s
    Mar 16 09:44:55.602: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:44:57.601: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.178981311s
    Mar 16 09:44:57.601: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 16 09:44:57.601: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 03/16/23 09:44:57.69
    Mar 16 09:44:57.783: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2364" to be "running and ready"
    Mar 16 09:44:57.872: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 88.999955ms
    Mar 16 09:44:57.872: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:44:59.962: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.179242855s
    Mar 16 09:44:59.962: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 16 09:44:59.962: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/16/23 09:45:00.051
    STEP: delete the pod with lifecycle hook 03/16/23 09:45:00.186
    Mar 16 09:45:00.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 16 09:45:00.365: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 16 09:45:02.367: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 16 09:45:02.456: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:02.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2364" for this suite. 03/16/23 09:45:02.633
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:02.723
Mar 16 09:45:02.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 03/16/23 09:45:02.724
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:02.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:03.168
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 16 09:45:03.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 16 09:45:03.524: INFO: Waiting for terminating namespaces to be deleted...
Mar 16 09:45:03.614: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
Mar 16 09:45:03.795: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 16 09:45:03.795: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container proxy ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 09:45:03.795: INFO: blackbox-exporter-5778995784-bpbqd from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 09:45:03.795: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 09:45:03.795: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 09:45:03.795: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 09:45:03.795: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 09:45:03.795: INFO: kube-proxy-worker-1-v1.26.1-jp7hq from kube-system started at 2023-03-16 09:40:52 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 09:45:03.795: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 09:45:03.795: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 09:45:03.795: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 09:45:03.795: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 09:45:03.795: INFO: node-local-dns-4xz8b from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 09:45:03.795: INFO: node-problem-detector-fv295 from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 09:45:03.795: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container vpn-shoot ready: true, restart count 0
Mar 16 09:45:03.795: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.795: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 16 09:45:03.795: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
Mar 16 09:45:03.907: INFO: pod-handle-http-request from container-lifecycle-hook-2364 started at 2023-03-16 09:44:53 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container container-handle-http-request ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container container-handle-https-request ready: true, restart count 0
Mar 16 09:45:03.907: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Mar 16 09:45:03.907: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container proxy ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 09:45:03.907: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 09:45:03.907: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 09:45:03.907: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 09:45:03.907: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 09:45:03.907: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 09:45:03.907: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 09:45:03.907: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container coredns ready: true, restart count 0
Mar 16 09:45:03.907: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container coredns ready: true, restart count 0
Mar 16 09:45:03.907: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 09:45:03.907: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 09:45:03.907: INFO: kube-proxy-worker-1-v1.26.1-6bxjn from kube-system started at 2023-03-16 09:37:01 +0000 UTC (2 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 09:45:03.907: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 09:45:03.907: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 09:45:03.907: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 09:45:03.907: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 09:45:03.907: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 09:45:03.907: INFO: node-local-dns-4kfz2 from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 09:45:03.907: INFO: node-problem-detector-5q5bx from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 09:45:03.907: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 09:45:03.907: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 09:45:03.907
Mar 16 09:45:04.001: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-437" to be "running"
Mar 16 09:45:04.091: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 89.076859ms
Mar 16 09:45:06.180: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.178715177s
Mar 16 09:45:06.180: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 09:45:06.269
STEP: Trying to apply a random label on the found node. 03/16/23 09:45:06.361
STEP: verifying the node has the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 42 03/16/23 09:45:06.543
STEP: Trying to relaunch the pod, now with labels. 03/16/23 09:45:06.632
Mar 16 09:45:06.724: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-437" to be "not pending"
Mar 16 09:45:06.813: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 88.887913ms
Mar 16 09:45:08.904: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.179077719s
Mar 16 09:45:08.904: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 off the node ip-10-250-19-136.ec2.internal 03/16/23 09:45:08.993
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 03/16/23 09:45:09.264
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-437" for this suite. 03/16/23 09:45:09.444
------------------------------
• [6.810 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:02.723
    Mar 16 09:45:02.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 03/16/23 09:45:02.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:02.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:03.168
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 16 09:45:03.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 16 09:45:03.524: INFO: Waiting for terminating namespaces to be deleted...
    Mar 16 09:45:03.614: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
    Mar 16 09:45:03.795: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: blackbox-exporter-5778995784-bpbqd from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: kube-proxy-worker-1-v1.26.1-jp7hq from kube-system started at 2023-03-16 09:40:52 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: node-local-dns-4xz8b from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: node-problem-detector-fv295 from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container vpn-shoot ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.795: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 16 09:45:03.795: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
    Mar 16 09:45:03.907: INFO: pod-handle-http-request from container-lifecycle-hook-2364 started at 2023-03-16 09:44:53 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container container-handle-http-request ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container container-handle-https-request ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: kube-proxy-worker-1-v1.26.1-6bxjn from kube-system started at 2023-03-16 09:37:01 +0000 UTC (2 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: node-local-dns-4kfz2 from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: node-problem-detector-5q5bx from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 09:45:03.907: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 09:45:03.907: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 09:45:03.907
    Mar 16 09:45:04.001: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-437" to be "running"
    Mar 16 09:45:04.091: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 89.076859ms
    Mar 16 09:45:06.180: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.178715177s
    Mar 16 09:45:06.180: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 09:45:06.269
    STEP: Trying to apply a random label on the found node. 03/16/23 09:45:06.361
    STEP: verifying the node has the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 42 03/16/23 09:45:06.543
    STEP: Trying to relaunch the pod, now with labels. 03/16/23 09:45:06.632
    Mar 16 09:45:06.724: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-437" to be "not pending"
    Mar 16 09:45:06.813: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 88.887913ms
    Mar 16 09:45:08.904: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.179077719s
    Mar 16 09:45:08.904: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 off the node ip-10-250-19-136.ec2.internal 03/16/23 09:45:08.993
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ac5f604-8ae7-45ad-856a-3a49fbf4d2c4 03/16/23 09:45:09.264
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-437" for this suite. 03/16/23 09:45:09.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:09.535
Mar 16 09:45:09.535: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 03/16/23 09:45:09.536
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:09.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:09.98
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 03/16/23 09:45:10.157
Mar 16 09:45:10.251: INFO: Waiting up to 5m0s for pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d" in namespace "containers-3972" to be "Succeeded or Failed"
Mar 16 09:45:10.340: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.074649ms
Mar 16 09:45:12.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178890792s
Mar 16 09:45:14.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179144964s
STEP: Saw pod success 03/16/23 09:45:14.43
Mar 16 09:45:14.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d" satisfied condition "Succeeded or Failed"
Mar 16 09:45:14.520: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d container agnhost-container: <nil>
STEP: delete the pod 03/16/23 09:45:14.67
Mar 16 09:45:14.767: INFO: Waiting for pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d to disappear
Mar 16 09:45:14.856: INFO: Pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:14.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3972" for this suite. 03/16/23 09:45:15.032
------------------------------
• [5.588 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:09.535
    Mar 16 09:45:09.535: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 03/16/23 09:45:09.536
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:09.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:09.98
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 03/16/23 09:45:10.157
    Mar 16 09:45:10.251: INFO: Waiting up to 5m0s for pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d" in namespace "containers-3972" to be "Succeeded or Failed"
    Mar 16 09:45:10.340: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.074649ms
    Mar 16 09:45:12.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178890792s
    Mar 16 09:45:14.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179144964s
    STEP: Saw pod success 03/16/23 09:45:14.43
    Mar 16 09:45:14.430: INFO: Pod "client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d" satisfied condition "Succeeded or Failed"
    Mar 16 09:45:14.520: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 09:45:14.67
    Mar 16 09:45:14.767: INFO: Waiting for pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d to disappear
    Mar 16 09:45:14.856: INFO: Pod client-containers-0ce49db6-6d75-48ac-aa79-8034d4d94b7d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:14.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3972" for this suite. 03/16/23 09:45:15.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:15.131
Mar 16 09:45:15.131: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 03/16/23 09:45:15.133
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:15.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:15.577
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/16/23 09:45:15.769
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/16/23 09:45:16.499
STEP: Deploying the custom resource conversion webhook pod 03/16/23 09:45:16.59
STEP: Wait for the deployment to be ready 03/16/23 09:45:16.77
Mar 16 09:45:17.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 09:45:19.127
STEP: Verifying the service has paired with the endpoint 03/16/23 09:45:19.225
Mar 16 09:45:20.226: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 16 09:45:20.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 03/16/23 09:45:23.133
STEP: Create a v2 custom resource 03/16/23 09:45:23.405
STEP: List CRs in v1 03/16/23 09:45:23.571
STEP: List CRs in v2 03/16/23 09:45:23.665
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4799" for this suite. 03/16/23 09:45:24.585
------------------------------
• [9.547 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:15.131
    Mar 16 09:45:15.131: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 03/16/23 09:45:15.133
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:15.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:15.577
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/16/23 09:45:15.769
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/16/23 09:45:16.499
    STEP: Deploying the custom resource conversion webhook pod 03/16/23 09:45:16.59
    STEP: Wait for the deployment to be ready 03/16/23 09:45:16.77
    Mar 16 09:45:17.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 09:45:19.127
    STEP: Verifying the service has paired with the endpoint 03/16/23 09:45:19.225
    Mar 16 09:45:20.226: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 16 09:45:20.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 03/16/23 09:45:23.133
    STEP: Create a v2 custom resource 03/16/23 09:45:23.405
    STEP: List CRs in v1 03/16/23 09:45:23.571
    STEP: List CRs in v2 03/16/23 09:45:23.665
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4799" for this suite. 03/16/23 09:45:24.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:24.678
Mar 16 09:45:24.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 03/16/23 09:45:24.679
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:24.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:25.136
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:25.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4664" for this suite. 03/16/23 09:45:25.494
------------------------------
• [0.906 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:24.678
    Mar 16 09:45:24.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 03/16/23 09:45:24.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:24.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:25.136
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:25.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4664" for this suite. 03/16/23 09:45:25.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:25.584
Mar 16 09:45:25.585: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:45:25.586
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:25.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:26.029
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Mar 16 09:45:26.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:45:30.364
Mar 16 09:45:30.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 create -f -'
Mar 16 09:45:32.141: INFO: stderr: ""
Mar 16 09:45:32.141: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 16 09:45:32.141: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 delete e2e-test-crd-publish-openapi-5444-crds test-cr'
Mar 16 09:45:32.602: INFO: stderr: ""
Mar 16 09:45:32.602: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 16 09:45:32.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 apply -f -'
Mar 16 09:45:33.830: INFO: stderr: ""
Mar 16 09:45:33.830: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 16 09:45:33.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 delete e2e-test-crd-publish-openapi-5444-crds test-cr'
Mar 16 09:45:34.296: INFO: stderr: ""
Mar 16 09:45:34.296: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/16/23 09:45:34.296
Mar 16 09:45:34.297: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 explain e2e-test-crd-publish-openapi-5444-crds'
Mar 16 09:45:34.879: INFO: stderr: ""
Mar 16 09:45:34.880: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5444-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:40.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8886" for this suite. 03/16/23 09:45:40.805
------------------------------
• [15.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:25.584
    Mar 16 09:45:25.585: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:45:25.586
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:25.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:26.029
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Mar 16 09:45:26.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:45:30.364
    Mar 16 09:45:30.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 create -f -'
    Mar 16 09:45:32.141: INFO: stderr: ""
    Mar 16 09:45:32.141: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 16 09:45:32.141: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 delete e2e-test-crd-publish-openapi-5444-crds test-cr'
    Mar 16 09:45:32.602: INFO: stderr: ""
    Mar 16 09:45:32.602: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 16 09:45:32.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 apply -f -'
    Mar 16 09:45:33.830: INFO: stderr: ""
    Mar 16 09:45:33.830: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 16 09:45:33.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 --namespace=crd-publish-openapi-8886 delete e2e-test-crd-publish-openapi-5444-crds test-cr'
    Mar 16 09:45:34.296: INFO: stderr: ""
    Mar 16 09:45:34.296: INFO: stdout: "e2e-test-crd-publish-openapi-5444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/16/23 09:45:34.296
    Mar 16 09:45:34.297: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8886 explain e2e-test-crd-publish-openapi-5444-crds'
    Mar 16 09:45:34.879: INFO: stderr: ""
    Mar 16 09:45:34.880: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5444-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:40.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8886" for this suite. 03/16/23 09:45:40.805
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:40.902
Mar 16 09:45:40.902: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 09:45:40.908
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:41.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:41.368
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/16/23 09:45:41.643
Mar 16 09:45:41.644: INFO: Creating simple deployment test-deployment-n7gd9
Mar 16 09:45:42.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-n7gd9-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 03/16/23 09:45:44.189
Mar 16 09:45:44.280: INFO: Deployment test-deployment-n7gd9 has Conditions: [{Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 03/16/23 09:45:44.28
Mar 16 09:45:44.463: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-n7gd9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/16/23 09:45:44.463
Mar 16 09:45:44.553: INFO: Observed &Deployment event: ADDED
Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
Mar 16 09:45:44.553: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 16 09:45:44.553: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-n7gd9-54bc444df" is progressing.}
Mar 16 09:45:44.554: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
Mar 16 09:45:44.554: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
Mar 16 09:45:44.554: INFO: Found Deployment test-deployment-n7gd9 in namespace deployment-7288 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 16 09:45:44.554: INFO: Deployment test-deployment-n7gd9 has an updated status
STEP: patching the Statefulset Status 03/16/23 09:45:44.554
Mar 16 09:45:44.554: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 16 09:45:44.646: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/16/23 09:45:44.646
Mar 16 09:45:44.737: INFO: Observed &Deployment event: ADDED
Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
Mar 16 09:45:44.737: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 16 09:45:44.737: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-n7gd9-54bc444df" is progressing.}
Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
Mar 16 09:45:44.738: INFO: Found deployment test-deployment-n7gd9 in namespace deployment-7288 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 16 09:45:44.738: INFO: Deployment test-deployment-n7gd9 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 09:45:44.828: INFO: Deployment "test-deployment-n7gd9":
&Deployment{ObjectMeta:{test-deployment-n7gd9  deployment-7288  8114a40b-4d56-41fd-9dd2-57bc86eff7c5 6608 1 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-16 09:45:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-16 09:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064fb148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 09:45:44 +0000 UTC,LastTransitionTime:2023-03-16 09:45:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.,LastUpdateTime:2023-03-16 09:45:44 +0000 UTC,LastTransitionTime:2023-03-16 09:45:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 16 09:45:44.919: INFO: New ReplicaSet "test-deployment-n7gd9-54bc444df" of Deployment "test-deployment-n7gd9":
&ReplicaSet{ObjectMeta:{test-deployment-n7gd9-54bc444df  deployment-7288  e99dc956-16c2-4a1b-9957-21db80bbc35e 6595 1 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-n7gd9 8114a40b-4d56-41fd-9dd2-57bc86eff7c5 0xc0064fb557 0xc0064fb558}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8114a40b-4d56-41fd-9dd2-57bc86eff7c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:45:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064fb608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 16 09:45:45.011: INFO: Pod "test-deployment-n7gd9-54bc444df-2k5mz" is available:
&Pod{ObjectMeta:{test-deployment-n7gd9-54bc444df-2k5mz test-deployment-n7gd9-54bc444df- deployment-7288  efd83ea7-71b9-48f0-8dc5-e758b19ddea8 6594 0 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:9714b95f651599686e93590feb5b2ff9cd28a293c76759ab9ad86ae5be512da6 cni.projectcalico.org/podIP:100.64.1.24/32 cni.projectcalico.org/podIPs:100.64.1.24/32] [{apps/v1 ReplicaSet test-deployment-n7gd9-54bc444df e99dc956-16c2-4a1b-9957-21db80bbc35e 0xc0064fb9b7 0xc0064fb9b8}] [] [{kube-controller-manager Update v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e99dc956-16c2-4a1b-9957-21db80bbc35e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:45:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4n9q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4n9q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.24,StartTime:2023-03-16 09:45:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:45:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa98e1e7364c5c63f3f8486bdffd648005a0c24b6fb7bde72ea2f67dd40d6261,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:45.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7288" for this suite. 03/16/23 09:45:45.103
------------------------------
• [4.294 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:40.902
    Mar 16 09:45:40.902: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 09:45:40.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:41.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:41.368
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/16/23 09:45:41.643
    Mar 16 09:45:41.644: INFO: Creating simple deployment test-deployment-n7gd9
    Mar 16 09:45:42.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-n7gd9-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 03/16/23 09:45:44.189
    Mar 16 09:45:44.280: INFO: Deployment test-deployment-n7gd9 has Conditions: [{Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 03/16/23 09:45:44.28
    Mar 16 09:45:44.463: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 45, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 45, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-n7gd9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/16/23 09:45:44.463
    Mar 16 09:45:44.553: INFO: Observed &Deployment event: ADDED
    Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
    Mar 16 09:45:44.553: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
    Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 16 09:45:44.553: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 16 09:45:44.553: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-n7gd9-54bc444df" is progressing.}
    Mar 16 09:45:44.554: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
    Mar 16 09:45:44.554: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 16 09:45:44.554: INFO: Observed Deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
    Mar 16 09:45:44.554: INFO: Found Deployment test-deployment-n7gd9 in namespace deployment-7288 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 16 09:45:44.554: INFO: Deployment test-deployment-n7gd9 has an updated status
    STEP: patching the Statefulset Status 03/16/23 09:45:44.554
    Mar 16 09:45:44.554: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 16 09:45:44.646: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/16/23 09:45:44.646
    Mar 16 09:45:44.737: INFO: Observed &Deployment event: ADDED
    Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
    Mar 16 09:45:44.737: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-n7gd9-54bc444df"}
    Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 16 09:45:44.737: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.737: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:41 +0000 UTC 2023-03-16 09:45:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-n7gd9-54bc444df" is progressing.}
    Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
    Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-16 09:45:43 +0000 UTC 2023-03-16 09:45:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.}
    Mar 16 09:45:44.738: INFO: Observed deployment test-deployment-n7gd9 in namespace deployment-7288 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 16 09:45:44.738: INFO: Observed &Deployment event: MODIFIED
    Mar 16 09:45:44.738: INFO: Found deployment test-deployment-n7gd9 in namespace deployment-7288 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 16 09:45:44.738: INFO: Deployment test-deployment-n7gd9 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 09:45:44.828: INFO: Deployment "test-deployment-n7gd9":
    &Deployment{ObjectMeta:{test-deployment-n7gd9  deployment-7288  8114a40b-4d56-41fd-9dd2-57bc86eff7c5 6608 1 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-16 09:45:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-16 09:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064fb148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 09:45:44 +0000 UTC,LastTransitionTime:2023-03-16 09:45:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-n7gd9-54bc444df" has successfully progressed.,LastUpdateTime:2023-03-16 09:45:44 +0000 UTC,LastTransitionTime:2023-03-16 09:45:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 16 09:45:44.919: INFO: New ReplicaSet "test-deployment-n7gd9-54bc444df" of Deployment "test-deployment-n7gd9":
    &ReplicaSet{ObjectMeta:{test-deployment-n7gd9-54bc444df  deployment-7288  e99dc956-16c2-4a1b-9957-21db80bbc35e 6595 1 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-n7gd9 8114a40b-4d56-41fd-9dd2-57bc86eff7c5 0xc0064fb557 0xc0064fb558}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8114a40b-4d56-41fd-9dd2-57bc86eff7c5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:45:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064fb608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 09:45:45.011: INFO: Pod "test-deployment-n7gd9-54bc444df-2k5mz" is available:
    &Pod{ObjectMeta:{test-deployment-n7gd9-54bc444df-2k5mz test-deployment-n7gd9-54bc444df- deployment-7288  efd83ea7-71b9-48f0-8dc5-e758b19ddea8 6594 0 2023-03-16 09:45:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:9714b95f651599686e93590feb5b2ff9cd28a293c76759ab9ad86ae5be512da6 cni.projectcalico.org/podIP:100.64.1.24/32 cni.projectcalico.org/podIPs:100.64.1.24/32] [{apps/v1 ReplicaSet test-deployment-n7gd9-54bc444df e99dc956-16c2-4a1b-9957-21db80bbc35e 0xc0064fb9b7 0xc0064fb9b8}] [] [{kube-controller-manager Update v1 2023-03-16 09:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e99dc956-16c2-4a1b-9957-21db80bbc35e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:45:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4n9q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4n9q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:45:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.24,StartTime:2023-03-16 09:45:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:45:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa98e1e7364c5c63f3f8486bdffd648005a0c24b6fb7bde72ea2f67dd40d6261,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:45.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7288" for this suite. 03/16/23 09:45:45.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:45.198
Mar 16 09:45:45.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:45:45.2
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:45.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:45.651
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 03/16/23 09:45:45.831
Mar 16 09:45:45.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e" in namespace "projected-3016" to be "Succeeded or Failed"
Mar 16 09:45:46.019: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Pending", Reason="", readiness=false. Elapsed: 92.811133ms
Mar 16 09:45:48.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184502368s
Mar 16 09:45:50.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18487291s
STEP: Saw pod success 03/16/23 09:45:50.111
Mar 16 09:45:50.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e" satisfied condition "Succeeded or Failed"
Mar 16 09:45:50.202: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e container client-container: <nil>
STEP: delete the pod 03/16/23 09:45:50.338
Mar 16 09:45:50.433: INFO: Waiting for pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e to disappear
Mar 16 09:45:50.524: INFO: Pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:50.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3016" for this suite. 03/16/23 09:45:50.705
------------------------------
• [5.598 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:45.198
    Mar 16 09:45:45.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:45:45.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:45.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:45.651
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 03/16/23 09:45:45.831
    Mar 16 09:45:45.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e" in namespace "projected-3016" to be "Succeeded or Failed"
    Mar 16 09:45:46.019: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Pending", Reason="", readiness=false. Elapsed: 92.811133ms
    Mar 16 09:45:48.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184502368s
    Mar 16 09:45:50.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18487291s
    STEP: Saw pod success 03/16/23 09:45:50.111
    Mar 16 09:45:50.111: INFO: Pod "downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e" satisfied condition "Succeeded or Failed"
    Mar 16 09:45:50.202: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e container client-container: <nil>
    STEP: delete the pod 03/16/23 09:45:50.338
    Mar 16 09:45:50.433: INFO: Waiting for pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e to disappear
    Mar 16 09:45:50.524: INFO: Pod downwardapi-volume-5694e1fd-1984-4709-8652-3656ad6f355e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:50.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3016" for this suite. 03/16/23 09:45:50.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:50.797
Mar 16 09:45:50.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 09:45:50.799
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:51.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:51.251
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 03/16/23 09:45:51.432
STEP: Ensuring job reaches completions 03/16/23 09:45:51.524
STEP: Ensuring pods with index for job exist 03/16/23 09:45:59.615
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 09:45:59.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-263" for this suite. 03/16/23 09:45:59.887
------------------------------
• [9.181 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:50.797
    Mar 16 09:45:50.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 09:45:50.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:45:51.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:45:51.251
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 03/16/23 09:45:51.432
    STEP: Ensuring job reaches completions 03/16/23 09:45:51.524
    STEP: Ensuring pods with index for job exist 03/16/23 09:45:59.615
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:45:59.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-263" for this suite. 03/16/23 09:45:59.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:45:59.979
Mar 16 09:45:59.979: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 09:45:59.98
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:00.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:00.432
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-12c122a0-32f5-4d1f-ac26-9c8755919efa 03/16/23 09:46:00.703
STEP: Creating the pod 03/16/23 09:46:00.794
Mar 16 09:46:00.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb" in namespace "configmap-2217" to be "running"
Mar 16 09:46:00.981: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb": Phase="Pending", Reason="", readiness=false. Elapsed: 90.806654ms
Mar 16 09:46:03.073: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb": Phase="Running", Reason="", readiness=false. Elapsed: 2.182849967s
Mar 16 09:46:03.073: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb" satisfied condition "running"
STEP: Waiting for pod with text data 03/16/23 09:46:03.073
STEP: Waiting for pod with binary data 03/16/23 09:46:03.211
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:03.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2217" for this suite. 03/16/23 09:46:03.487
------------------------------
• [3.599 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:45:59.979
    Mar 16 09:45:59.979: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 09:45:59.98
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:00.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:00.432
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-12c122a0-32f5-4d1f-ac26-9c8755919efa 03/16/23 09:46:00.703
    STEP: Creating the pod 03/16/23 09:46:00.794
    Mar 16 09:46:00.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb" in namespace "configmap-2217" to be "running"
    Mar 16 09:46:00.981: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb": Phase="Pending", Reason="", readiness=false. Elapsed: 90.806654ms
    Mar 16 09:46:03.073: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb": Phase="Running", Reason="", readiness=false. Elapsed: 2.182849967s
    Mar 16 09:46:03.073: INFO: Pod "pod-configmaps-44694bc0-fe70-434e-ab0e-64779569debb" satisfied condition "running"
    STEP: Waiting for pod with text data 03/16/23 09:46:03.073
    STEP: Waiting for pod with binary data 03/16/23 09:46:03.211
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:03.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2217" for this suite. 03/16/23 09:46:03.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:03.58
Mar 16 09:46:03.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 03/16/23 09:46:03.581
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:03.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:04.033
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/16/23 09:46:04.213
Mar 16 09:46:04.304: INFO: created test-podtemplate-1
Mar 16 09:46:04.395: INFO: created test-podtemplate-2
Mar 16 09:46:04.487: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/16/23 09:46:04.487
STEP: delete collection of pod templates 03/16/23 09:46:04.577
Mar 16 09:46:04.577: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/16/23 09:46:04.673
Mar 16 09:46:04.673: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7411" for this suite. 03/16/23 09:46:04.856
------------------------------
• [1.368 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:03.58
    Mar 16 09:46:03.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 03/16/23 09:46:03.581
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:03.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:04.033
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/16/23 09:46:04.213
    Mar 16 09:46:04.304: INFO: created test-podtemplate-1
    Mar 16 09:46:04.395: INFO: created test-podtemplate-2
    Mar 16 09:46:04.487: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/16/23 09:46:04.487
    STEP: delete collection of pod templates 03/16/23 09:46:04.577
    Mar 16 09:46:04.577: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/16/23 09:46:04.673
    Mar 16 09:46:04.673: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:04.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7411" for this suite. 03/16/23 09:46:04.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:04.949
Mar 16 09:46:04.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 09:46:04.95
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:05.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:05.403
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 16 09:46:05.583: INFO: Creating deployment "test-recreate-deployment"
Mar 16 09:46:05.674: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 16 09:46:05.855: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 16 09:46:05.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 09:46:08.036: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 16 09:46:08.219: INFO: Updating deployment test-recreate-deployment
Mar 16 09:46:08.219: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 09:46:08.400: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7396  598c0577-0c44-40ba-88c4-8ccdfff6586c 6902 2 2023-03-16 09:46:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006870268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 09:46:08 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-16 09:46:08 +0000 UTC,LastTransitionTime:2023-03-16 09:46:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 16 09:46:08.491: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7396  e9e1cd58-87dc-4b22-9a17-0f4512cb4428 6901 1 2023-03-16 09:46:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 598c0577-0c44-40ba-88c4-8ccdfff6586c 0xc0071b65f0 0xc0071b65f1}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598c0577-0c44-40ba-88c4-8ccdfff6586c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071b6688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 09:46:08.491: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 16 09:46:08.491: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7396  0a45da89-d2e5-4bdc-9438-1c6b7a89d309 6894 2 2023-03-16 09:46:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 598c0577-0c44-40ba-88c4-8ccdfff6586c 0xc0071b64e7 0xc0071b64e8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598c0577-0c44-40ba-88c4-8ccdfff6586c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071b6598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 09:46:08.582: INFO: Pod "test-recreate-deployment-cff6dc657-v96lg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-v96lg test-recreate-deployment-cff6dc657- deployment-7396  af67d329-1d00-4a18-ac14-4fc286944cb2 6903 0 2023-03-16 09:46:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e9e1cd58-87dc-4b22-9a17-0f4512cb4428 0xc0071b6ae0 0xc0071b6ae1}] [] [{kube-controller-manager Update v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9e1cd58-87dc-4b22-9a17-0f4512cb4428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kswnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kswnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:46:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7396" for this suite. 03/16/23 09:46:08.762
------------------------------
• [3.905 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:04.949
    Mar 16 09:46:04.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 09:46:04.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:05.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:05.403
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 16 09:46:05.583: INFO: Creating deployment "test-recreate-deployment"
    Mar 16 09:46:05.674: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 16 09:46:05.855: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 16 09:46:05.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 09:46:08.036: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 16 09:46:08.219: INFO: Updating deployment test-recreate-deployment
    Mar 16 09:46:08.219: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 09:46:08.400: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7396  598c0577-0c44-40ba-88c4-8ccdfff6586c 6902 2 2023-03-16 09:46:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006870268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 09:46:08 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-16 09:46:08 +0000 UTC,LastTransitionTime:2023-03-16 09:46:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 16 09:46:08.491: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7396  e9e1cd58-87dc-4b22-9a17-0f4512cb4428 6901 1 2023-03-16 09:46:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 598c0577-0c44-40ba-88c4-8ccdfff6586c 0xc0071b65f0 0xc0071b65f1}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598c0577-0c44-40ba-88c4-8ccdfff6586c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071b6688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 09:46:08.491: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 16 09:46:08.491: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7396  0a45da89-d2e5-4bdc-9438-1c6b7a89d309 6894 2 2023-03-16 09:46:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 598c0577-0c44-40ba-88c4-8ccdfff6586c 0xc0071b64e7 0xc0071b64e8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598c0577-0c44-40ba-88c4-8ccdfff6586c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071b6598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 09:46:08.582: INFO: Pod "test-recreate-deployment-cff6dc657-v96lg" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-v96lg test-recreate-deployment-cff6dc657- deployment-7396  af67d329-1d00-4a18-ac14-4fc286944cb2 6903 0 2023-03-16 09:46:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e9e1cd58-87dc-4b22-9a17-0f4512cb4428 0xc0071b6ae0 0xc0071b6ae1}] [] [{kube-controller-manager Update v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9e1cd58-87dc-4b22-9a17-0f4512cb4428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:46:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kswnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kswnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:46:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:46:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7396" for this suite. 03/16/23 09:46:08.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:08.854
Mar 16 09:46:08.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:46:08.855
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:09.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:09.307
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-5ed3db57-f098-4b2c-aa8f-3e4110880d98 03/16/23 09:46:09.486
STEP: Creating a pod to test consume secrets 03/16/23 09:46:09.577
Mar 16 09:46:09.673: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50" in namespace "projected-4787" to be "Succeeded or Failed"
Mar 16 09:46:09.764: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Pending", Reason="", readiness=false. Elapsed: 90.500044ms
Mar 16 09:46:11.855: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Running", Reason="", readiness=false. Elapsed: 2.182145657s
Mar 16 09:46:13.856: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183026561s
STEP: Saw pod success 03/16/23 09:46:13.856
Mar 16 09:46:13.856: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50" satisfied condition "Succeeded or Failed"
Mar 16 09:46:13.946: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/16/23 09:46:14.042
Mar 16 09:46:14.136: INFO: Waiting for pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 to disappear
Mar 16 09:46:14.226: INFO: Pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:14.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4787" for this suite. 03/16/23 09:46:14.407
------------------------------
• [5.646 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:08.854
    Mar 16 09:46:08.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:46:08.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:09.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:09.307
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-5ed3db57-f098-4b2c-aa8f-3e4110880d98 03/16/23 09:46:09.486
    STEP: Creating a pod to test consume secrets 03/16/23 09:46:09.577
    Mar 16 09:46:09.673: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50" in namespace "projected-4787" to be "Succeeded or Failed"
    Mar 16 09:46:09.764: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Pending", Reason="", readiness=false. Elapsed: 90.500044ms
    Mar 16 09:46:11.855: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Running", Reason="", readiness=false. Elapsed: 2.182145657s
    Mar 16 09:46:13.856: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183026561s
    STEP: Saw pod success 03/16/23 09:46:13.856
    Mar 16 09:46:13.856: INFO: Pod "pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50" satisfied condition "Succeeded or Failed"
    Mar 16 09:46:13.946: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 09:46:14.042
    Mar 16 09:46:14.136: INFO: Waiting for pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 to disappear
    Mar 16 09:46:14.226: INFO: Pod pod-projected-secrets-7249fc12-e485-4369-92c5-bf980adcdc50 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:14.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4787" for this suite. 03/16/23 09:46:14.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:14.503
Mar 16 09:46:14.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 03/16/23 09:46:14.504
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:14.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:14.956
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 16 09:46:15.237: INFO: Waiting up to 5m0s for pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95" in namespace "kubelet-test-6326" to be "running and ready"
Mar 16 09:46:15.327: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95": Phase="Pending", Reason="", readiness=false. Elapsed: 90.525812ms
Mar 16 09:46:15.327: INFO: The phase of Pod busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:46:17.420: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95": Phase="Running", Reason="", readiness=true. Elapsed: 2.182872686s
Mar 16 09:46:17.420: INFO: The phase of Pod busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95 is Running (Ready = true)
Mar 16 09:46:17.420: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6326" for this suite. 03/16/23 09:46:17.785
------------------------------
• [3.374 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:14.503
    Mar 16 09:46:14.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 03/16/23 09:46:14.504
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:14.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:14.956
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 16 09:46:15.237: INFO: Waiting up to 5m0s for pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95" in namespace "kubelet-test-6326" to be "running and ready"
    Mar 16 09:46:15.327: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95": Phase="Pending", Reason="", readiness=false. Elapsed: 90.525812ms
    Mar 16 09:46:15.327: INFO: The phase of Pod busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:46:17.420: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95": Phase="Running", Reason="", readiness=true. Elapsed: 2.182872686s
    Mar 16 09:46:17.420: INFO: The phase of Pod busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95 is Running (Ready = true)
    Mar 16 09:46:17.420: INFO: Pod "busybox-scheduling-5b00d3b1-04dc-4878-891c-f13de74ffb95" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6326" for this suite. 03/16/23 09:46:17.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:17.877
Mar 16 09:46:17.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 03/16/23 09:46:17.878
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:18.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:18.329
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:18.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2773" for this suite. 03/16/23 09:46:18.788
------------------------------
• [1.003 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:17.877
    Mar 16 09:46:17.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 03/16/23 09:46:17.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:18.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:18.329
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:18.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2773" for this suite. 03/16/23 09:46:18.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:18.88
Mar 16 09:46:18.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 09:46:18.881
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:19.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:19.333
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 09:46:19.696
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 09:46:20.715
STEP: Deploying the webhook pod 03/16/23 09:46:20.808
STEP: Wait for the deployment to be ready 03/16/23 09:46:20.991
Mar 16 09:46:21.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 09:46:23.356
STEP: Verifying the service has paired with the endpoint 03/16/23 09:46:23.452
Mar 16 09:46:24.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 03/16/23 09:46:25.456
STEP: Creating a configMap that should be mutated 03/16/23 09:46:25.816
STEP: Deleting the collection of validation webhooks 03/16/23 09:46:26.579
STEP: Creating a configMap that should not be mutated 03/16/23 09:46:26.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:26.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1014" for this suite. 03/16/23 09:46:27.422
STEP: Destroying namespace "webhook-1014-markers" for this suite. 03/16/23 09:46:27.514
------------------------------
• [8.726 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:18.88
    Mar 16 09:46:18.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 09:46:18.881
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:19.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:19.333
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 09:46:19.696
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 09:46:20.715
    STEP: Deploying the webhook pod 03/16/23 09:46:20.808
    STEP: Wait for the deployment to be ready 03/16/23 09:46:20.991
    Mar 16 09:46:21.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 46, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 09:46:23.356
    STEP: Verifying the service has paired with the endpoint 03/16/23 09:46:23.452
    Mar 16 09:46:24.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 03/16/23 09:46:25.456
    STEP: Creating a configMap that should be mutated 03/16/23 09:46:25.816
    STEP: Deleting the collection of validation webhooks 03/16/23 09:46:26.579
    STEP: Creating a configMap that should not be mutated 03/16/23 09:46:26.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:26.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1014" for this suite. 03/16/23 09:46:27.422
    STEP: Destroying namespace "webhook-1014-markers" for this suite. 03/16/23 09:46:27.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:27.609
Mar 16 09:46:27.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 03/16/23 09:46:27.611
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:27.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:28.063
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 03/16/23 09:46:28.334
STEP: Updating PodDisruptionBudget status 03/16/23 09:46:28.425
STEP: Waiting for all pods to be running 03/16/23 09:46:28.52
Mar 16 09:46:28.617: INFO: running pods: 0 < 1
STEP: locating a running pod 03/16/23 09:46:30.709
STEP: Waiting for the pdb to be processed 03/16/23 09:46:30.981
STEP: Patching PodDisruptionBudget status 03/16/23 09:46:31.164
STEP: Waiting for the pdb to be processed 03/16/23 09:46:31.347
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 16 09:46:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2222" for this suite. 03/16/23 09:46:31.618
------------------------------
• [4.100 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:27.609
    Mar 16 09:46:27.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 03/16/23 09:46:27.611
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:27.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:28.063
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 03/16/23 09:46:28.334
    STEP: Updating PodDisruptionBudget status 03/16/23 09:46:28.425
    STEP: Waiting for all pods to be running 03/16/23 09:46:28.52
    Mar 16 09:46:28.617: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/16/23 09:46:30.709
    STEP: Waiting for the pdb to be processed 03/16/23 09:46:30.981
    STEP: Patching PodDisruptionBudget status 03/16/23 09:46:31.164
    STEP: Waiting for the pdb to be processed 03/16/23 09:46:31.347
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:46:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2222" for this suite. 03/16/23 09:46:31.618
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:46:31.71
Mar 16 09:46:31.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 09:46:31.711
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:31.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:32.164
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 09:47:32.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5066" for this suite. 03/16/23 09:47:32.711
------------------------------
• [61.093 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:46:31.71
    Mar 16 09:46:31.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 09:46:31.711
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:46:31.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:46:32.164
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:47:32.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5066" for this suite. 03/16/23 09:47:32.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:47:32.803
Mar 16 09:47:32.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 03/16/23 09:47:32.804
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:33.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:33.256
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 16 09:47:33.436: INFO: Creating pod...
Mar 16 09:47:33.531: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4175" to be "running"
Mar 16 09:47:33.622: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 90.67591ms
Mar 16 09:47:35.713: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.182072052s
Mar 16 09:47:35.713: INFO: Pod "agnhost" satisfied condition "running"
Mar 16 09:47:35.713: INFO: Creating service...
Mar 16 09:47:35.808: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/DELETE
Mar 16 09:47:35.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 16 09:47:35.995: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/GET
Mar 16 09:47:36.130: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 16 09:47:36.130: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/HEAD
Mar 16 09:47:36.271: INFO: http.Client request:HEAD | StatusCode:200
Mar 16 09:47:36.271: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 16 09:47:36.363: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 16 09:47:36.363: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/PATCH
Mar 16 09:47:36.457: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 16 09:47:36.457: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/POST
Mar 16 09:47:36.550: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 16 09:47:36.550: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/PUT
Mar 16 09:47:36.643: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 16 09:47:36.643: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/DELETE
Mar 16 09:47:36.736: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 16 09:47:36.736: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/GET
Mar 16 09:47:36.830: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 16 09:47:36.830: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/HEAD
Mar 16 09:47:36.924: INFO: http.Client request:HEAD | StatusCode:200
Mar 16 09:47:36.924: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/OPTIONS
Mar 16 09:47:37.017: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 16 09:47:37.017: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/PATCH
Mar 16 09:47:37.147: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 16 09:47:37.147: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/POST
Mar 16 09:47:37.241: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 16 09:47:37.241: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/PUT
Mar 16 09:47:37.335: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 16 09:47:37.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4175" for this suite. 03/16/23 09:47:37.515
------------------------------
• [4.803 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:47:32.803
    Mar 16 09:47:32.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 03/16/23 09:47:32.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:33.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:33.256
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 16 09:47:33.436: INFO: Creating pod...
    Mar 16 09:47:33.531: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4175" to be "running"
    Mar 16 09:47:33.622: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 90.67591ms
    Mar 16 09:47:35.713: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.182072052s
    Mar 16 09:47:35.713: INFO: Pod "agnhost" satisfied condition "running"
    Mar 16 09:47:35.713: INFO: Creating service...
    Mar 16 09:47:35.808: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/DELETE
    Mar 16 09:47:35.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 16 09:47:35.995: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/GET
    Mar 16 09:47:36.130: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 16 09:47:36.130: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/HEAD
    Mar 16 09:47:36.271: INFO: http.Client request:HEAD | StatusCode:200
    Mar 16 09:47:36.271: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 16 09:47:36.363: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 16 09:47:36.363: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/PATCH
    Mar 16 09:47:36.457: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 16 09:47:36.457: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/POST
    Mar 16 09:47:36.550: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 16 09:47:36.550: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/pods/agnhost/proxy/some/path/with/PUT
    Mar 16 09:47:36.643: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 16 09:47:36.643: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/DELETE
    Mar 16 09:47:36.736: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 16 09:47:36.736: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/GET
    Mar 16 09:47:36.830: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 16 09:47:36.830: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/HEAD
    Mar 16 09:47:36.924: INFO: http.Client request:HEAD | StatusCode:200
    Mar 16 09:47:36.924: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/OPTIONS
    Mar 16 09:47:37.017: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 16 09:47:37.017: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/PATCH
    Mar 16 09:47:37.147: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 16 09:47:37.147: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/POST
    Mar 16 09:47:37.241: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 16 09:47:37.241: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4175/services/test-service/proxy/some/path/with/PUT
    Mar 16 09:47:37.335: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:47:37.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4175" for this suite. 03/16/23 09:47:37.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:47:37.606
Mar 16 09:47:37.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange 03/16/23 09:47:37.608
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:37.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:38.06
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-xn5mx" in namespace "limitrange-2078" 03/16/23 09:47:38.24
STEP: Creating another limitRange in another namespace 03/16/23 09:47:38.331
Mar 16 09:47:38.603: INFO: Namespace "e2e-limitrange-xn5mx-2996" created
Mar 16 09:47:38.603: INFO: Creating LimitRange "e2e-limitrange-xn5mx" in namespace "e2e-limitrange-xn5mx-2996"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-xn5mx" 03/16/23 09:47:38.694
Mar 16 09:47:38.785: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-xn5mx" in "limitrange-2078" namespace 03/16/23 09:47:38.785
Mar 16 09:47:38.878: INFO: LimitRange "e2e-limitrange-xn5mx" has been patched
STEP: Delete LimitRange "e2e-limitrange-xn5mx" by Collection with labelSelector: "e2e-limitrange-xn5mx=patched" 03/16/23 09:47:38.878
STEP: Confirm that the limitRange "e2e-limitrange-xn5mx" has been deleted 03/16/23 09:47:38.971
Mar 16 09:47:38.971: INFO: Requesting list of LimitRange to confirm quantity
Mar 16 09:47:39.061: INFO: Found 0 LimitRange with label "e2e-limitrange-xn5mx=patched"
Mar 16 09:47:39.061: INFO: LimitRange "e2e-limitrange-xn5mx" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-xn5mx" 03/16/23 09:47:39.061
Mar 16 09:47:39.152: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 16 09:47:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2078" for this suite. 03/16/23 09:47:39.243
STEP: Destroying namespace "e2e-limitrange-xn5mx-2996" for this suite. 03/16/23 09:47:39.334
------------------------------
• [1.820 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:47:37.606
    Mar 16 09:47:37.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename limitrange 03/16/23 09:47:37.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:37.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:38.06
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-xn5mx" in namespace "limitrange-2078" 03/16/23 09:47:38.24
    STEP: Creating another limitRange in another namespace 03/16/23 09:47:38.331
    Mar 16 09:47:38.603: INFO: Namespace "e2e-limitrange-xn5mx-2996" created
    Mar 16 09:47:38.603: INFO: Creating LimitRange "e2e-limitrange-xn5mx" in namespace "e2e-limitrange-xn5mx-2996"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-xn5mx" 03/16/23 09:47:38.694
    Mar 16 09:47:38.785: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-xn5mx" in "limitrange-2078" namespace 03/16/23 09:47:38.785
    Mar 16 09:47:38.878: INFO: LimitRange "e2e-limitrange-xn5mx" has been patched
    STEP: Delete LimitRange "e2e-limitrange-xn5mx" by Collection with labelSelector: "e2e-limitrange-xn5mx=patched" 03/16/23 09:47:38.878
    STEP: Confirm that the limitRange "e2e-limitrange-xn5mx" has been deleted 03/16/23 09:47:38.971
    Mar 16 09:47:38.971: INFO: Requesting list of LimitRange to confirm quantity
    Mar 16 09:47:39.061: INFO: Found 0 LimitRange with label "e2e-limitrange-xn5mx=patched"
    Mar 16 09:47:39.061: INFO: LimitRange "e2e-limitrange-xn5mx" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-xn5mx" 03/16/23 09:47:39.061
    Mar 16 09:47:39.152: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:47:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2078" for this suite. 03/16/23 09:47:39.243
    STEP: Destroying namespace "e2e-limitrange-xn5mx-2996" for this suite. 03/16/23 09:47:39.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:47:39.427
Mar 16 09:47:39.427: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 03/16/23 09:47:39.428
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:39.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:39.879
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/16/23 09:47:40.059
STEP: Watching for error events or started pod 03/16/23 09:47:40.155
STEP: Waiting for pod completion 03/16/23 09:47:42.247
Mar 16 09:47:42.247: INFO: Waiting up to 3m0s for pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d" in namespace "sysctl-7579" to be "completed"
Mar 16 09:47:42.338: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 90.501925ms
Mar 16 09:47:44.429: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182081047s
Mar 16 09:47:44.429: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/16/23 09:47:44.52
STEP: Getting logs from the pod 03/16/23 09:47:44.52
STEP: Checking that the sysctl is actually updated 03/16/23 09:47:44.687
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:47:44.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7579" for this suite. 03/16/23 09:47:44.871
------------------------------
• [5.535 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:47:39.427
    Mar 16 09:47:39.427: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 03/16/23 09:47:39.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:39.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:39.879
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/16/23 09:47:40.059
    STEP: Watching for error events or started pod 03/16/23 09:47:40.155
    STEP: Waiting for pod completion 03/16/23 09:47:42.247
    Mar 16 09:47:42.247: INFO: Waiting up to 3m0s for pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d" in namespace "sysctl-7579" to be "completed"
    Mar 16 09:47:42.338: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 90.501925ms
    Mar 16 09:47:44.429: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182081047s
    Mar 16 09:47:44.429: INFO: Pod "sysctl-effbac63-5b46-4df0-9506-416f485b8f6d" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/16/23 09:47:44.52
    STEP: Getting logs from the pod 03/16/23 09:47:44.52
    STEP: Checking that the sysctl is actually updated 03/16/23 09:47:44.687
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:47:44.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7579" for this suite. 03/16/23 09:47:44.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:47:44.963
Mar 16 09:47:44.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 03/16/23 09:47:44.964
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:45.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:45.416
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 16 09:47:45.872: INFO: Waiting up to 5m0s for pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc" in namespace "emptydir-wrapper-2957" to be "running and ready"
Mar 16 09:47:45.963: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc": Phase="Pending", Reason="", readiness=false. Elapsed: 90.684303ms
Mar 16 09:47:45.963: INFO: The phase of Pod pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:47:48.055: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.182633231s
Mar 16 09:47:48.055: INFO: The phase of Pod pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc is Running (Ready = true)
Mar 16 09:47:48.055: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/16/23 09:47:48.146
STEP: Cleaning up the configmap 03/16/23 09:47:48.24
STEP: Cleaning up the pod 03/16/23 09:47:48.332
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 09:47:48.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2957" for this suite. 03/16/23 09:47:48.606
------------------------------
• [3.734 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:47:44.963
    Mar 16 09:47:44.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 03/16/23 09:47:44.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:45.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:45.416
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 16 09:47:45.872: INFO: Waiting up to 5m0s for pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc" in namespace "emptydir-wrapper-2957" to be "running and ready"
    Mar 16 09:47:45.963: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc": Phase="Pending", Reason="", readiness=false. Elapsed: 90.684303ms
    Mar 16 09:47:45.963: INFO: The phase of Pod pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:47:48.055: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.182633231s
    Mar 16 09:47:48.055: INFO: The phase of Pod pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc is Running (Ready = true)
    Mar 16 09:47:48.055: INFO: Pod "pod-secrets-8ad3b946-a7fd-4997-82e5-44cab52d79fc" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/16/23 09:47:48.146
    STEP: Cleaning up the configmap 03/16/23 09:47:48.24
    STEP: Cleaning up the pod 03/16/23 09:47:48.332
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:47:48.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2957" for this suite. 03/16/23 09:47:48.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:47:48.698
Mar 16 09:47:48.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 09:47:48.699
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:48.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:49.152
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/16/23 09:47:49.423
STEP: create the rc2 03/16/23 09:47:49.515
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/16/23 09:47:54.699
STEP: delete the rc simpletest-rc-to-be-deleted 03/16/23 09:47:59.533
STEP: wait for the rc to be deleted 03/16/23 09:47:59.624
Mar 16 09:48:04.908: INFO: 71 pods remaining
Mar 16 09:48:04.908: INFO: 71 pods has nil DeletionTimestamp
Mar 16 09:48:04.908: INFO: 
STEP: Gathering metrics 03/16/23 09:48:09.9
W0316 09:48:10.085623    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 09:48:10.085: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 16 09:48:10.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k4fk" in namespace "gc-3030"
Mar 16 09:48:10.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-477l2" in namespace "gc-3030"
Mar 16 09:48:10.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-49c5x" in namespace "gc-3030"
Mar 16 09:48:10.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-49cx4" in namespace "gc-3030"
Mar 16 09:48:10.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gz2q" in namespace "gc-3030"
Mar 16 09:48:10.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-524zk" in namespace "gc-3030"
Mar 16 09:48:10.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-58g6v" in namespace "gc-3030"
Mar 16 09:48:10.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dmkx" in namespace "gc-3030"
Mar 16 09:48:10.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vlzr" in namespace "gc-3030"
Mar 16 09:48:10.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fj9v" in namespace "gc-3030"
Mar 16 09:48:11.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gnh7" in namespace "gc-3030"
Mar 16 09:48:11.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qc56" in namespace "gc-3030"
Mar 16 09:48:11.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xgvv" in namespace "gc-3030"
Mar 16 09:48:11.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m79f" in namespace "gc-3030"
Mar 16 09:48:11.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cdbv" in namespace "gc-3030"
Mar 16 09:48:11.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lxww" in namespace "gc-3030"
Mar 16 09:48:11.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xsbc" in namespace "gc-3030"
Mar 16 09:48:11.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b89z" in namespace "gc-3030"
Mar 16 09:48:11.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ckkd" in namespace "gc-3030"
Mar 16 09:48:11.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g9j2" in namespace "gc-3030"
Mar 16 09:48:11.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nnb9" in namespace "gc-3030"
Mar 16 09:48:12.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tkj5" in namespace "gc-3030"
Mar 16 09:48:12.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-b92x9" in namespace "gc-3030"
Mar 16 09:48:12.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpz4c" in namespace "gc-3030"
Mar 16 09:48:12.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqc9d" in namespace "gc-3030"
Mar 16 09:48:12.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-clms2" in namespace "gc-3030"
Mar 16 09:48:12.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqdc8" in namespace "gc-3030"
Mar 16 09:48:12.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-djx59" in namespace "gc-3030"
Mar 16 09:48:12.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4njs" in namespace "gc-3030"
Mar 16 09:48:12.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-f576h" in namespace "gc-3030"
Mar 16 09:48:12.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5rbj" in namespace "gc-3030"
Mar 16 09:48:13.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5vf2" in namespace "gc-3030"
Mar 16 09:48:13.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbln7" in namespace "gc-3030"
Mar 16 09:48:13.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg26z" in namespace "gc-3030"
Mar 16 09:48:13.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjfgg" in namespace "gc-3030"
Mar 16 09:48:13.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp4wt" in namespace "gc-3030"
Mar 16 09:48:13.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqk5f" in namespace "gc-3030"
Mar 16 09:48:13.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs4rn" in namespace "gc-3030"
Mar 16 09:48:13.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv2d8" in namespace "gc-3030"
Mar 16 09:48:13.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2l2n" in namespace "gc-3030"
Mar 16 09:48:13.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd2kj" in namespace "gc-3030"
Mar 16 09:48:13.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh7nl" in namespace "gc-3030"
Mar 16 09:48:14.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjhd7" in namespace "gc-3030"
Mar 16 09:48:14.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6mhg" in namespace "gc-3030"
Mar 16 09:48:14.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8fj4" in namespace "gc-3030"
Mar 16 09:48:14.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8gd9" in namespace "gc-3030"
Mar 16 09:48:14.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxjbm" in namespace "gc-3030"
Mar 16 09:48:14.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-j768m" in namespace "gc-3030"
Mar 16 09:48:14.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8ddm" in namespace "gc-3030"
Mar 16 09:48:14.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-khhwj" in namespace "gc-3030"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:14.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3030" for this suite. 03/16/23 09:48:14.898
------------------------------
• [26.292 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:47:48.698
    Mar 16 09:47:48.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 09:47:48.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:47:48.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:47:49.152
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/16/23 09:47:49.423
    STEP: create the rc2 03/16/23 09:47:49.515
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/16/23 09:47:54.699
    STEP: delete the rc simpletest-rc-to-be-deleted 03/16/23 09:47:59.533
    STEP: wait for the rc to be deleted 03/16/23 09:47:59.624
    Mar 16 09:48:04.908: INFO: 71 pods remaining
    Mar 16 09:48:04.908: INFO: 71 pods has nil DeletionTimestamp
    Mar 16 09:48:04.908: INFO: 
    STEP: Gathering metrics 03/16/23 09:48:09.9
    W0316 09:48:10.085623    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 09:48:10.085: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 16 09:48:10.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k4fk" in namespace "gc-3030"
    Mar 16 09:48:10.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-477l2" in namespace "gc-3030"
    Mar 16 09:48:10.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-49c5x" in namespace "gc-3030"
    Mar 16 09:48:10.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-49cx4" in namespace "gc-3030"
    Mar 16 09:48:10.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gz2q" in namespace "gc-3030"
    Mar 16 09:48:10.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-524zk" in namespace "gc-3030"
    Mar 16 09:48:10.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-58g6v" in namespace "gc-3030"
    Mar 16 09:48:10.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dmkx" in namespace "gc-3030"
    Mar 16 09:48:10.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vlzr" in namespace "gc-3030"
    Mar 16 09:48:10.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fj9v" in namespace "gc-3030"
    Mar 16 09:48:11.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gnh7" in namespace "gc-3030"
    Mar 16 09:48:11.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qc56" in namespace "gc-3030"
    Mar 16 09:48:11.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xgvv" in namespace "gc-3030"
    Mar 16 09:48:11.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m79f" in namespace "gc-3030"
    Mar 16 09:48:11.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cdbv" in namespace "gc-3030"
    Mar 16 09:48:11.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lxww" in namespace "gc-3030"
    Mar 16 09:48:11.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xsbc" in namespace "gc-3030"
    Mar 16 09:48:11.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b89z" in namespace "gc-3030"
    Mar 16 09:48:11.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ckkd" in namespace "gc-3030"
    Mar 16 09:48:11.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g9j2" in namespace "gc-3030"
    Mar 16 09:48:11.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nnb9" in namespace "gc-3030"
    Mar 16 09:48:12.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tkj5" in namespace "gc-3030"
    Mar 16 09:48:12.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-b92x9" in namespace "gc-3030"
    Mar 16 09:48:12.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpz4c" in namespace "gc-3030"
    Mar 16 09:48:12.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqc9d" in namespace "gc-3030"
    Mar 16 09:48:12.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-clms2" in namespace "gc-3030"
    Mar 16 09:48:12.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqdc8" in namespace "gc-3030"
    Mar 16 09:48:12.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-djx59" in namespace "gc-3030"
    Mar 16 09:48:12.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4njs" in namespace "gc-3030"
    Mar 16 09:48:12.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-f576h" in namespace "gc-3030"
    Mar 16 09:48:12.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5rbj" in namespace "gc-3030"
    Mar 16 09:48:13.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5vf2" in namespace "gc-3030"
    Mar 16 09:48:13.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbln7" in namespace "gc-3030"
    Mar 16 09:48:13.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg26z" in namespace "gc-3030"
    Mar 16 09:48:13.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjfgg" in namespace "gc-3030"
    Mar 16 09:48:13.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp4wt" in namespace "gc-3030"
    Mar 16 09:48:13.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqk5f" in namespace "gc-3030"
    Mar 16 09:48:13.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs4rn" in namespace "gc-3030"
    Mar 16 09:48:13.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv2d8" in namespace "gc-3030"
    Mar 16 09:48:13.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2l2n" in namespace "gc-3030"
    Mar 16 09:48:13.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd2kj" in namespace "gc-3030"
    Mar 16 09:48:13.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh7nl" in namespace "gc-3030"
    Mar 16 09:48:14.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjhd7" in namespace "gc-3030"
    Mar 16 09:48:14.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6mhg" in namespace "gc-3030"
    Mar 16 09:48:14.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8fj4" in namespace "gc-3030"
    Mar 16 09:48:14.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8gd9" in namespace "gc-3030"
    Mar 16 09:48:14.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxjbm" in namespace "gc-3030"
    Mar 16 09:48:14.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-j768m" in namespace "gc-3030"
    Mar 16 09:48:14.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8ddm" in namespace "gc-3030"
    Mar 16 09:48:14.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-khhwj" in namespace "gc-3030"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:14.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3030" for this suite. 03/16/23 09:48:14.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:14.992
Mar 16 09:48:14.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 09:48:14.994
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:15.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:15.445
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 03/16/23 09:48:15.719
STEP: watching for Pod to be ready 03/16/23 09:48:15.814
Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
Mar 16 09:48:16.248: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
Mar 16 09:48:17.312: INFO: Found Pod pod-test in namespace pods-5652 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/16/23 09:48:17.402
STEP: getting the Pod and ensuring that it's patched 03/16/23 09:48:17.586
STEP: replacing the Pod's status Ready condition to False 03/16/23 09:48:17.676
STEP: check the Pod again to ensure its Ready conditions are False 03/16/23 09:48:17.861
STEP: deleting the Pod via a Collection with a LabelSelector 03/16/23 09:48:17.861
STEP: watching for the Pod to be deleted 03/16/23 09:48:17.961
Mar 16 09:48:18.051: INFO: observed event type MODIFIED
Mar 16 09:48:19.319: INFO: observed event type MODIFIED
Mar 16 09:48:19.468: INFO: observed event type MODIFIED
Mar 16 09:48:20.324: INFO: observed event type MODIFIED
Mar 16 09:48:20.330: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:20.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5652" for this suite. 03/16/23 09:48:20.602
------------------------------
• [5.702 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:14.992
    Mar 16 09:48:14.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 09:48:14.994
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:15.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:15.445
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 03/16/23 09:48:15.719
    STEP: watching for Pod to be ready 03/16/23 09:48:15.814
    Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
    Mar 16 09:48:15.904: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
    Mar 16 09:48:16.248: INFO: observed Pod pod-test in namespace pods-5652 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
    Mar 16 09:48:17.312: INFO: Found Pod pod-test in namespace pods-5652 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 09:48:15 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/16/23 09:48:17.402
    STEP: getting the Pod and ensuring that it's patched 03/16/23 09:48:17.586
    STEP: replacing the Pod's status Ready condition to False 03/16/23 09:48:17.676
    STEP: check the Pod again to ensure its Ready conditions are False 03/16/23 09:48:17.861
    STEP: deleting the Pod via a Collection with a LabelSelector 03/16/23 09:48:17.861
    STEP: watching for the Pod to be deleted 03/16/23 09:48:17.961
    Mar 16 09:48:18.051: INFO: observed event type MODIFIED
    Mar 16 09:48:19.319: INFO: observed event type MODIFIED
    Mar 16 09:48:19.468: INFO: observed event type MODIFIED
    Mar 16 09:48:20.324: INFO: observed event type MODIFIED
    Mar 16 09:48:20.330: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:20.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5652" for this suite. 03/16/23 09:48:20.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:20.694
Mar 16 09:48:20.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 09:48:20.696
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:20.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:21.154
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/16/23 09:48:21.334
STEP: submitting the pod to kubernetes 03/16/23 09:48:21.334
STEP: verifying QOS class is set on the pod 03/16/23 09:48:21.435
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:21.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2001" for this suite. 03/16/23 09:48:21.617
------------------------------
• [1.014 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:20.694
    Mar 16 09:48:20.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 09:48:20.696
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:20.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:21.154
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/16/23 09:48:21.334
    STEP: submitting the pod to kubernetes 03/16/23 09:48:21.334
    STEP: verifying QOS class is set on the pod 03/16/23 09:48:21.435
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:21.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2001" for this suite. 03/16/23 09:48:21.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:21.709
Mar 16 09:48:21.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:48:21.711
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:21.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:22.164
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 03/16/23 09:48:22.344
Mar 16 09:48:22.344: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7706 create -f -'
Mar 16 09:48:23.671: INFO: stderr: ""
Mar 16 09:48:23.672: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/16/23 09:48:23.672
Mar 16 09:48:24.766: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:48:24.766: INFO: Found 1 / 1
Mar 16 09:48:24.766: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/16/23 09:48:24.766
Mar 16 09:48:24.857: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:48:24.857: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 16 09:48:24.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7706 patch pod agnhost-primary-85gvm -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 16 09:48:25.297: INFO: stderr: ""
Mar 16 09:48:25.297: INFO: stdout: "pod/agnhost-primary-85gvm patched\n"
STEP: checking annotations 03/16/23 09:48:25.297
Mar 16 09:48:25.388: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 09:48:25.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:25.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7706" for this suite. 03/16/23 09:48:25.568
------------------------------
• [3.950 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:21.709
    Mar 16 09:48:21.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:48:21.711
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:21.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:22.164
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 03/16/23 09:48:22.344
    Mar 16 09:48:22.344: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7706 create -f -'
    Mar 16 09:48:23.671: INFO: stderr: ""
    Mar 16 09:48:23.672: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/16/23 09:48:23.672
    Mar 16 09:48:24.766: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:48:24.766: INFO: Found 1 / 1
    Mar 16 09:48:24.766: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/16/23 09:48:24.766
    Mar 16 09:48:24.857: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:48:24.857: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 16 09:48:24.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7706 patch pod agnhost-primary-85gvm -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 16 09:48:25.297: INFO: stderr: ""
    Mar 16 09:48:25.297: INFO: stdout: "pod/agnhost-primary-85gvm patched\n"
    STEP: checking annotations 03/16/23 09:48:25.297
    Mar 16 09:48:25.388: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 09:48:25.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:25.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7706" for this suite. 03/16/23 09:48:25.568
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:25.66
Mar 16 09:48:25.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 09:48:25.661
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:25.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:26.112
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-26a04f42-870b-4f94-8be4-7bb378a3be50 03/16/23 09:48:26.292
STEP: Creating a pod to test consume configMaps 03/16/23 09:48:26.384
Mar 16 09:48:26.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4" in namespace "configmap-1756" to be "Succeeded or Failed"
Mar 16 09:48:26.571: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Pending", Reason="", readiness=false. Elapsed: 91.032948ms
Mar 16 09:48:28.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183168529s
Mar 16 09:48:30.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183053417s
STEP: Saw pod success 03/16/23 09:48:30.663
Mar 16 09:48:30.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4" satisfied condition "Succeeded or Failed"
Mar 16 09:48:30.754: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 09:48:30.895
Mar 16 09:48:30.992: INFO: Waiting for pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 to disappear
Mar 16 09:48:31.082: INFO: Pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:31.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1756" for this suite. 03/16/23 09:48:31.262
------------------------------
• [5.694 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:25.66
    Mar 16 09:48:25.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 09:48:25.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:25.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:26.112
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-26a04f42-870b-4f94-8be4-7bb378a3be50 03/16/23 09:48:26.292
    STEP: Creating a pod to test consume configMaps 03/16/23 09:48:26.384
    Mar 16 09:48:26.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4" in namespace "configmap-1756" to be "Succeeded or Failed"
    Mar 16 09:48:26.571: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Pending", Reason="", readiness=false. Elapsed: 91.032948ms
    Mar 16 09:48:28.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183168529s
    Mar 16 09:48:30.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183053417s
    STEP: Saw pod success 03/16/23 09:48:30.663
    Mar 16 09:48:30.663: INFO: Pod "pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4" satisfied condition "Succeeded or Failed"
    Mar 16 09:48:30.754: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 09:48:30.895
    Mar 16 09:48:30.992: INFO: Waiting for pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 to disappear
    Mar 16 09:48:31.082: INFO: Pod pod-configmaps-5e822058-7908-4e4a-a693-9dfff2e85ba4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:31.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1756" for this suite. 03/16/23 09:48:31.262
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:31.354
Mar 16 09:48:31.354: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:48:31.355
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:31.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:31.807
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 03/16/23 09:48:31.987
Mar 16 09:48:32.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32" in namespace "projected-3031" to be "Succeeded or Failed"
Mar 16 09:48:32.173: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Pending", Reason="", readiness=false. Elapsed: 90.372039ms
Mar 16 09:48:34.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182004448s
Mar 16 09:48:36.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182178769s
STEP: Saw pod success 03/16/23 09:48:36.265
Mar 16 09:48:36.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32" satisfied condition "Succeeded or Failed"
Mar 16 09:48:36.356: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 container client-container: <nil>
STEP: delete the pod 03/16/23 09:48:36.499
Mar 16 09:48:36.593: INFO: Waiting for pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 to disappear
Mar 16 09:48:36.686: INFO: Pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:36.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3031" for this suite. 03/16/23 09:48:36.866
------------------------------
• [5.604 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:31.354
    Mar 16 09:48:31.354: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:48:31.355
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:31.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:31.807
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 03/16/23 09:48:31.987
    Mar 16 09:48:32.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32" in namespace "projected-3031" to be "Succeeded or Failed"
    Mar 16 09:48:32.173: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Pending", Reason="", readiness=false. Elapsed: 90.372039ms
    Mar 16 09:48:34.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182004448s
    Mar 16 09:48:36.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182178769s
    STEP: Saw pod success 03/16/23 09:48:36.265
    Mar 16 09:48:36.265: INFO: Pod "downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32" satisfied condition "Succeeded or Failed"
    Mar 16 09:48:36.356: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 container client-container: <nil>
    STEP: delete the pod 03/16/23 09:48:36.499
    Mar 16 09:48:36.593: INFO: Waiting for pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 to disappear
    Mar 16 09:48:36.686: INFO: Pod downwardapi-volume-98182ff2-f3e2-451e-8158-b1366ff5ee32 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:36.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3031" for this suite. 03/16/23 09:48:36.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:36.959
Mar 16 09:48:36.959: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 09:48:36.96
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:37.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:37.412
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7496/configmap-test-0b40278b-9450-4a49-adaf-ce324a528fdb 03/16/23 09:48:37.591
STEP: Creating a pod to test consume configMaps 03/16/23 09:48:37.682
Mar 16 09:48:37.778: INFO: Waiting up to 5m0s for pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66" in namespace "configmap-7496" to be "Succeeded or Failed"
Mar 16 09:48:37.868: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Pending", Reason="", readiness=false. Elapsed: 90.578407ms
Mar 16 09:48:39.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182275465s
Mar 16 09:48:41.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181818686s
STEP: Saw pod success 03/16/23 09:48:41.96
Mar 16 09:48:41.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66" satisfied condition "Succeeded or Failed"
Mar 16 09:48:42.050: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 container env-test: <nil>
STEP: delete the pod 03/16/23 09:48:42.145
Mar 16 09:48:42.241: INFO: Waiting for pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 to disappear
Mar 16 09:48:42.331: INFO: Pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:42.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7496" for this suite. 03/16/23 09:48:42.512
------------------------------
• [5.645 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:36.959
    Mar 16 09:48:36.959: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 09:48:36.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:37.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:37.412
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7496/configmap-test-0b40278b-9450-4a49-adaf-ce324a528fdb 03/16/23 09:48:37.591
    STEP: Creating a pod to test consume configMaps 03/16/23 09:48:37.682
    Mar 16 09:48:37.778: INFO: Waiting up to 5m0s for pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66" in namespace "configmap-7496" to be "Succeeded or Failed"
    Mar 16 09:48:37.868: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Pending", Reason="", readiness=false. Elapsed: 90.578407ms
    Mar 16 09:48:39.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182275465s
    Mar 16 09:48:41.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181818686s
    STEP: Saw pod success 03/16/23 09:48:41.96
    Mar 16 09:48:41.960: INFO: Pod "pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66" satisfied condition "Succeeded or Failed"
    Mar 16 09:48:42.050: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 container env-test: <nil>
    STEP: delete the pod 03/16/23 09:48:42.145
    Mar 16 09:48:42.241: INFO: Waiting for pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 to disappear
    Mar 16 09:48:42.331: INFO: Pod pod-configmaps-1085cc5c-3bb5-44c8-a061-650913034f66 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:42.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7496" for this suite. 03/16/23 09:48:42.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:42.604
Mar 16 09:48:42.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:48:42.606
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:42.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:43.058
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Mar 16 09:48:43.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:48:48.632
Mar 16 09:48:48.632: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 create -f -'
Mar 16 09:48:50.208: INFO: stderr: ""
Mar 16 09:48:50.208: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 16 09:48:50.208: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 delete e2e-test-crd-publish-openapi-4525-crds test-cr'
Mar 16 09:48:50.640: INFO: stderr: ""
Mar 16 09:48:50.640: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 16 09:48:50.640: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 apply -f -'
Mar 16 09:48:51.378: INFO: stderr: ""
Mar 16 09:48:51.378: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 16 09:48:51.378: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 delete e2e-test-crd-publish-openapi-4525-crds test-cr'
Mar 16 09:48:51.817: INFO: stderr: ""
Mar 16 09:48:51.817: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/16/23 09:48:51.817
Mar 16 09:48:51.817: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 explain e2e-test-crd-publish-openapi-4525-crds'
Mar 16 09:48:52.673: INFO: stderr: ""
Mar 16 09:48:52.673: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4525-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:57.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-970" for this suite. 03/16/23 09:48:57.366
------------------------------
• [14.851 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:42.604
    Mar 16 09:48:42.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:48:42.606
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:42.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:43.058
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Mar 16 09:48:43.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:48:48.632
    Mar 16 09:48:48.632: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 create -f -'
    Mar 16 09:48:50.208: INFO: stderr: ""
    Mar 16 09:48:50.208: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 16 09:48:50.208: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 delete e2e-test-crd-publish-openapi-4525-crds test-cr'
    Mar 16 09:48:50.640: INFO: stderr: ""
    Mar 16 09:48:50.640: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 16 09:48:50.640: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 apply -f -'
    Mar 16 09:48:51.378: INFO: stderr: ""
    Mar 16 09:48:51.378: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 16 09:48:51.378: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 --namespace=crd-publish-openapi-970 delete e2e-test-crd-publish-openapi-4525-crds test-cr'
    Mar 16 09:48:51.817: INFO: stderr: ""
    Mar 16 09:48:51.817: INFO: stdout: "e2e-test-crd-publish-openapi-4525-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/16/23 09:48:51.817
    Mar 16 09:48:51.817: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-970 explain e2e-test-crd-publish-openapi-4525-crds'
    Mar 16 09:48:52.673: INFO: stderr: ""
    Mar 16 09:48:52.673: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4525-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:57.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-970" for this suite. 03/16/23 09:48:57.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:57.457
Mar 16 09:48:57.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:48:57.459
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:57.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:57.903
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 03/16/23 09:48:58.079
Mar 16 09:48:58.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9417 cluster-info'
Mar 16 09:48:58.428: INFO: stderr: ""
Mar 16 09:48:58.428: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 09:48:58.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9417" for this suite. 03/16/23 09:48:58.605
------------------------------
• [1.238 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:57.457
    Mar 16 09:48:57.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:48:57.459
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:57.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:57.903
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 03/16/23 09:48:58.079
    Mar 16 09:48:58.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9417 cluster-info'
    Mar 16 09:48:58.428: INFO: stderr: ""
    Mar 16 09:48:58.428: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:48:58.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9417" for this suite. 03/16/23 09:48:58.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:48:58.696
Mar 16 09:48:58.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 09:48:58.697
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:58.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:59.141
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 03/16/23 09:48:59.318
Mar 16 09:48:59.412: INFO: Waiting up to 5m0s for pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac" in namespace "var-expansion-9743" to be "Succeeded or Failed"
Mar 16 09:48:59.501: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Pending", Reason="", readiness=false. Elapsed: 88.996857ms
Mar 16 09:49:01.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178979381s
Mar 16 09:49:03.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179004407s
STEP: Saw pod success 03/16/23 09:49:03.591
Mar 16 09:49:03.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac" satisfied condition "Succeeded or Failed"
Mar 16 09:49:03.680: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac container dapi-container: <nil>
STEP: delete the pod 03/16/23 09:49:03.821
Mar 16 09:49:03.918: INFO: Waiting for pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac to disappear
Mar 16 09:49:04.006: INFO: Pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:04.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9743" for this suite. 03/16/23 09:49:04.183
------------------------------
• [5.578 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:48:58.696
    Mar 16 09:48:58.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 09:48:58.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:48:58.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:48:59.141
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 03/16/23 09:48:59.318
    Mar 16 09:48:59.412: INFO: Waiting up to 5m0s for pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac" in namespace "var-expansion-9743" to be "Succeeded or Failed"
    Mar 16 09:48:59.501: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Pending", Reason="", readiness=false. Elapsed: 88.996857ms
    Mar 16 09:49:01.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178979381s
    Mar 16 09:49:03.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179004407s
    STEP: Saw pod success 03/16/23 09:49:03.591
    Mar 16 09:49:03.591: INFO: Pod "var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac" satisfied condition "Succeeded or Failed"
    Mar 16 09:49:03.680: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac container dapi-container: <nil>
    STEP: delete the pod 03/16/23 09:49:03.821
    Mar 16 09:49:03.918: INFO: Waiting for pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac to disappear
    Mar 16 09:49:04.006: INFO: Pod var-expansion-6156010e-c3db-4d0a-b7ae-205fb31778ac no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:04.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9743" for this suite. 03/16/23 09:49:04.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:04.274
Mar 16 09:49:04.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:49:04.275
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:04.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:04.72
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Mar 16 09:49:04.897: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:49:09.04
Mar 16 09:49:09.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 create -f -'
Mar 16 09:49:10.669: INFO: stderr: ""
Mar 16 09:49:10.670: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 16 09:49:10.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
Mar 16 09:49:11.119: INFO: stderr: ""
Mar 16 09:49:11.119: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 16 09:49:11.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 apply -f -'
Mar 16 09:49:12.210: INFO: stderr: ""
Mar 16 09:49:12.210: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 16 09:49:12.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
Mar 16 09:49:12.665: INFO: stderr: ""
Mar 16 09:49:12.665: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/16/23 09:49:12.665
Mar 16 09:49:12.665: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 explain e2e-test-crd-publish-openapi-7178-crds'
Mar 16 09:49:13.160: INFO: stderr: ""
Mar 16 09:49:13.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7178-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3076" for this suite. 03/16/23 09:49:17.84
------------------------------
• [13.657 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:04.274
    Mar 16 09:49:04.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:49:04.275
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:04.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:04.72
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Mar 16 09:49:04.897: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/16/23 09:49:09.04
    Mar 16 09:49:09.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 create -f -'
    Mar 16 09:49:10.669: INFO: stderr: ""
    Mar 16 09:49:10.670: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 16 09:49:10.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
    Mar 16 09:49:11.119: INFO: stderr: ""
    Mar 16 09:49:11.119: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 16 09:49:11.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 apply -f -'
    Mar 16 09:49:12.210: INFO: stderr: ""
    Mar 16 09:49:12.210: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 16 09:49:12.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 --namespace=crd-publish-openapi-3076 delete e2e-test-crd-publish-openapi-7178-crds test-cr'
    Mar 16 09:49:12.665: INFO: stderr: ""
    Mar 16 09:49:12.665: INFO: stdout: "e2e-test-crd-publish-openapi-7178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/16/23 09:49:12.665
    Mar 16 09:49:12.665: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3076 explain e2e-test-crd-publish-openapi-7178-crds'
    Mar 16 09:49:13.160: INFO: stderr: ""
    Mar 16 09:49:13.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7178-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3076" for this suite. 03/16/23 09:49:17.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:17.932
Mar 16 09:49:17.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange 03/16/23 09:49:17.934
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:18.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:18.383
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 03/16/23 09:49:18.563
STEP: Setting up watch 03/16/23 09:49:18.563
STEP: Submitting a LimitRange 03/16/23 09:49:18.754
STEP: Verifying LimitRange creation was observed 03/16/23 09:49:18.845
STEP: Fetching the LimitRange to ensure it has proper values 03/16/23 09:49:18.845
Mar 16 09:49:18.935: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 16 09:49:18.935: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/16/23 09:49:18.935
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/16/23 09:49:19.029
Mar 16 09:49:19.119: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 16 09:49:19.120: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/16/23 09:49:19.12
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/16/23 09:49:19.213
Mar 16 09:49:19.304: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 16 09:49:19.304: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/16/23 09:49:19.304
STEP: Failing to create a Pod with more than max resources 03/16/23 09:49:19.396
STEP: Updating a LimitRange 03/16/23 09:49:19.489
STEP: Verifying LimitRange updating is effective 03/16/23 09:49:19.58
STEP: Creating a Pod with less than former min resources 03/16/23 09:49:21.671
STEP: Failing to create a Pod with more than max resources 03/16/23 09:49:21.765
STEP: Deleting a LimitRange 03/16/23 09:49:21.857
STEP: Verifying the LimitRange was deleted 03/16/23 09:49:21.949
Mar 16 09:49:27.040: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/16/23 09:49:27.04
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3486" for this suite. 03/16/23 09:49:27.314
------------------------------
• [9.473 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:17.932
    Mar 16 09:49:17.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename limitrange 03/16/23 09:49:17.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:18.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:18.383
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 03/16/23 09:49:18.563
    STEP: Setting up watch 03/16/23 09:49:18.563
    STEP: Submitting a LimitRange 03/16/23 09:49:18.754
    STEP: Verifying LimitRange creation was observed 03/16/23 09:49:18.845
    STEP: Fetching the LimitRange to ensure it has proper values 03/16/23 09:49:18.845
    Mar 16 09:49:18.935: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 16 09:49:18.935: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/16/23 09:49:18.935
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/16/23 09:49:19.029
    Mar 16 09:49:19.119: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 16 09:49:19.120: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/16/23 09:49:19.12
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/16/23 09:49:19.213
    Mar 16 09:49:19.304: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 16 09:49:19.304: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/16/23 09:49:19.304
    STEP: Failing to create a Pod with more than max resources 03/16/23 09:49:19.396
    STEP: Updating a LimitRange 03/16/23 09:49:19.489
    STEP: Verifying LimitRange updating is effective 03/16/23 09:49:19.58
    STEP: Creating a Pod with less than former min resources 03/16/23 09:49:21.671
    STEP: Failing to create a Pod with more than max resources 03/16/23 09:49:21.765
    STEP: Deleting a LimitRange 03/16/23 09:49:21.857
    STEP: Verifying the LimitRange was deleted 03/16/23 09:49:21.949
    Mar 16 09:49:27.040: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/16/23 09:49:27.04
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3486" for this suite. 03/16/23 09:49:27.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:27.406
Mar 16 09:49:27.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 03/16/23 09:49:27.407
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:27.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:27.857
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Mar 16 09:49:28.131: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a" in namespace "security-context-test-6394" to be "Succeeded or Failed"
Mar 16 09:49:28.222: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Pending", Reason="", readiness=false. Elapsed: 90.558455ms
Mar 16 09:49:30.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182132984s
Mar 16 09:49:32.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181407796s
Mar 16 09:49:32.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a" satisfied condition "Succeeded or Failed"
Mar 16 09:49:32.411: INFO: Got logs for pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:32.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6394" for this suite. 03/16/23 09:49:32.591
------------------------------
• [5.276 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:27.406
    Mar 16 09:49:27.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 03/16/23 09:49:27.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:27.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:27.857
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Mar 16 09:49:28.131: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a" in namespace "security-context-test-6394" to be "Succeeded or Failed"
    Mar 16 09:49:28.222: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Pending", Reason="", readiness=false. Elapsed: 90.558455ms
    Mar 16 09:49:30.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182132984s
    Mar 16 09:49:32.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181407796s
    Mar 16 09:49:32.313: INFO: Pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a" satisfied condition "Succeeded or Failed"
    Mar 16 09:49:32.411: INFO: Got logs for pod "busybox-privileged-false-71f96074-1c68-47f1-a451-e24d1a89d95a": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:32.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6394" for this suite. 03/16/23 09:49:32.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:32.684
Mar 16 09:49:32.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 09:49:32.685
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:32.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:33.135
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 03/16/23 09:49:33.314
Mar 16 09:49:33.410: INFO: Waiting up to 5m0s for pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f" in namespace "emptydir-6834" to be "Succeeded or Failed"
Mar 16 09:49:33.500: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 90.269602ms
Mar 16 09:49:35.592: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181999671s
Mar 16 09:49:37.591: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181536089s
STEP: Saw pod success 03/16/23 09:49:37.591
Mar 16 09:49:37.591: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f" satisfied condition "Succeeded or Failed"
Mar 16 09:49:37.682: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f container test-container: <nil>
STEP: delete the pod 03/16/23 09:49:37.817
Mar 16 09:49:37.910: INFO: Waiting for pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f to disappear
Mar 16 09:49:38.000: INFO: Pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:38.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6834" for this suite. 03/16/23 09:49:38.179
------------------------------
• [5.586 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:32.684
    Mar 16 09:49:32.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 09:49:32.685
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:32.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:33.135
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/16/23 09:49:33.314
    Mar 16 09:49:33.410: INFO: Waiting up to 5m0s for pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f" in namespace "emptydir-6834" to be "Succeeded or Failed"
    Mar 16 09:49:33.500: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 90.269602ms
    Mar 16 09:49:35.592: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181999671s
    Mar 16 09:49:37.591: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181536089s
    STEP: Saw pod success 03/16/23 09:49:37.591
    Mar 16 09:49:37.591: INFO: Pod "pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f" satisfied condition "Succeeded or Failed"
    Mar 16 09:49:37.682: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f container test-container: <nil>
    STEP: delete the pod 03/16/23 09:49:37.817
    Mar 16 09:49:37.910: INFO: Waiting for pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f to disappear
    Mar 16 09:49:38.000: INFO: Pod pod-3e0814bf-6de4-44dc-b49a-bf6973560e8f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:38.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6834" for this suite. 03/16/23 09:49:38.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:38.271
Mar 16 09:49:38.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:49:38.272
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:38.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:38.722
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 03/16/23 09:49:38.901
Mar 16 09:49:38.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905" in namespace "projected-2765" to be "Succeeded or Failed"
Mar 16 09:49:39.087: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Pending", Reason="", readiness=false. Elapsed: 90.265923ms
Mar 16 09:49:41.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181829175s
Mar 16 09:49:43.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180986087s
STEP: Saw pod success 03/16/23 09:49:43.178
Mar 16 09:49:43.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905" satisfied condition "Succeeded or Failed"
Mar 16 09:49:43.268: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 container client-container: <nil>
STEP: delete the pod 03/16/23 09:49:43.364
Mar 16 09:49:43.459: INFO: Waiting for pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 to disappear
Mar 16 09:49:43.549: INFO: Pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 09:49:43.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2765" for this suite. 03/16/23 09:49:43.728
------------------------------
• [5.549 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:38.271
    Mar 16 09:49:38.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:49:38.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:38.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:38.722
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 03/16/23 09:49:38.901
    Mar 16 09:49:38.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905" in namespace "projected-2765" to be "Succeeded or Failed"
    Mar 16 09:49:39.087: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Pending", Reason="", readiness=false. Elapsed: 90.265923ms
    Mar 16 09:49:41.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181829175s
    Mar 16 09:49:43.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180986087s
    STEP: Saw pod success 03/16/23 09:49:43.178
    Mar 16 09:49:43.178: INFO: Pod "downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905" satisfied condition "Succeeded or Failed"
    Mar 16 09:49:43.268: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 container client-container: <nil>
    STEP: delete the pod 03/16/23 09:49:43.364
    Mar 16 09:49:43.459: INFO: Waiting for pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 to disappear
    Mar 16 09:49:43.549: INFO: Pod downwardapi-volume-91cefdaa-1aba-4dc9-88bd-aae089991905 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:49:43.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2765" for this suite. 03/16/23 09:49:43.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:49:43.82
Mar 16 09:49:43.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 09:49:43.821
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:44.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:44.273
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8359 03/16/23 09:49:44.452
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 03/16/23 09:49:44.543
Mar 16 09:49:44.724: INFO: Found 1 stateful pods, waiting for 3
Mar 16 09:49:54.820: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 09:49:54.820: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 09:49:54.820: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/16/23 09:49:55.091
Mar 16 09:49:55.283: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/16/23 09:49:55.283
STEP: Not applying an update when the partition is greater than the number of replicas 03/16/23 09:49:55.465
STEP: Performing a canary update 03/16/23 09:49:55.465
Mar 16 09:49:55.658: INFO: Updating stateful set ss2
Mar 16 09:49:55.839: INFO: Waiting for Pod statefulset-8359/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 03/16/23 09:50:06.023
Mar 16 09:50:06.303: INFO: Found 2 stateful pods, waiting for 3
Mar 16 09:50:16.398: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 09:50:16.398: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 09:50:16.398: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/16/23 09:50:16.579
Mar 16 09:50:16.771: INFO: Updating stateful set ss2
Mar 16 09:50:16.952: INFO: Waiting for Pod statefulset-8359/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Mar 16 09:50:27.327: INFO: Updating stateful set ss2
Mar 16 09:50:27.508: INFO: Waiting for StatefulSet statefulset-8359/ss2 to complete update
Mar 16 09:50:27.508: INFO: Waiting for Pod statefulset-8359/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 09:50:37.690: INFO: Deleting all statefulset in ns statefulset-8359
Mar 16 09:50:37.780: INFO: Scaling statefulset ss2 to 0
Mar 16 09:50:48.144: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 09:50:48.234: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 09:50:48.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8359" for this suite. 03/16/23 09:50:48.685
------------------------------
• [64.957 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:49:43.82
    Mar 16 09:49:43.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 09:49:43.821
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:49:44.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:49:44.273
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8359 03/16/23 09:49:44.452
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 03/16/23 09:49:44.543
    Mar 16 09:49:44.724: INFO: Found 1 stateful pods, waiting for 3
    Mar 16 09:49:54.820: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 09:49:54.820: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 09:49:54.820: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/16/23 09:49:55.091
    Mar 16 09:49:55.283: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/16/23 09:49:55.283
    STEP: Not applying an update when the partition is greater than the number of replicas 03/16/23 09:49:55.465
    STEP: Performing a canary update 03/16/23 09:49:55.465
    Mar 16 09:49:55.658: INFO: Updating stateful set ss2
    Mar 16 09:49:55.839: INFO: Waiting for Pod statefulset-8359/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 03/16/23 09:50:06.023
    Mar 16 09:50:06.303: INFO: Found 2 stateful pods, waiting for 3
    Mar 16 09:50:16.398: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 09:50:16.398: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 09:50:16.398: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/16/23 09:50:16.579
    Mar 16 09:50:16.771: INFO: Updating stateful set ss2
    Mar 16 09:50:16.952: INFO: Waiting for Pod statefulset-8359/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Mar 16 09:50:27.327: INFO: Updating stateful set ss2
    Mar 16 09:50:27.508: INFO: Waiting for StatefulSet statefulset-8359/ss2 to complete update
    Mar 16 09:50:27.508: INFO: Waiting for Pod statefulset-8359/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 09:50:37.690: INFO: Deleting all statefulset in ns statefulset-8359
    Mar 16 09:50:37.780: INFO: Scaling statefulset ss2 to 0
    Mar 16 09:50:48.144: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 09:50:48.234: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:50:48.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8359" for this suite. 03/16/23 09:50:48.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:50:48.78
Mar 16 09:50:48.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 09:50:48.781
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:50:49.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:50:49.232
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Mar 16 09:50:49.411: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/16/23 09:50:49.592
STEP: Checking rc "condition-test" has the desired failure condition set 03/16/23 09:50:49.684
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/16/23 09:50:49.774
Mar 16 09:50:49.956: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/16/23 09:50:49.956
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 09:50:50.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8790" for this suite. 03/16/23 09:50:50.138
------------------------------
• [1.449 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:50:48.78
    Mar 16 09:50:48.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 09:50:48.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:50:49.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:50:49.232
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Mar 16 09:50:49.411: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/16/23 09:50:49.592
    STEP: Checking rc "condition-test" has the desired failure condition set 03/16/23 09:50:49.684
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/16/23 09:50:49.774
    Mar 16 09:50:49.956: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/16/23 09:50:49.956
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:50:50.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8790" for this suite. 03/16/23 09:50:50.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:50:50.23
Mar 16 09:50:50.230: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 03/16/23 09:50:50.231
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:50:50.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:50:50.681
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 03/16/23 09:50:50.861
Mar 16 09:50:50.861: INFO: PodSpec: initContainers in spec.initContainers
Mar 16 09:51:30.922: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2079a52e-7be9-4b01-aea1-649b902d8d1b", GenerateName:"", Namespace:"init-container-5272", SelfLink:"", UID:"0bc69d09-5348-431d-a62f-9b5ed5251445", ResourceVersion:"10590", Generation:0, CreationTimestamp:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"861381579"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"dc8ef68729effa017c7fe4f758ebc0eac4bf71d1df7ee359e82537c5b4122745", "cni.projectcalico.org/podIP":"100.64.1.108/32", "cni.projectcalico.org/podIPs":"100.64.1.108/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0073460a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 50, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007346108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 51, 30, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007346168), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bm7cs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0072e4020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00490c120), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-19-136.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004280e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490c1a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490c1c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00490c1c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00490c1cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000fe2060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.19.136", PodIP:"100.64.1.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.108"}}, StartTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004281c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000428230)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a1cf3bfd4106a1c5273f3eb232f5824eb51364b7242da424e0ca9acb11401aef", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0072e40a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0072e4080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00490c24f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:51:30.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5272" for this suite. 03/16/23 09:51:31.102
------------------------------
• [40.963 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:50:50.23
    Mar 16 09:50:50.230: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 03/16/23 09:50:50.231
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:50:50.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:50:50.681
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 03/16/23 09:50:50.861
    Mar 16 09:50:50.861: INFO: PodSpec: initContainers in spec.initContainers
    Mar 16 09:51:30.922: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2079a52e-7be9-4b01-aea1-649b902d8d1b", GenerateName:"", Namespace:"init-container-5272", SelfLink:"", UID:"0bc69d09-5348-431d-a62f-9b5ed5251445", ResourceVersion:"10590", Generation:0, CreationTimestamp:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"861381579"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"dc8ef68729effa017c7fe4f758ebc0eac4bf71d1df7ee359e82537c5b4122745", "cni.projectcalico.org/podIP":"100.64.1.108/32", "cni.projectcalico.org/podIPs":"100.64.1.108/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0073460a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 50, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007346108), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 16, 9, 51, 30, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007346168), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bm7cs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0072e4020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bm7cs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00490c120), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-19-136.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004280e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490c1a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490c1c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00490c1c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00490c1cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000fe2060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.19.136", PodIP:"100.64.1.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.108"}}, StartTime:time.Date(2023, time.March, 16, 9, 50, 50, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004281c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000428230)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a1cf3bfd4106a1c5273f3eb232f5824eb51364b7242da424e0ca9acb11401aef", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0072e40a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0072e4080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00490c24f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:51:30.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5272" for this suite. 03/16/23 09:51:31.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:51:31.195
Mar 16 09:51:31.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 09:51:31.197
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:51:31.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:51:31.648
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 09:51:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-31" for this suite. 03/16/23 09:51:32.008
------------------------------
• [0.920 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:51:31.195
    Mar 16 09:51:31.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 09:51:31.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:51:31.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:51:31.648
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:51:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-31" for this suite. 03/16/23 09:51:32.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:51:32.116
Mar 16 09:51:32.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 03/16/23 09:51:32.117
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:51:32.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:51:32.567
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 16 09:51:33.017: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 09:52:33.926: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:52:34.016
Mar 16 09:52:34.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 03/16/23 09:52:34.017
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:52:34.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:52:34.467
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 03/16/23 09:52:34.646
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 09:52:34.646
Mar 16 09:52:34.752: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1024" to be "running"
Mar 16 09:52:34.842: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 90.474391ms
Mar 16 09:52:36.933: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.181188962s
Mar 16 09:52:36.933: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 09:52:37.023
Mar 16 09:52:37.116: INFO: found a healthy node: ip-10-250-19-136.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Mar 16 09:52:44.576: INFO: pods created so far: [1 1 1]
Mar 16 09:52:44.576: INFO: length of pods created so far: 3
Mar 16 09:52:46.761: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Mar 16 09:52:53.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:52:54.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1024" for this suite. 03/16/23 09:52:54.964
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9440" for this suite. 03/16/23 09:52:55.055
------------------------------
• [83.031 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:51:32.116
    Mar 16 09:51:32.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 03/16/23 09:51:32.117
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:51:32.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:51:32.567
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 16 09:51:33.017: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 09:52:33.926: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:52:34.016
    Mar 16 09:52:34.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 03/16/23 09:52:34.017
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:52:34.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:52:34.467
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 03/16/23 09:52:34.646
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 09:52:34.646
    Mar 16 09:52:34.752: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1024" to be "running"
    Mar 16 09:52:34.842: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 90.474391ms
    Mar 16 09:52:36.933: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.181188962s
    Mar 16 09:52:36.933: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 09:52:37.023
    Mar 16 09:52:37.116: INFO: found a healthy node: ip-10-250-19-136.ec2.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Mar 16 09:52:44.576: INFO: pods created so far: [1 1 1]
    Mar 16 09:52:44.576: INFO: length of pods created so far: 3
    Mar 16 09:52:46.761: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:52:53.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:52:54.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1024" for this suite. 03/16/23 09:52:54.964
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9440" for this suite. 03/16/23 09:52:55.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:52:55.148
Mar 16 09:52:55.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 03/16/23 09:52:55.149
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:52:55.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:52:55.6
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/16/23 09:52:55.779
STEP: creating a watch on configmaps with label B 03/16/23 09:52:55.868
STEP: creating a watch on configmaps with label A or B 03/16/23 09:52:55.957
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.047
Mar 16 09:52:56.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11222 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:52:56.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11222 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.138
Mar 16 09:52:56.319: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11223 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:52:56.320: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11223 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/16/23 09:52:56.32
Mar 16 09:52:56.500: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11224 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:52:56.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11224 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.501
Mar 16 09:52:56.592: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11225 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:52:56.593: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11225 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/16/23 09:52:56.593
Mar 16 09:52:56.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11228 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:52:56.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11228 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/16/23 09:53:06.685
Mar 16 09:53:06.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11310 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:53:06.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11310 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 16 09:53:16.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2313" for this suite. 03/16/23 09:53:16.958
------------------------------
• [21.900 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:52:55.148
    Mar 16 09:52:55.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 03/16/23 09:52:55.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:52:55.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:52:55.6
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/16/23 09:52:55.779
    STEP: creating a watch on configmaps with label B 03/16/23 09:52:55.868
    STEP: creating a watch on configmaps with label A or B 03/16/23 09:52:55.957
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.047
    Mar 16 09:52:56.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11222 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:52:56.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11222 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.138
    Mar 16 09:52:56.319: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11223 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:52:56.320: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11223 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/16/23 09:52:56.32
    Mar 16 09:52:56.500: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11224 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:52:56.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11224 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/16/23 09:52:56.501
    Mar 16 09:52:56.592: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11225 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:52:56.593: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2313  473e7ba1-efb3-4827-90b8-e6e3b6201bb8 11225 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/16/23 09:52:56.593
    Mar 16 09:52:56.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11228 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:52:56.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11228 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/16/23 09:53:06.685
    Mar 16 09:53:06.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11310 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:53:06.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2313  d2527128-63a6-4918-8df2-1e66bda87649 11310 0 2023-03-16 09:52:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-16 09:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:53:16.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2313" for this suite. 03/16/23 09:53:16.958
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:53:17.049
Mar 16 09:53:17.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 09:53:17.05
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:17.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:17.5
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Mar 16 09:53:17.780: INFO: Waiting up to 2m0s for pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" in namespace "var-expansion-4469" to be "container 0 failed with reason CreateContainerConfigError"
Mar 16 09:53:17.871: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.423656ms
Mar 16 09:53:19.962: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181990761s
Mar 16 09:53:19.962: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 16 09:53:19.962: INFO: Deleting pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" in namespace "var-expansion-4469"
Mar 16 09:53:20.053: INFO: Wait up to 5m0s for pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 09:53:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4469" for this suite. 03/16/23 09:53:22.414
------------------------------
• [5.456 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:53:17.049
    Mar 16 09:53:17.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 09:53:17.05
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:17.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:17.5
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Mar 16 09:53:17.780: INFO: Waiting up to 2m0s for pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" in namespace "var-expansion-4469" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 16 09:53:17.871: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.423656ms
    Mar 16 09:53:19.962: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181990761s
    Mar 16 09:53:19.962: INFO: Pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 16 09:53:19.962: INFO: Deleting pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" in namespace "var-expansion-4469"
    Mar 16 09:53:20.053: INFO: Wait up to 5m0s for pod "var-expansion-13a4e7c4-3d15-44ab-aa1e-d4bf3da61f9c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:53:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4469" for this suite. 03/16/23 09:53:22.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:53:22.506
Mar 16 09:53:22.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 09:53:22.508
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:22.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:22.961
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 09:53:23.323
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 09:53:23.993
STEP: Deploying the webhook pod 03/16/23 09:53:24.085
STEP: Wait for the deployment to be ready 03/16/23 09:53:24.267
Mar 16 09:53:24.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 09:53:26.629
STEP: Verifying the service has paired with the endpoint 03/16/23 09:53:26.724
Mar 16 09:53:27.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 03/16/23 09:53:27.815
STEP: create a pod that should be denied by the webhook 03/16/23 09:53:28.08
STEP: create a pod that causes the webhook to hang 03/16/23 09:53:28.261
STEP: create a configmap that should be denied by the webhook 03/16/23 09:53:38.444
STEP: create a configmap that should be admitted by the webhook 03/16/23 09:53:38.562
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/16/23 09:53:38.711
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/16/23 09:53:38.94
STEP: create a namespace that bypass the webhook 03/16/23 09:53:39.038
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/16/23 09:53:39.131
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:53:39.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4200" for this suite. 03/16/23 09:53:40.018
STEP: Destroying namespace "webhook-4200-markers" for this suite. 03/16/23 09:53:40.109
------------------------------
• [17.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:53:22.506
    Mar 16 09:53:22.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 09:53:22.508
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:22.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:22.961
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 09:53:23.323
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 09:53:23.993
    STEP: Deploying the webhook pod 03/16/23 09:53:24.085
    STEP: Wait for the deployment to be ready 03/16/23 09:53:24.267
    Mar 16 09:53:24.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 9, 53, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 09:53:26.629
    STEP: Verifying the service has paired with the endpoint 03/16/23 09:53:26.724
    Mar 16 09:53:27.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 03/16/23 09:53:27.815
    STEP: create a pod that should be denied by the webhook 03/16/23 09:53:28.08
    STEP: create a pod that causes the webhook to hang 03/16/23 09:53:28.261
    STEP: create a configmap that should be denied by the webhook 03/16/23 09:53:38.444
    STEP: create a configmap that should be admitted by the webhook 03/16/23 09:53:38.562
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/16/23 09:53:38.711
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/16/23 09:53:38.94
    STEP: create a namespace that bypass the webhook 03/16/23 09:53:39.038
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/16/23 09:53:39.131
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:53:39.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4200" for this suite. 03/16/23 09:53:40.018
    STEP: Destroying namespace "webhook-4200-markers" for this suite. 03/16/23 09:53:40.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:53:40.201
Mar 16 09:53:40.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:53:40.203
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:40.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:40.653
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-dfc030f6-4177-4ed3-b32d-748e480ecb89 03/16/23 09:53:40.832
STEP: Creating a pod to test consume secrets 03/16/23 09:53:40.922
Mar 16 09:53:41.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9" in namespace "projected-152" to be "Succeeded or Failed"
Mar 16 09:53:41.108: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Pending", Reason="", readiness=false. Elapsed: 90.08057ms
Mar 16 09:53:43.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181667271s
Mar 16 09:53:45.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181541886s
STEP: Saw pod success 03/16/23 09:53:45.199
Mar 16 09:53:45.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9" satisfied condition "Succeeded or Failed"
Mar 16 09:53:45.289: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/16/23 09:53:45.427
Mar 16 09:53:45.527: INFO: Waiting for pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 to disappear
Mar 16 09:53:45.617: INFO: Pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 09:53:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-152" for this suite. 03/16/23 09:53:45.796
------------------------------
• [5.686 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:53:40.201
    Mar 16 09:53:40.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:53:40.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:40.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:40.653
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-dfc030f6-4177-4ed3-b32d-748e480ecb89 03/16/23 09:53:40.832
    STEP: Creating a pod to test consume secrets 03/16/23 09:53:40.922
    Mar 16 09:53:41.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9" in namespace "projected-152" to be "Succeeded or Failed"
    Mar 16 09:53:41.108: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Pending", Reason="", readiness=false. Elapsed: 90.08057ms
    Mar 16 09:53:43.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181667271s
    Mar 16 09:53:45.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181541886s
    STEP: Saw pod success 03/16/23 09:53:45.199
    Mar 16 09:53:45.199: INFO: Pod "pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9" satisfied condition "Succeeded or Failed"
    Mar 16 09:53:45.289: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 09:53:45.427
    Mar 16 09:53:45.527: INFO: Waiting for pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 to disappear
    Mar 16 09:53:45.617: INFO: Pod pod-projected-secrets-99484f2f-dc16-4a54-8dff-44b355234df9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:53:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-152" for this suite. 03/16/23 09:53:45.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:53:45.889
Mar 16 09:53:45.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 09:53:45.89
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:46.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:46.34
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/16/23 09:53:46.519
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:53:46.611
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:53:46.611
STEP: creating a pod to probe DNS 03/16/23 09:53:46.611
STEP: submitting the pod to kubernetes 03/16/23 09:53:46.611
Mar 16 09:53:46.708: INFO: Waiting up to 15m0s for pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186" in namespace "dns-3378" to be "running"
Mar 16 09:53:46.798: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 90.489854ms
Mar 16 09:53:48.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181653823s
Mar 16 09:53:50.890: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18273751s
Mar 16 09:53:52.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Running", Reason="", readiness=true. Elapsed: 6.181563117s
Mar 16 09:53:52.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186" satisfied condition "running"
STEP: retrieving the pod 03/16/23 09:53:52.889
STEP: looking for the results for each expected name from probers 03/16/23 09:53:52.98
Mar 16 09:53:53.335: INFO: DNS probes using dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186 succeeded

STEP: deleting the pod 03/16/23 09:53:53.335
STEP: changing the externalName to bar.example.com 03/16/23 09:53:53.429
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:53:53.61
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:53:53.611
STEP: creating a second pod to probe DNS 03/16/23 09:53:53.611
STEP: submitting the pod to kubernetes 03/16/23 09:53:53.611
Mar 16 09:53:53.706: INFO: Waiting up to 15m0s for pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c" in namespace "dns-3378" to be "running"
Mar 16 09:53:53.796: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.127772ms
Mar 16 09:53:55.888: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.181482296s
Mar 16 09:53:55.888: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c" satisfied condition "running"
STEP: retrieving the pod 03/16/23 09:53:55.888
STEP: looking for the results for each expected name from probers 03/16/23 09:53:55.978
Mar 16 09:53:56.171: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:53:56.307: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:53:56.307: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

Mar 16 09:54:01.451: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:01.544: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:01.544: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

Mar 16 09:54:06.402: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:06.495: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:06.495: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

Mar 16 09:54:11.401: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:11.494: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:11.494: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

Mar 16 09:54:16.401: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:16.494: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 16 09:54:16.494: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

Mar 16 09:54:21.494: INFO: DNS probes using dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c succeeded

STEP: deleting the pod 03/16/23 09:54:21.494
STEP: changing the service to type=ClusterIP 03/16/23 09:54:21.591
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:54:21.777
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
 03/16/23 09:54:21.777
STEP: creating a third pod to probe DNS 03/16/23 09:54:21.777
STEP: submitting the pod to kubernetes 03/16/23 09:54:21.867
Mar 16 09:54:21.963: INFO: Waiting up to 15m0s for pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c" in namespace "dns-3378" to be "running"
Mar 16 09:54:22.053: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.427648ms
Mar 16 09:54:24.145: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c": Phase="Running", Reason="", readiness=true. Elapsed: 2.182354714s
Mar 16 09:54:24.145: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c" satisfied condition "running"
STEP: retrieving the pod 03/16/23 09:54:24.145
STEP: looking for the results for each expected name from probers 03/16/23 09:54:24.236
Mar 16 09:54:24.567: INFO: DNS probes using dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c succeeded

STEP: deleting the pod 03/16/23 09:54:24.567
STEP: deleting the test externalName service 03/16/23 09:54:24.663
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 09:54:24.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3378" for this suite. 03/16/23 09:54:24.938
------------------------------
• [39.140 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:53:45.889
    Mar 16 09:53:45.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 09:53:45.89
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:53:46.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:53:46.34
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/16/23 09:53:46.519
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:53:46.611
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:53:46.611
    STEP: creating a pod to probe DNS 03/16/23 09:53:46.611
    STEP: submitting the pod to kubernetes 03/16/23 09:53:46.611
    Mar 16 09:53:46.708: INFO: Waiting up to 15m0s for pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186" in namespace "dns-3378" to be "running"
    Mar 16 09:53:46.798: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 90.489854ms
    Mar 16 09:53:48.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181653823s
    Mar 16 09:53:50.890: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18273751s
    Mar 16 09:53:52.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186": Phase="Running", Reason="", readiness=true. Elapsed: 6.181563117s
    Mar 16 09:53:52.889: INFO: Pod "dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 09:53:52.889
    STEP: looking for the results for each expected name from probers 03/16/23 09:53:52.98
    Mar 16 09:53:53.335: INFO: DNS probes using dns-test-3c399b5b-f143-42b0-80c8-3f5f8b91a186 succeeded

    STEP: deleting the pod 03/16/23 09:53:53.335
    STEP: changing the externalName to bar.example.com 03/16/23 09:53:53.429
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:53:53.61
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:53:53.611
    STEP: creating a second pod to probe DNS 03/16/23 09:53:53.611
    STEP: submitting the pod to kubernetes 03/16/23 09:53:53.611
    Mar 16 09:53:53.706: INFO: Waiting up to 15m0s for pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c" in namespace "dns-3378" to be "running"
    Mar 16 09:53:53.796: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.127772ms
    Mar 16 09:53:55.888: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.181482296s
    Mar 16 09:53:55.888: INFO: Pod "dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 09:53:55.888
    STEP: looking for the results for each expected name from probers 03/16/23 09:53:55.978
    Mar 16 09:53:56.171: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:53:56.307: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:53:56.307: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

    Mar 16 09:54:01.451: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:01.544: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:01.544: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

    Mar 16 09:54:06.402: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:06.495: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:06.495: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

    Mar 16 09:54:11.401: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:11.494: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:11.494: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

    Mar 16 09:54:16.401: INFO: File wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:16.494: INFO: File jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local from pod  dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 16 09:54:16.494: INFO: Lookups using dns-3378/dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c failed for: [wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local]

    Mar 16 09:54:21.494: INFO: DNS probes using dns-test-d609f227-34af-48e0-9a5a-baad47a7ce6c succeeded

    STEP: deleting the pod 03/16/23 09:54:21.494
    STEP: changing the service to type=ClusterIP 03/16/23 09:54:21.591
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:54:21.777
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3378.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3378.svc.cluster.local; sleep 1; done
     03/16/23 09:54:21.777
    STEP: creating a third pod to probe DNS 03/16/23 09:54:21.777
    STEP: submitting the pod to kubernetes 03/16/23 09:54:21.867
    Mar 16 09:54:21.963: INFO: Waiting up to 15m0s for pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c" in namespace "dns-3378" to be "running"
    Mar 16 09:54:22.053: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.427648ms
    Mar 16 09:54:24.145: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c": Phase="Running", Reason="", readiness=true. Elapsed: 2.182354714s
    Mar 16 09:54:24.145: INFO: Pod "dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 09:54:24.145
    STEP: looking for the results for each expected name from probers 03/16/23 09:54:24.236
    Mar 16 09:54:24.567: INFO: DNS probes using dns-test-79fb676f-bd49-4c70-bf9c-abf01cf2a83c succeeded

    STEP: deleting the pod 03/16/23 09:54:24.567
    STEP: deleting the test externalName service 03/16/23 09:54:24.663
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:54:24.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3378" for this suite. 03/16/23 09:54:24.938
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:54:25.03
Mar 16 09:54:25.030: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 09:54:25.031
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:25.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:25.483
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Mar 16 09:54:25.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 03/16/23 09:54:25.664
STEP: submitting the pod to kubernetes 03/16/23 09:54:25.664
Mar 16 09:54:25.759: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631" in namespace "pods-3003" to be "running and ready"
Mar 16 09:54:25.849: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631": Phase="Pending", Reason="", readiness=false. Elapsed: 90.24584ms
Mar 16 09:54:25.849: INFO: The phase of Pod pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:54:27.940: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631": Phase="Running", Reason="", readiness=true. Elapsed: 2.180751978s
Mar 16 09:54:27.940: INFO: The phase of Pod pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631 is Running (Ready = true)
Mar 16 09:54:27.940: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 09:54:28.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3003" for this suite. 03/16/23 09:54:28.539
------------------------------
• [3.601 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:54:25.03
    Mar 16 09:54:25.030: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 09:54:25.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:25.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:25.483
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Mar 16 09:54:25.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 03/16/23 09:54:25.664
    STEP: submitting the pod to kubernetes 03/16/23 09:54:25.664
    Mar 16 09:54:25.759: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631" in namespace "pods-3003" to be "running and ready"
    Mar 16 09:54:25.849: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631": Phase="Pending", Reason="", readiness=false. Elapsed: 90.24584ms
    Mar 16 09:54:25.849: INFO: The phase of Pod pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:54:27.940: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631": Phase="Running", Reason="", readiness=true. Elapsed: 2.180751978s
    Mar 16 09:54:27.940: INFO: The phase of Pod pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631 is Running (Ready = true)
    Mar 16 09:54:27.940: INFO: Pod "pod-logs-websocket-7de9b545-cd64-4342-80a9-299686212631" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:54:28.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3003" for this suite. 03/16/23 09:54:28.539
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:54:28.631
Mar 16 09:54:28.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 09:54:28.632
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:28.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:29.083
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 03/16/23 09:54:29.263
Mar 16 09:54:29.357: INFO: Waiting up to 5m0s for pod "pod-xd6z2" in namespace "pods-5336" to be "running"
Mar 16 09:54:29.448: INFO: Pod "pod-xd6z2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.289584ms
Mar 16 09:54:31.539: INFO: Pod "pod-xd6z2": Phase="Running", Reason="", readiness=true. Elapsed: 2.181671219s
Mar 16 09:54:31.539: INFO: Pod "pod-xd6z2" satisfied condition "running"
STEP: patching /status 03/16/23 09:54:31.539
Mar 16 09:54:31.633: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 09:54:31.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5336" for this suite. 03/16/23 09:54:31.813
------------------------------
• [3.276 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:54:28.631
    Mar 16 09:54:28.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 09:54:28.632
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:28.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:29.083
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 03/16/23 09:54:29.263
    Mar 16 09:54:29.357: INFO: Waiting up to 5m0s for pod "pod-xd6z2" in namespace "pods-5336" to be "running"
    Mar 16 09:54:29.448: INFO: Pod "pod-xd6z2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.289584ms
    Mar 16 09:54:31.539: INFO: Pod "pod-xd6z2": Phase="Running", Reason="", readiness=true. Elapsed: 2.181671219s
    Mar 16 09:54:31.539: INFO: Pod "pod-xd6z2" satisfied condition "running"
    STEP: patching /status 03/16/23 09:54:31.539
    Mar 16 09:54:31.633: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:54:31.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5336" for this suite. 03/16/23 09:54:31.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:54:31.907
Mar 16 09:54:31.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 09:54:31.908
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:32.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:32.359
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/16/23 09:54:32.538
STEP: Wait for the Deployment to create new ReplicaSet 03/16/23 09:54:32.629
STEP: delete the deployment 03/16/23 09:54:32.81
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/16/23 09:54:32.901
STEP: Gathering metrics 03/16/23 09:54:33.353
W0316 09:54:33.450803    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 09:54:33.450: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 09:54:33.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2842" for this suite. 03/16/23 09:54:33.542
------------------------------
• [1.726 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:54:31.907
    Mar 16 09:54:31.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 09:54:31.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:32.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:32.359
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/16/23 09:54:32.538
    STEP: Wait for the Deployment to create new ReplicaSet 03/16/23 09:54:32.629
    STEP: delete the deployment 03/16/23 09:54:32.81
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/16/23 09:54:32.901
    STEP: Gathering metrics 03/16/23 09:54:33.353
    W0316 09:54:33.450803    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 09:54:33.450: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:54:33.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2842" for this suite. 03/16/23 09:54:33.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:54:33.635
Mar 16 09:54:33.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 03/16/23 09:54:33.637
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:33.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:34.087
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/16/23 09:54:34.361
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/16/23 09:54:52.997
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/16/23 09:54:53.087
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/16/23 09:54:53.268
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/16/23 09:54:53.269
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/16/23 09:54:53.546
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/16/23 09:54:55.818
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/16/23 09:54:58.09
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/16/23 09:54:58.271
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/16/23 09:54:58.272
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/16/23 09:54:58.549
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/16/23 09:54:58.64
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/16/23 09:55:02.003
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/16/23 09:55:02.184
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/16/23 09:55:02.184
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 16 09:55:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7535" for this suite. 03/16/23 09:55:02.817
------------------------------
• [29.273 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:54:33.635
    Mar 16 09:54:33.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 03/16/23 09:54:33.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:54:33.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:54:34.087
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/16/23 09:54:34.361
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/16/23 09:54:52.997
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/16/23 09:54:53.087
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/16/23 09:54:53.268
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/16/23 09:54:53.269
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/16/23 09:54:53.546
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/16/23 09:54:55.818
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/16/23 09:54:58.09
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/16/23 09:54:58.271
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/16/23 09:54:58.272
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/16/23 09:54:58.549
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/16/23 09:54:58.64
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/16/23 09:55:02.003
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/16/23 09:55:02.184
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/16/23 09:55:02.184
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:55:02.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7535" for this suite. 03/16/23 09:55:02.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:55:02.909
Mar 16 09:55:02.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 03/16/23 09:55:02.91
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:55:03.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:55:03.361
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 16 09:55:03.811: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 09:56:04.635: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 03/16/23 09:56:04.726
Mar 16 09:56:04.921: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 16 09:56:05.015: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 16 09:56:05.206: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 16 09:56:05.301: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/16/23 09:56:05.301
Mar 16 09:56:05.301: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3348" to be "running"
Mar 16 09:56:05.391: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 90.272761ms
Mar 16 09:56:07.484: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.183073125s
Mar 16 09:56:07.484: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 16 09:56:07.484: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
Mar 16 09:56:07.575: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.646994ms
Mar 16 09:56:07.575: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 16 09:56:07.575: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
Mar 16 09:56:07.665: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.335508ms
Mar 16 09:56:07.665: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 16 09:56:07.665: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
Mar 16 09:56:07.755: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.30248ms
Mar 16 09:56:07.755: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/16/23 09:56:07.755
Mar 16 09:56:07.859: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 16 09:56:07.949: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 90.265911ms
Mar 16 09:56:10.040: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181371797s
Mar 16 09:56:12.041: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.182598642s
Mar 16 09:56:12.041: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:56:12.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3348" for this suite. 03/16/23 09:56:13.327
------------------------------
• [70.509 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:55:02.909
    Mar 16 09:55:02.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 03/16/23 09:55:02.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:55:03.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:55:03.361
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 16 09:55:03.811: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 09:56:04.635: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 03/16/23 09:56:04.726
    Mar 16 09:56:04.921: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 16 09:56:05.015: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 16 09:56:05.206: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 16 09:56:05.301: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/16/23 09:56:05.301
    Mar 16 09:56:05.301: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3348" to be "running"
    Mar 16 09:56:05.391: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 90.272761ms
    Mar 16 09:56:07.484: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.183073125s
    Mar 16 09:56:07.484: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 16 09:56:07.484: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
    Mar 16 09:56:07.575: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.646994ms
    Mar 16 09:56:07.575: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 16 09:56:07.575: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
    Mar 16 09:56:07.665: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.335508ms
    Mar 16 09:56:07.665: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 16 09:56:07.665: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3348" to be "running"
    Mar 16 09:56:07.755: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.30248ms
    Mar 16 09:56:07.755: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/16/23 09:56:07.755
    Mar 16 09:56:07.859: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 16 09:56:07.949: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 90.265911ms
    Mar 16 09:56:10.040: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181371797s
    Mar 16 09:56:12.041: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.182598642s
    Mar 16 09:56:12.041: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:56:12.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3348" for this suite. 03/16/23 09:56:13.327
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:56:13.418
Mar 16 09:56:13.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 09:56:13.419
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:13.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:13.87
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 03/16/23 09:56:14.049
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/16/23 09:56:14.14
STEP: patching the secret 03/16/23 09:56:14.233
STEP: deleting the secret using a LabelSelector 03/16/23 09:56:14.415
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/16/23 09:56:14.507
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 09:56:14.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7092" for this suite. 03/16/23 09:56:14.691
------------------------------
• [1.364 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:56:13.418
    Mar 16 09:56:13.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 09:56:13.419
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:13.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:13.87
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 03/16/23 09:56:14.049
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/16/23 09:56:14.14
    STEP: patching the secret 03/16/23 09:56:14.233
    STEP: deleting the secret using a LabelSelector 03/16/23 09:56:14.415
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/16/23 09:56:14.507
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:56:14.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7092" for this suite. 03/16/23 09:56:14.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:56:14.783
Mar 16 09:56:14.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:56:14.784
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:15.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:15.235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Mar 16 09:56:15.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/16/23 09:56:19.547
Mar 16 09:56:19.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
Mar 16 09:56:21.159: INFO: stderr: ""
Mar 16 09:56:21.159: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 16 09:56:21.159: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 delete e2e-test-crd-publish-openapi-292-crds test-foo'
Mar 16 09:56:21.626: INFO: stderr: ""
Mar 16 09:56:21.626: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 16 09:56:21.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
Mar 16 09:56:22.361: INFO: stderr: ""
Mar 16 09:56:22.361: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 16 09:56:22.361: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 delete e2e-test-crd-publish-openapi-292-crds test-foo'
Mar 16 09:56:22.801: INFO: stderr: ""
Mar 16 09:56:22.801: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/16/23 09:56:22.801
Mar 16 09:56:22.801: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
Mar 16 09:56:23.382: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/16/23 09:56:23.382
Mar 16 09:56:23.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
Mar 16 09:56:24.348: INFO: rc: 1
Mar 16 09:56:24.348: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
Mar 16 09:56:25.101: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/16/23 09:56:25.101
Mar 16 09:56:25.102: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
Mar 16 09:56:25.668: INFO: rc: 1
Mar 16 09:56:25.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
Mar 16 09:56:26.411: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/16/23 09:56:26.411
Mar 16 09:56:26.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds'
Mar 16 09:56:26.881: INFO: stderr: ""
Mar 16 09:56:26.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/16/23 09:56:26.881
Mar 16 09:56:26.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.metadata'
Mar 16 09:56:27.360: INFO: stderr: ""
Mar 16 09:56:27.360: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 16 09:56:27.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec'
Mar 16 09:56:27.850: INFO: stderr: ""
Mar 16 09:56:27.850: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 16 09:56:27.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec.bars'
Mar 16 09:56:28.333: INFO: stderr: ""
Mar 16 09:56:28.333: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/16/23 09:56:28.334
Mar 16 09:56:28.334: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec.bars2'
Mar 16 09:56:28.808: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:56:34.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4482" for this suite. 03/16/23 09:56:34.687
------------------------------
• [19.996 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:56:14.783
    Mar 16 09:56:14.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:56:14.784
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:15.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:15.235
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Mar 16 09:56:15.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/16/23 09:56:19.547
    Mar 16 09:56:19.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
    Mar 16 09:56:21.159: INFO: stderr: ""
    Mar 16 09:56:21.159: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 16 09:56:21.159: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 delete e2e-test-crd-publish-openapi-292-crds test-foo'
    Mar 16 09:56:21.626: INFO: stderr: ""
    Mar 16 09:56:21.626: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 16 09:56:21.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
    Mar 16 09:56:22.361: INFO: stderr: ""
    Mar 16 09:56:22.361: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 16 09:56:22.361: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 delete e2e-test-crd-publish-openapi-292-crds test-foo'
    Mar 16 09:56:22.801: INFO: stderr: ""
    Mar 16 09:56:22.801: INFO: stdout: "e2e-test-crd-publish-openapi-292-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/16/23 09:56:22.801
    Mar 16 09:56:22.801: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
    Mar 16 09:56:23.382: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/16/23 09:56:23.382
    Mar 16 09:56:23.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
    Mar 16 09:56:24.348: INFO: rc: 1
    Mar 16 09:56:24.348: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
    Mar 16 09:56:25.101: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/16/23 09:56:25.101
    Mar 16 09:56:25.102: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 create -f -'
    Mar 16 09:56:25.668: INFO: rc: 1
    Mar 16 09:56:25.668: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 --namespace=crd-publish-openapi-4482 apply -f -'
    Mar 16 09:56:26.411: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/16/23 09:56:26.411
    Mar 16 09:56:26.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds'
    Mar 16 09:56:26.881: INFO: stderr: ""
    Mar 16 09:56:26.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/16/23 09:56:26.881
    Mar 16 09:56:26.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.metadata'
    Mar 16 09:56:27.360: INFO: stderr: ""
    Mar 16 09:56:27.360: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 16 09:56:27.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec'
    Mar 16 09:56:27.850: INFO: stderr: ""
    Mar 16 09:56:27.850: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 16 09:56:27.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec.bars'
    Mar 16 09:56:28.333: INFO: stderr: ""
    Mar 16 09:56:28.333: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-292-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/16/23 09:56:28.334
    Mar 16 09:56:28.334: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4482 explain e2e-test-crd-publish-openapi-292-crds.spec.bars2'
    Mar 16 09:56:28.808: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:56:34.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4482" for this suite. 03/16/23 09:56:34.687
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:56:34.779
Mar 16 09:56:34.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:56:34.781
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:35.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:35.232
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/16/23 09:56:35.411
Mar 16 09:56:35.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/16/23 09:57:01.254
Mar 16 09:57:01.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 09:57:06.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:28.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-765" for this suite. 03/16/23 09:57:28.756
------------------------------
• [54.068 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:56:34.779
    Mar 16 09:56:34.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 09:56:34.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:56:35.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:56:35.232
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/16/23 09:56:35.411
    Mar 16 09:56:35.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/16/23 09:57:01.254
    Mar 16 09:57:01.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 09:57:06.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:28.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-765" for this suite. 03/16/23 09:57:28.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:28.848
Mar 16 09:57:28.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 09:57:28.849
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:29.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:29.298
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/16/23 09:57:29.477
Mar 16 09:57:29.575: INFO: Waiting up to 5m0s for pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd" in namespace "emptydir-6160" to be "Succeeded or Failed"
Mar 16 09:57:29.665: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 90.091559ms
Mar 16 09:57:31.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181642119s
Mar 16 09:57:33.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181615461s
STEP: Saw pod success 03/16/23 09:57:33.756
Mar 16 09:57:33.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd" satisfied condition "Succeeded or Failed"
Mar 16 09:57:33.846: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd container test-container: <nil>
STEP: delete the pod 03/16/23 09:57:33.982
Mar 16 09:57:34.077: INFO: Waiting for pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd to disappear
Mar 16 09:57:34.167: INFO: Pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:34.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6160" for this suite. 03/16/23 09:57:34.346
------------------------------
• [5.588 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:28.848
    Mar 16 09:57:28.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 09:57:28.849
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:29.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:29.298
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/16/23 09:57:29.477
    Mar 16 09:57:29.575: INFO: Waiting up to 5m0s for pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd" in namespace "emptydir-6160" to be "Succeeded or Failed"
    Mar 16 09:57:29.665: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 90.091559ms
    Mar 16 09:57:31.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181642119s
    Mar 16 09:57:33.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181615461s
    STEP: Saw pod success 03/16/23 09:57:33.756
    Mar 16 09:57:33.756: INFO: Pod "pod-7ae1919b-a03d-40f5-9c19-719d32783ffd" satisfied condition "Succeeded or Failed"
    Mar 16 09:57:33.846: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd container test-container: <nil>
    STEP: delete the pod 03/16/23 09:57:33.982
    Mar 16 09:57:34.077: INFO: Waiting for pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd to disappear
    Mar 16 09:57:34.167: INFO: Pod pod-7ae1919b-a03d-40f5-9c19-719d32783ffd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:34.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6160" for this suite. 03/16/23 09:57:34.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:34.437
Mar 16 09:57:34.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 03/16/23 09:57:34.438
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:34.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:34.887
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/16/23 09:57:35.066
STEP: modifying the configmap once 03/16/23 09:57:35.157
STEP: modifying the configmap a second time 03/16/23 09:57:35.338
STEP: deleting the configmap 03/16/23 09:57:35.518
STEP: creating a watch on configmaps from the resource version returned by the first update 03/16/23 09:57:35.609
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/16/23 09:57:35.698
Mar 16 09:57:35.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2443  52d2a940-2ec2-491a-a966-389a7735d433 13297 0 2023-03-16 09:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-16 09:57:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 09:57:35.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2443  52d2a940-2ec2-491a-a966-389a7735d433 13298 0 2023-03-16 09:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-16 09:57:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:35.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2443" for this suite. 03/16/23 09:57:35.79
------------------------------
• [1.443 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:34.437
    Mar 16 09:57:34.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 03/16/23 09:57:34.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:34.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:34.887
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/16/23 09:57:35.066
    STEP: modifying the configmap once 03/16/23 09:57:35.157
    STEP: modifying the configmap a second time 03/16/23 09:57:35.338
    STEP: deleting the configmap 03/16/23 09:57:35.518
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/16/23 09:57:35.609
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/16/23 09:57:35.698
    Mar 16 09:57:35.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2443  52d2a940-2ec2-491a-a966-389a7735d433 13297 0 2023-03-16 09:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-16 09:57:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 09:57:35.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2443  52d2a940-2ec2-491a-a966-389a7735d433 13298 0 2023-03-16 09:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-16 09:57:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:35.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2443" for this suite. 03/16/23 09:57:35.79
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:35.881
Mar 16 09:57:35.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring 03/16/23 09:57:35.882
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:36.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:36.333
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/16/23 09:57:36.606
STEP: mirroring an update to a custom Endpoint 03/16/23 09:57:36.786
STEP: mirroring deletion of a custom Endpoint 03/16/23 09:57:36.967
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:37.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-379" for this suite. 03/16/23 09:57:37.239
------------------------------
• [1.449 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:35.881
    Mar 16 09:57:35.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslicemirroring 03/16/23 09:57:35.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:36.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:36.333
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/16/23 09:57:36.606
    STEP: mirroring an update to a custom Endpoint 03/16/23 09:57:36.786
    STEP: mirroring deletion of a custom Endpoint 03/16/23 09:57:36.967
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:37.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-379" for this suite. 03/16/23 09:57:37.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:37.331
Mar 16 09:57:37.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:57:37.332
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:37.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:37.781
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 03/16/23 09:57:37.96
Mar 16 09:57:37.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2504 api-versions'
Mar 16 09:57:38.656: INFO: stderr: ""
Mar 16 09:57:38.656: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:38.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2504" for this suite. 03/16/23 09:57:38.834
------------------------------
• [1.594 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:37.331
    Mar 16 09:57:37.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:57:37.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:37.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:37.781
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 03/16/23 09:57:37.96
    Mar 16 09:57:37.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2504 api-versions'
    Mar 16 09:57:38.656: INFO: stderr: ""
    Mar 16 09:57:38.656: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:38.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2504" for this suite. 03/16/23 09:57:38.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:38.928
Mar 16 09:57:38.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csistoragecapacity 03/16/23 09:57:38.93
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:39.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:39.379
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/16/23 09:57:39.558
STEP: getting /apis/storage.k8s.io 03/16/23 09:57:39.737
STEP: getting /apis/storage.k8s.io/v1 03/16/23 09:57:39.826
STEP: creating 03/16/23 09:57:39.915
STEP: watching 03/16/23 09:57:40.186
Mar 16 09:57:40.186: INFO: starting watch
STEP: getting 03/16/23 09:57:40.455
STEP: listing in namespace 03/16/23 09:57:40.545
STEP: listing across namespaces 03/16/23 09:57:40.636
STEP: patching 03/16/23 09:57:40.726
STEP: updating 03/16/23 09:57:40.817
Mar 16 09:57:40.907: INFO: waiting for watch events with expected annotations in namespace
Mar 16 09:57:40.907: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/16/23 09:57:40.907
STEP: deleting a collection 03/16/23 09:57:41.178
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:41.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-9542" for this suite. 03/16/23 09:57:41.452
------------------------------
• [2.615 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:38.928
    Mar 16 09:57:38.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csistoragecapacity 03/16/23 09:57:38.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:39.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:39.379
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/16/23 09:57:39.558
    STEP: getting /apis/storage.k8s.io 03/16/23 09:57:39.737
    STEP: getting /apis/storage.k8s.io/v1 03/16/23 09:57:39.826
    STEP: creating 03/16/23 09:57:39.915
    STEP: watching 03/16/23 09:57:40.186
    Mar 16 09:57:40.186: INFO: starting watch
    STEP: getting 03/16/23 09:57:40.455
    STEP: listing in namespace 03/16/23 09:57:40.545
    STEP: listing across namespaces 03/16/23 09:57:40.636
    STEP: patching 03/16/23 09:57:40.726
    STEP: updating 03/16/23 09:57:40.817
    Mar 16 09:57:40.907: INFO: waiting for watch events with expected annotations in namespace
    Mar 16 09:57:40.907: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/16/23 09:57:40.907
    STEP: deleting a collection 03/16/23 09:57:41.178
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:41.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-9542" for this suite. 03/16/23 09:57:41.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:41.544
Mar 16 09:57:41.544: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 09:57:41.545
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:41.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:41.994
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 03/16/23 09:57:42.172
STEP: Ensuring ResourceQuota status is calculated 03/16/23 09:57:42.262
STEP: Creating a ResourceQuota with not best effort scope 03/16/23 09:57:44.355
STEP: Ensuring ResourceQuota status is calculated 03/16/23 09:57:44.446
STEP: Creating a best-effort pod 03/16/23 09:57:46.537
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/16/23 09:57:46.633
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/16/23 09:57:48.724
STEP: Deleting the pod 03/16/23 09:57:50.817
STEP: Ensuring resource quota status released the pod usage 03/16/23 09:57:50.908
STEP: Creating a not best-effort pod 03/16/23 09:57:53
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/16/23 09:57:53.096
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/16/23 09:57:55.187
STEP: Deleting the pod 03/16/23 09:57:57.278
STEP: Ensuring resource quota status released the pod usage 03/16/23 09:57:57.37
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 09:57:59.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-75" for this suite. 03/16/23 09:57:59.641
------------------------------
• [18.188 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:41.544
    Mar 16 09:57:41.544: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 09:57:41.545
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:57:41.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:57:41.994
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 03/16/23 09:57:42.172
    STEP: Ensuring ResourceQuota status is calculated 03/16/23 09:57:42.262
    STEP: Creating a ResourceQuota with not best effort scope 03/16/23 09:57:44.355
    STEP: Ensuring ResourceQuota status is calculated 03/16/23 09:57:44.446
    STEP: Creating a best-effort pod 03/16/23 09:57:46.537
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/16/23 09:57:46.633
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/16/23 09:57:48.724
    STEP: Deleting the pod 03/16/23 09:57:50.817
    STEP: Ensuring resource quota status released the pod usage 03/16/23 09:57:50.908
    STEP: Creating a not best-effort pod 03/16/23 09:57:53
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/16/23 09:57:53.096
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/16/23 09:57:55.187
    STEP: Deleting the pod 03/16/23 09:57:57.278
    STEP: Ensuring resource quota status released the pod usage 03/16/23 09:57:57.37
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:57:59.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-75" for this suite. 03/16/23 09:57:59.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:57:59.733
Mar 16 09:57:59.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 09:57:59.734
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:00.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:00.183
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8221 03/16/23 09:58:00.362
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/16/23 09:58:00.457
STEP: creating service externalsvc in namespace services-8221 03/16/23 09:58:00.458
STEP: creating replication controller externalsvc in namespace services-8221 03/16/23 09:58:00.552
I0316 09:58:00.642884    8588 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8221, replica count: 2
I0316 09:58:03.745164    8588 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/16/23 09:58:03.835
Mar 16 09:58:04.021: INFO: Creating new exec pod
Mar 16 09:58:04.117: INFO: Waiting up to 5m0s for pod "execpodnw8c5" in namespace "services-8221" to be "running"
Mar 16 09:58:04.208: INFO: Pod "execpodnw8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 90.94417ms
Mar 16 09:58:06.299: INFO: Pod "execpodnw8c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.181983246s
Mar 16 09:58:06.299: INFO: Pod "execpodnw8c5" satisfied condition "running"
Mar 16 09:58:06.299: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8221 exec execpodnw8c5 -- /bin/sh -x -c nslookup nodeport-service.services-8221.svc.cluster.local'
Mar 16 09:58:07.419: INFO: stderr: "+ nslookup nodeport-service.services-8221.svc.cluster.local\n"
Mar 16 09:58:07.419: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-8221.svc.cluster.local\tcanonical name = externalsvc.services-8221.svc.cluster.local.\nName:\texternalsvc.services-8221.svc.cluster.local\nAddress: 100.110.209.19\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8221, will wait for the garbage collector to delete the pods 03/16/23 09:58:07.42
Mar 16 09:58:07.702: INFO: Deleting ReplicationController externalsvc took: 90.905811ms
Mar 16 09:58:07.803: INFO: Terminating ReplicationController externalsvc pods took: 101.095074ms
Mar 16 09:58:09.597: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8221" for this suite. 03/16/23 09:58:09.868
------------------------------
• [10.226 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:57:59.733
    Mar 16 09:57:59.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 09:57:59.734
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:00.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:00.183
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8221 03/16/23 09:58:00.362
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/16/23 09:58:00.457
    STEP: creating service externalsvc in namespace services-8221 03/16/23 09:58:00.458
    STEP: creating replication controller externalsvc in namespace services-8221 03/16/23 09:58:00.552
    I0316 09:58:00.642884    8588 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8221, replica count: 2
    I0316 09:58:03.745164    8588 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/16/23 09:58:03.835
    Mar 16 09:58:04.021: INFO: Creating new exec pod
    Mar 16 09:58:04.117: INFO: Waiting up to 5m0s for pod "execpodnw8c5" in namespace "services-8221" to be "running"
    Mar 16 09:58:04.208: INFO: Pod "execpodnw8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 90.94417ms
    Mar 16 09:58:06.299: INFO: Pod "execpodnw8c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.181983246s
    Mar 16 09:58:06.299: INFO: Pod "execpodnw8c5" satisfied condition "running"
    Mar 16 09:58:06.299: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8221 exec execpodnw8c5 -- /bin/sh -x -c nslookup nodeport-service.services-8221.svc.cluster.local'
    Mar 16 09:58:07.419: INFO: stderr: "+ nslookup nodeport-service.services-8221.svc.cluster.local\n"
    Mar 16 09:58:07.419: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-8221.svc.cluster.local\tcanonical name = externalsvc.services-8221.svc.cluster.local.\nName:\texternalsvc.services-8221.svc.cluster.local\nAddress: 100.110.209.19\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8221, will wait for the garbage collector to delete the pods 03/16/23 09:58:07.42
    Mar 16 09:58:07.702: INFO: Deleting ReplicationController externalsvc took: 90.905811ms
    Mar 16 09:58:07.803: INFO: Terminating ReplicationController externalsvc pods took: 101.095074ms
    Mar 16 09:58:09.597: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8221" for this suite. 03/16/23 09:58:09.868
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:09.96
Mar 16 09:58:09.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop 03/16/23 09:58:09.961
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:10.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:10.41
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9194 03/16/23 09:58:10.589
STEP: Waiting for pods to come up. 03/16/23 09:58:10.685
Mar 16 09:58:10.685: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9194" to be "running"
Mar 16 09:58:10.775: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 89.808306ms
Mar 16 09:58:12.865: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.180427269s
Mar 16 09:58:12.865: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9194 03/16/23 09:58:12.955
Mar 16 09:58:13.049: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9194" to be "running"
Mar 16 09:58:13.138: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 89.557139ms
Mar 16 09:58:15.232: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.182868439s
Mar 16 09:58:15.232: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/16/23 09:58:15.232
Mar 16 09:58:20.515: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/16/23 09:58:20.515
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9194" for this suite. 03/16/23 09:58:20.786
------------------------------
• [10.917 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:09.96
    Mar 16 09:58:09.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename prestop 03/16/23 09:58:09.961
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:10.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:10.41
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9194 03/16/23 09:58:10.589
    STEP: Waiting for pods to come up. 03/16/23 09:58:10.685
    Mar 16 09:58:10.685: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9194" to be "running"
    Mar 16 09:58:10.775: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 89.808306ms
    Mar 16 09:58:12.865: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.180427269s
    Mar 16 09:58:12.865: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9194 03/16/23 09:58:12.955
    Mar 16 09:58:13.049: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9194" to be "running"
    Mar 16 09:58:13.138: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 89.557139ms
    Mar 16 09:58:15.232: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.182868439s
    Mar 16 09:58:15.232: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/16/23 09:58:15.232
    Mar 16 09:58:20.515: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/16/23 09:58:20.515
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9194" for this suite. 03/16/23 09:58:20.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:20.878
Mar 16 09:58:20.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 09:58:20.879
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:21.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:21.327
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-f599efa5-255d-4a96-b820-e472869a7f5b 03/16/23 09:58:21.506
STEP: Creating a pod to test consume secrets 03/16/23 09:58:21.596
Mar 16 09:58:21.691: INFO: Waiting up to 5m0s for pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee" in namespace "secrets-8977" to be "Succeeded or Failed"
Mar 16 09:58:21.781: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Pending", Reason="", readiness=false. Elapsed: 89.918664ms
Mar 16 09:58:23.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181257872s
Mar 16 09:58:25.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181132246s
STEP: Saw pod success 03/16/23 09:58:25.872
Mar 16 09:58:25.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee" satisfied condition "Succeeded or Failed"
Mar 16 09:58:25.963: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee container secret-env-test: <nil>
STEP: delete the pod 03/16/23 09:58:26.058
Mar 16 09:58:26.152: INFO: Waiting for pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee to disappear
Mar 16 09:58:26.241: INFO: Pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:26.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8977" for this suite. 03/16/23 09:58:26.42
------------------------------
• [5.633 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:20.878
    Mar 16 09:58:20.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 09:58:20.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:21.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:21.327
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-f599efa5-255d-4a96-b820-e472869a7f5b 03/16/23 09:58:21.506
    STEP: Creating a pod to test consume secrets 03/16/23 09:58:21.596
    Mar 16 09:58:21.691: INFO: Waiting up to 5m0s for pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee" in namespace "secrets-8977" to be "Succeeded or Failed"
    Mar 16 09:58:21.781: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Pending", Reason="", readiness=false. Elapsed: 89.918664ms
    Mar 16 09:58:23.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181257872s
    Mar 16 09:58:25.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181132246s
    STEP: Saw pod success 03/16/23 09:58:25.872
    Mar 16 09:58:25.872: INFO: Pod "pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee" satisfied condition "Succeeded or Failed"
    Mar 16 09:58:25.963: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee container secret-env-test: <nil>
    STEP: delete the pod 03/16/23 09:58:26.058
    Mar 16 09:58:26.152: INFO: Waiting for pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee to disappear
    Mar 16 09:58:26.241: INFO: Pod pod-secrets-dee7e111-1b8a-43e8-9db7-935e4b8414ee no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:26.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8977" for this suite. 03/16/23 09:58:26.42
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:26.511
Mar 16 09:58:26.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 09:58:26.513
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:26.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:26.961
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-55deca77-fde5-4a1b-9c9d-b4699d9dc35a 03/16/23 09:58:27.231
STEP: Creating the pod 03/16/23 09:58:27.321
Mar 16 09:58:27.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447" in namespace "configmap-4655" to be "running and ready"
Mar 16 09:58:27.506: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447": Phase="Pending", Reason="", readiness=false. Elapsed: 89.892647ms
Mar 16 09:58:27.506: INFO: The phase of Pod pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:58:29.597: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447": Phase="Running", Reason="", readiness=true. Elapsed: 2.180989441s
Mar 16 09:58:29.597: INFO: The phase of Pod pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447 is Running (Ready = true)
Mar 16 09:58:29.597: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-55deca77-fde5-4a1b-9c9d-b4699d9dc35a 03/16/23 09:58:29.787
STEP: waiting to observe update in volume 03/16/23 09:58:29.877
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:34.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4655" for this suite. 03/16/23 09:58:34.342
------------------------------
• [7.921 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:26.511
    Mar 16 09:58:26.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 09:58:26.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:26.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:26.961
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-55deca77-fde5-4a1b-9c9d-b4699d9dc35a 03/16/23 09:58:27.231
    STEP: Creating the pod 03/16/23 09:58:27.321
    Mar 16 09:58:27.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447" in namespace "configmap-4655" to be "running and ready"
    Mar 16 09:58:27.506: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447": Phase="Pending", Reason="", readiness=false. Elapsed: 89.892647ms
    Mar 16 09:58:27.506: INFO: The phase of Pod pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:58:29.597: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447": Phase="Running", Reason="", readiness=true. Elapsed: 2.180989441s
    Mar 16 09:58:29.597: INFO: The phase of Pod pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447 is Running (Ready = true)
    Mar 16 09:58:29.597: INFO: Pod "pod-configmaps-abfbeec1-bb6f-4847-8028-78177c9cd447" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-55deca77-fde5-4a1b-9c9d-b4699d9dc35a 03/16/23 09:58:29.787
    STEP: waiting to observe update in volume 03/16/23 09:58:29.877
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:34.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4655" for this suite. 03/16/23 09:58:34.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:34.434
Mar 16 09:58:34.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery 03/16/23 09:58:34.435
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:34.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:34.884
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/16/23 09:58:35.151
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 16 09:58:35.726: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 16 09:58:35.815: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 16 09:58:35.815: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 16 09:58:35.815: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 16 09:58:35.815: INFO: Checking APIGroup: apps
Mar 16 09:58:35.904: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 16 09:58:35.904: INFO: Versions found [{apps/v1 v1}]
Mar 16 09:58:35.904: INFO: apps/v1 matches apps/v1
Mar 16 09:58:35.904: INFO: Checking APIGroup: events.k8s.io
Mar 16 09:58:35.993: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 16 09:58:35.993: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 16 09:58:35.993: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 16 09:58:35.993: INFO: Checking APIGroup: authentication.k8s.io
Mar 16 09:58:36.082: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 16 09:58:36.082: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 16 09:58:36.082: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 16 09:58:36.082: INFO: Checking APIGroup: authorization.k8s.io
Mar 16 09:58:36.171: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 16 09:58:36.171: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 16 09:58:36.171: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 16 09:58:36.171: INFO: Checking APIGroup: autoscaling
Mar 16 09:58:36.260: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 16 09:58:36.260: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Mar 16 09:58:36.260: INFO: autoscaling/v2 matches autoscaling/v2
Mar 16 09:58:36.260: INFO: Checking APIGroup: batch
Mar 16 09:58:36.349: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 16 09:58:36.349: INFO: Versions found [{batch/v1 v1}]
Mar 16 09:58:36.349: INFO: batch/v1 matches batch/v1
Mar 16 09:58:36.349: INFO: Checking APIGroup: certificates.k8s.io
Mar 16 09:58:36.438: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 16 09:58:36.438: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 16 09:58:36.438: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 16 09:58:36.438: INFO: Checking APIGroup: networking.k8s.io
Mar 16 09:58:36.527: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 16 09:58:36.527: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 16 09:58:36.527: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 16 09:58:36.527: INFO: Checking APIGroup: policy
Mar 16 09:58:36.616: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 16 09:58:36.616: INFO: Versions found [{policy/v1 v1}]
Mar 16 09:58:36.616: INFO: policy/v1 matches policy/v1
Mar 16 09:58:36.616: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 16 09:58:36.705: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 16 09:58:36.705: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 16 09:58:36.705: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 16 09:58:36.705: INFO: Checking APIGroup: storage.k8s.io
Mar 16 09:58:36.794: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 16 09:58:36.794: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 16 09:58:36.794: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 16 09:58:36.794: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 16 09:58:36.883: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 16 09:58:36.883: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 16 09:58:36.883: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 16 09:58:36.883: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 16 09:58:36.972: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 16 09:58:36.972: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 16 09:58:36.972: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 16 09:58:36.972: INFO: Checking APIGroup: scheduling.k8s.io
Mar 16 09:58:37.061: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 16 09:58:37.061: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 16 09:58:37.061: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 16 09:58:37.061: INFO: Checking APIGroup: coordination.k8s.io
Mar 16 09:58:37.150: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 16 09:58:37.150: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 16 09:58:37.150: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 16 09:58:37.150: INFO: Checking APIGroup: node.k8s.io
Mar 16 09:58:37.239: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 16 09:58:37.239: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 16 09:58:37.239: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 16 09:58:37.239: INFO: Checking APIGroup: discovery.k8s.io
Mar 16 09:58:37.328: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 16 09:58:37.328: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 16 09:58:37.328: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 16 09:58:37.328: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 16 09:58:37.417: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Mar 16 09:58:37.417: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Mar 16 09:58:37.417: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Mar 16 09:58:37.417: INFO: Checking APIGroup: autoscaling.k8s.io
Mar 16 09:58:37.506: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Mar 16 09:58:37.506: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Mar 16 09:58:37.506: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Mar 16 09:58:37.506: INFO: Checking APIGroup: crd.projectcalico.org
Mar 16 09:58:37.595: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 16 09:58:37.595: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 16 09:58:37.595: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar 16 09:58:37.595: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 16 09:58:37.684: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar 16 09:58:37.684: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Mar 16 09:58:37.684: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar 16 09:58:37.684: INFO: Checking APIGroup: cert.gardener.cloud
Mar 16 09:58:37.773: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Mar 16 09:58:37.773: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Mar 16 09:58:37.773: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Mar 16 09:58:37.773: INFO: Checking APIGroup: dns.gardener.cloud
Mar 16 09:58:37.862: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Mar 16 09:58:37.862: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Mar 16 09:58:37.862: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Mar 16 09:58:37.862: INFO: Checking APIGroup: metrics.k8s.io
Mar 16 09:58:37.951: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 16 09:58:37.951: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 16 09:58:37.951: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:37.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1101" for this suite. 03/16/23 09:58:38.13
------------------------------
• [3.787 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:34.434
    Mar 16 09:58:34.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename discovery 03/16/23 09:58:34.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:34.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:34.884
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/16/23 09:58:35.151
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 16 09:58:35.726: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 16 09:58:35.815: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 16 09:58:35.815: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 16 09:58:35.815: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 16 09:58:35.815: INFO: Checking APIGroup: apps
    Mar 16 09:58:35.904: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 16 09:58:35.904: INFO: Versions found [{apps/v1 v1}]
    Mar 16 09:58:35.904: INFO: apps/v1 matches apps/v1
    Mar 16 09:58:35.904: INFO: Checking APIGroup: events.k8s.io
    Mar 16 09:58:35.993: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 16 09:58:35.993: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 16 09:58:35.993: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 16 09:58:35.993: INFO: Checking APIGroup: authentication.k8s.io
    Mar 16 09:58:36.082: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 16 09:58:36.082: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 16 09:58:36.082: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 16 09:58:36.082: INFO: Checking APIGroup: authorization.k8s.io
    Mar 16 09:58:36.171: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 16 09:58:36.171: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 16 09:58:36.171: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 16 09:58:36.171: INFO: Checking APIGroup: autoscaling
    Mar 16 09:58:36.260: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 16 09:58:36.260: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Mar 16 09:58:36.260: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 16 09:58:36.260: INFO: Checking APIGroup: batch
    Mar 16 09:58:36.349: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 16 09:58:36.349: INFO: Versions found [{batch/v1 v1}]
    Mar 16 09:58:36.349: INFO: batch/v1 matches batch/v1
    Mar 16 09:58:36.349: INFO: Checking APIGroup: certificates.k8s.io
    Mar 16 09:58:36.438: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 16 09:58:36.438: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 16 09:58:36.438: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 16 09:58:36.438: INFO: Checking APIGroup: networking.k8s.io
    Mar 16 09:58:36.527: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 16 09:58:36.527: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 16 09:58:36.527: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 16 09:58:36.527: INFO: Checking APIGroup: policy
    Mar 16 09:58:36.616: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 16 09:58:36.616: INFO: Versions found [{policy/v1 v1}]
    Mar 16 09:58:36.616: INFO: policy/v1 matches policy/v1
    Mar 16 09:58:36.616: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 16 09:58:36.705: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 16 09:58:36.705: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 16 09:58:36.705: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 16 09:58:36.705: INFO: Checking APIGroup: storage.k8s.io
    Mar 16 09:58:36.794: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 16 09:58:36.794: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 16 09:58:36.794: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 16 09:58:36.794: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 16 09:58:36.883: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 16 09:58:36.883: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 16 09:58:36.883: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 16 09:58:36.883: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 16 09:58:36.972: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 16 09:58:36.972: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 16 09:58:36.972: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 16 09:58:36.972: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 16 09:58:37.061: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 16 09:58:37.061: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 16 09:58:37.061: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 16 09:58:37.061: INFO: Checking APIGroup: coordination.k8s.io
    Mar 16 09:58:37.150: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 16 09:58:37.150: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 16 09:58:37.150: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 16 09:58:37.150: INFO: Checking APIGroup: node.k8s.io
    Mar 16 09:58:37.239: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 16 09:58:37.239: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 16 09:58:37.239: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 16 09:58:37.239: INFO: Checking APIGroup: discovery.k8s.io
    Mar 16 09:58:37.328: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 16 09:58:37.328: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 16 09:58:37.328: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 16 09:58:37.328: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 16 09:58:37.417: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Mar 16 09:58:37.417: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Mar 16 09:58:37.417: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Mar 16 09:58:37.417: INFO: Checking APIGroup: autoscaling.k8s.io
    Mar 16 09:58:37.506: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
    Mar 16 09:58:37.506: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
    Mar 16 09:58:37.506: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
    Mar 16 09:58:37.506: INFO: Checking APIGroup: crd.projectcalico.org
    Mar 16 09:58:37.595: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar 16 09:58:37.595: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar 16 09:58:37.595: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar 16 09:58:37.595: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar 16 09:58:37.684: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar 16 09:58:37.684: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Mar 16 09:58:37.684: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar 16 09:58:37.684: INFO: Checking APIGroup: cert.gardener.cloud
    Mar 16 09:58:37.773: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
    Mar 16 09:58:37.773: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
    Mar 16 09:58:37.773: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
    Mar 16 09:58:37.773: INFO: Checking APIGroup: dns.gardener.cloud
    Mar 16 09:58:37.862: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
    Mar 16 09:58:37.862: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
    Mar 16 09:58:37.862: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
    Mar 16 09:58:37.862: INFO: Checking APIGroup: metrics.k8s.io
    Mar 16 09:58:37.951: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 16 09:58:37.951: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 16 09:58:37.951: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:37.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1101" for this suite. 03/16/23 09:58:38.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:38.223
Mar 16 09:58:38.223: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test 03/16/23 09:58:38.224
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:38.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:38.672
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-4968" for this suite. 03/16/23 09:58:40.116
------------------------------
• [1.984 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:38.223
    Mar 16 09:58:38.223: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename lease-test 03/16/23 09:58:38.224
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:38.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:38.672
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-4968" for this suite. 03/16/23 09:58:40.116
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:40.207
Mar 16 09:58:40.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 09:58:40.208
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:40.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:40.657
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 03/16/23 09:58:40.835
Mar 16 09:58:40.931: INFO: Waiting up to 5m0s for pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71" in namespace "emptydir-6387" to be "Succeeded or Failed"
Mar 16 09:58:41.021: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Pending", Reason="", readiness=false. Elapsed: 90.031945ms
Mar 16 09:58:43.114: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183040482s
Mar 16 09:58:45.111: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180305378s
STEP: Saw pod success 03/16/23 09:58:45.111
Mar 16 09:58:45.111: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71" satisfied condition "Succeeded or Failed"
Mar 16 09:58:45.205: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 container test-container: <nil>
STEP: delete the pod 03/16/23 09:58:45.299
Mar 16 09:58:45.394: INFO: Waiting for pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 to disappear
Mar 16 09:58:45.484: INFO: Pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6387" for this suite. 03/16/23 09:58:45.662
------------------------------
• [5.546 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:40.207
    Mar 16 09:58:40.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 09:58:40.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:40.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:40.657
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/16/23 09:58:40.835
    Mar 16 09:58:40.931: INFO: Waiting up to 5m0s for pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71" in namespace "emptydir-6387" to be "Succeeded or Failed"
    Mar 16 09:58:41.021: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Pending", Reason="", readiness=false. Elapsed: 90.031945ms
    Mar 16 09:58:43.114: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183040482s
    Mar 16 09:58:45.111: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180305378s
    STEP: Saw pod success 03/16/23 09:58:45.111
    Mar 16 09:58:45.111: INFO: Pod "pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71" satisfied condition "Succeeded or Failed"
    Mar 16 09:58:45.205: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 container test-container: <nil>
    STEP: delete the pod 03/16/23 09:58:45.299
    Mar 16 09:58:45.394: INFO: Waiting for pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 to disappear
    Mar 16 09:58:45.484: INFO: Pod pod-94fc70ec-56d3-4ba3-94ac-d6ac8b308c71 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6387" for this suite. 03/16/23 09:58:45.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:45.754
Mar 16 09:58:45.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 09:58:45.755
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:46.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:46.204
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 16 09:58:46.653: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 09:58:46.653
Mar 16 09:58:46.653: INFO: Waiting up to 5m0s for pod "test-rs-nn74v" in namespace "replicaset-4480" to be "running"
Mar 16 09:58:46.743: INFO: Pod "test-rs-nn74v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.906466ms
Mar 16 09:58:48.835: INFO: Pod "test-rs-nn74v": Phase="Running", Reason="", readiness=true. Elapsed: 2.181192741s
Mar 16 09:58:48.835: INFO: Pod "test-rs-nn74v" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  03/16/23 09:58:48.835
Mar 16 09:58:49.016: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/16/23 09:58:49.016
W0316 09:58:49.109562    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
Mar 16 09:58:50.147: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 2, AvailableReplicas 2
Mar 16 09:58:50.276: INFO: observed Replicaset test-rs in namespace replicaset-4480 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4480" for this suite. 03/16/23 09:58:50.455
------------------------------
• [4.792 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:45.754
    Mar 16 09:58:45.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 09:58:45.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:46.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:46.204
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 16 09:58:46.653: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 09:58:46.653
    Mar 16 09:58:46.653: INFO: Waiting up to 5m0s for pod "test-rs-nn74v" in namespace "replicaset-4480" to be "running"
    Mar 16 09:58:46.743: INFO: Pod "test-rs-nn74v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.906466ms
    Mar 16 09:58:48.835: INFO: Pod "test-rs-nn74v": Phase="Running", Reason="", readiness=true. Elapsed: 2.181192741s
    Mar 16 09:58:48.835: INFO: Pod "test-rs-nn74v" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  03/16/23 09:58:48.835
    Mar 16 09:58:49.016: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/16/23 09:58:49.016
    W0316 09:58:49.109562    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
    Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
    Mar 16 09:58:49.198: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 1, AvailableReplicas 1
    Mar 16 09:58:50.147: INFO: observed ReplicaSet test-rs in namespace replicaset-4480 with ReadyReplicas 2, AvailableReplicas 2
    Mar 16 09:58:50.276: INFO: observed Replicaset test-rs in namespace replicaset-4480 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4480" for this suite. 03/16/23 09:58:50.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:50.546
Mar 16 09:58:50.547: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 09:58:50.548
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:50.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:50.997
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-3f75e22b-4241-4481-907e-74cf2b478508 03/16/23 09:58:51.175
STEP: Creating a pod to test consume configMaps 03/16/23 09:58:51.269
Mar 16 09:58:51.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33" in namespace "projected-4779" to be "Succeeded or Failed"
Mar 16 09:58:51.456: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Pending", Reason="", readiness=false. Elapsed: 92.673422ms
Mar 16 09:58:53.547: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183542708s
Mar 16 09:58:55.548: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.184406792s
STEP: Saw pod success 03/16/23 09:58:55.548
Mar 16 09:58:55.548: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33" satisfied condition "Succeeded or Failed"
Mar 16 09:58:55.638: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/16/23 09:58:55.738
Mar 16 09:58:55.832: INFO: Waiting for pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 to disappear
Mar 16 09:58:55.922: INFO: Pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 09:58:55.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4779" for this suite. 03/16/23 09:58:56.102
------------------------------
• [5.646 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:50.546
    Mar 16 09:58:50.547: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 09:58:50.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:50.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:50.997
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-3f75e22b-4241-4481-907e-74cf2b478508 03/16/23 09:58:51.175
    STEP: Creating a pod to test consume configMaps 03/16/23 09:58:51.269
    Mar 16 09:58:51.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33" in namespace "projected-4779" to be "Succeeded or Failed"
    Mar 16 09:58:51.456: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Pending", Reason="", readiness=false. Elapsed: 92.673422ms
    Mar 16 09:58:53.547: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183542708s
    Mar 16 09:58:55.548: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.184406792s
    STEP: Saw pod success 03/16/23 09:58:55.548
    Mar 16 09:58:55.548: INFO: Pod "pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33" satisfied condition "Succeeded or Failed"
    Mar 16 09:58:55.638: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/16/23 09:58:55.738
    Mar 16 09:58:55.832: INFO: Waiting for pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 to disappear
    Mar 16 09:58:55.922: INFO: Pod pod-projected-configmaps-c67dbe9d-a91d-417b-89bc-dd94a25a6e33 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:58:55.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4779" for this suite. 03/16/23 09:58:56.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:58:56.194
Mar 16 09:58:56.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 09:58:56.195
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:56.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:56.645
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Mar 16 09:58:56.918: INFO: Waiting up to 5m0s for pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c" in namespace "container-probe-407" to be "running and ready"
Mar 16 09:58:57.008: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.065675ms
Mar 16 09:58:57.008: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Pending, waiting for it to be Running (with Ready = true)
Mar 16 09:58:59.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 2.18094144s
Mar 16 09:58:59.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:01.100: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 4.181860129s
Mar 16 09:59:01.100: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:03.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 6.180390769s
Mar 16 09:59:03.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:05.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 8.180467818s
Mar 16 09:59:05.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:07.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 10.180477158s
Mar 16 09:59:07.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:09.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 12.180578726s
Mar 16 09:59:09.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:11.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 14.180363828s
Mar 16 09:59:11.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:13.098: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 16.180126645s
Mar 16 09:59:13.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:15.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 18.180453689s
Mar 16 09:59:15.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:17.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 20.180889458s
Mar 16 09:59:17.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
Mar 16 09:59:19.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=true. Elapsed: 22.180709505s
Mar 16 09:59:19.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = true)
Mar 16 09:59:19.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c" satisfied condition "running and ready"
Mar 16 09:59:19.189: INFO: Container started at 2023-03-16 09:58:57 +0000 UTC, pod became ready at 2023-03-16 09:59:17 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 09:59:19.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-407" for this suite. 03/16/23 09:59:19.368
------------------------------
• [23.265 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:58:56.194
    Mar 16 09:58:56.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 09:58:56.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:58:56.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:58:56.645
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Mar 16 09:58:56.918: INFO: Waiting up to 5m0s for pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c" in namespace "container-probe-407" to be "running and ready"
    Mar 16 09:58:57.008: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.065675ms
    Mar 16 09:58:57.008: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 09:58:59.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 2.18094144s
    Mar 16 09:58:59.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:01.100: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 4.181860129s
    Mar 16 09:59:01.100: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:03.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 6.180390769s
    Mar 16 09:59:03.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:05.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 8.180467818s
    Mar 16 09:59:05.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:07.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 10.180477158s
    Mar 16 09:59:07.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:09.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 12.180578726s
    Mar 16 09:59:09.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:11.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 14.180363828s
    Mar 16 09:59:11.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:13.098: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 16.180126645s
    Mar 16 09:59:13.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:15.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 18.180453689s
    Mar 16 09:59:15.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:17.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=false. Elapsed: 20.180889458s
    Mar 16 09:59:17.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = false)
    Mar 16 09:59:19.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c": Phase="Running", Reason="", readiness=true. Elapsed: 22.180709505s
    Mar 16 09:59:19.099: INFO: The phase of Pod test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c is Running (Ready = true)
    Mar 16 09:59:19.099: INFO: Pod "test-webserver-37786e5d-fdaa-45de-98fd-e4f9d22f2f6c" satisfied condition "running and ready"
    Mar 16 09:59:19.189: INFO: Container started at 2023-03-16 09:58:57 +0000 UTC, pod became ready at 2023-03-16 09:59:17 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:59:19.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-407" for this suite. 03/16/23 09:59:19.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:59:19.459
Mar 16 09:59:19.460: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 09:59:19.461
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:19.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:19.909
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 16 09:59:20.088: INFO: Creating deployment "webserver-deployment"
Mar 16 09:59:20.178: INFO: Waiting for observed generation 1
Mar 16 09:59:20.268: INFO: Waiting for all required pods to come up
Mar 16 09:59:20.359: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/16/23 09:59:20.359
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z9dvz" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sc2wl" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qkjhz" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-97r4q" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l2fbv" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t92w4" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q24k6" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5gdsg" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xlmsz" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vrfhs" in namespace "deployment-6840" to be "running"
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q": Phase="Pending", Reason="", readiness=false. Elapsed: 90.201243ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz": Phase="Pending", Reason="", readiness=false. Elapsed: 90.465656ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz": Phase="Pending", Reason="", readiness=false. Elapsed: 90.270211ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg": Phase="Pending", Reason="", readiness=false. Elapsed: 90.188943ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv": Phase="Pending", Reason="", readiness=false. Elapsed: 90.146714ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4": Phase="Pending", Reason="", readiness=false. Elapsed: 90.123437ms
Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl": Phase="Pending", Reason="", readiness=false. Elapsed: 90.250287ms
Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz": Phase="Pending", Reason="", readiness=false. Elapsed: 176.68871ms
Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs": Phase="Pending", Reason="", readiness=false. Elapsed: 176.649278ms
Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6": Phase="Pending", Reason="", readiness=false. Elapsed: 176.761993ms
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4": Phase="Running", Reason="", readiness=true. Elapsed: 2.180948765s
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4" satisfied condition "running"
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.181075783s
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl" satisfied condition "running"
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.18102188s
Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg": Phase="Running", Reason="", readiness=true. Elapsed: 2.26866625s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268996323s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268801732s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268993686s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.268899934s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6": Phase="Running", Reason="", readiness=true. Elapsed: 2.268866642s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.268835117s
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6" satisfied condition "running"
Mar 16 09:59:22.628: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 16 09:59:22.808: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 16 09:59:22.989: INFO: Updating deployment webserver-deployment
Mar 16 09:59:22.989: INFO: Waiting for observed generation 2
Mar 16 09:59:23.079: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 16 09:59:23.169: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 16 09:59:23.259: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 16 09:59:23.528: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 16 09:59:23.528: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 16 09:59:23.618: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 16 09:59:23.799: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 16 09:59:23.799: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 16 09:59:23.980: INFO: Updating deployment webserver-deployment
Mar 16 09:59:23.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 16 09:59:24.162: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 16 09:59:24.251: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 09:59:24.431: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6840  756a3c91-28d7-4c62-97d4-e84b8bd3c718 14463 3 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005027258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 09:59:23 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-16 09:59:24 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 16 09:59:24.521: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6840  d75efc25-28c3-4981-9a39-992204e0b277 14462 3 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 756a3c91-28d7-4c62-97d4-e84b8bd3c718 0xc001afd5c7 0xc001afd5c8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"756a3c91-28d7-4c62-97d4-e84b8bd3c718\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001afd668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 09:59:24.521: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 16 09:59:24.521: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6840  00229663-bec9-4a36-90d4-bcc41c1ea246 14458 3 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 756a3c91-28d7-4c62-97d4-e84b8bd3c718 0xc001afd4d7 0xc001afd4d8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"756a3c91-28d7-4c62-97d4-e84b8bd3c718\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001afd568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 16 09:59:24.703: INFO: Pod "webserver-deployment-7f5969cbc7-48rzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-48rzj webserver-deployment-7f5969cbc7- deployment-6840  1e2b2e6f-eeb9-4730-be67-c84f2d3ee71a 14454 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdb57 0xc001afdb58}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktmfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktmfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.703: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gdsg webserver-deployment-7f5969cbc7- deployment-6840  aa319b4d-3dc3-4df9-8898-ca91d5e01faa 14333 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ca56588ac4aa9f042c40877b8e62a4a173224c74e62500f2f036bd2c003e50cd cni.projectcalico.org/podIP:100.64.0.83/32 cni.projectcalico.org/podIPs:100.64.0.83/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdd37 0xc001afdd38}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh9n8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh9n8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.83,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c457125fa737b81dd6d815065b55acef339092a1b20e605f02d48a201580269,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-5gr8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gr8s webserver-deployment-7f5969cbc7- deployment-6840  db31fff6-3ddd-4298-8253-726ee93dc354 14452 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdf20 0xc001afdf21}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz9zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz9zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-6mpng" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6mpng webserver-deployment-7f5969cbc7- deployment-6840  d63c4aa0-206e-4111-b427-7d138a6eae8e 14439 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc0d7 0xc0052fc0d8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9zl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9zl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-97r4q webserver-deployment-7f5969cbc7- deployment-6840  d9be3498-875a-4e1c-a0ba-d45aef5372c8 14314 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db0013c579656a19006e942787f6439956f003e457233cb1bc4b53f8a11a7f5b cni.projectcalico.org/podIP:100.64.1.143/32 cni.projectcalico.org/podIPs:100.64.1.143/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc2b7 0xc0052fc2b8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6bmts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6bmts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.143,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9e56d1dc17cd37501faa051f89d7a6fbcd4bc9708b4c56a714cf8de66d199eb2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-cbvqb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbvqb webserver-deployment-7f5969cbc7- deployment-6840  a84afd2a-29f9-4e2c-a5d0-2d5f713130b3 14453 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc4a7 0xc0052fc4a8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxpgm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxpgm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-hmbv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hmbv7 webserver-deployment-7f5969cbc7- deployment-6840  0825d9f5-acb2-4acd-821b-85c79b39e5e1 14449 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc667 0xc0052fc668}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p2z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p2z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-khqh7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-khqh7 webserver-deployment-7f5969cbc7- deployment-6840  78767a79-98b2-4a6a-841d-a0d43d4f731a 14446 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc827 0xc0052fc828}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlk2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlk2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-mtl6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtl6v webserver-deployment-7f5969cbc7- deployment-6840  1993a9ec-ed94-46a3-a36c-bed8fddafd49 14437 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc9e7 0xc0052fc9e8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lb58x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lb58x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q24k6 webserver-deployment-7f5969cbc7- deployment-6840  5c11624f-4a72-4f9f-98c5-8f0f9ad0029d 14330 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e4b7c27bf3195ef9b6463e7e91314cfd45ab29d7f3cd390a3373169e4c28778c cni.projectcalico.org/podIP:100.64.0.82/32 cni.projectcalico.org/podIPs:100.64.0.82/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcbc7 0xc0052fcbc8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnbxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnbxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.82,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0b9b3580939db8516840a692fe2bffdcea8445420a6663a73e700b80c8424c59,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-qjzhp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qjzhp webserver-deployment-7f5969cbc7- deployment-6840  e62a5e61-f24f-4d3d-984f-be537a96cc94 14451 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcdb0 0xc0052fcdb1}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nl7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nl7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qkjhz webserver-deployment-7f5969cbc7- deployment-6840  a6674c2b-f5ee-44cf-b080-180f91a87094 14323 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d028954d91972856260cda0edd6129d1ad681b11ccba93102c4b46a3e7feb6fd cni.projectcalico.org/podIP:100.64.1.145/32 cni.projectcalico.org/podIPs:100.64.1.145/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcf87 0xc0052fcf88}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgczd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgczd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.145,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://17ee18018e3dfa17a9e8078f0b6064c59d4761866a4e13be9c7f3e79f8ad4b20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sc2wl webserver-deployment-7f5969cbc7- deployment-6840  39599ad9-82fb-46fc-a092-c3d4570a0d7e 14317 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b711c21cf96a105c303fbd82c91cffe59feba3656463f2beb55e00738c0a3dc6 cni.projectcalico.org/podIP:100.64.1.141/32 cni.projectcalico.org/podIPs:100.64.1.141/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd197 0xc0052fd198}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wwkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wwkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.141,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8cebad65276223a8619ec61439593088661ac03772dbbddc129d4b8d79771253,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t92w4 webserver-deployment-7f5969cbc7- deployment-6840  55931a24-2de7-42b9-b3fa-fb03f1c5fe85 14320 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:321ee34cb740698e28c1df0f3d1f7685a5afd3549a434c93f259249b2cfdff43 cni.projectcalico.org/podIP:100.64.1.142/32 cni.projectcalico.org/podIPs:100.64.1.142/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd3a7 0xc0052fd3a8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b4bm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b4bm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.142,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://09361faf6bc5a54e00dfef3b7775805767309e42694862105b256c7f58188517,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-tstqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tstqz webserver-deployment-7f5969cbc7- deployment-6840  2920cffa-7c5d-4591-a285-983999ff0c7f 14448 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd597 0xc0052fd598}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptwj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptwj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vrfhs webserver-deployment-7f5969cbc7- deployment-6840  9f65bab1-850d-4eaa-b973-5fd6c005fe6a 14304 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ffde90a16444dca966ea27d99e85b5efb805de605adc92700cb778279af37d8f cni.projectcalico.org/podIP:100.64.0.81/32 cni.projectcalico.org/podIPs:100.64.0.81/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd777 0xc0052fd778}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75g9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75g9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.81,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://73acdf6cc8edc45144a0c49a081b6c8cac2c4609e7a0836183b7a03b1fd2fd02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-vvq2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vvq2r webserver-deployment-7f5969cbc7- deployment-6840  5a0bcdd9-15c3-4d44-94ed-103c7c3da844 14467 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:eb76b0809008b21a5c91efc11c5cf4aa5e31ba5240d9a8d8052f78a9f202cdbb cni.projectcalico.org/podIP:100.64.0.88/32 cni.projectcalico.org/podIPs:100.64.0.88/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd980 0xc0052fd981}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5nrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5nrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-wsn98" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wsn98 webserver-deployment-7f5969cbc7- deployment-6840  9a2fb637-0c92-4eaa-855d-0055ed1c9d7b 14450 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdb57 0xc0052fdb58}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2mql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2mql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z9dvz webserver-deployment-7f5969cbc7- deployment-6840  5d30fc71-188d-4650-a3f8-373a69f9f78a 14336 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:84a113ccb8ccddff309ae540bb0a3ce34ce4db673dcf2915ca5c2747b34492c7 cni.projectcalico.org/podIP:100.64.0.84/32 cni.projectcalico.org/podIPs:100.64.0.84/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdd37 0xc0052fdd38}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkggv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkggv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.84,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1cbd9e3be78597590fe1a0f81b786cb423fb09402433993030983abaf50b3080,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-zc6k5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zc6k5 webserver-deployment-7f5969cbc7- deployment-6840  3bd2661a-ef40-4899-837d-f70311cd0167 14445 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdf20 0xc0052fdf21}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vwwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vwwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-7jb8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jb8n webserver-deployment-d9f79cb5- deployment-6840  d896085d-7911-4b05-b327-bd50fc975a90 14465 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f428b6ff630724bf9d26cb9d1486e68ca4f104462f5eb53563193d1a7acaba64 cni.projectcalico.org/podIP:100.64.0.87/32 cni.projectcalico.org/podIPs:100.64.0.87/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c0f7 0xc00541c0f8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98gq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98gq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.87,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-8wtjf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8wtjf webserver-deployment-d9f79cb5- deployment-6840  c672551b-4173-41fd-babc-fa642d9d2aa0 14441 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c30f 0xc00541c320}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wp74b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wp74b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-b6p47" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6p47 webserver-deployment-d9f79cb5- deployment-6840  9aa1e4e0-2604-4f5f-9278-11e508231ab9 14461 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2bb99d7a8fe556280374c8f097fcd9c6d4dd00dbbd4efbf100be75ab27d2a3e8 cni.projectcalico.org/podIP:100.64.1.147/32 cni.projectcalico.org/podIPs:100.64.1.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c517 0xc00541c518}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvg6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvg6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.147,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-dbcw6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dbcw6 webserver-deployment-d9f79cb5- deployment-6840  78d5ff16-c4c6-45ad-9202-17a8cf5e15d3 14444 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c747 0xc00541c748}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ct65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ct65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-j6psw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j6psw webserver-deployment-d9f79cb5- deployment-6840  91bcf8d8-1086-4cec-bfbd-a09df8970a30 14442 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c927 0xc00541c928}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd968,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd968,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-knvjc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-knvjc webserver-deployment-d9f79cb5- deployment-6840  3c3f6638-2e58-46c1-b5e7-4debf770f19a 14447 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cb07 0xc00541cb08}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx6bk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx6bk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-mp5bj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mp5bj webserver-deployment-d9f79cb5- deployment-6840  b12c29c0-b814-4a21-80c7-0a1cd355edd9 14440 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cce7 0xc00541cce8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wd2mr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wd2mr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-rlwwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rlwwx webserver-deployment-d9f79cb5- deployment-6840  4fe6ee85-1db8-4acf-8646-937dfb2c8cf2 14464 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7acff7d0c880a974dba08164488b9e57603ed6686b835ad19817bd5ed135566a cni.projectcalico.org/podIP:100.64.0.86/32 cni.projectcalico.org/podIPs:100.64.0.86/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cee7 0xc00541cee8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r9cp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r9cp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.86,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-sf269" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sf269 webserver-deployment-d9f79cb5- deployment-6840  b3320ca7-fbf6-4bc5-8e91-c76a929df115 14457 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d57d66032f09842e4bbfe0c0027b0c719714256e134e5b79c431e83997324d71 cni.projectcalico.org/podIP:100.64.1.148/32 cni.projectcalico.org/podIPs:100.64.1.148/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d0ff 0xc00541d130}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggvhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggvhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.148,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-t6nz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6nz9 webserver-deployment-d9f79cb5- deployment-6840  42115053-fae4-4bc5-8092-633024915217 14460 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:71a29cd94b19e78968fcdce2ddd55350226aeafb22a8b87924d5476cafb915b5 cni.projectcalico.org/podIP:100.64.1.146/32 cni.projectcalico.org/podIPs:100.64.1.146/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d377 0xc00541d378}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnnkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnnkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.146,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-t7c4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t7c4c webserver-deployment-d9f79cb5- deployment-6840  96732012-b544-4917-b1bb-7334fcd592be 14473 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b5afffc38f70c40ce7d6dc76f517572b2f087192c424b74b2699ae958241812d cni.projectcalico.org/podIP:100.64.1.149/32 cni.projectcalico.org/podIPs:100.64.1.149/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d5c7 0xc00541d5c8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9wpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9wpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-ttgng" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ttgng webserver-deployment-d9f79cb5- deployment-6840  a399ad83-b00b-4a41-89f3-6eb48028062f 14443 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d7c7 0xc00541d7c8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5g7nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5g7nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 09:59:24.709: INFO: Pod "webserver-deployment-d9f79cb5-v5pk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v5pk2 webserver-deployment-d9f79cb5- deployment-6840  ee83ee11-f10f-402c-bcef-415a5301b5fc 14438 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d9a7 0xc00541d9a8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xv5ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xv5ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 09:59:24.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6840" for this suite. 03/16/23 09:59:24.8
------------------------------
• [5.432 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:59:19.459
    Mar 16 09:59:19.460: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 09:59:19.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:19.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:19.909
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 16 09:59:20.088: INFO: Creating deployment "webserver-deployment"
    Mar 16 09:59:20.178: INFO: Waiting for observed generation 1
    Mar 16 09:59:20.268: INFO: Waiting for all required pods to come up
    Mar 16 09:59:20.359: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/16/23 09:59:20.359
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z9dvz" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sc2wl" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qkjhz" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-97r4q" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l2fbv" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t92w4" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q24k6" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5gdsg" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xlmsz" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.359: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vrfhs" in namespace "deployment-6840" to be "running"
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q": Phase="Pending", Reason="", readiness=false. Elapsed: 90.201243ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz": Phase="Pending", Reason="", readiness=false. Elapsed: 90.465656ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz": Phase="Pending", Reason="", readiness=false. Elapsed: 90.270211ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg": Phase="Pending", Reason="", readiness=false. Elapsed: 90.188943ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv": Phase="Pending", Reason="", readiness=false. Elapsed: 90.146714ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4": Phase="Pending", Reason="", readiness=false. Elapsed: 90.123437ms
    Mar 16 09:59:20.450: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl": Phase="Pending", Reason="", readiness=false. Elapsed: 90.250287ms
    Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz": Phase="Pending", Reason="", readiness=false. Elapsed: 176.68871ms
    Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs": Phase="Pending", Reason="", readiness=false. Elapsed: 176.649278ms
    Mar 16 09:59:20.536: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6": Phase="Pending", Reason="", readiness=false. Elapsed: 176.761993ms
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4": Phase="Running", Reason="", readiness=true. Elapsed: 2.180948765s
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4" satisfied condition "running"
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.181075783s
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl" satisfied condition "running"
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.18102188s
    Mar 16 09:59:22.540: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg": Phase="Running", Reason="", readiness=true. Elapsed: 2.26866625s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268996323s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268801732s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz": Phase="Running", Reason="", readiness=true. Elapsed: 2.268993686s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.268899934s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-l2fbv" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6": Phase="Running", Reason="", readiness=true. Elapsed: 2.268866642s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.268835117s
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-xlmsz" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6" satisfied condition "running"
    Mar 16 09:59:22.628: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 16 09:59:22.808: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 16 09:59:22.989: INFO: Updating deployment webserver-deployment
    Mar 16 09:59:22.989: INFO: Waiting for observed generation 2
    Mar 16 09:59:23.079: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 16 09:59:23.169: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 16 09:59:23.259: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 16 09:59:23.528: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 16 09:59:23.528: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 16 09:59:23.618: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 16 09:59:23.799: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 16 09:59:23.799: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 16 09:59:23.980: INFO: Updating deployment webserver-deployment
    Mar 16 09:59:23.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 16 09:59:24.162: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 16 09:59:24.251: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 09:59:24.431: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6840  756a3c91-28d7-4c62-97d4-e84b8bd3c718 14463 3 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005027258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 09:59:23 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-16 09:59:24 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 16 09:59:24.521: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6840  d75efc25-28c3-4981-9a39-992204e0b277 14462 3 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 756a3c91-28d7-4c62-97d4-e84b8bd3c718 0xc001afd5c7 0xc001afd5c8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"756a3c91-28d7-4c62-97d4-e84b8bd3c718\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001afd668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 09:59:24.521: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 16 09:59:24.521: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6840  00229663-bec9-4a36-90d4-bcc41c1ea246 14458 3 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 756a3c91-28d7-4c62-97d4-e84b8bd3c718 0xc001afd4d7 0xc001afd4d8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"756a3c91-28d7-4c62-97d4-e84b8bd3c718\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001afd568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 09:59:24.703: INFO: Pod "webserver-deployment-7f5969cbc7-48rzj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-48rzj webserver-deployment-7f5969cbc7- deployment-6840  1e2b2e6f-eeb9-4730-be67-c84f2d3ee71a 14454 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdb57 0xc001afdb58}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktmfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktmfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.703: INFO: Pod "webserver-deployment-7f5969cbc7-5gdsg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gdsg webserver-deployment-7f5969cbc7- deployment-6840  aa319b4d-3dc3-4df9-8898-ca91d5e01faa 14333 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ca56588ac4aa9f042c40877b8e62a4a173224c74e62500f2f036bd2c003e50cd cni.projectcalico.org/podIP:100.64.0.83/32 cni.projectcalico.org/podIPs:100.64.0.83/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdd37 0xc001afdd38}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh9n8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh9n8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.83,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c457125fa737b81dd6d815065b55acef339092a1b20e605f02d48a201580269,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-5gr8s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gr8s webserver-deployment-7f5969cbc7- deployment-6840  db31fff6-3ddd-4298-8253-726ee93dc354 14452 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc001afdf20 0xc001afdf21}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz9zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz9zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-6mpng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6mpng webserver-deployment-7f5969cbc7- deployment-6840  d63c4aa0-206e-4111-b427-7d138a6eae8e 14439 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc0d7 0xc0052fc0d8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9zl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9zl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-97r4q" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-97r4q webserver-deployment-7f5969cbc7- deployment-6840  d9be3498-875a-4e1c-a0ba-d45aef5372c8 14314 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db0013c579656a19006e942787f6439956f003e457233cb1bc4b53f8a11a7f5b cni.projectcalico.org/podIP:100.64.1.143/32 cni.projectcalico.org/podIPs:100.64.1.143/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc2b7 0xc0052fc2b8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6bmts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6bmts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.143,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9e56d1dc17cd37501faa051f89d7a6fbcd4bc9708b4c56a714cf8de66d199eb2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-cbvqb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbvqb webserver-deployment-7f5969cbc7- deployment-6840  a84afd2a-29f9-4e2c-a5d0-2d5f713130b3 14453 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc4a7 0xc0052fc4a8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxpgm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxpgm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-hmbv7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hmbv7 webserver-deployment-7f5969cbc7- deployment-6840  0825d9f5-acb2-4acd-821b-85c79b39e5e1 14449 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc667 0xc0052fc668}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p2z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p2z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.704: INFO: Pod "webserver-deployment-7f5969cbc7-khqh7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-khqh7 webserver-deployment-7f5969cbc7- deployment-6840  78767a79-98b2-4a6a-841d-a0d43d4f731a 14446 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc827 0xc0052fc828}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlk2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlk2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-mtl6v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtl6v webserver-deployment-7f5969cbc7- deployment-6840  1993a9ec-ed94-46a3-a36c-bed8fddafd49 14437 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fc9e7 0xc0052fc9e8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lb58x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lb58x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-q24k6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q24k6 webserver-deployment-7f5969cbc7- deployment-6840  5c11624f-4a72-4f9f-98c5-8f0f9ad0029d 14330 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e4b7c27bf3195ef9b6463e7e91314cfd45ab29d7f3cd390a3373169e4c28778c cni.projectcalico.org/podIP:100.64.0.82/32 cni.projectcalico.org/podIPs:100.64.0.82/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcbc7 0xc0052fcbc8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnbxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnbxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.82,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0b9b3580939db8516840a692fe2bffdcea8445420a6663a73e700b80c8424c59,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-qjzhp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qjzhp webserver-deployment-7f5969cbc7- deployment-6840  e62a5e61-f24f-4d3d-984f-be537a96cc94 14451 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcdb0 0xc0052fcdb1}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nl7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nl7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-qkjhz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qkjhz webserver-deployment-7f5969cbc7- deployment-6840  a6674c2b-f5ee-44cf-b080-180f91a87094 14323 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d028954d91972856260cda0edd6129d1ad681b11ccba93102c4b46a3e7feb6fd cni.projectcalico.org/podIP:100.64.1.145/32 cni.projectcalico.org/podIPs:100.64.1.145/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fcf87 0xc0052fcf88}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgczd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgczd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.145,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://17ee18018e3dfa17a9e8078f0b6064c59d4761866a4e13be9c7f3e79f8ad4b20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.705: INFO: Pod "webserver-deployment-7f5969cbc7-sc2wl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sc2wl webserver-deployment-7f5969cbc7- deployment-6840  39599ad9-82fb-46fc-a092-c3d4570a0d7e 14317 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b711c21cf96a105c303fbd82c91cffe59feba3656463f2beb55e00738c0a3dc6 cni.projectcalico.org/podIP:100.64.1.141/32 cni.projectcalico.org/podIPs:100.64.1.141/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd197 0xc0052fd198}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wwkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wwkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.141,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8cebad65276223a8619ec61439593088661ac03772dbbddc129d4b8d79771253,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-t92w4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t92w4 webserver-deployment-7f5969cbc7- deployment-6840  55931a24-2de7-42b9-b3fa-fb03f1c5fe85 14320 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:321ee34cb740698e28c1df0f3d1f7685a5afd3549a434c93f259249b2cfdff43 cni.projectcalico.org/podIP:100.64.1.142/32 cni.projectcalico.org/podIPs:100.64.1.142/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd3a7 0xc0052fd3a8}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b4bm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b4bm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.142,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://09361faf6bc5a54e00dfef3b7775805767309e42694862105b256c7f58188517,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-tstqz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tstqz webserver-deployment-7f5969cbc7- deployment-6840  2920cffa-7c5d-4591-a285-983999ff0c7f 14448 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd597 0xc0052fd598}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptwj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptwj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-vrfhs" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vrfhs webserver-deployment-7f5969cbc7- deployment-6840  9f65bab1-850d-4eaa-b973-5fd6c005fe6a 14304 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ffde90a16444dca966ea27d99e85b5efb805de605adc92700cb778279af37d8f cni.projectcalico.org/podIP:100.64.0.81/32 cni.projectcalico.org/podIPs:100.64.0.81/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd777 0xc0052fd778}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75g9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75g9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.81,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://73acdf6cc8edc45144a0c49a081b6c8cac2c4609e7a0836183b7a03b1fd2fd02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-vvq2r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vvq2r webserver-deployment-7f5969cbc7- deployment-6840  5a0bcdd9-15c3-4d44-94ed-103c7c3da844 14467 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:eb76b0809008b21a5c91efc11c5cf4aa5e31ba5240d9a8d8052f78a9f202cdbb cni.projectcalico.org/podIP:100.64.0.88/32 cni.projectcalico.org/podIPs:100.64.0.88/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fd980 0xc0052fd981}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p5nrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p5nrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-wsn98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wsn98 webserver-deployment-7f5969cbc7- deployment-6840  9a2fb637-0c92-4eaa-855d-0055ed1c9d7b 14450 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdb57 0xc0052fdb58}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2mql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2mql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-z9dvz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z9dvz webserver-deployment-7f5969cbc7- deployment-6840  5d30fc71-188d-4650-a3f8-373a69f9f78a 14336 0 2023-03-16 09:59:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:84a113ccb8ccddff309ae540bb0a3ce34ce4db673dcf2915ca5c2747b34492c7 cni.projectcalico.org/podIP:100.64.0.84/32 cni.projectcalico.org/podIPs:100.64.0.84/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdd37 0xc0052fdd38}] [] [{calico Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 09:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkggv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkggv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.84,StartTime:2023-03-16 09:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 09:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1cbd9e3be78597590fe1a0f81b786cb423fb09402433993030983abaf50b3080,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.706: INFO: Pod "webserver-deployment-7f5969cbc7-zc6k5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zc6k5 webserver-deployment-7f5969cbc7- deployment-6840  3bd2661a-ef40-4899-837d-f70311cd0167 14445 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 00229663-bec9-4a36-90d4-bcc41c1ea246 0xc0052fdf20 0xc0052fdf21}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00229663-bec9-4a36-90d4-bcc41c1ea246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vwwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vwwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-7jb8n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jb8n webserver-deployment-d9f79cb5- deployment-6840  d896085d-7911-4b05-b327-bd50fc975a90 14465 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f428b6ff630724bf9d26cb9d1486e68ca4f104462f5eb53563193d1a7acaba64 cni.projectcalico.org/podIP:100.64.0.87/32 cni.projectcalico.org/podIPs:100.64.0.87/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c0f7 0xc00541c0f8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98gq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98gq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.87,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-8wtjf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8wtjf webserver-deployment-d9f79cb5- deployment-6840  c672551b-4173-41fd-babc-fa642d9d2aa0 14441 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c30f 0xc00541c320}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wp74b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wp74b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-b6p47" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6p47 webserver-deployment-d9f79cb5- deployment-6840  9aa1e4e0-2604-4f5f-9278-11e508231ab9 14461 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2bb99d7a8fe556280374c8f097fcd9c6d4dd00dbbd4efbf100be75ab27d2a3e8 cni.projectcalico.org/podIP:100.64.1.147/32 cni.projectcalico.org/podIPs:100.64.1.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c517 0xc00541c518}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvg6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvg6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.147,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-dbcw6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dbcw6 webserver-deployment-d9f79cb5- deployment-6840  78d5ff16-c4c6-45ad-9202-17a8cf5e15d3 14444 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c747 0xc00541c748}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ct65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ct65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-j6psw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j6psw webserver-deployment-d9f79cb5- deployment-6840  91bcf8d8-1086-4cec-bfbd-a09df8970a30 14442 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541c927 0xc00541c928}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd968,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd968,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.707: INFO: Pod "webserver-deployment-d9f79cb5-knvjc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-knvjc webserver-deployment-d9f79cb5- deployment-6840  3c3f6638-2e58-46c1-b5e7-4debf770f19a 14447 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cb07 0xc00541cb08}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx6bk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx6bk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-mp5bj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mp5bj webserver-deployment-d9f79cb5- deployment-6840  b12c29c0-b814-4a21-80c7-0a1cd355edd9 14440 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cce7 0xc00541cce8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wd2mr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wd2mr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-rlwwx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rlwwx webserver-deployment-d9f79cb5- deployment-6840  4fe6ee85-1db8-4acf-8646-937dfb2c8cf2 14464 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7acff7d0c880a974dba08164488b9e57603ed6686b835ad19817bd5ed135566a cni.projectcalico.org/podIP:100.64.0.86/32 cni.projectcalico.org/podIPs:100.64.0.86/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541cee7 0xc00541cee8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r9cp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r9cp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:100.64.0.86,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-sf269" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sf269 webserver-deployment-d9f79cb5- deployment-6840  b3320ca7-fbf6-4bc5-8e91-c76a929df115 14457 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d57d66032f09842e4bbfe0c0027b0c719714256e134e5b79c431e83997324d71 cni.projectcalico.org/podIP:100.64.1.148/32 cni.projectcalico.org/podIPs:100.64.1.148/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d0ff 0xc00541d130}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggvhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggvhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.148,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-t6nz9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6nz9 webserver-deployment-d9f79cb5- deployment-6840  42115053-fae4-4bc5-8092-633024915217 14460 0 2023-03-16 09:59:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:71a29cd94b19e78968fcdce2ddd55350226aeafb22a8b87924d5476cafb915b5 cni.projectcalico.org/podIP:100.64.1.146/32 cni.projectcalico.org/podIPs:100.64.1.146/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d377 0xc00541d378}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnnkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnnkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.146,StartTime:2023-03-16 09:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-t7c4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t7c4c webserver-deployment-d9f79cb5- deployment-6840  96732012-b544-4917-b1bb-7334fcd592be 14473 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b5afffc38f70c40ce7d6dc76f517572b2f087192c424b74b2699ae958241812d cni.projectcalico.org/podIP:100.64.1.149/32 cni.projectcalico.org/podIPs:100.64.1.149/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d5c7 0xc00541d5c8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9wpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9wpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.708: INFO: Pod "webserver-deployment-d9f79cb5-ttgng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ttgng webserver-deployment-d9f79cb5- deployment-6840  a399ad83-b00b-4a41-89f3-6eb48028062f 14443 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d7c7 0xc00541d7c8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5g7nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5g7nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 09:59:24.709: INFO: Pod "webserver-deployment-d9f79cb5-v5pk2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v5pk2 webserver-deployment-d9f79cb5- deployment-6840  ee83ee11-f10f-402c-bcef-415a5301b5fc 14438 0 2023-03-16 09:59:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d75efc25-28c3-4981-9a39-992204e0b277 0xc00541d9a7 0xc00541d9a8}] [] [{kube-controller-manager Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75efc25-28c3-4981-9a39-992204e0b277\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 09:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xv5ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xv5ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 09:59:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 09:59:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:59:24.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6840" for this suite. 03/16/23 09:59:24.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:59:24.893
Mar 16 09:59:24.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 09:59:24.894
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:25.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:25.343
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 03/16/23 09:59:25.611
STEP: Patching the Job 03/16/23 09:59:25.702
STEP: Watching for Job to be patched 03/16/23 09:59:25.798
Mar 16 09:59:25.888: INFO: Event ADDED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 16 09:59:25.888: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 16 09:59:25.888: INFO: Event MODIFIED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/16/23 09:59:25.888
STEP: Watching for Job to be updated 03/16/23 09:59:26.069
Mar 16 09:59:26.158: INFO: Event MODIFIED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:26.158: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/16/23 09:59:26.158
Mar 16 09:59:26.248: INFO: Job: e2e-zh864 as labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched]
STEP: Waiting for job to complete 03/16/23 09:59:26.248
STEP: Delete a job collection with a labelselector 03/16/23 09:59:34.338
STEP: Watching for Job to be deleted 03/16/23 09:59:34.43
Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 16 09:59:34.519: INFO: Event DELETED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/16/23 09:59:34.519
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 09:59:34.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7532" for this suite. 03/16/23 09:59:34.787
------------------------------
• [9.985 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:59:24.893
    Mar 16 09:59:24.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 09:59:24.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:25.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:25.343
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 03/16/23 09:59:25.611
    STEP: Patching the Job 03/16/23 09:59:25.702
    STEP: Watching for Job to be patched 03/16/23 09:59:25.798
    Mar 16 09:59:25.888: INFO: Event ADDED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 16 09:59:25.888: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 16 09:59:25.888: INFO: Event MODIFIED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/16/23 09:59:25.888
    STEP: Watching for Job to be updated 03/16/23 09:59:26.069
    Mar 16 09:59:26.158: INFO: Event MODIFIED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:26.158: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/16/23 09:59:26.158
    Mar 16 09:59:26.248: INFO: Job: e2e-zh864 as labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched]
    STEP: Waiting for job to complete 03/16/23 09:59:26.248
    STEP: Delete a job collection with a labelselector 03/16/23 09:59:34.338
    STEP: Watching for Job to be deleted 03/16/23 09:59:34.43
    Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:34.519: INFO: Event MODIFIED observed for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 16 09:59:34.519: INFO: Event DELETED found for Job e2e-zh864 in namespace job-7532 with labels: map[e2e-job-label:e2e-zh864 e2e-zh864:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/16/23 09:59:34.519
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:59:34.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7532" for this suite. 03/16/23 09:59:34.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:59:34.879
Mar 16 09:59:34.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 09:59:34.88
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:35.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:35.328
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 03/16/23 09:59:35.506
Mar 16 09:59:35.596: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/16/23 09:59:35.596
Mar 16 09:59:35.687: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/16/23 09:59:35.687
Mar 16 09:59:35.867: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 09:59:35.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7102" for this suite. 03/16/23 09:59:35.958
------------------------------
• [1.170 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:59:34.879
    Mar 16 09:59:34.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 09:59:34.88
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:35.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:35.328
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 03/16/23 09:59:35.506
    Mar 16 09:59:35.596: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/16/23 09:59:35.596
    Mar 16 09:59:35.687: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/16/23 09:59:35.687
    Mar 16 09:59:35.867: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 09:59:35.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7102" for this suite. 03/16/23 09:59:35.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 09:59:36.049
Mar 16 09:59:36.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 09:59:36.05
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:36.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:36.498
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 03/16/23 09:59:36.676
Mar 16 09:59:36.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 create -f -'
Mar 16 09:59:37.940: INFO: stderr: ""
Mar 16 09:59:37.940: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:37.94
Mar 16 09:59:37.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:38.382: INFO: stderr: ""
Mar 16 09:59:38.382: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
Mar 16 09:59:38.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 09:59:38.729: INFO: stderr: ""
Mar 16 09:59:38.729: INFO: stdout: ""
Mar 16 09:59:38.729: INFO: update-demo-nautilus-sg6cj is created but not running
Mar 16 09:59:43.730: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:44.186: INFO: stderr: ""
Mar 16 09:59:44.186: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
Mar 16 09:59:44.186: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 09:59:44.535: INFO: stderr: ""
Mar 16 09:59:44.535: INFO: stdout: "true"
Mar 16 09:59:44.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 09:59:44.887: INFO: stderr: ""
Mar 16 09:59:44.887: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 09:59:44.887: INFO: validating pod update-demo-nautilus-sg6cj
Mar 16 09:59:45.070: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 09:59:45.070: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 09:59:45.070: INFO: update-demo-nautilus-sg6cj is verified up and running
Mar 16 09:59:45.070: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-vjlzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 09:59:45.417: INFO: stderr: ""
Mar 16 09:59:45.417: INFO: stdout: "true"
Mar 16 09:59:45.417: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-vjlzc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 09:59:45.760: INFO: stderr: ""
Mar 16 09:59:45.760: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 09:59:45.760: INFO: validating pod update-demo-nautilus-vjlzc
Mar 16 09:59:45.950: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 09:59:45.950: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 09:59:45.950: INFO: update-demo-nautilus-vjlzc is verified up and running
STEP: scaling down the replication controller 03/16/23 09:59:45.95
Mar 16 09:59:45.952: INFO: scanned /root for discovery docs: <nil>
Mar 16 09:59:45.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 16 09:59:46.482: INFO: stderr: ""
Mar 16 09:59:46.482: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:46.482
Mar 16 09:59:46.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:46.970: INFO: stderr: ""
Mar 16 09:59:46.970: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/16/23 09:59:46.97
Mar 16 09:59:51.971: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:52.331: INFO: stderr: ""
Mar 16 09:59:52.331: INFO: stdout: "update-demo-nautilus-sg6cj "
Mar 16 09:59:52.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 09:59:52.680: INFO: stderr: ""
Mar 16 09:59:52.680: INFO: stdout: "true"
Mar 16 09:59:52.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 09:59:53.030: INFO: stderr: ""
Mar 16 09:59:53.030: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 09:59:53.030: INFO: validating pod update-demo-nautilus-sg6cj
Mar 16 09:59:53.166: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 09:59:53.166: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 09:59:53.166: INFO: update-demo-nautilus-sg6cj is verified up and running
STEP: scaling up the replication controller 03/16/23 09:59:53.166
Mar 16 09:59:53.168: INFO: scanned /root for discovery docs: <nil>
Mar 16 09:59:53.168: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 16 09:59:53.716: INFO: stderr: ""
Mar 16 09:59:53.716: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:53.716
Mar 16 09:59:53.716: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:54.171: INFO: stderr: ""
Mar 16 09:59:54.171: INFO: stdout: "update-demo-nautilus-f8jx8 update-demo-nautilus-sg6cj "
Mar 16 09:59:54.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 09:59:54.518: INFO: stderr: ""
Mar 16 09:59:54.518: INFO: stdout: ""
Mar 16 09:59:54.518: INFO: update-demo-nautilus-f8jx8 is created but not running
Mar 16 09:59:59.519: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 09:59:59.953: INFO: stderr: ""
Mar 16 09:59:59.953: INFO: stdout: "update-demo-nautilus-f8jx8 update-demo-nautilus-sg6cj "
Mar 16 09:59:59.953: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 10:00:00.298: INFO: stderr: ""
Mar 16 10:00:00.298: INFO: stdout: "true"
Mar 16 10:00:00.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 10:00:00.641: INFO: stderr: ""
Mar 16 10:00:00.641: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 10:00:00.641: INFO: validating pod update-demo-nautilus-f8jx8
Mar 16 10:00:00.830: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 10:00:00.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 10:00:00.830: INFO: update-demo-nautilus-f8jx8 is verified up and running
Mar 16 10:00:00.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 10:00:01.179: INFO: stderr: ""
Mar 16 10:00:01.179: INFO: stdout: "true"
Mar 16 10:00:01.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 10:00:01.526: INFO: stderr: ""
Mar 16 10:00:01.526: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 10:00:01.526: INFO: validating pod update-demo-nautilus-sg6cj
Mar 16 10:00:01.662: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 10:00:01.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 10:00:01.662: INFO: update-demo-nautilus-sg6cj is verified up and running
STEP: using delete to clean up resources 03/16/23 10:00:01.662
Mar 16 10:00:01.662: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 delete --grace-period=0 --force -f -'
Mar 16 10:00:02.103: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:00:02.103: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 16 10:00:02.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get rc,svc -l name=update-demo --no-headers'
Mar 16 10:00:02.547: INFO: stderr: "No resources found in kubectl-1320 namespace.\n"
Mar 16 10:00:02.547: INFO: stdout: ""
Mar 16 10:00:02.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 16 10:00:02.931: INFO: stderr: ""
Mar 16 10:00:02.931: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:02.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1320" for this suite. 03/16/23 10:00:03.11
------------------------------
• [27.151 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 09:59:36.049
    Mar 16 09:59:36.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 09:59:36.05
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 09:59:36.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 09:59:36.498
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 03/16/23 09:59:36.676
    Mar 16 09:59:36.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 create -f -'
    Mar 16 09:59:37.940: INFO: stderr: ""
    Mar 16 09:59:37.940: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:37.94
    Mar 16 09:59:37.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:38.382: INFO: stderr: ""
    Mar 16 09:59:38.382: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
    Mar 16 09:59:38.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 09:59:38.729: INFO: stderr: ""
    Mar 16 09:59:38.729: INFO: stdout: ""
    Mar 16 09:59:38.729: INFO: update-demo-nautilus-sg6cj is created but not running
    Mar 16 09:59:43.730: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:44.186: INFO: stderr: ""
    Mar 16 09:59:44.186: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
    Mar 16 09:59:44.186: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 09:59:44.535: INFO: stderr: ""
    Mar 16 09:59:44.535: INFO: stdout: "true"
    Mar 16 09:59:44.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 09:59:44.887: INFO: stderr: ""
    Mar 16 09:59:44.887: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 09:59:44.887: INFO: validating pod update-demo-nautilus-sg6cj
    Mar 16 09:59:45.070: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 09:59:45.070: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 09:59:45.070: INFO: update-demo-nautilus-sg6cj is verified up and running
    Mar 16 09:59:45.070: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-vjlzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 09:59:45.417: INFO: stderr: ""
    Mar 16 09:59:45.417: INFO: stdout: "true"
    Mar 16 09:59:45.417: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-vjlzc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 09:59:45.760: INFO: stderr: ""
    Mar 16 09:59:45.760: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 09:59:45.760: INFO: validating pod update-demo-nautilus-vjlzc
    Mar 16 09:59:45.950: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 09:59:45.950: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 09:59:45.950: INFO: update-demo-nautilus-vjlzc is verified up and running
    STEP: scaling down the replication controller 03/16/23 09:59:45.95
    Mar 16 09:59:45.952: INFO: scanned /root for discovery docs: <nil>
    Mar 16 09:59:45.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 16 09:59:46.482: INFO: stderr: ""
    Mar 16 09:59:46.482: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:46.482
    Mar 16 09:59:46.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:46.970: INFO: stderr: ""
    Mar 16 09:59:46.970: INFO: stdout: "update-demo-nautilus-sg6cj update-demo-nautilus-vjlzc "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/16/23 09:59:46.97
    Mar 16 09:59:51.971: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:52.331: INFO: stderr: ""
    Mar 16 09:59:52.331: INFO: stdout: "update-demo-nautilus-sg6cj "
    Mar 16 09:59:52.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 09:59:52.680: INFO: stderr: ""
    Mar 16 09:59:52.680: INFO: stdout: "true"
    Mar 16 09:59:52.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 09:59:53.030: INFO: stderr: ""
    Mar 16 09:59:53.030: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 09:59:53.030: INFO: validating pod update-demo-nautilus-sg6cj
    Mar 16 09:59:53.166: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 09:59:53.166: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 09:59:53.166: INFO: update-demo-nautilus-sg6cj is verified up and running
    STEP: scaling up the replication controller 03/16/23 09:59:53.166
    Mar 16 09:59:53.168: INFO: scanned /root for discovery docs: <nil>
    Mar 16 09:59:53.168: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 16 09:59:53.716: INFO: stderr: ""
    Mar 16 09:59:53.716: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 09:59:53.716
    Mar 16 09:59:53.716: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:54.171: INFO: stderr: ""
    Mar 16 09:59:54.171: INFO: stdout: "update-demo-nautilus-f8jx8 update-demo-nautilus-sg6cj "
    Mar 16 09:59:54.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 09:59:54.518: INFO: stderr: ""
    Mar 16 09:59:54.518: INFO: stdout: ""
    Mar 16 09:59:54.518: INFO: update-demo-nautilus-f8jx8 is created but not running
    Mar 16 09:59:59.519: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 09:59:59.953: INFO: stderr: ""
    Mar 16 09:59:59.953: INFO: stdout: "update-demo-nautilus-f8jx8 update-demo-nautilus-sg6cj "
    Mar 16 09:59:59.953: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 10:00:00.298: INFO: stderr: ""
    Mar 16 10:00:00.298: INFO: stdout: "true"
    Mar 16 10:00:00.298: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-f8jx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 10:00:00.641: INFO: stderr: ""
    Mar 16 10:00:00.641: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 10:00:00.641: INFO: validating pod update-demo-nautilus-f8jx8
    Mar 16 10:00:00.830: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 10:00:00.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 10:00:00.830: INFO: update-demo-nautilus-f8jx8 is verified up and running
    Mar 16 10:00:00.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 10:00:01.179: INFO: stderr: ""
    Mar 16 10:00:01.179: INFO: stdout: "true"
    Mar 16 10:00:01.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods update-demo-nautilus-sg6cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 10:00:01.526: INFO: stderr: ""
    Mar 16 10:00:01.526: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 10:00:01.526: INFO: validating pod update-demo-nautilus-sg6cj
    Mar 16 10:00:01.662: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 10:00:01.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 10:00:01.662: INFO: update-demo-nautilus-sg6cj is verified up and running
    STEP: using delete to clean up resources 03/16/23 10:00:01.662
    Mar 16 10:00:01.662: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 delete --grace-period=0 --force -f -'
    Mar 16 10:00:02.103: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:00:02.103: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 16 10:00:02.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get rc,svc -l name=update-demo --no-headers'
    Mar 16 10:00:02.547: INFO: stderr: "No resources found in kubectl-1320 namespace.\n"
    Mar 16 10:00:02.547: INFO: stdout: ""
    Mar 16 10:00:02.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1320 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 16 10:00:02.931: INFO: stderr: ""
    Mar 16 10:00:02.931: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:02.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1320" for this suite. 03/16/23 10:00:03.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:03.201
Mar 16 10:00:03.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 10:00:03.202
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:03.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:03.65
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/16/23 10:00:03.918
Mar 16 10:00:04.013: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7997" to be "running and ready"
Mar 16 10:00:04.105: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 91.840825ms
Mar 16 10:00:04.105: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:00:06.196: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.182657739s
Mar 16 10:00:06.196: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 16 10:00:06.196: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 03/16/23 10:00:06.285
Mar 16 10:00:06.379: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7997" to be "running and ready"
Mar 16 10:00:06.469: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.778949ms
Mar 16 10:00:06.469: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:00:08.559: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180198528s
Mar 16 10:00:08.559: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 16 10:00:08.559: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/16/23 10:00:08.649
Mar 16 10:00:08.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 16 10:00:08.834: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 16 10:00:10.835: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 16 10:00:10.925: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/16/23 10:00:10.925
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:11.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7997" for this suite. 03/16/23 10:00:11.239
------------------------------
• [8.129 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:03.201
    Mar 16 10:00:03.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 10:00:03.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:03.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:03.65
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/16/23 10:00:03.918
    Mar 16 10:00:04.013: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7997" to be "running and ready"
    Mar 16 10:00:04.105: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 91.840825ms
    Mar 16 10:00:04.105: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:00:06.196: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.182657739s
    Mar 16 10:00:06.196: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 16 10:00:06.196: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 03/16/23 10:00:06.285
    Mar 16 10:00:06.379: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7997" to be "running and ready"
    Mar 16 10:00:06.469: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.778949ms
    Mar 16 10:00:06.469: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:00:08.559: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180198528s
    Mar 16 10:00:08.559: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 16 10:00:08.559: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/16/23 10:00:08.649
    Mar 16 10:00:08.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 16 10:00:08.834: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 16 10:00:10.835: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 16 10:00:10.925: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/16/23 10:00:10.925
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:11.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7997" for this suite. 03/16/23 10:00:11.239
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:11.331
Mar 16 10:00:11.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 03/16/23 10:00:11.332
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:11.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:11.779
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Mar 16 10:00:12.226: INFO: Endpoints addresses: [10.243.116.206] , ports: [443]
Mar 16 10:00:12.226: INFO: EndpointSlices addresses: [10.243.116.206] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1869" for this suite. 03/16/23 10:00:12.317
------------------------------
• [1.078 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:11.331
    Mar 16 10:00:11.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 03/16/23 10:00:11.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:11.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:11.779
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Mar 16 10:00:12.226: INFO: Endpoints addresses: [10.243.116.206] , ports: [443]
    Mar 16 10:00:12.226: INFO: EndpointSlices addresses: [10.243.116.206] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1869" for this suite. 03/16/23 10:00:12.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:12.409
Mar 16 10:00:12.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:00:12.41
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:12.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:12.858
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 03/16/23 10:00:13.036
Mar 16 10:00:13.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd 03/16/23 10:00:29.614
STEP: check the unserved version gets removed 03/16/23 10:00:29.98
STEP: check the other version is not changed 03/16/23 10:00:33.97
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:45.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5726" for this suite. 03/16/23 10:00:46.185
------------------------------
• [33.867 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:12.409
    Mar 16 10:00:12.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:00:12.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:12.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:12.858
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 03/16/23 10:00:13.036
    Mar 16 10:00:13.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: mark a version not serverd 03/16/23 10:00:29.614
    STEP: check the unserved version gets removed 03/16/23 10:00:29.98
    STEP: check the other version is not changed 03/16/23 10:00:33.97
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:45.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5726" for this suite. 03/16/23 10:00:46.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:46.276
Mar 16 10:00:46.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:00:46.278
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:46.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:46.725
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-1c891fec-3ade-4c2a-8f1e-e3d335e6ab04 03/16/23 10:00:46.903
STEP: Creating a pod to test consume configMaps 03/16/23 10:00:46.993
Mar 16 10:00:47.087: INFO: Waiting up to 5m0s for pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7" in namespace "configmap-1992" to be "Succeeded or Failed"
Mar 16 10:00:47.177: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765844ms
Mar 16 10:00:49.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180375821s
Mar 16 10:00:51.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180287616s
STEP: Saw pod success 03/16/23 10:00:51.268
Mar 16 10:00:51.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7" satisfied condition "Succeeded or Failed"
Mar 16 10:00:51.357: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:00:51.494
Mar 16 10:00:51.591: INFO: Waiting for pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 to disappear
Mar 16 10:00:51.681: INFO: Pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1992" for this suite. 03/16/23 10:00:51.859
------------------------------
• [5.673 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:46.276
    Mar 16 10:00:46.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:00:46.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:46.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:46.725
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-1c891fec-3ade-4c2a-8f1e-e3d335e6ab04 03/16/23 10:00:46.903
    STEP: Creating a pod to test consume configMaps 03/16/23 10:00:46.993
    Mar 16 10:00:47.087: INFO: Waiting up to 5m0s for pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7" in namespace "configmap-1992" to be "Succeeded or Failed"
    Mar 16 10:00:47.177: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765844ms
    Mar 16 10:00:49.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180375821s
    Mar 16 10:00:51.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180287616s
    STEP: Saw pod success 03/16/23 10:00:51.268
    Mar 16 10:00:51.268: INFO: Pod "pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7" satisfied condition "Succeeded or Failed"
    Mar 16 10:00:51.357: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:00:51.494
    Mar 16 10:00:51.591: INFO: Waiting for pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 to disappear
    Mar 16 10:00:51.681: INFO: Pod pod-configmaps-86ae5dee-6d73-4389-a7f6-5a5260d854d7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1992" for this suite. 03/16/23 10:00:51.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:51.95
Mar 16 10:00:51.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:00:51.951
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:52.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:52.4
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-df931383-820c-4cfd-9d6d-fc1fa80b143a 03/16/23 10:00:52.578
STEP: Creating a pod to test consume configMaps 03/16/23 10:00:52.669
Mar 16 10:00:52.763: INFO: Waiting up to 5m0s for pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e" in namespace "configmap-5310" to be "Succeeded or Failed"
Mar 16 10:00:52.853: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.505542ms
Mar 16 10:00:54.944: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180217688s
Mar 16 10:00:56.945: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181247526s
STEP: Saw pod success 03/16/23 10:00:56.945
Mar 16 10:00:56.945: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e" satisfied condition "Succeeded or Failed"
Mar 16 10:00:57.034: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e container configmap-volume-test: <nil>
STEP: delete the pod 03/16/23 10:00:57.17
Mar 16 10:00:57.265: INFO: Waiting for pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e to disappear
Mar 16 10:00:57.355: INFO: Pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:00:57.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5310" for this suite. 03/16/23 10:00:57.533
------------------------------
• [5.673 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:51.95
    Mar 16 10:00:51.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:00:51.951
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:52.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:52.4
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-df931383-820c-4cfd-9d6d-fc1fa80b143a 03/16/23 10:00:52.578
    STEP: Creating a pod to test consume configMaps 03/16/23 10:00:52.669
    Mar 16 10:00:52.763: INFO: Waiting up to 5m0s for pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e" in namespace "configmap-5310" to be "Succeeded or Failed"
    Mar 16 10:00:52.853: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.505542ms
    Mar 16 10:00:54.944: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180217688s
    Mar 16 10:00:56.945: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181247526s
    STEP: Saw pod success 03/16/23 10:00:56.945
    Mar 16 10:00:56.945: INFO: Pod "pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e" satisfied condition "Succeeded or Failed"
    Mar 16 10:00:57.034: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e container configmap-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:00:57.17
    Mar 16 10:00:57.265: INFO: Waiting for pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e to disappear
    Mar 16 10:00:57.355: INFO: Pod pod-configmaps-98d9f308-ee41-4289-a631-f0a2e33faf7e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:00:57.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5310" for this suite. 03/16/23 10:00:57.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:00:57.624
Mar 16 10:00:57.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:00:57.625
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:57.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:58.072
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 03/16/23 10:00:58.251
Mar 16 10:00:58.345: INFO: Waiting up to 5m0s for pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b" in namespace "emptydir-369" to be "Succeeded or Failed"
Mar 16 10:00:58.435: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.617099ms
Mar 16 10:01:00.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179796346s
Mar 16 10:01:02.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179714888s
STEP: Saw pod success 03/16/23 10:01:02.525
Mar 16 10:01:02.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b" satisfied condition "Succeeded or Failed"
Mar 16 10:01:02.615: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b container test-container: <nil>
STEP: delete the pod 03/16/23 10:01:02.709
Mar 16 10:01:02.802: INFO: Waiting for pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b to disappear
Mar 16 10:01:02.892: INFO: Pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:02.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-369" for this suite. 03/16/23 10:01:03.069
------------------------------
• [5.535 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:00:57.624
    Mar 16 10:00:57.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:00:57.625
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:00:57.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:00:58.072
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/16/23 10:00:58.251
    Mar 16 10:00:58.345: INFO: Waiting up to 5m0s for pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b" in namespace "emptydir-369" to be "Succeeded or Failed"
    Mar 16 10:00:58.435: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.617099ms
    Mar 16 10:01:00.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179796346s
    Mar 16 10:01:02.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179714888s
    STEP: Saw pod success 03/16/23 10:01:02.525
    Mar 16 10:01:02.525: INFO: Pod "pod-a0e683d3-9636-4e4d-a3df-7635adfc177b" satisfied condition "Succeeded or Failed"
    Mar 16 10:01:02.615: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b container test-container: <nil>
    STEP: delete the pod 03/16/23 10:01:02.709
    Mar 16 10:01:02.802: INFO: Waiting for pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b to disappear
    Mar 16 10:01:02.892: INFO: Pod pod-a0e683d3-9636-4e4d-a3df-7635adfc177b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:02.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-369" for this suite. 03/16/23 10:01:03.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:03.161
Mar 16 10:01:03.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:01:03.162
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:03.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:03.608
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Mar 16 10:01:03.786: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod 03/16/23 10:01:03.787
STEP: submitting the pod to kubernetes 03/16/23 10:01:03.787
Mar 16 10:01:03.882: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e" in namespace "pods-7820" to be "running and ready"
Mar 16 10:01:03.971: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.600514ms
Mar 16 10:01:03.972: INFO: The phase of Pod pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:01:06.063: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e": Phase="Running", Reason="", readiness=true. Elapsed: 2.180780983s
Mar 16 10:01:06.063: INFO: The phase of Pod pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e is Running (Ready = true)
Mar 16 10:01:06.063: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:06.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7820" for this suite. 03/16/23 10:01:06.821
------------------------------
• [3.751 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:03.161
    Mar 16 10:01:03.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:01:03.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:03.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:03.608
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Mar 16 10:01:03.786: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating the pod 03/16/23 10:01:03.787
    STEP: submitting the pod to kubernetes 03/16/23 10:01:03.787
    Mar 16 10:01:03.882: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e" in namespace "pods-7820" to be "running and ready"
    Mar 16 10:01:03.971: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.600514ms
    Mar 16 10:01:03.972: INFO: The phase of Pod pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:01:06.063: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e": Phase="Running", Reason="", readiness=true. Elapsed: 2.180780983s
    Mar 16 10:01:06.063: INFO: The phase of Pod pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e is Running (Ready = true)
    Mar 16 10:01:06.063: INFO: Pod "pod-exec-websocket-4554c7c7-42c5-4d4e-8987-dbef939bf14e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:06.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7820" for this suite. 03/16/23 10:01:06.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:06.913
Mar 16 10:01:06.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 10:01:06.914
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:07.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:07.36
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa in namespace container-probe-1889 03/16/23 10:01:07.538
Mar 16 10:01:07.632: INFO: Waiting up to 5m0s for pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa" in namespace "container-probe-1889" to be "not pending"
Mar 16 10:01:07.722: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa": Phase="Pending", Reason="", readiness=false. Elapsed: 89.610467ms
Mar 16 10:01:09.813: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.180427895s
Mar 16 10:01:09.813: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa" satisfied condition "not pending"
Mar 16 10:01:09.813: INFO: Started pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa in namespace container-probe-1889
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:01:09.813
Mar 16 10:01:09.903: INFO: Initial restart count of pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa is 0
Mar 16 10:01:30.900: INFO: Restart count of pod container-probe-1889/liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa is now 1 (20.996719125s elapsed)
STEP: deleting the pod 03/16/23 10:01:30.9
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:30.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1889" for this suite. 03/16/23 10:01:31.17
------------------------------
• [24.348 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:06.913
    Mar 16 10:01:06.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 10:01:06.914
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:07.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:07.36
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa in namespace container-probe-1889 03/16/23 10:01:07.538
    Mar 16 10:01:07.632: INFO: Waiting up to 5m0s for pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa" in namespace "container-probe-1889" to be "not pending"
    Mar 16 10:01:07.722: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa": Phase="Pending", Reason="", readiness=false. Elapsed: 89.610467ms
    Mar 16 10:01:09.813: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.180427895s
    Mar 16 10:01:09.813: INFO: Pod "liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa" satisfied condition "not pending"
    Mar 16 10:01:09.813: INFO: Started pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa in namespace container-probe-1889
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:01:09.813
    Mar 16 10:01:09.903: INFO: Initial restart count of pod liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa is 0
    Mar 16 10:01:30.900: INFO: Restart count of pod container-probe-1889/liveness-627cdc96-f80c-45ca-96a0-f322e055bfaa is now 1 (20.996719125s elapsed)
    STEP: deleting the pod 03/16/23 10:01:30.9
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:30.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1889" for this suite. 03/16/23 10:01:31.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:31.262
Mar 16 10:01:31.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:01:31.264
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:31.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:31.711
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:01:32.071
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:01:32.412
STEP: Deploying the webhook pod 03/16/23 10:01:32.503
STEP: Wait for the deployment to be ready 03/16/23 10:01:32.684
Mar 16 10:01:32.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:01:35.044
STEP: Verifying the service has paired with the endpoint 03/16/23 10:01:35.138
Mar 16 10:01:36.138: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Mar 16 10:01:36.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/16/23 10:01:36.407
STEP: Creating a custom resource that should be denied by the webhook 03/16/23 10:01:36.671
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/16/23 10:01:38.844
STEP: Updating the custom resource with disallowed data should be denied 03/16/23 10:01:38.98
STEP: Deleting the custom resource should be denied 03/16/23 10:01:39.207
STEP: Remove the offending key and value from the custom resource data 03/16/23 10:01:39.347
STEP: Deleting the updated custom resource should be successful 03/16/23 10:01:39.575
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5322" for this suite. 03/16/23 10:01:40.527
STEP: Destroying namespace "webhook-5322-markers" for this suite. 03/16/23 10:01:40.617
------------------------------
• [9.445 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:31.262
    Mar 16 10:01:31.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:01:31.264
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:31.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:31.711
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:01:32.071
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:01:32.412
    STEP: Deploying the webhook pod 03/16/23 10:01:32.503
    STEP: Wait for the deployment to be ready 03/16/23 10:01:32.684
    Mar 16 10:01:32.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 1, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:01:35.044
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:01:35.138
    Mar 16 10:01:36.138: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Mar 16 10:01:36.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/16/23 10:01:36.407
    STEP: Creating a custom resource that should be denied by the webhook 03/16/23 10:01:36.671
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/16/23 10:01:38.844
    STEP: Updating the custom resource with disallowed data should be denied 03/16/23 10:01:38.98
    STEP: Deleting the custom resource should be denied 03/16/23 10:01:39.207
    STEP: Remove the offending key and value from the custom resource data 03/16/23 10:01:39.347
    STEP: Deleting the updated custom resource should be successful 03/16/23 10:01:39.575
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5322" for this suite. 03/16/23 10:01:40.527
    STEP: Destroying namespace "webhook-5322-markers" for this suite. 03/16/23 10:01:40.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:40.71
Mar 16 10:01:40.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:01:40.712
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:40.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:41.158
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 03/16/23 10:01:41.336
Mar 16 10:01:41.431: INFO: Waiting up to 5m0s for pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36" in namespace "downward-api-7013" to be "running and ready"
Mar 16 10:01:41.521: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36": Phase="Pending", Reason="", readiness=false. Elapsed: 89.638654ms
Mar 16 10:01:41.521: INFO: The phase of Pod annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:01:43.611: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36": Phase="Running", Reason="", readiness=true. Elapsed: 2.180440956s
Mar 16 10:01:43.611: INFO: The phase of Pod annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36 is Running (Ready = true)
Mar 16 10:01:43.611: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36" satisfied condition "running and ready"
Mar 16 10:01:44.525: INFO: Successfully updated pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:46.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7013" for this suite. 03/16/23 10:01:46.904
------------------------------
• [6.284 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:40.71
    Mar 16 10:01:40.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:01:40.712
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:40.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:41.158
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 03/16/23 10:01:41.336
    Mar 16 10:01:41.431: INFO: Waiting up to 5m0s for pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36" in namespace "downward-api-7013" to be "running and ready"
    Mar 16 10:01:41.521: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36": Phase="Pending", Reason="", readiness=false. Elapsed: 89.638654ms
    Mar 16 10:01:41.521: INFO: The phase of Pod annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:01:43.611: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36": Phase="Running", Reason="", readiness=true. Elapsed: 2.180440956s
    Mar 16 10:01:43.611: INFO: The phase of Pod annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36 is Running (Ready = true)
    Mar 16 10:01:43.611: INFO: Pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36" satisfied condition "running and ready"
    Mar 16 10:01:44.525: INFO: Successfully updated pod "annotationupdated6128754-12d9-4033-ad2c-b7dbdf8a0d36"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:46.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7013" for this suite. 03/16/23 10:01:46.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:46.995
Mar 16 10:01:46.995: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:01:46.996
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:47.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:47.446
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1357 03/16/23 10:01:47.625
STEP: creating replication controller nodeport-test in namespace services-1357 03/16/23 10:01:47.72
I0316 10:01:47.811314    8588 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1357, replica count: 2
I0316 10:01:50.913180    8588 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:01:50.913: INFO: Creating new exec pod
Mar 16 10:01:51.007: INFO: Waiting up to 5m0s for pod "execpodvcslq" in namespace "services-1357" to be "running"
Mar 16 10:01:51.096: INFO: Pod "execpodvcslq": Phase="Pending", Reason="", readiness=false. Elapsed: 89.498285ms
Mar 16 10:01:53.186: INFO: Pod "execpodvcslq": Phase="Running", Reason="", readiness=true. Elapsed: 2.17949018s
Mar 16 10:01:53.186: INFO: Pod "execpodvcslq" satisfied condition "running"
Mar 16 10:01:54.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Mar 16 10:01:55.516: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 16 10:01:55.516: INFO: stdout: ""
Mar 16 10:01:55.516: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 100.106.162.29 80'
Mar 16 10:01:56.621: INFO: stderr: "+ nc -v -z -w 2 100.106.162.29 80\nConnection to 100.106.162.29 80 port [tcp/http] succeeded!\n"
Mar 16 10:01:56.621: INFO: stdout: ""
Mar 16 10:01:56.622: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 30461'
Mar 16 10:01:57.721: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 30461\nConnection to 10.250.19.136 30461 port [tcp/*] succeeded!\n"
Mar 16 10:01:57.721: INFO: stdout: ""
Mar 16 10:01:57.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 30461'
Mar 16 10:01:58.913: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 30461\nConnection to 10.250.19.246 30461 port [tcp/*] succeeded!\n"
Mar 16 10:01:58.913: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:01:58.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1357" for this suite. 03/16/23 10:01:59.091
------------------------------
• [12.187 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:46.995
    Mar 16 10:01:46.995: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:01:46.996
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:47.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:47.446
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1357 03/16/23 10:01:47.625
    STEP: creating replication controller nodeport-test in namespace services-1357 03/16/23 10:01:47.72
    I0316 10:01:47.811314    8588 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1357, replica count: 2
    I0316 10:01:50.913180    8588 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:01:50.913: INFO: Creating new exec pod
    Mar 16 10:01:51.007: INFO: Waiting up to 5m0s for pod "execpodvcslq" in namespace "services-1357" to be "running"
    Mar 16 10:01:51.096: INFO: Pod "execpodvcslq": Phase="Pending", Reason="", readiness=false. Elapsed: 89.498285ms
    Mar 16 10:01:53.186: INFO: Pod "execpodvcslq": Phase="Running", Reason="", readiness=true. Elapsed: 2.17949018s
    Mar 16 10:01:53.186: INFO: Pod "execpodvcslq" satisfied condition "running"
    Mar 16 10:01:54.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Mar 16 10:01:55.516: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 16 10:01:55.516: INFO: stdout: ""
    Mar 16 10:01:55.516: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 100.106.162.29 80'
    Mar 16 10:01:56.621: INFO: stderr: "+ nc -v -z -w 2 100.106.162.29 80\nConnection to 100.106.162.29 80 port [tcp/http] succeeded!\n"
    Mar 16 10:01:56.621: INFO: stdout: ""
    Mar 16 10:01:56.622: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 30461'
    Mar 16 10:01:57.721: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 30461\nConnection to 10.250.19.136 30461 port [tcp/*] succeeded!\n"
    Mar 16 10:01:57.721: INFO: stdout: ""
    Mar 16 10:01:57.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1357 exec execpodvcslq -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 30461'
    Mar 16 10:01:58.913: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 30461\nConnection to 10.250.19.246 30461 port [tcp/*] succeeded!\n"
    Mar 16 10:01:58.913: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:01:58.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1357" for this suite. 03/16/23 10:01:59.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:01:59.183
Mar 16 10:01:59.183: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 10:01:59.184
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:59.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:59.63
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/16/23 10:01:59.898
STEP: delete the rc 03/16/23 10:02:05.08
STEP: wait for the rc to be deleted 03/16/23 10:02:05.17
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/16/23 10:02:10.261
STEP: Gathering metrics 03/16/23 10:02:40.446
W0316 10:02:40.543733    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 10:02:40.543: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 16 10:02:40.543: INFO: Deleting pod "simpletest.rc-2m4pn" in namespace "gc-4239"
Mar 16 10:02:40.636: INFO: Deleting pod "simpletest.rc-2mtdm" in namespace "gc-4239"
Mar 16 10:02:40.729: INFO: Deleting pod "simpletest.rc-2q7qx" in namespace "gc-4239"
Mar 16 10:02:40.822: INFO: Deleting pod "simpletest.rc-2ss2k" in namespace "gc-4239"
Mar 16 10:02:40.913: INFO: Deleting pod "simpletest.rc-2xxpl" in namespace "gc-4239"
Mar 16 10:02:41.007: INFO: Deleting pod "simpletest.rc-45654" in namespace "gc-4239"
Mar 16 10:02:41.100: INFO: Deleting pod "simpletest.rc-49x9z" in namespace "gc-4239"
Mar 16 10:02:41.193: INFO: Deleting pod "simpletest.rc-4s7vk" in namespace "gc-4239"
Mar 16 10:02:41.287: INFO: Deleting pod "simpletest.rc-56s86" in namespace "gc-4239"
Mar 16 10:02:41.379: INFO: Deleting pod "simpletest.rc-66smv" in namespace "gc-4239"
Mar 16 10:02:41.473: INFO: Deleting pod "simpletest.rc-69m8l" in namespace "gc-4239"
Mar 16 10:02:41.567: INFO: Deleting pod "simpletest.rc-6cnvv" in namespace "gc-4239"
Mar 16 10:02:41.660: INFO: Deleting pod "simpletest.rc-6d59f" in namespace "gc-4239"
Mar 16 10:02:41.753: INFO: Deleting pod "simpletest.rc-6g9sl" in namespace "gc-4239"
Mar 16 10:02:41.847: INFO: Deleting pod "simpletest.rc-6lrt8" in namespace "gc-4239"
Mar 16 10:02:41.939: INFO: Deleting pod "simpletest.rc-6t2zw" in namespace "gc-4239"
Mar 16 10:02:42.031: INFO: Deleting pod "simpletest.rc-6zmtr" in namespace "gc-4239"
Mar 16 10:02:42.125: INFO: Deleting pod "simpletest.rc-7jvxh" in namespace "gc-4239"
Mar 16 10:02:42.216: INFO: Deleting pod "simpletest.rc-7tttr" in namespace "gc-4239"
Mar 16 10:02:42.308: INFO: Deleting pod "simpletest.rc-7vhp5" in namespace "gc-4239"
Mar 16 10:02:42.400: INFO: Deleting pod "simpletest.rc-84pgs" in namespace "gc-4239"
Mar 16 10:02:42.493: INFO: Deleting pod "simpletest.rc-8sdj7" in namespace "gc-4239"
Mar 16 10:02:42.587: INFO: Deleting pod "simpletest.rc-9f7zh" in namespace "gc-4239"
Mar 16 10:02:42.679: INFO: Deleting pod "simpletest.rc-b44kh" in namespace "gc-4239"
Mar 16 10:02:42.773: INFO: Deleting pod "simpletest.rc-b5jvl" in namespace "gc-4239"
Mar 16 10:02:42.865: INFO: Deleting pod "simpletest.rc-bh4xv" in namespace "gc-4239"
Mar 16 10:02:42.957: INFO: Deleting pod "simpletest.rc-bjwvc" in namespace "gc-4239"
Mar 16 10:02:43.049: INFO: Deleting pod "simpletest.rc-bkqbp" in namespace "gc-4239"
Mar 16 10:02:43.141: INFO: Deleting pod "simpletest.rc-bmv95" in namespace "gc-4239"
Mar 16 10:02:43.235: INFO: Deleting pod "simpletest.rc-brpz7" in namespace "gc-4239"
Mar 16 10:02:43.328: INFO: Deleting pod "simpletest.rc-ck7c9" in namespace "gc-4239"
Mar 16 10:02:43.421: INFO: Deleting pod "simpletest.rc-cnxmh" in namespace "gc-4239"
Mar 16 10:02:43.514: INFO: Deleting pod "simpletest.rc-cr27j" in namespace "gc-4239"
Mar 16 10:02:43.606: INFO: Deleting pod "simpletest.rc-cxgcc" in namespace "gc-4239"
Mar 16 10:02:43.700: INFO: Deleting pod "simpletest.rc-cxlff" in namespace "gc-4239"
Mar 16 10:02:43.793: INFO: Deleting pod "simpletest.rc-dgvxq" in namespace "gc-4239"
Mar 16 10:02:43.887: INFO: Deleting pod "simpletest.rc-djft4" in namespace "gc-4239"
Mar 16 10:02:43.980: INFO: Deleting pod "simpletest.rc-dwrth" in namespace "gc-4239"
Mar 16 10:02:44.075: INFO: Deleting pod "simpletest.rc-f2vzq" in namespace "gc-4239"
Mar 16 10:02:44.169: INFO: Deleting pod "simpletest.rc-f57f6" in namespace "gc-4239"
Mar 16 10:02:44.262: INFO: Deleting pod "simpletest.rc-fqcnq" in namespace "gc-4239"
Mar 16 10:02:44.356: INFO: Deleting pod "simpletest.rc-gdvbq" in namespace "gc-4239"
Mar 16 10:02:44.450: INFO: Deleting pod "simpletest.rc-gn76n" in namespace "gc-4239"
Mar 16 10:02:44.543: INFO: Deleting pod "simpletest.rc-gqplm" in namespace "gc-4239"
Mar 16 10:02:44.636: INFO: Deleting pod "simpletest.rc-gtrp7" in namespace "gc-4239"
Mar 16 10:02:44.730: INFO: Deleting pod "simpletest.rc-hkjz7" in namespace "gc-4239"
Mar 16 10:02:44.824: INFO: Deleting pod "simpletest.rc-hl7rt" in namespace "gc-4239"
Mar 16 10:02:44.917: INFO: Deleting pod "simpletest.rc-jbgtk" in namespace "gc-4239"
Mar 16 10:02:45.010: INFO: Deleting pod "simpletest.rc-kf4zf" in namespace "gc-4239"
Mar 16 10:02:45.102: INFO: Deleting pod "simpletest.rc-kpghz" in namespace "gc-4239"
Mar 16 10:02:45.195: INFO: Deleting pod "simpletest.rc-krqlw" in namespace "gc-4239"
Mar 16 10:02:45.288: INFO: Deleting pod "simpletest.rc-l54rw" in namespace "gc-4239"
Mar 16 10:02:45.380: INFO: Deleting pod "simpletest.rc-l5s74" in namespace "gc-4239"
Mar 16 10:02:45.472: INFO: Deleting pod "simpletest.rc-lcdvd" in namespace "gc-4239"
Mar 16 10:02:45.564: INFO: Deleting pod "simpletest.rc-lhhr2" in namespace "gc-4239"
Mar 16 10:02:45.657: INFO: Deleting pod "simpletest.rc-lhhwj" in namespace "gc-4239"
Mar 16 10:02:45.756: INFO: Deleting pod "simpletest.rc-lqdq5" in namespace "gc-4239"
Mar 16 10:02:45.850: INFO: Deleting pod "simpletest.rc-md7cf" in namespace "gc-4239"
Mar 16 10:02:45.943: INFO: Deleting pod "simpletest.rc-mkpbt" in namespace "gc-4239"
Mar 16 10:02:46.037: INFO: Deleting pod "simpletest.rc-mmg5g" in namespace "gc-4239"
Mar 16 10:02:46.130: INFO: Deleting pod "simpletest.rc-mqlh8" in namespace "gc-4239"
Mar 16 10:02:46.223: INFO: Deleting pod "simpletest.rc-n46wl" in namespace "gc-4239"
Mar 16 10:02:46.316: INFO: Deleting pod "simpletest.rc-ndgw7" in namespace "gc-4239"
Mar 16 10:02:46.410: INFO: Deleting pod "simpletest.rc-nlwpw" in namespace "gc-4239"
Mar 16 10:02:46.503: INFO: Deleting pod "simpletest.rc-nnpqs" in namespace "gc-4239"
Mar 16 10:02:46.598: INFO: Deleting pod "simpletest.rc-nrlgx" in namespace "gc-4239"
Mar 16 10:02:46.691: INFO: Deleting pod "simpletest.rc-pd8hp" in namespace "gc-4239"
Mar 16 10:02:46.784: INFO: Deleting pod "simpletest.rc-ph7x9" in namespace "gc-4239"
Mar 16 10:02:46.876: INFO: Deleting pod "simpletest.rc-ptt7c" in namespace "gc-4239"
Mar 16 10:02:46.969: INFO: Deleting pod "simpletest.rc-q7bbf" in namespace "gc-4239"
Mar 16 10:02:47.063: INFO: Deleting pod "simpletest.rc-q8z79" in namespace "gc-4239"
Mar 16 10:02:47.157: INFO: Deleting pod "simpletest.rc-qdggc" in namespace "gc-4239"
Mar 16 10:02:47.250: INFO: Deleting pod "simpletest.rc-r79s5" in namespace "gc-4239"
Mar 16 10:02:47.343: INFO: Deleting pod "simpletest.rc-r828w" in namespace "gc-4239"
Mar 16 10:02:47.437: INFO: Deleting pod "simpletest.rc-rb84z" in namespace "gc-4239"
Mar 16 10:02:47.531: INFO: Deleting pod "simpletest.rc-rlfvh" in namespace "gc-4239"
Mar 16 10:02:47.623: INFO: Deleting pod "simpletest.rc-rm4tw" in namespace "gc-4239"
Mar 16 10:02:47.716: INFO: Deleting pod "simpletest.rc-s2s4q" in namespace "gc-4239"
Mar 16 10:02:47.809: INFO: Deleting pod "simpletest.rc-s4l52" in namespace "gc-4239"
Mar 16 10:02:47.902: INFO: Deleting pod "simpletest.rc-scpfq" in namespace "gc-4239"
Mar 16 10:02:47.996: INFO: Deleting pod "simpletest.rc-sf9g5" in namespace "gc-4239"
Mar 16 10:02:48.093: INFO: Deleting pod "simpletest.rc-snms5" in namespace "gc-4239"
Mar 16 10:02:48.187: INFO: Deleting pod "simpletest.rc-sv2bw" in namespace "gc-4239"
Mar 16 10:02:48.281: INFO: Deleting pod "simpletest.rc-tdxft" in namespace "gc-4239"
Mar 16 10:02:48.374: INFO: Deleting pod "simpletest.rc-tgm66" in namespace "gc-4239"
Mar 16 10:02:48.466: INFO: Deleting pod "simpletest.rc-vbn7s" in namespace "gc-4239"
Mar 16 10:02:48.559: INFO: Deleting pod "simpletest.rc-vd876" in namespace "gc-4239"
Mar 16 10:02:48.651: INFO: Deleting pod "simpletest.rc-vj8gn" in namespace "gc-4239"
Mar 16 10:02:48.743: INFO: Deleting pod "simpletest.rc-vxksd" in namespace "gc-4239"
Mar 16 10:02:48.837: INFO: Deleting pod "simpletest.rc-w9rvk" in namespace "gc-4239"
Mar 16 10:02:48.930: INFO: Deleting pod "simpletest.rc-wkfjx" in namespace "gc-4239"
Mar 16 10:02:49.024: INFO: Deleting pod "simpletest.rc-wl845" in namespace "gc-4239"
Mar 16 10:02:49.117: INFO: Deleting pod "simpletest.rc-xp4vp" in namespace "gc-4239"
Mar 16 10:02:49.211: INFO: Deleting pod "simpletest.rc-xtb4n" in namespace "gc-4239"
Mar 16 10:02:49.303: INFO: Deleting pod "simpletest.rc-xtlfp" in namespace "gc-4239"
Mar 16 10:02:49.397: INFO: Deleting pod "simpletest.rc-zb7nl" in namespace "gc-4239"
Mar 16 10:02:49.491: INFO: Deleting pod "simpletest.rc-zctgg" in namespace "gc-4239"
Mar 16 10:02:49.584: INFO: Deleting pod "simpletest.rc-zhqbc" in namespace "gc-4239"
Mar 16 10:02:49.677: INFO: Deleting pod "simpletest.rc-znqtz" in namespace "gc-4239"
Mar 16 10:02:49.771: INFO: Deleting pod "simpletest.rc-zrdgj" in namespace "gc-4239"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 10:02:49.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4239" for this suite. 03/16/23 10:02:49.955
------------------------------
• [50.863 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:01:59.183
    Mar 16 10:01:59.183: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 10:01:59.184
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:01:59.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:01:59.63
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/16/23 10:01:59.898
    STEP: delete the rc 03/16/23 10:02:05.08
    STEP: wait for the rc to be deleted 03/16/23 10:02:05.17
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/16/23 10:02:10.261
    STEP: Gathering metrics 03/16/23 10:02:40.446
    W0316 10:02:40.543733    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 10:02:40.543: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 16 10:02:40.543: INFO: Deleting pod "simpletest.rc-2m4pn" in namespace "gc-4239"
    Mar 16 10:02:40.636: INFO: Deleting pod "simpletest.rc-2mtdm" in namespace "gc-4239"
    Mar 16 10:02:40.729: INFO: Deleting pod "simpletest.rc-2q7qx" in namespace "gc-4239"
    Mar 16 10:02:40.822: INFO: Deleting pod "simpletest.rc-2ss2k" in namespace "gc-4239"
    Mar 16 10:02:40.913: INFO: Deleting pod "simpletest.rc-2xxpl" in namespace "gc-4239"
    Mar 16 10:02:41.007: INFO: Deleting pod "simpletest.rc-45654" in namespace "gc-4239"
    Mar 16 10:02:41.100: INFO: Deleting pod "simpletest.rc-49x9z" in namespace "gc-4239"
    Mar 16 10:02:41.193: INFO: Deleting pod "simpletest.rc-4s7vk" in namespace "gc-4239"
    Mar 16 10:02:41.287: INFO: Deleting pod "simpletest.rc-56s86" in namespace "gc-4239"
    Mar 16 10:02:41.379: INFO: Deleting pod "simpletest.rc-66smv" in namespace "gc-4239"
    Mar 16 10:02:41.473: INFO: Deleting pod "simpletest.rc-69m8l" in namespace "gc-4239"
    Mar 16 10:02:41.567: INFO: Deleting pod "simpletest.rc-6cnvv" in namespace "gc-4239"
    Mar 16 10:02:41.660: INFO: Deleting pod "simpletest.rc-6d59f" in namespace "gc-4239"
    Mar 16 10:02:41.753: INFO: Deleting pod "simpletest.rc-6g9sl" in namespace "gc-4239"
    Mar 16 10:02:41.847: INFO: Deleting pod "simpletest.rc-6lrt8" in namespace "gc-4239"
    Mar 16 10:02:41.939: INFO: Deleting pod "simpletest.rc-6t2zw" in namespace "gc-4239"
    Mar 16 10:02:42.031: INFO: Deleting pod "simpletest.rc-6zmtr" in namespace "gc-4239"
    Mar 16 10:02:42.125: INFO: Deleting pod "simpletest.rc-7jvxh" in namespace "gc-4239"
    Mar 16 10:02:42.216: INFO: Deleting pod "simpletest.rc-7tttr" in namespace "gc-4239"
    Mar 16 10:02:42.308: INFO: Deleting pod "simpletest.rc-7vhp5" in namespace "gc-4239"
    Mar 16 10:02:42.400: INFO: Deleting pod "simpletest.rc-84pgs" in namespace "gc-4239"
    Mar 16 10:02:42.493: INFO: Deleting pod "simpletest.rc-8sdj7" in namespace "gc-4239"
    Mar 16 10:02:42.587: INFO: Deleting pod "simpletest.rc-9f7zh" in namespace "gc-4239"
    Mar 16 10:02:42.679: INFO: Deleting pod "simpletest.rc-b44kh" in namespace "gc-4239"
    Mar 16 10:02:42.773: INFO: Deleting pod "simpletest.rc-b5jvl" in namespace "gc-4239"
    Mar 16 10:02:42.865: INFO: Deleting pod "simpletest.rc-bh4xv" in namespace "gc-4239"
    Mar 16 10:02:42.957: INFO: Deleting pod "simpletest.rc-bjwvc" in namespace "gc-4239"
    Mar 16 10:02:43.049: INFO: Deleting pod "simpletest.rc-bkqbp" in namespace "gc-4239"
    Mar 16 10:02:43.141: INFO: Deleting pod "simpletest.rc-bmv95" in namespace "gc-4239"
    Mar 16 10:02:43.235: INFO: Deleting pod "simpletest.rc-brpz7" in namespace "gc-4239"
    Mar 16 10:02:43.328: INFO: Deleting pod "simpletest.rc-ck7c9" in namespace "gc-4239"
    Mar 16 10:02:43.421: INFO: Deleting pod "simpletest.rc-cnxmh" in namespace "gc-4239"
    Mar 16 10:02:43.514: INFO: Deleting pod "simpletest.rc-cr27j" in namespace "gc-4239"
    Mar 16 10:02:43.606: INFO: Deleting pod "simpletest.rc-cxgcc" in namespace "gc-4239"
    Mar 16 10:02:43.700: INFO: Deleting pod "simpletest.rc-cxlff" in namespace "gc-4239"
    Mar 16 10:02:43.793: INFO: Deleting pod "simpletest.rc-dgvxq" in namespace "gc-4239"
    Mar 16 10:02:43.887: INFO: Deleting pod "simpletest.rc-djft4" in namespace "gc-4239"
    Mar 16 10:02:43.980: INFO: Deleting pod "simpletest.rc-dwrth" in namespace "gc-4239"
    Mar 16 10:02:44.075: INFO: Deleting pod "simpletest.rc-f2vzq" in namespace "gc-4239"
    Mar 16 10:02:44.169: INFO: Deleting pod "simpletest.rc-f57f6" in namespace "gc-4239"
    Mar 16 10:02:44.262: INFO: Deleting pod "simpletest.rc-fqcnq" in namespace "gc-4239"
    Mar 16 10:02:44.356: INFO: Deleting pod "simpletest.rc-gdvbq" in namespace "gc-4239"
    Mar 16 10:02:44.450: INFO: Deleting pod "simpletest.rc-gn76n" in namespace "gc-4239"
    Mar 16 10:02:44.543: INFO: Deleting pod "simpletest.rc-gqplm" in namespace "gc-4239"
    Mar 16 10:02:44.636: INFO: Deleting pod "simpletest.rc-gtrp7" in namespace "gc-4239"
    Mar 16 10:02:44.730: INFO: Deleting pod "simpletest.rc-hkjz7" in namespace "gc-4239"
    Mar 16 10:02:44.824: INFO: Deleting pod "simpletest.rc-hl7rt" in namespace "gc-4239"
    Mar 16 10:02:44.917: INFO: Deleting pod "simpletest.rc-jbgtk" in namespace "gc-4239"
    Mar 16 10:02:45.010: INFO: Deleting pod "simpletest.rc-kf4zf" in namespace "gc-4239"
    Mar 16 10:02:45.102: INFO: Deleting pod "simpletest.rc-kpghz" in namespace "gc-4239"
    Mar 16 10:02:45.195: INFO: Deleting pod "simpletest.rc-krqlw" in namespace "gc-4239"
    Mar 16 10:02:45.288: INFO: Deleting pod "simpletest.rc-l54rw" in namespace "gc-4239"
    Mar 16 10:02:45.380: INFO: Deleting pod "simpletest.rc-l5s74" in namespace "gc-4239"
    Mar 16 10:02:45.472: INFO: Deleting pod "simpletest.rc-lcdvd" in namespace "gc-4239"
    Mar 16 10:02:45.564: INFO: Deleting pod "simpletest.rc-lhhr2" in namespace "gc-4239"
    Mar 16 10:02:45.657: INFO: Deleting pod "simpletest.rc-lhhwj" in namespace "gc-4239"
    Mar 16 10:02:45.756: INFO: Deleting pod "simpletest.rc-lqdq5" in namespace "gc-4239"
    Mar 16 10:02:45.850: INFO: Deleting pod "simpletest.rc-md7cf" in namespace "gc-4239"
    Mar 16 10:02:45.943: INFO: Deleting pod "simpletest.rc-mkpbt" in namespace "gc-4239"
    Mar 16 10:02:46.037: INFO: Deleting pod "simpletest.rc-mmg5g" in namespace "gc-4239"
    Mar 16 10:02:46.130: INFO: Deleting pod "simpletest.rc-mqlh8" in namespace "gc-4239"
    Mar 16 10:02:46.223: INFO: Deleting pod "simpletest.rc-n46wl" in namespace "gc-4239"
    Mar 16 10:02:46.316: INFO: Deleting pod "simpletest.rc-ndgw7" in namespace "gc-4239"
    Mar 16 10:02:46.410: INFO: Deleting pod "simpletest.rc-nlwpw" in namespace "gc-4239"
    Mar 16 10:02:46.503: INFO: Deleting pod "simpletest.rc-nnpqs" in namespace "gc-4239"
    Mar 16 10:02:46.598: INFO: Deleting pod "simpletest.rc-nrlgx" in namespace "gc-4239"
    Mar 16 10:02:46.691: INFO: Deleting pod "simpletest.rc-pd8hp" in namespace "gc-4239"
    Mar 16 10:02:46.784: INFO: Deleting pod "simpletest.rc-ph7x9" in namespace "gc-4239"
    Mar 16 10:02:46.876: INFO: Deleting pod "simpletest.rc-ptt7c" in namespace "gc-4239"
    Mar 16 10:02:46.969: INFO: Deleting pod "simpletest.rc-q7bbf" in namespace "gc-4239"
    Mar 16 10:02:47.063: INFO: Deleting pod "simpletest.rc-q8z79" in namespace "gc-4239"
    Mar 16 10:02:47.157: INFO: Deleting pod "simpletest.rc-qdggc" in namespace "gc-4239"
    Mar 16 10:02:47.250: INFO: Deleting pod "simpletest.rc-r79s5" in namespace "gc-4239"
    Mar 16 10:02:47.343: INFO: Deleting pod "simpletest.rc-r828w" in namespace "gc-4239"
    Mar 16 10:02:47.437: INFO: Deleting pod "simpletest.rc-rb84z" in namespace "gc-4239"
    Mar 16 10:02:47.531: INFO: Deleting pod "simpletest.rc-rlfvh" in namespace "gc-4239"
    Mar 16 10:02:47.623: INFO: Deleting pod "simpletest.rc-rm4tw" in namespace "gc-4239"
    Mar 16 10:02:47.716: INFO: Deleting pod "simpletest.rc-s2s4q" in namespace "gc-4239"
    Mar 16 10:02:47.809: INFO: Deleting pod "simpletest.rc-s4l52" in namespace "gc-4239"
    Mar 16 10:02:47.902: INFO: Deleting pod "simpletest.rc-scpfq" in namespace "gc-4239"
    Mar 16 10:02:47.996: INFO: Deleting pod "simpletest.rc-sf9g5" in namespace "gc-4239"
    Mar 16 10:02:48.093: INFO: Deleting pod "simpletest.rc-snms5" in namespace "gc-4239"
    Mar 16 10:02:48.187: INFO: Deleting pod "simpletest.rc-sv2bw" in namespace "gc-4239"
    Mar 16 10:02:48.281: INFO: Deleting pod "simpletest.rc-tdxft" in namespace "gc-4239"
    Mar 16 10:02:48.374: INFO: Deleting pod "simpletest.rc-tgm66" in namespace "gc-4239"
    Mar 16 10:02:48.466: INFO: Deleting pod "simpletest.rc-vbn7s" in namespace "gc-4239"
    Mar 16 10:02:48.559: INFO: Deleting pod "simpletest.rc-vd876" in namespace "gc-4239"
    Mar 16 10:02:48.651: INFO: Deleting pod "simpletest.rc-vj8gn" in namespace "gc-4239"
    Mar 16 10:02:48.743: INFO: Deleting pod "simpletest.rc-vxksd" in namespace "gc-4239"
    Mar 16 10:02:48.837: INFO: Deleting pod "simpletest.rc-w9rvk" in namespace "gc-4239"
    Mar 16 10:02:48.930: INFO: Deleting pod "simpletest.rc-wkfjx" in namespace "gc-4239"
    Mar 16 10:02:49.024: INFO: Deleting pod "simpletest.rc-wl845" in namespace "gc-4239"
    Mar 16 10:02:49.117: INFO: Deleting pod "simpletest.rc-xp4vp" in namespace "gc-4239"
    Mar 16 10:02:49.211: INFO: Deleting pod "simpletest.rc-xtb4n" in namespace "gc-4239"
    Mar 16 10:02:49.303: INFO: Deleting pod "simpletest.rc-xtlfp" in namespace "gc-4239"
    Mar 16 10:02:49.397: INFO: Deleting pod "simpletest.rc-zb7nl" in namespace "gc-4239"
    Mar 16 10:02:49.491: INFO: Deleting pod "simpletest.rc-zctgg" in namespace "gc-4239"
    Mar 16 10:02:49.584: INFO: Deleting pod "simpletest.rc-zhqbc" in namespace "gc-4239"
    Mar 16 10:02:49.677: INFO: Deleting pod "simpletest.rc-znqtz" in namespace "gc-4239"
    Mar 16 10:02:49.771: INFO: Deleting pod "simpletest.rc-zrdgj" in namespace "gc-4239"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:02:49.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4239" for this suite. 03/16/23 10:02:49.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:02:50.047
Mar 16 10:02:50.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:02:50.048
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:02:50.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:02:50.494
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-568a5f9f-31ee-4428-bf69-d27bdc82b01d 03/16/23 10:02:50.762
STEP: Creating secret with name s-test-opt-upd-adc5f335-3bb2-45a4-a3cd-1455d58a877d 03/16/23 10:02:50.852
STEP: Creating the pod 03/16/23 10:02:50.942
Mar 16 10:02:51.038: INFO: Waiting up to 5m0s for pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850" in namespace "secrets-1237" to be "running and ready"
Mar 16 10:02:51.128: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850": Phase="Pending", Reason="", readiness=false. Elapsed: 89.679597ms
Mar 16 10:02:51.128: INFO: The phase of Pod pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:02:53.219: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850": Phase="Running", Reason="", readiness=true. Elapsed: 2.180459197s
Mar 16 10:02:53.219: INFO: The phase of Pod pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850 is Running (Ready = true)
Mar 16 10:02:53.219: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-568a5f9f-31ee-4428-bf69-d27bdc82b01d 03/16/23 10:02:53.68
STEP: Updating secret s-test-opt-upd-adc5f335-3bb2-45a4-a3cd-1455d58a877d 03/16/23 10:02:53.771
STEP: Creating secret with name s-test-opt-create-58f36623-925c-45ac-b482-32ca6ba172d6 03/16/23 10:02:53.862
STEP: waiting to observe update in volume 03/16/23 10:02:53.952
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:02:58.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1237" for this suite. 03/16/23 10:02:58.607
------------------------------
• [8.651 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:02:50.047
    Mar 16 10:02:50.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:02:50.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:02:50.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:02:50.494
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-568a5f9f-31ee-4428-bf69-d27bdc82b01d 03/16/23 10:02:50.762
    STEP: Creating secret with name s-test-opt-upd-adc5f335-3bb2-45a4-a3cd-1455d58a877d 03/16/23 10:02:50.852
    STEP: Creating the pod 03/16/23 10:02:50.942
    Mar 16 10:02:51.038: INFO: Waiting up to 5m0s for pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850" in namespace "secrets-1237" to be "running and ready"
    Mar 16 10:02:51.128: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850": Phase="Pending", Reason="", readiness=false. Elapsed: 89.679597ms
    Mar 16 10:02:51.128: INFO: The phase of Pod pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:02:53.219: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850": Phase="Running", Reason="", readiness=true. Elapsed: 2.180459197s
    Mar 16 10:02:53.219: INFO: The phase of Pod pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850 is Running (Ready = true)
    Mar 16 10:02:53.219: INFO: Pod "pod-secrets-46ed6f5c-1aec-44f5-929b-8393f42d7850" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-568a5f9f-31ee-4428-bf69-d27bdc82b01d 03/16/23 10:02:53.68
    STEP: Updating secret s-test-opt-upd-adc5f335-3bb2-45a4-a3cd-1455d58a877d 03/16/23 10:02:53.771
    STEP: Creating secret with name s-test-opt-create-58f36623-925c-45ac-b482-32ca6ba172d6 03/16/23 10:02:53.862
    STEP: waiting to observe update in volume 03/16/23 10:02:53.952
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:02:58.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1237" for this suite. 03/16/23 10:02:58.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:02:58.698
Mar 16 10:02:58.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:02:58.699
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:02:58.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:02:59.145
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 03/16/23 10:02:59.323
STEP: Getting a ResourceQuota 03/16/23 10:02:59.413
STEP: Updating a ResourceQuota 03/16/23 10:02:59.503
STEP: Verifying a ResourceQuota was modified 03/16/23 10:02:59.594
STEP: Deleting a ResourceQuota 03/16/23 10:02:59.683
STEP: Verifying the deleted ResourceQuota 03/16/23 10:02:59.774
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:02:59.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3470" for this suite. 03/16/23 10:02:59.953
------------------------------
• [1.346 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:02:58.698
    Mar 16 10:02:58.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:02:58.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:02:58.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:02:59.145
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 03/16/23 10:02:59.323
    STEP: Getting a ResourceQuota 03/16/23 10:02:59.413
    STEP: Updating a ResourceQuota 03/16/23 10:02:59.503
    STEP: Verifying a ResourceQuota was modified 03/16/23 10:02:59.594
    STEP: Deleting a ResourceQuota 03/16/23 10:02:59.683
    STEP: Verifying the deleted ResourceQuota 03/16/23 10:02:59.774
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:02:59.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3470" for this suite. 03/16/23 10:02:59.953
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:00.044
Mar 16 10:03:00.044: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:03:00.045
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:00.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:00.495
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 16 10:03:00.857: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6231 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:01.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6231" for this suite. 03/16/23 10:03:01.127
------------------------------
• [1.173 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:00.044
    Mar 16 10:03:00.044: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:03:00.045
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:00.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:00.495
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 16 10:03:00.857: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6231 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:01.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6231" for this suite. 03/16/23 10:03:01.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:01.218
Mar 16 10:03:01.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 03/16/23 10:03:01.219
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:01.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:01.666
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 03/16/23 10:03:01.843
STEP: wait for the container to reach Succeeded 03/16/23 10:03:01.937
STEP: get the container status 03/16/23 10:03:06.389
STEP: the container should be terminated 03/16/23 10:03:06.479
STEP: the termination message should be set 03/16/23 10:03:06.479
Mar 16 10:03:06.479: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/16/23 10:03:06.479
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:06.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3181" for this suite. 03/16/23 10:03:06.84
------------------------------
• [5.713 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:01.218
    Mar 16 10:03:01.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 03/16/23 10:03:01.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:01.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:01.666
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 03/16/23 10:03:01.843
    STEP: wait for the container to reach Succeeded 03/16/23 10:03:01.937
    STEP: get the container status 03/16/23 10:03:06.389
    STEP: the container should be terminated 03/16/23 10:03:06.479
    STEP: the termination message should be set 03/16/23 10:03:06.479
    Mar 16 10:03:06.479: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/16/23 10:03:06.479
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:06.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3181" for this suite. 03/16/23 10:03:06.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:06.932
Mar 16 10:03:06.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 03/16/23 10:03:06.933
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:07.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:07.381
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/16/23 10:03:07.558
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-msm4 03/16/23 10:03:07.738
STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:03:07.739
Mar 16 10:03:07.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-msm4" in namespace "subpath-1779" to be "Succeeded or Failed"
Mar 16 10:03:07.924: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.643146ms
Mar 16 10:03:10.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 2.180106796s
Mar 16 10:03:12.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 4.179528903s
Mar 16 10:03:14.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 6.18118181s
Mar 16 10:03:16.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 8.180351198s
Mar 16 10:03:18.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 10.180241677s
Mar 16 10:03:20.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 12.17972085s
Mar 16 10:03:22.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 14.179660914s
Mar 16 10:03:24.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 16.181556539s
Mar 16 10:03:26.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 18.181373796s
Mar 16 10:03:28.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 20.180042684s
Mar 16 10:03:30.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=false. Elapsed: 22.179800464s
Mar 16 10:03:32.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.181256846s
STEP: Saw pod success 03/16/23 10:03:32.016
Mar 16 10:03:32.016: INFO: Pod "pod-subpath-test-secret-msm4" satisfied condition "Succeeded or Failed"
Mar 16 10:03:32.106: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-secret-msm4 container test-container-subpath-secret-msm4: <nil>
STEP: delete the pod 03/16/23 10:03:32.201
Mar 16 10:03:32.294: INFO: Waiting for pod pod-subpath-test-secret-msm4 to disappear
Mar 16 10:03:32.383: INFO: Pod pod-subpath-test-secret-msm4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-msm4 03/16/23 10:03:32.384
Mar 16 10:03:32.384: INFO: Deleting pod "pod-subpath-test-secret-msm4" in namespace "subpath-1779"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:32.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1779" for this suite. 03/16/23 10:03:32.652
------------------------------
• [25.811 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:06.932
    Mar 16 10:03:06.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 03/16/23 10:03:06.933
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:07.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:07.381
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/16/23 10:03:07.558
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-msm4 03/16/23 10:03:07.738
    STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:03:07.739
    Mar 16 10:03:07.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-msm4" in namespace "subpath-1779" to be "Succeeded or Failed"
    Mar 16 10:03:07.924: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.643146ms
    Mar 16 10:03:10.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 2.180106796s
    Mar 16 10:03:12.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 4.179528903s
    Mar 16 10:03:14.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 6.18118181s
    Mar 16 10:03:16.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 8.180351198s
    Mar 16 10:03:18.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 10.180241677s
    Mar 16 10:03:20.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 12.17972085s
    Mar 16 10:03:22.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 14.179660914s
    Mar 16 10:03:24.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 16.181556539s
    Mar 16 10:03:26.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 18.181373796s
    Mar 16 10:03:28.015: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=true. Elapsed: 20.180042684s
    Mar 16 10:03:30.014: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Running", Reason="", readiness=false. Elapsed: 22.179800464s
    Mar 16 10:03:32.016: INFO: Pod "pod-subpath-test-secret-msm4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.181256846s
    STEP: Saw pod success 03/16/23 10:03:32.016
    Mar 16 10:03:32.016: INFO: Pod "pod-subpath-test-secret-msm4" satisfied condition "Succeeded or Failed"
    Mar 16 10:03:32.106: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-secret-msm4 container test-container-subpath-secret-msm4: <nil>
    STEP: delete the pod 03/16/23 10:03:32.201
    Mar 16 10:03:32.294: INFO: Waiting for pod pod-subpath-test-secret-msm4 to disappear
    Mar 16 10:03:32.383: INFO: Pod pod-subpath-test-secret-msm4 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-msm4 03/16/23 10:03:32.384
    Mar 16 10:03:32.384: INFO: Deleting pod "pod-subpath-test-secret-msm4" in namespace "subpath-1779"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:32.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1779" for this suite. 03/16/23 10:03:32.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:32.743
Mar 16 10:03:32.744: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 03/16/23 10:03:32.745
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:33.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:33.192
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/16/23 10:03:33.374
STEP: Replace a pod template 03/16/23 10:03:33.466
Mar 16 10:03:33.646: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:33.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9449" for this suite. 03/16/23 10:03:33.737
------------------------------
• [1.084 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:32.743
    Mar 16 10:03:32.744: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 03/16/23 10:03:32.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:33.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:33.192
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/16/23 10:03:33.374
    STEP: Replace a pod template 03/16/23 10:03:33.466
    Mar 16 10:03:33.646: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:33.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9449" for this suite. 03/16/23 10:03:33.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:33.828
Mar 16 10:03:33.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ephemeral-containers-test 03/16/23 10:03:33.829
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:34.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:34.277
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/16/23 10:03:34.455
Mar 16 10:03:34.551: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5806" to be "running and ready"
Mar 16 10:03:34.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.679565ms
Mar 16 10:03:34.641: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:03:36.730: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179705114s
Mar 16 10:03:36.731: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 16 10:03:36.731: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/16/23 10:03:36.82
Mar 16 10:03:36.917: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5806" to be "container debugger running"
Mar 16 10:03:37.007: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.684434ms
Mar 16 10:03:39.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179906101s
Mar 16 10:03:41.098: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.181203924s
Mar 16 10:03:41.098: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/16/23 10:03:41.098
Mar 16 10:03:41.098: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5806 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:03:41.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:03:41.099: INFO: ExecWithOptions: Clientset creation
Mar 16 10:03:41.099: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-5806/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 16 10:03:41.868: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-5806" for this suite. 03/16/23 10:03:42.185
------------------------------
• [8.447 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:33.828
    Mar 16 10:03:33.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/16/23 10:03:33.829
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:34.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:34.277
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/16/23 10:03:34.455
    Mar 16 10:03:34.551: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5806" to be "running and ready"
    Mar 16 10:03:34.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.679565ms
    Mar 16 10:03:34.641: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:03:36.730: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179705114s
    Mar 16 10:03:36.731: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 16 10:03:36.731: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/16/23 10:03:36.82
    Mar 16 10:03:36.917: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5806" to be "container debugger running"
    Mar 16 10:03:37.007: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.684434ms
    Mar 16 10:03:39.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179906101s
    Mar 16 10:03:41.098: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.181203924s
    Mar 16 10:03:41.098: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/16/23 10:03:41.098
    Mar 16 10:03:41.098: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5806 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:03:41.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:03:41.099: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:03:41.099: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/ephemeral-containers-test-5806/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 16 10:03:41.868: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-5806" for this suite. 03/16/23 10:03:42.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:42.276
Mar 16 10:03:42.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 10:03:42.277
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:42.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:42.724
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 03/16/23 10:03:42.902
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:43.171
STEP: Creating a pod in the namespace 03/16/23 10:03:43.348
STEP: Waiting for the pod to have running status 03/16/23 10:03:43.442
Mar 16 10:03:43.443: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4535" to be "running"
Mar 16 10:03:43.533: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.916206ms
Mar 16 10:03:45.623: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180533871s
Mar 16 10:03:45.623: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/16/23 10:03:45.623
STEP: Waiting for the namespace to be removed. 03/16/23 10:03:45.714
STEP: Recreating the namespace 03/16/23 10:03:56.804
STEP: Verifying there are no pods in the namespace 03/16/23 10:03:57.074
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-169" for this suite. 03/16/23 10:03:57.341
STEP: Destroying namespace "nsdeletetest-4535" for this suite. 03/16/23 10:03:57.432
Mar 16 10:03:57.521: INFO: Namespace nsdeletetest-4535 was already deleted
STEP: Destroying namespace "nsdeletetest-3645" for this suite. 03/16/23 10:03:57.521
------------------------------
• [15.337 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:42.276
    Mar 16 10:03:42.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 10:03:42.277
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:42.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:42.724
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 03/16/23 10:03:42.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:43.171
    STEP: Creating a pod in the namespace 03/16/23 10:03:43.348
    STEP: Waiting for the pod to have running status 03/16/23 10:03:43.442
    Mar 16 10:03:43.443: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4535" to be "running"
    Mar 16 10:03:43.533: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.916206ms
    Mar 16 10:03:45.623: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180533871s
    Mar 16 10:03:45.623: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/16/23 10:03:45.623
    STEP: Waiting for the namespace to be removed. 03/16/23 10:03:45.714
    STEP: Recreating the namespace 03/16/23 10:03:56.804
    STEP: Verifying there are no pods in the namespace 03/16/23 10:03:57.074
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-169" for this suite. 03/16/23 10:03:57.341
    STEP: Destroying namespace "nsdeletetest-4535" for this suite. 03/16/23 10:03:57.432
    Mar 16 10:03:57.521: INFO: Namespace nsdeletetest-4535 was already deleted
    STEP: Destroying namespace "nsdeletetest-3645" for this suite. 03/16/23 10:03:57.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:57.614
Mar 16 10:03:57.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate 03/16/23 10:03:57.616
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:57.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:58.063
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 16 10:03:58.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4499" for this suite. 03/16/23 10:03:58.961
------------------------------
• [1.438 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:57.614
    Mar 16 10:03:57.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename podtemplate 03/16/23 10:03:57.616
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:57.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:58.063
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:03:58.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4499" for this suite. 03/16/23 10:03:58.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:03:59.053
Mar 16 10:03:59.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 03/16/23 10:03:59.054
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:59.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:59.5
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 03/16/23 10:03:59.678
STEP: getting /apis/discovery.k8s.io 03/16/23 10:03:59.855
STEP: getting /apis/discovery.k8s.iov1 03/16/23 10:03:59.944
STEP: creating 03/16/23 10:04:00.032
STEP: getting 03/16/23 10:04:00.303
STEP: listing 03/16/23 10:04:00.392
STEP: watching 03/16/23 10:04:00.482
Mar 16 10:04:00.482: INFO: starting watch
STEP: cluster-wide listing 03/16/23 10:04:00.57
STEP: cluster-wide watching 03/16/23 10:04:00.662
Mar 16 10:04:00.662: INFO: starting watch
STEP: patching 03/16/23 10:04:00.75
STEP: updating 03/16/23 10:04:00.841
Mar 16 10:04:01.021: INFO: waiting for watch events with expected annotations
Mar 16 10:04:01.021: INFO: saw patched and updated annotations
STEP: deleting 03/16/23 10:04:01.021
STEP: deleting a collection 03/16/23 10:04:01.29
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 16 10:04:01.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2869" for this suite. 03/16/23 10:04:01.562
------------------------------
• [2.600 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:03:59.053
    Mar 16 10:03:59.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 03/16/23 10:03:59.054
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:03:59.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:03:59.5
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 03/16/23 10:03:59.678
    STEP: getting /apis/discovery.k8s.io 03/16/23 10:03:59.855
    STEP: getting /apis/discovery.k8s.iov1 03/16/23 10:03:59.944
    STEP: creating 03/16/23 10:04:00.032
    STEP: getting 03/16/23 10:04:00.303
    STEP: listing 03/16/23 10:04:00.392
    STEP: watching 03/16/23 10:04:00.482
    Mar 16 10:04:00.482: INFO: starting watch
    STEP: cluster-wide listing 03/16/23 10:04:00.57
    STEP: cluster-wide watching 03/16/23 10:04:00.662
    Mar 16 10:04:00.662: INFO: starting watch
    STEP: patching 03/16/23 10:04:00.75
    STEP: updating 03/16/23 10:04:00.841
    Mar 16 10:04:01.021: INFO: waiting for watch events with expected annotations
    Mar 16 10:04:01.021: INFO: saw patched and updated annotations
    STEP: deleting 03/16/23 10:04:01.021
    STEP: deleting a collection 03/16/23 10:04:01.29
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:04:01.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2869" for this suite. 03/16/23 10:04:01.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:04:01.653
Mar 16 10:04:01.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress 03/16/23 10:04:01.654
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:04:01.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:04:02.102
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/16/23 10:04:02.28
STEP: getting /apis/networking.k8s.io 03/16/23 10:04:02.458
STEP: getting /apis/networking.k8s.iov1 03/16/23 10:04:02.546
STEP: creating 03/16/23 10:04:02.635
STEP: getting 03/16/23 10:04:02.905
STEP: listing 03/16/23 10:04:02.995
STEP: watching 03/16/23 10:04:03.085
Mar 16 10:04:03.086: INFO: starting watch
STEP: cluster-wide listing 03/16/23 10:04:03.174
STEP: cluster-wide watching 03/16/23 10:04:03.264
Mar 16 10:04:03.264: INFO: starting watch
STEP: patching 03/16/23 10:04:03.353
STEP: updating 03/16/23 10:04:03.443
Mar 16 10:04:03.623: INFO: waiting for watch events with expected annotations
Mar 16 10:04:03.623: INFO: saw patched and updated annotations
STEP: patching /status 03/16/23 10:04:03.623
STEP: updating /status 03/16/23 10:04:03.714
STEP: get /status 03/16/23 10:04:03.894
STEP: deleting 03/16/23 10:04:03.984
STEP: deleting a collection 03/16/23 10:04:04.253
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Mar 16 10:04:04.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-2443" for this suite. 03/16/23 10:04:04.527
------------------------------
• [2.964 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:04:01.653
    Mar 16 10:04:01.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingress 03/16/23 10:04:01.654
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:04:01.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:04:02.102
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/16/23 10:04:02.28
    STEP: getting /apis/networking.k8s.io 03/16/23 10:04:02.458
    STEP: getting /apis/networking.k8s.iov1 03/16/23 10:04:02.546
    STEP: creating 03/16/23 10:04:02.635
    STEP: getting 03/16/23 10:04:02.905
    STEP: listing 03/16/23 10:04:02.995
    STEP: watching 03/16/23 10:04:03.085
    Mar 16 10:04:03.086: INFO: starting watch
    STEP: cluster-wide listing 03/16/23 10:04:03.174
    STEP: cluster-wide watching 03/16/23 10:04:03.264
    Mar 16 10:04:03.264: INFO: starting watch
    STEP: patching 03/16/23 10:04:03.353
    STEP: updating 03/16/23 10:04:03.443
    Mar 16 10:04:03.623: INFO: waiting for watch events with expected annotations
    Mar 16 10:04:03.623: INFO: saw patched and updated annotations
    STEP: patching /status 03/16/23 10:04:03.623
    STEP: updating /status 03/16/23 10:04:03.714
    STEP: get /status 03/16/23 10:04:03.894
    STEP: deleting 03/16/23 10:04:03.984
    STEP: deleting a collection 03/16/23 10:04:04.253
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:04:04.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-2443" for this suite. 03/16/23 10:04:04.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:04:04.618
Mar 16 10:04:04.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:04:04.619
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:04:04.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:04:05.067
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-vcthq" 03/16/23 10:04:05.335
Mar 16 10:04:05.515: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard cpu limit of 500m
Mar 16 10:04:05.515: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-vcthq" /status 03/16/23 10:04:05.515
STEP: Confirm /status for "e2e-rq-status-vcthq" resourceQuota via watch 03/16/23 10:04:05.695
Mar 16 10:04:05.784: INFO: observed resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList(nil)
Mar 16 10:04:05.784: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 16 10:04:05.784: INFO: ResourceQuota "e2e-rq-status-vcthq" /status was updated
STEP: Patching hard spec values for cpu & memory 03/16/23 10:04:05.873
Mar 16 10:04:05.964: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard cpu limit of 1
Mar 16 10:04:05.964: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-vcthq" /status 03/16/23 10:04:05.964
STEP: Confirm /status for "e2e-rq-status-vcthq" resourceQuota via watch 03/16/23 10:04:06.054
Mar 16 10:04:06.143: INFO: observed resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 16 10:04:06.143: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Mar 16 10:04:06.143: INFO: ResourceQuota "e2e-rq-status-vcthq" /status was patched
STEP: Get "e2e-rq-status-vcthq" /status 03/16/23 10:04:06.143
Mar 16 10:04:06.234: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard cpu of 1
Mar 16 10:04:06.234: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-vcthq" /status before checking Spec is unchanged 03/16/23 10:04:06.323
Mar 16 10:04:06.413: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard cpu of 2
Mar 16 10:04:06.414: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard memory of 2Gi
Mar 16 10:04:06.502: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Mar 16 10:08:11.686: INFO: ResourceQuota "e2e-rq-status-vcthq" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:08:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-237" for this suite. 03/16/23 10:08:11.865
------------------------------
• [247.338 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:04:04.618
    Mar 16 10:04:04.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:04:04.619
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:04:04.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:04:05.067
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-vcthq" 03/16/23 10:04:05.335
    Mar 16 10:04:05.515: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard cpu limit of 500m
    Mar 16 10:04:05.515: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-vcthq" /status 03/16/23 10:04:05.515
    STEP: Confirm /status for "e2e-rq-status-vcthq" resourceQuota via watch 03/16/23 10:04:05.695
    Mar 16 10:04:05.784: INFO: observed resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList(nil)
    Mar 16 10:04:05.784: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 16 10:04:05.784: INFO: ResourceQuota "e2e-rq-status-vcthq" /status was updated
    STEP: Patching hard spec values for cpu & memory 03/16/23 10:04:05.873
    Mar 16 10:04:05.964: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard cpu limit of 1
    Mar 16 10:04:05.964: INFO: Resource quota "e2e-rq-status-vcthq" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-vcthq" /status 03/16/23 10:04:05.964
    STEP: Confirm /status for "e2e-rq-status-vcthq" resourceQuota via watch 03/16/23 10:04:06.054
    Mar 16 10:04:06.143: INFO: observed resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 16 10:04:06.143: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Mar 16 10:04:06.143: INFO: ResourceQuota "e2e-rq-status-vcthq" /status was patched
    STEP: Get "e2e-rq-status-vcthq" /status 03/16/23 10:04:06.143
    Mar 16 10:04:06.234: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard cpu of 1
    Mar 16 10:04:06.234: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-vcthq" /status before checking Spec is unchanged 03/16/23 10:04:06.323
    Mar 16 10:04:06.413: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard cpu of 2
    Mar 16 10:04:06.414: INFO: Resourcequota "e2e-rq-status-vcthq" reports status: hard memory of 2Gi
    Mar 16 10:04:06.502: INFO: Found resourceQuota "e2e-rq-status-vcthq" in namespace "resourcequota-237" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Mar 16 10:08:11.686: INFO: ResourceQuota "e2e-rq-status-vcthq" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:08:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-237" for this suite. 03/16/23 10:08:11.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:08:11.956
Mar 16 10:08:11.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 03/16/23 10:08:11.958
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:08:12.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:08:12.407
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 03/16/23 10:08:12.585
STEP: wait for the container to reach Succeeded 03/16/23 10:08:12.682
STEP: get the container status 03/16/23 10:08:16.044
STEP: the container should be terminated 03/16/23 10:08:16.134
STEP: the termination message should be set 03/16/23 10:08:16.135
Mar 16 10:08:16.135: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/16/23 10:08:16.135
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 16 10:08:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8017" for this suite. 03/16/23 10:08:16.496
------------------------------
• [4.630 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:08:11.956
    Mar 16 10:08:11.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 03/16/23 10:08:11.958
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:08:12.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:08:12.407
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 03/16/23 10:08:12.585
    STEP: wait for the container to reach Succeeded 03/16/23 10:08:12.682
    STEP: get the container status 03/16/23 10:08:16.044
    STEP: the container should be terminated 03/16/23 10:08:16.134
    STEP: the termination message should be set 03/16/23 10:08:16.135
    Mar 16 10:08:16.135: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/16/23 10:08:16.135
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:08:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8017" for this suite. 03/16/23 10:08:16.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:08:16.587
Mar 16 10:08:16.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 03/16/23 10:08:16.589
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:08:16.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:08:17.037
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/16/23 10:08:17.215
STEP: Ensuring a job is scheduled 03/16/23 10:08:17.306
STEP: Ensuring exactly one is scheduled 03/16/23 10:09:01.402
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/16/23 10:09:01.492
STEP: Ensuring the job is replaced with a new one 03/16/23 10:09:01.583
STEP: Removing cronjob 03/16/23 10:10:01.674
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 16 10:10:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-401" for this suite. 03/16/23 10:10:01.944
------------------------------
• [105.447 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:08:16.587
    Mar 16 10:08:16.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 03/16/23 10:08:16.589
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:08:16.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:08:17.037
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/16/23 10:08:17.215
    STEP: Ensuring a job is scheduled 03/16/23 10:08:17.306
    STEP: Ensuring exactly one is scheduled 03/16/23 10:09:01.402
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/16/23 10:09:01.492
    STEP: Ensuring the job is replaced with a new one 03/16/23 10:09:01.583
    STEP: Removing cronjob 03/16/23 10:10:01.674
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:10:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-401" for this suite. 03/16/23 10:10:01.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:10:02.035
Mar 16 10:10:02.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:10:02.037
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:02.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:02.486
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/16/23 10:10:02.664
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/16/23 10:10:02.754
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/16/23 10:10:02.754
STEP: creating a pod to probe DNS 03/16/23 10:10:02.754
STEP: submitting the pod to kubernetes 03/16/23 10:10:02.755
Mar 16 10:10:02.853: INFO: Waiting up to 15m0s for pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c" in namespace "dns-8790" to be "running"
Mar 16 10:10:02.943: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.066668ms
Mar 16 10:10:05.033: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.180672135s
Mar 16 10:10:05.034: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c" satisfied condition "running"
STEP: retrieving the pod 03/16/23 10:10:05.034
STEP: looking for the results for each expected name from probers 03/16/23 10:10:05.123
Mar 16 10:10:05.679: INFO: DNS probes using dns-8790/dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c succeeded

STEP: deleting the pod 03/16/23 10:10:05.679
STEP: deleting the test headless service 03/16/23 10:10:05.778
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:10:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8790" for this suite. 03/16/23 10:10:06.049
------------------------------
• [4.104 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:10:02.035
    Mar 16 10:10:02.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:10:02.037
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:02.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:02.486
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/16/23 10:10:02.664
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/16/23 10:10:02.754
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8790.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/16/23 10:10:02.754
    STEP: creating a pod to probe DNS 03/16/23 10:10:02.754
    STEP: submitting the pod to kubernetes 03/16/23 10:10:02.755
    Mar 16 10:10:02.853: INFO: Waiting up to 15m0s for pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c" in namespace "dns-8790" to be "running"
    Mar 16 10:10:02.943: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.066668ms
    Mar 16 10:10:05.033: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.180672135s
    Mar 16 10:10:05.034: INFO: Pod "dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 10:10:05.034
    STEP: looking for the results for each expected name from probers 03/16/23 10:10:05.123
    Mar 16 10:10:05.679: INFO: DNS probes using dns-8790/dns-test-458f3cb6-1535-407a-aa1a-25d8bd61db3c succeeded

    STEP: deleting the pod 03/16/23 10:10:05.679
    STEP: deleting the test headless service 03/16/23 10:10:05.778
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:10:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8790" for this suite. 03/16/23 10:10:06.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:10:06.14
Mar 16 10:10:06.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:10:06.141
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:06.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:06.589
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:10:06.767
Mar 16 10:10:06.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095" in namespace "projected-9607" to be "Succeeded or Failed"
Mar 16 10:10:06.952: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765491ms
Mar 16 10:10:09.042: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179844783s
Mar 16 10:10:11.043: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180054331s
STEP: Saw pod success 03/16/23 10:10:11.043
Mar 16 10:10:11.043: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095" satisfied condition "Succeeded or Failed"
Mar 16 10:10:11.132: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 container client-container: <nil>
STEP: delete the pod 03/16/23 10:10:11.237
Mar 16 10:10:11.331: INFO: Waiting for pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 to disappear
Mar 16 10:10:11.421: INFO: Pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:10:11.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9607" for this suite. 03/16/23 10:10:11.6
------------------------------
• [5.551 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:10:06.14
    Mar 16 10:10:06.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:10:06.141
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:06.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:06.589
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:10:06.767
    Mar 16 10:10:06.862: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095" in namespace "projected-9607" to be "Succeeded or Failed"
    Mar 16 10:10:06.952: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765491ms
    Mar 16 10:10:09.042: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179844783s
    Mar 16 10:10:11.043: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180054331s
    STEP: Saw pod success 03/16/23 10:10:11.043
    Mar 16 10:10:11.043: INFO: Pod "downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095" satisfied condition "Succeeded or Failed"
    Mar 16 10:10:11.132: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:10:11.237
    Mar 16 10:10:11.331: INFO: Waiting for pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 to disappear
    Mar 16 10:10:11.421: INFO: Pod downwardapi-volume-27ac72c8-ea50-46db-819c-8f6663595095 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:10:11.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9607" for this suite. 03/16/23 10:10:11.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:10:11.692
Mar 16 10:10:11.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 10:10:11.693
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:11.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:12.142
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1573 03/16/23 10:10:12.32
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 03/16/23 10:10:12.411
STEP: Creating stateful set ss in namespace statefulset-1573 03/16/23 10:10:12.501
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1573 03/16/23 10:10:12.592
Mar 16 10:10:12.682: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 10:10:22.773: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/16/23 10:10:22.773
Mar 16 10:10:22.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:10:23.965: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:10:23.965: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:10:23.965: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:10:24.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 16 10:10:34.147: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:10:34.147: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:10:34.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999479s
Mar 16 10:10:35.598: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.908640381s
Mar 16 10:10:36.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.818605089s
Mar 16 10:10:37.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.727385024s
Mar 16 10:10:38.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.637180241s
Mar 16 10:10:39.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.547014819s
Mar 16 10:10:41.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.456500598s
Mar 16 10:10:42.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.36632713s
Mar 16 10:10:43.232: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.275451935s
Mar 16 10:10:44.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 185.143866ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1573 03/16/23 10:10:45.322
Mar 16 10:10:45.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:10:46.491: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 10:10:46.491: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:10:46.491: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:10:46.581: INFO: Found 1 stateful pods, waiting for 3
Mar 16 10:10:56.675: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:10:56.675: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:10:56.675: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/16/23 10:10:56.675
STEP: Scale down will halt with unhealthy stateful pod 03/16/23 10:10:56.675
Mar 16 10:10:56.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:10:58.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:10:58.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:10:58.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:10:58.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:10:59.108: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:10:59.108: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:10:59.108: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:10:59.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:11:00.219: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:11:00.219: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:11:00.219: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:11:00.219: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:11:00.308: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 16 10:11:10.489: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:11:10.489: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:11:10.489: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:11:10.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999956s
Mar 16 10:11:11.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.908847958s
Mar 16 10:11:12.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.818278796s
Mar 16 10:11:14.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.726959943s
Mar 16 10:11:15.124: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.635760426s
Mar 16 10:11:16.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.545167362s
Mar 16 10:11:17.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.453815562s
Mar 16 10:11:18.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.362467752s
Mar 16 10:11:19.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.271178276s
Mar 16 10:11:20.579: INFO: Verifying statefulset ss doesn't scale past 3 for another 180.61282ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 03/16/23 10:11:21.579
Mar 16 10:11:21.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:11:22.787: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 10:11:22.787: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:11:22.787: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:11:22.787: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:11:23.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 10:11:23.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:11:23.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:11:23.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:11:24.634: INFO: rc: 1
Mar 16 10:11:24.634: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 16 10:11:34.636: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:11:35.124: INFO: rc: 1
Mar 16 10:11:35.125: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:11:45.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:11:55.559: INFO: rc: 1
Mar 16 10:11:55.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:05.560: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:06.013: INFO: rc: 1
Mar 16 10:12:06.014: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:16.017: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:16.465: INFO: rc: 1
Mar 16 10:12:16.465: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:26.469: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:26.918: INFO: rc: 1
Mar 16 10:12:26.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:36.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:37.355: INFO: rc: 1
Mar 16 10:12:37.355: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:47.355: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:47.796: INFO: rc: 1
Mar 16 10:12:47.796: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:12:57.797: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:12:58.242: INFO: rc: 1
Mar 16 10:12:58.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:13:08.243: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:13:08.691: INFO: rc: 1
Mar 16 10:13:08.691: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:13:18.692: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:13:19.130: INFO: rc: 1
Mar 16 10:13:19.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:13:29.132: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:13:29.567: INFO: rc: 1
Mar 16 10:13:29.567: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:13:39.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:13:40.010: INFO: rc: 1
Mar 16 10:13:40.010: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:13:50.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:13:50.449: INFO: rc: 1
Mar 16 10:13:50.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:00.452: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:00.892: INFO: rc: 1
Mar 16 10:14:00.892: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:10.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:11.328: INFO: rc: 1
Mar 16 10:14:11.328: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:21.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:21.768: INFO: rc: 1
Mar 16 10:14:21.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:31.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:32.218: INFO: rc: 1
Mar 16 10:14:32.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:42.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:42.653: INFO: rc: 1
Mar 16 10:14:42.653: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:14:52.655: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:14:53.095: INFO: rc: 1
Mar 16 10:14:53.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:15:03.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:03.530: INFO: rc: 1
Mar 16 10:15:03.531: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
------------------------------
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m0.72s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m0.001s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 (Step Runtime: 3m50.832s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 11352 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc004ad7ad0, 0x10}, {0xc004ad7a9c, 0x4}, {0xc006fa8680, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x8022ee8?, 0xc0041281a0?}, 0xc00055de20?, {0xc006fa8680, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x8022ee8, 0xc0041281a0}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func10.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x7fbcad8, 0xc002a25320})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Mar 16 10:15:13.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:13.967: INFO: rc: 1
Mar 16 10:15:13.967: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:15:23.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:24.411: INFO: rc: 1
Mar 16 10:15:24.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m20.722s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m20.003s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 (Step Runtime: 4m10.834s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 11352 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc004ad7ad0, 0x10}, {0xc004ad7a9c, 0x4}, {0xc006fa8680, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x8022ee8?, 0xc0041281a0?}, 0xc00055de20?, {0xc006fa8680, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x8022ee8, 0xc0041281a0}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func10.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x7fbcad8, 0xc002a25320})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Mar 16 10:15:34.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:34.860: INFO: rc: 1
Mar 16 10:15:34.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:15:44.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:45.307: INFO: rc: 1
Mar 16 10:15:45.307: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m40.724s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 5m40.005s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 (Step Runtime: 4m30.836s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 11352 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc004ad7ad0, 0x10}, {0xc004ad7a9c, 0x4}, {0xc006fa8680, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x8022ee8?, 0xc0041281a0?}, 0xc00055de20?, {0xc006fa8680, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x8022ee8, 0xc0041281a0}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func10.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x7fbcad8, 0xc002a25320})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Mar 16 10:15:55.309: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:15:55.740: INFO: rc: 1
Mar 16 10:15:55.740: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:16:05.744: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:16:06.179: INFO: rc: 1
Mar 16 10:16:06.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Automatically polling progress:
  [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 6m0.726s)
    test/e2e/apps/statefulset.go:587
    In [It] (Node Runtime: 6m0.007s)
      test/e2e/apps/statefulset.go:587
      At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 (Step Runtime: 4m50.838s)
        test/e2e/apps/statefulset.go:683

      Spec Goroutine
      goroutine 11352 [sleep]
        time.Sleep(0x2540be400)
          /usr/local/go/src/runtime/time.go:195
        k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc004ad7ad0, 0x10}, {0xc004ad7a9c, 0x4}, {0xc006fa8680, 0x38}, 0x3?, 0x45d964b800)
          test/e2e/framework/pod/output/output.go:113
        k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x8022ee8?, 0xc0041281a0?}, 0xc00055de20?, {0xc006fa8680, 0x38})
          test/e2e/framework/statefulset/rest.go:240
      > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x8022ee8, 0xc0041281a0}, 0x0?)
          test/e2e/apps/statefulset.go:1728
      > k8s.io/kubernetes/test/e2e/apps.glob..func10.2.10()
          test/e2e/apps/statefulset.go:684
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0x7fbcad8, 0xc002a25320})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Mar 16 10:16:16.182: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:16:16.622: INFO: rc: 1
Mar 16 10:16:16.622: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 16 10:16:26.625: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:16:27.064: INFO: rc: 1
Mar 16 10:16:27.064: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar 16 10:16:27.064: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/16/23 10:16:27.335
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 10:16:27.335: INFO: Deleting all statefulset in ns statefulset-1573
Mar 16 10:16:27.425: INFO: Scaling statefulset ss to 0
Mar 16 10:16:27.696: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:16:27.788: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:16:28.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1573" for this suite. 03/16/23 10:16:28.236
• [SLOW TEST] [376.637 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:10:11.692
    Mar 16 10:10:11.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 10:10:11.693
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:10:11.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:10:12.142
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1573 03/16/23 10:10:12.32
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/16/23 10:10:12.411
    STEP: Creating stateful set ss in namespace statefulset-1573 03/16/23 10:10:12.501
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1573 03/16/23 10:10:12.592
    Mar 16 10:10:12.682: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 10:10:22.773: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/16/23 10:10:22.773
    Mar 16 10:10:22.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:10:23.965: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:10:23.965: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:10:23.965: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:10:24.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 16 10:10:34.147: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:10:34.147: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:10:34.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999479s
    Mar 16 10:10:35.598: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.908640381s
    Mar 16 10:10:36.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.818605089s
    Mar 16 10:10:37.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.727385024s
    Mar 16 10:10:38.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.637180241s
    Mar 16 10:10:39.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.547014819s
    Mar 16 10:10:41.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.456500598s
    Mar 16 10:10:42.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.36632713s
    Mar 16 10:10:43.232: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.275451935s
    Mar 16 10:10:44.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 185.143866ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1573 03/16/23 10:10:45.322
    Mar 16 10:10:45.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:10:46.491: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 10:10:46.491: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:10:46.491: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:10:46.581: INFO: Found 1 stateful pods, waiting for 3
    Mar 16 10:10:56.675: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:10:56.675: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:10:56.675: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/16/23 10:10:56.675
    STEP: Scale down will halt with unhealthy stateful pod 03/16/23 10:10:56.675
    Mar 16 10:10:56.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:10:58.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:10:58.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:10:58.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:10:58.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:10:59.108: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:10:59.108: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:10:59.108: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:10:59.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:11:00.219: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:11:00.219: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:11:00.219: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:11:00.219: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:11:00.308: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Mar 16 10:11:10.489: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:11:10.489: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:11:10.489: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:11:10.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999956s
    Mar 16 10:11:11.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.908847958s
    Mar 16 10:11:12.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.818278796s
    Mar 16 10:11:14.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.726959943s
    Mar 16 10:11:15.124: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.635760426s
    Mar 16 10:11:16.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.545167362s
    Mar 16 10:11:17.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.453815562s
    Mar 16 10:11:18.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.362467752s
    Mar 16 10:11:19.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.271178276s
    Mar 16 10:11:20.579: INFO: Verifying statefulset ss doesn't scale past 3 for another 180.61282ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1573 03/16/23 10:11:21.579
    Mar 16 10:11:21.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:11:22.787: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 10:11:22.787: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:11:22.787: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:11:22.787: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:11:23.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 10:11:23.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:11:23.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:11:23.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:11:24.634: INFO: rc: 1
    Mar 16 10:11:24.634: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: unable to upgrade connection: container not found ("webserver")

    error:
    exit status 1
    Mar 16 10:11:34.636: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:11:35.124: INFO: rc: 1
    Mar 16 10:11:35.125: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:11:45.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:11:55.559: INFO: rc: 1
    Mar 16 10:11:55.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:05.560: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:06.013: INFO: rc: 1
    Mar 16 10:12:06.014: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:16.017: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:16.465: INFO: rc: 1
    Mar 16 10:12:16.465: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:26.469: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:26.918: INFO: rc: 1
    Mar 16 10:12:26.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:36.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:37.355: INFO: rc: 1
    Mar 16 10:12:37.355: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:47.355: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:47.796: INFO: rc: 1
    Mar 16 10:12:47.796: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:12:57.797: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:12:58.242: INFO: rc: 1
    Mar 16 10:12:58.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:13:08.243: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:13:08.691: INFO: rc: 1
    Mar 16 10:13:08.691: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:13:18.692: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:13:19.130: INFO: rc: 1
    Mar 16 10:13:19.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:13:29.132: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:13:29.567: INFO: rc: 1
    Mar 16 10:13:29.567: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:13:39.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:13:40.010: INFO: rc: 1
    Mar 16 10:13:40.010: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:13:50.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:13:50.449: INFO: rc: 1
    Mar 16 10:13:50.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:00.452: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:00.892: INFO: rc: 1
    Mar 16 10:14:00.892: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:10.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:11.328: INFO: rc: 1
    Mar 16 10:14:11.328: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:21.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:21.768: INFO: rc: 1
    Mar 16 10:14:21.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:31.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:32.218: INFO: rc: 1
    Mar 16 10:14:32.218: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:42.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:42.653: INFO: rc: 1
    Mar 16 10:14:42.653: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:14:52.655: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:14:53.095: INFO: rc: 1
    Mar 16 10:14:53.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:03.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:03.530: INFO: rc: 1
    Mar 16 10:15:03.531: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:13.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:13.967: INFO: rc: 1
    Mar 16 10:15:13.967: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:23.969: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:24.411: INFO: rc: 1
    Mar 16 10:15:24.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:34.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:34.860: INFO: rc: 1
    Mar 16 10:15:34.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:44.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:45.307: INFO: rc: 1
    Mar 16 10:15:45.307: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:15:55.309: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:15:55.740: INFO: rc: 1
    Mar 16 10:15:55.740: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:16:05.744: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:16:06.179: INFO: rc: 1
    Mar 16 10:16:06.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:16:16.182: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:16:16.622: INFO: rc: 1
    Mar 16 10:16:16.622: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 16 10:16:26.625: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1573 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:16:27.064: INFO: rc: 1
    Mar 16 10:16:27.064: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Mar 16 10:16:27.064: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/16/23 10:16:27.335
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 10:16:27.335: INFO: Deleting all statefulset in ns statefulset-1573
    Mar 16 10:16:27.425: INFO: Scaling statefulset ss to 0
    Mar 16 10:16:27.696: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:16:27.788: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:16:28.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1573" for this suite. 03/16/23 10:16:28.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:16:28.33
Mar 16 10:16:28.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 10:16:28.331
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:28.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:28.779
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9122 03/16/23 10:16:28.957
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9122 03/16/23 10:16:29.048
Mar 16 10:16:29.229: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 10:16:39.320: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/16/23 10:16:39.5
STEP: updating a scale subresource 03/16/23 10:16:39.59
STEP: verifying the statefulset Spec.Replicas was modified 03/16/23 10:16:39.681
STEP: Patch a scale subresource 03/16/23 10:16:39.771
STEP: verifying the statefulset Spec.Replicas was modified 03/16/23 10:16:39.863
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 10:16:39.953: INFO: Deleting all statefulset in ns statefulset-9122
Mar 16 10:16:40.043: INFO: Scaling statefulset ss to 0
Mar 16 10:16:50.405: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:16:50.497: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:16:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9122" for this suite. 03/16/23 10:16:50.945
------------------------------
• [22.706 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:16:28.33
    Mar 16 10:16:28.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 10:16:28.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:28.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:28.779
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9122 03/16/23 10:16:28.957
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9122 03/16/23 10:16:29.048
    Mar 16 10:16:29.229: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 10:16:39.320: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/16/23 10:16:39.5
    STEP: updating a scale subresource 03/16/23 10:16:39.59
    STEP: verifying the statefulset Spec.Replicas was modified 03/16/23 10:16:39.681
    STEP: Patch a scale subresource 03/16/23 10:16:39.771
    STEP: verifying the statefulset Spec.Replicas was modified 03/16/23 10:16:39.863
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 10:16:39.953: INFO: Deleting all statefulset in ns statefulset-9122
    Mar 16 10:16:40.043: INFO: Scaling statefulset ss to 0
    Mar 16 10:16:50.405: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:16:50.497: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:16:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9122" for this suite. 03/16/23 10:16:50.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:16:51.037
Mar 16 10:16:51.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:16:51.038
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:51.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:51.486
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-5892/secret-test-abec1c7c-2ba8-413b-aa03-d6cc4249ba3b 03/16/23 10:16:51.664
STEP: Creating a pod to test consume secrets 03/16/23 10:16:51.755
Mar 16 10:16:51.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3" in namespace "secrets-5892" to be "Succeeded or Failed"
Mar 16 10:16:51.943: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Pending", Reason="", readiness=false. Elapsed: 90.029019ms
Mar 16 10:16:54.034: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Running", Reason="", readiness=false. Elapsed: 2.181154739s
Mar 16 10:16:56.035: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181905559s
STEP: Saw pod success 03/16/23 10:16:56.035
Mar 16 10:16:56.035: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3" satisfied condition "Succeeded or Failed"
Mar 16 10:16:56.125: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 container env-test: <nil>
STEP: delete the pod 03/16/23 10:16:56.222
Mar 16 10:16:56.315: INFO: Waiting for pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 to disappear
Mar 16 10:16:56.405: INFO: Pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:16:56.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5892" for this suite. 03/16/23 10:16:56.583
------------------------------
• [5.638 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:16:51.037
    Mar 16 10:16:51.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:16:51.038
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:51.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:51.486
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-5892/secret-test-abec1c7c-2ba8-413b-aa03-d6cc4249ba3b 03/16/23 10:16:51.664
    STEP: Creating a pod to test consume secrets 03/16/23 10:16:51.755
    Mar 16 10:16:51.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3" in namespace "secrets-5892" to be "Succeeded or Failed"
    Mar 16 10:16:51.943: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Pending", Reason="", readiness=false. Elapsed: 90.029019ms
    Mar 16 10:16:54.034: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Running", Reason="", readiness=false. Elapsed: 2.181154739s
    Mar 16 10:16:56.035: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181905559s
    STEP: Saw pod success 03/16/23 10:16:56.035
    Mar 16 10:16:56.035: INFO: Pod "pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3" satisfied condition "Succeeded or Failed"
    Mar 16 10:16:56.125: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 container env-test: <nil>
    STEP: delete the pod 03/16/23 10:16:56.222
    Mar 16 10:16:56.315: INFO: Waiting for pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 to disappear
    Mar 16 10:16:56.405: INFO: Pod pod-configmaps-0bdf636e-0f6f-4230-a7a2-f47bc62783a3 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:16:56.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5892" for this suite. 03/16/23 10:16:56.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:16:56.675
Mar 16 10:16:56.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 10:16:56.676
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:56.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:57.125
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-pvhkk" 03/16/23 10:16:57.303
Mar 16 10:16:57.579: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-pvhkk-2293" 03/16/23 10:16:57.579
Mar 16 10:16:57.759: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-pvhkk-2293" 03/16/23 10:16:57.759
Mar 16 10:16:57.940: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:16:57.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6797" for this suite. 03/16/23 10:16:58.03
STEP: Destroying namespace "e2e-ns-pvhkk-2293" for this suite. 03/16/23 10:16:58.121
------------------------------
• [1.536 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:16:56.675
    Mar 16 10:16:56.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 10:16:56.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:56.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:57.125
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-pvhkk" 03/16/23 10:16:57.303
    Mar 16 10:16:57.579: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-pvhkk-2293" 03/16/23 10:16:57.579
    Mar 16 10:16:57.759: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-pvhkk-2293" 03/16/23 10:16:57.759
    Mar 16 10:16:57.940: INFO: Namespace "e2e-ns-pvhkk-2293" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:16:57.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6797" for this suite. 03/16/23 10:16:58.03
    STEP: Destroying namespace "e2e-ns-pvhkk-2293" for this suite. 03/16/23 10:16:58.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:16:58.213
Mar 16 10:16:58.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:16:58.214
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:58.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:58.665
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-40ad1080-9004-497e-978e-023261e01aed 03/16/23 10:16:58.843
STEP: Creating secret with name secret-projected-all-test-volume-10bf547b-db05-4e59-b945-d0feb0f5698d 03/16/23 10:16:58.934
STEP: Creating a pod to test Check all projections for projected volume plugin 03/16/23 10:16:59.024
Mar 16 10:16:59.119: INFO: Waiting up to 5m0s for pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8" in namespace "projected-9096" to be "Succeeded or Failed"
Mar 16 10:16:59.209: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.844229ms
Mar 16 10:17:01.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180973854s
Mar 16 10:17:03.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180348939s
STEP: Saw pod success 03/16/23 10:17:03.3
Mar 16 10:17:03.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8" satisfied condition "Succeeded or Failed"
Mar 16 10:17:03.390: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 container projected-all-volume-test: <nil>
STEP: delete the pod 03/16/23 10:17:03.53
Mar 16 10:17:03.625: INFO: Waiting for pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 to disappear
Mar 16 10:17:03.714: INFO: Pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9096" for this suite. 03/16/23 10:17:03.893
------------------------------
• [5.771 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:16:58.213
    Mar 16 10:16:58.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:16:58.214
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:16:58.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:16:58.665
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-40ad1080-9004-497e-978e-023261e01aed 03/16/23 10:16:58.843
    STEP: Creating secret with name secret-projected-all-test-volume-10bf547b-db05-4e59-b945-d0feb0f5698d 03/16/23 10:16:58.934
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/16/23 10:16:59.024
    Mar 16 10:16:59.119: INFO: Waiting up to 5m0s for pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8" in namespace "projected-9096" to be "Succeeded or Failed"
    Mar 16 10:16:59.209: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.844229ms
    Mar 16 10:17:01.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180973854s
    Mar 16 10:17:03.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180348939s
    STEP: Saw pod success 03/16/23 10:17:03.3
    Mar 16 10:17:03.300: INFO: Pod "projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8" satisfied condition "Succeeded or Failed"
    Mar 16 10:17:03.390: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 container projected-all-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:17:03.53
    Mar 16 10:17:03.625: INFO: Waiting for pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 to disappear
    Mar 16 10:17:03.714: INFO: Pod projected-volume-52e415f3-4bae-41e9-bf50-14398900ecc8 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9096" for this suite. 03/16/23 10:17:03.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:03.984
Mar 16 10:17:03.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:17:03.985
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:04.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:04.433
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:17:04.612
Mar 16 10:17:04.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d" in namespace "downward-api-8780" to be "Succeeded or Failed"
Mar 16 10:17:04.796: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.613455ms
Mar 16 10:17:06.886: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180040798s
Mar 16 10:17:08.887: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180391014s
STEP: Saw pod success 03/16/23 10:17:08.887
Mar 16 10:17:08.887: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d" satisfied condition "Succeeded or Failed"
Mar 16 10:17:08.977: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d container client-container: <nil>
STEP: delete the pod 03/16/23 10:17:09.071
Mar 16 10:17:09.167: INFO: Waiting for pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d to disappear
Mar 16 10:17:09.256: INFO: Pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:09.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8780" for this suite. 03/16/23 10:17:09.434
------------------------------
• [5.540 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:03.984
    Mar 16 10:17:03.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:17:03.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:04.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:04.433
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:17:04.612
    Mar 16 10:17:04.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d" in namespace "downward-api-8780" to be "Succeeded or Failed"
    Mar 16 10:17:04.796: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.613455ms
    Mar 16 10:17:06.886: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180040798s
    Mar 16 10:17:08.887: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180391014s
    STEP: Saw pod success 03/16/23 10:17:08.887
    Mar 16 10:17:08.887: INFO: Pod "downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d" satisfied condition "Succeeded or Failed"
    Mar 16 10:17:08.977: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d container client-container: <nil>
    STEP: delete the pod 03/16/23 10:17:09.071
    Mar 16 10:17:09.167: INFO: Waiting for pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d to disappear
    Mar 16 10:17:09.256: INFO: Pod downwardapi-volume-2b93f028-32f5-4fd4-bb5a-c4689098ff0d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:09.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8780" for this suite. 03/16/23 10:17:09.434
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:09.525
Mar 16 10:17:09.525: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 10:17:09.526
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:09.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:09.973
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/16/23 10:17:10.151
STEP: Verify that the required pods have come up 03/16/23 10:17:10.242
Mar 16 10:17:10.332: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/16/23 10:17:10.332
Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-wj4wn" in namespace "replicaset-2778" to be "running"
Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-2jk8v" in namespace "replicaset-2778" to be "running"
Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-8dhrf" in namespace "replicaset-2778" to be "running"
Mar 16 10:17:10.422: INFO: Pod "test-rs-wj4wn": Phase="Pending", Reason="", readiness=false. Elapsed: 89.702045ms
Mar 16 10:17:10.422: INFO: Pod "test-rs-2jk8v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.647767ms
Mar 16 10:17:10.422: INFO: Pod "test-rs-8dhrf": Phase="Pending", Reason="", readiness=false. Elapsed: 89.625658ms
Mar 16 10:17:12.512: INFO: Pod "test-rs-2jk8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.180106455s
Mar 16 10:17:12.513: INFO: Pod "test-rs-2jk8v" satisfied condition "running"
Mar 16 10:17:12.512: INFO: Pod "test-rs-8dhrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.180084387s
Mar 16 10:17:12.513: INFO: Pod "test-rs-8dhrf" satisfied condition "running"
Mar 16 10:17:12.512: INFO: Pod "test-rs-wj4wn": Phase="Running", Reason="", readiness=true. Elapsed: 2.180175529s
Mar 16 10:17:12.513: INFO: Pod "test-rs-wj4wn" satisfied condition "running"
Mar 16 10:17:12.603: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/16/23 10:17:12.603
STEP: DeleteCollection of the ReplicaSets 03/16/23 10:17:12.694
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/16/23 10:17:12.785
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2778" for this suite. 03/16/23 10:17:13.054
------------------------------
• [3.620 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:09.525
    Mar 16 10:17:09.525: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 10:17:09.526
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:09.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:09.973
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/16/23 10:17:10.151
    STEP: Verify that the required pods have come up 03/16/23 10:17:10.242
    Mar 16 10:17:10.332: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/16/23 10:17:10.332
    Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-wj4wn" in namespace "replicaset-2778" to be "running"
    Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-2jk8v" in namespace "replicaset-2778" to be "running"
    Mar 16 10:17:10.332: INFO: Waiting up to 5m0s for pod "test-rs-8dhrf" in namespace "replicaset-2778" to be "running"
    Mar 16 10:17:10.422: INFO: Pod "test-rs-wj4wn": Phase="Pending", Reason="", readiness=false. Elapsed: 89.702045ms
    Mar 16 10:17:10.422: INFO: Pod "test-rs-2jk8v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.647767ms
    Mar 16 10:17:10.422: INFO: Pod "test-rs-8dhrf": Phase="Pending", Reason="", readiness=false. Elapsed: 89.625658ms
    Mar 16 10:17:12.512: INFO: Pod "test-rs-2jk8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.180106455s
    Mar 16 10:17:12.513: INFO: Pod "test-rs-2jk8v" satisfied condition "running"
    Mar 16 10:17:12.512: INFO: Pod "test-rs-8dhrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.180084387s
    Mar 16 10:17:12.513: INFO: Pod "test-rs-8dhrf" satisfied condition "running"
    Mar 16 10:17:12.512: INFO: Pod "test-rs-wj4wn": Phase="Running", Reason="", readiness=true. Elapsed: 2.180175529s
    Mar 16 10:17:12.513: INFO: Pod "test-rs-wj4wn" satisfied condition "running"
    Mar 16 10:17:12.603: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/16/23 10:17:12.603
    STEP: DeleteCollection of the ReplicaSets 03/16/23 10:17:12.694
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/16/23 10:17:12.785
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2778" for this suite. 03/16/23 10:17:13.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:13.147
Mar 16 10:17:13.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 03/16/23 10:17:13.148
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:13.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:13.595
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 03/16/23 10:17:13.774
Mar 16 10:17:13.774: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:17.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-252" for this suite. 03/16/23 10:17:17.838
------------------------------
• [4.781 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:13.147
    Mar 16 10:17:13.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 03/16/23 10:17:13.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:13.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:13.595
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 03/16/23 10:17:13.774
    Mar 16 10:17:13.774: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:17.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-252" for this suite. 03/16/23 10:17:17.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:17.929
Mar 16 10:17:17.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/16/23 10:17:17.931
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:18.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:18.383
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/16/23 10:17:18.57
STEP: Creating hostNetwork=false pod 03/16/23 10:17:18.57
Mar 16 10:17:18.665: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5158" to be "running and ready"
Mar 16 10:17:18.755: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824481ms
Mar 16 10:17:18.755: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:17:20.846: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180745921s
Mar 16 10:17:20.846: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 16 10:17:20.846: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/16/23 10:17:20.937
Mar 16 10:17:21.031: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5158" to be "running and ready"
Mar 16 10:17:21.121: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.897171ms
Mar 16 10:17:21.121: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:17:23.213: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.181736936s
Mar 16 10:17:23.213: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 16 10:17:23.213: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/16/23 10:17:23.302
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/16/23 10:17:23.303
Mar 16 10:17:23.303: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:23.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:23.303: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:23.303: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 16 10:17:24.102: INFO: Exec stderr: ""
Mar 16 10:17:24.102: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:24.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:24.103: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:24.103: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 16 10:17:24.846: INFO: Exec stderr: ""
Mar 16 10:17:24.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:24.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:24.847: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:24.847: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 16 10:17:25.591: INFO: Exec stderr: ""
Mar 16 10:17:25.591: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:25.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:25.592: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:25.592: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 16 10:17:26.339: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/16/23 10:17:26.339
Mar 16 10:17:26.339: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:26.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:26.340: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:26.340: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 16 10:17:27.094: INFO: Exec stderr: ""
Mar 16 10:17:27.094: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:27.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:27.095: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:27.095: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 16 10:17:27.834: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/16/23 10:17:27.834
Mar 16 10:17:27.834: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:27.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:27.835: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:27.835: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 16 10:17:28.592: INFO: Exec stderr: ""
Mar 16 10:17:28.592: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:28.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:28.593: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:28.593: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 16 10:17:29.332: INFO: Exec stderr: ""
Mar 16 10:17:29.332: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:29.332: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:29.333: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:29.333: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 16 10:17:30.056: INFO: Exec stderr: ""
Mar 16 10:17:30.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:17:30.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:17:30.057: INFO: ExecWithOptions: Clientset creation
Mar 16 10:17:30.057: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 16 10:17:30.795: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5158" for this suite. 03/16/23 10:17:30.974
------------------------------
• [13.135 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:17.929
    Mar 16 10:17:17.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/16/23 10:17:17.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:18.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:18.383
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/16/23 10:17:18.57
    STEP: Creating hostNetwork=false pod 03/16/23 10:17:18.57
    Mar 16 10:17:18.665: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5158" to be "running and ready"
    Mar 16 10:17:18.755: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824481ms
    Mar 16 10:17:18.755: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:17:20.846: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180745921s
    Mar 16 10:17:20.846: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 16 10:17:20.846: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/16/23 10:17:20.937
    Mar 16 10:17:21.031: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5158" to be "running and ready"
    Mar 16 10:17:21.121: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.897171ms
    Mar 16 10:17:21.121: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:17:23.213: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.181736936s
    Mar 16 10:17:23.213: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 16 10:17:23.213: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/16/23 10:17:23.302
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/16/23 10:17:23.303
    Mar 16 10:17:23.303: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:23.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:23.303: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:23.303: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 16 10:17:24.102: INFO: Exec stderr: ""
    Mar 16 10:17:24.102: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:24.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:24.103: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:24.103: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 16 10:17:24.846: INFO: Exec stderr: ""
    Mar 16 10:17:24.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:24.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:24.847: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:24.847: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 16 10:17:25.591: INFO: Exec stderr: ""
    Mar 16 10:17:25.591: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:25.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:25.592: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:25.592: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 16 10:17:26.339: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/16/23 10:17:26.339
    Mar 16 10:17:26.339: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:26.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:26.340: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:26.340: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 16 10:17:27.094: INFO: Exec stderr: ""
    Mar 16 10:17:27.094: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:27.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:27.095: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:27.095: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 16 10:17:27.834: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/16/23 10:17:27.834
    Mar 16 10:17:27.834: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:27.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:27.835: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:27.835: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 16 10:17:28.592: INFO: Exec stderr: ""
    Mar 16 10:17:28.592: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:28.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:28.593: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:28.593: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 16 10:17:29.332: INFO: Exec stderr: ""
    Mar 16 10:17:29.332: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:29.332: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:29.333: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:29.333: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 16 10:17:30.056: INFO: Exec stderr: ""
    Mar 16 10:17:30.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:17:30.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:17:30.057: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:17:30.057: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-5158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 16 10:17:30.795: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5158" for this suite. 03/16/23 10:17:30.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:31.066
Mar 16 10:17:31.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:17:31.068
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:31.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:31.515
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 03/16/23 10:17:31.694
Mar 16 10:17:31.789: INFO: Waiting up to 5m0s for pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f" in namespace "projected-368" to be "running and ready"
Mar 16 10:17:31.879: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.649093ms
Mar 16 10:17:31.879: INFO: The phase of Pod annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:17:33.970: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.180862272s
Mar 16 10:17:33.970: INFO: The phase of Pod annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f is Running (Ready = true)
Mar 16 10:17:33.970: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f" satisfied condition "running and ready"
Mar 16 10:17:34.837: INFO: Successfully updated pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:37.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-368" for this suite. 03/16/23 10:17:37.204
------------------------------
• [6.228 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:31.066
    Mar 16 10:17:31.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:17:31.068
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:31.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:31.515
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 03/16/23 10:17:31.694
    Mar 16 10:17:31.789: INFO: Waiting up to 5m0s for pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f" in namespace "projected-368" to be "running and ready"
    Mar 16 10:17:31.879: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.649093ms
    Mar 16 10:17:31.879: INFO: The phase of Pod annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:17:33.970: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.180862272s
    Mar 16 10:17:33.970: INFO: The phase of Pod annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f is Running (Ready = true)
    Mar 16 10:17:33.970: INFO: Pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f" satisfied condition "running and ready"
    Mar 16 10:17:34.837: INFO: Successfully updated pod "annotationupdate76cac9bb-2ea8-434e-8feb-61edcd358b9f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:37.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-368" for this suite. 03/16/23 10:17:37.204
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:37.295
Mar 16 10:17:37.295: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:17:37.296
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:37.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:37.744
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:17:38.103
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:17:38.446
STEP: Deploying the webhook pod 03/16/23 10:17:38.537
STEP: Wait for the deployment to be ready 03/16/23 10:17:38.719
Mar 16 10:17:38.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:17:41.079
STEP: Verifying the service has paired with the endpoint 03/16/23 10:17:41.173
Mar 16 10:17:42.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 03/16/23 10:17:42.263
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/16/23 10:17:42.573
STEP: Creating a configMap that should not be mutated 03/16/23 10:17:42.665
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/16/23 10:17:42.845
STEP: Creating a configMap that should be mutated 03/16/23 10:17:42.937
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:43.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6546" for this suite. 03/16/23 10:17:43.842
STEP: Destroying namespace "webhook-6546-markers" for this suite. 03/16/23 10:17:43.933
------------------------------
• [6.729 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:37.295
    Mar 16 10:17:37.295: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:17:37.296
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:37.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:37.744
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:17:38.103
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:17:38.446
    STEP: Deploying the webhook pod 03/16/23 10:17:38.537
    STEP: Wait for the deployment to be ready 03/16/23 10:17:38.719
    Mar 16 10:17:38.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 17, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:17:41.079
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:17:41.173
    Mar 16 10:17:42.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 03/16/23 10:17:42.263
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/16/23 10:17:42.573
    STEP: Creating a configMap that should not be mutated 03/16/23 10:17:42.665
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/16/23 10:17:42.845
    STEP: Creating a configMap that should be mutated 03/16/23 10:17:42.937
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:43.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6546" for this suite. 03/16/23 10:17:43.842
    STEP: Destroying namespace "webhook-6546-markers" for this suite. 03/16/23 10:17:43.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:44.024
Mar 16 10:17:44.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:17:44.026
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:44.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:44.472
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 03/16/23 10:17:44.65
STEP: watching for the ServiceAccount to be added 03/16/23 10:17:44.83
STEP: patching the ServiceAccount 03/16/23 10:17:44.918
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/16/23 10:17:45.01
STEP: deleting the ServiceAccount 03/16/23 10:17:45.101
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:17:45.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9555" for this suite. 03/16/23 10:17:45.283
------------------------------
• [1.349 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:44.024
    Mar 16 10:17:44.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:17:44.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:44.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:44.472
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 03/16/23 10:17:44.65
    STEP: watching for the ServiceAccount to be added 03/16/23 10:17:44.83
    STEP: patching the ServiceAccount 03/16/23 10:17:44.918
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/16/23 10:17:45.01
    STEP: deleting the ServiceAccount 03/16/23 10:17:45.101
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:17:45.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9555" for this suite. 03/16/23 10:17:45.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:17:45.375
Mar 16 10:17:45.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:17:45.376
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:45.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:45.823
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 03/16/23 10:17:46.001
STEP: Creating a ResourceQuota 03/16/23 10:17:51.091
STEP: Ensuring resource quota status is calculated 03/16/23 10:17:51.181
STEP: Creating a Pod that fits quota 03/16/23 10:17:53.272
STEP: Ensuring ResourceQuota status captures the pod usage 03/16/23 10:17:53.372
STEP: Not allowing a pod to be created that exceeds remaining quota 03/16/23 10:17:55.463
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/16/23 10:17:55.555
STEP: Ensuring a pod cannot update its resource requirements 03/16/23 10:17:55.648
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/16/23 10:17:55.738
STEP: Deleting the pod 03/16/23 10:17:57.829
STEP: Ensuring resource quota status released the pod usage 03/16/23 10:17:57.921
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:00.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3378" for this suite. 03/16/23 10:18:00.19
------------------------------
• [14.906 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:17:45.375
    Mar 16 10:17:45.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:17:45.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:17:45.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:17:45.823
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 03/16/23 10:17:46.001
    STEP: Creating a ResourceQuota 03/16/23 10:17:51.091
    STEP: Ensuring resource quota status is calculated 03/16/23 10:17:51.181
    STEP: Creating a Pod that fits quota 03/16/23 10:17:53.272
    STEP: Ensuring ResourceQuota status captures the pod usage 03/16/23 10:17:53.372
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/16/23 10:17:55.463
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/16/23 10:17:55.555
    STEP: Ensuring a pod cannot update its resource requirements 03/16/23 10:17:55.648
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/16/23 10:17:55.738
    STEP: Deleting the pod 03/16/23 10:17:57.829
    STEP: Ensuring resource quota status released the pod usage 03/16/23 10:17:57.921
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:00.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3378" for this suite. 03/16/23 10:18:00.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:00.283
Mar 16 10:18:00.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 03/16/23 10:18:00.284
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:00.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:00.735
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:00.913
Mar 16 10:18:00.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2 03/16/23 10:18:00.915
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:01.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:01.362
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.631
STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.811
STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.99
STEP: listing a collection of PDBs across all namespaces 03/16/23 10:18:02.079
STEP: listing a collection of PDBs in namespace disruption-382 03/16/23 10:18:02.169
STEP: deleting a collection of PDBs 03/16/23 10:18:02.259
STEP: Waiting for the PDB collection to be deleted 03/16/23 10:18:02.352
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:02.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4366" for this suite. 03/16/23 10:18:02.624
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-382" for this suite. 03/16/23 10:18:02.715
------------------------------
• [2.522 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:00.283
    Mar 16 10:18:00.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 03/16/23 10:18:00.284
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:00.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:00.735
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:00.913
    Mar 16 10:18:00.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption-2 03/16/23 10:18:00.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:01.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:01.362
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.631
    STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.811
    STEP: Waiting for the pdb to be processed 03/16/23 10:18:01.99
    STEP: listing a collection of PDBs across all namespaces 03/16/23 10:18:02.079
    STEP: listing a collection of PDBs in namespace disruption-382 03/16/23 10:18:02.169
    STEP: deleting a collection of PDBs 03/16/23 10:18:02.259
    STEP: Waiting for the PDB collection to be deleted 03/16/23 10:18:02.352
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:02.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4366" for this suite. 03/16/23 10:18:02.624
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-382" for this suite. 03/16/23 10:18:02.715
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:02.805
Mar 16 10:18:02.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:18:02.807
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:03.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:03.253
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 03/16/23 10:18:03.431
STEP: Creating a ResourceQuota 03/16/23 10:18:08.52
STEP: Ensuring resource quota status is calculated 03/16/23 10:18:08.611
STEP: Creating a Service 03/16/23 10:18:10.702
STEP: Creating a NodePort Service 03/16/23 10:18:10.801
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/16/23 10:18:10.9
STEP: Ensuring resource quota status captures service creation 03/16/23 10:18:11.003
STEP: Deleting Services 03/16/23 10:18:13.093
STEP: Ensuring resource quota status released usage 03/16/23 10:18:13.286
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:15.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3274" for this suite. 03/16/23 10:18:15.554
------------------------------
• [12.839 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:02.805
    Mar 16 10:18:02.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:18:02.807
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:03.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:03.253
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 03/16/23 10:18:03.431
    STEP: Creating a ResourceQuota 03/16/23 10:18:08.52
    STEP: Ensuring resource quota status is calculated 03/16/23 10:18:08.611
    STEP: Creating a Service 03/16/23 10:18:10.702
    STEP: Creating a NodePort Service 03/16/23 10:18:10.801
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/16/23 10:18:10.9
    STEP: Ensuring resource quota status captures service creation 03/16/23 10:18:11.003
    STEP: Deleting Services 03/16/23 10:18:13.093
    STEP: Ensuring resource quota status released usage 03/16/23 10:18:13.286
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:15.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3274" for this suite. 03/16/23 10:18:15.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:15.646
Mar 16 10:18:15.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:18:15.648
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:15.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:16.095
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 03/16/23 10:18:16.273
Mar 16 10:18:16.367: INFO: Waiting up to 5m0s for pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64" in namespace "downward-api-5274" to be "running and ready"
Mar 16 10:18:16.457: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64": Phase="Pending", Reason="", readiness=false. Elapsed: 89.693279ms
Mar 16 10:18:16.457: INFO: The phase of Pod labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:18:18.548: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64": Phase="Running", Reason="", readiness=true. Elapsed: 2.180188487s
Mar 16 10:18:18.548: INFO: The phase of Pod labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64 is Running (Ready = true)
Mar 16 10:18:18.548: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64" satisfied condition "running and ready"
Mar 16 10:18:19.417: INFO: Successfully updated pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:21.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5274" for this suite. 03/16/23 10:18:21.828
------------------------------
• [6.273 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:15.646
    Mar 16 10:18:15.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:18:15.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:15.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:16.095
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 03/16/23 10:18:16.273
    Mar 16 10:18:16.367: INFO: Waiting up to 5m0s for pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64" in namespace "downward-api-5274" to be "running and ready"
    Mar 16 10:18:16.457: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64": Phase="Pending", Reason="", readiness=false. Elapsed: 89.693279ms
    Mar 16 10:18:16.457: INFO: The phase of Pod labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:18:18.548: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64": Phase="Running", Reason="", readiness=true. Elapsed: 2.180188487s
    Mar 16 10:18:18.548: INFO: The phase of Pod labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64 is Running (Ready = true)
    Mar 16 10:18:18.548: INFO: Pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64" satisfied condition "running and ready"
    Mar 16 10:18:19.417: INFO: Successfully updated pod "labelsupdate66e01df5-8606-446b-bb85-4ddd6abfcb64"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:21.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5274" for this suite. 03/16/23 10:18:21.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:21.922
Mar 16 10:18:21.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 03/16/23 10:18:21.923
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:22.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:22.369
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/16/23 10:18:22.547
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-tnsk 03/16/23 10:18:22.727
STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:18:22.727
Mar 16 10:18:22.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tnsk" in namespace "subpath-6528" to be "Succeeded or Failed"
Mar 16 10:18:22.912: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Pending", Reason="", readiness=false. Elapsed: 89.500994ms
Mar 16 10:18:25.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.179868528s
Mar 16 10:18:27.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 4.180006874s
Mar 16 10:18:29.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 6.180354169s
Mar 16 10:18:31.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 8.180094609s
Mar 16 10:18:33.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 10.180401546s
Mar 16 10:18:35.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 12.180209232s
Mar 16 10:18:37.001: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 14.179290136s
Mar 16 10:18:39.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 16.180578962s
Mar 16 10:18:41.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.180473326s
Mar 16 10:18:43.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 20.18062925s
Mar 16 10:18:45.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=false. Elapsed: 22.180290487s
Mar 16 10:18:47.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180393254s
STEP: Saw pod success 03/16/23 10:18:47.003
Mar 16 10:18:47.003: INFO: Pod "pod-subpath-test-projected-tnsk" satisfied condition "Succeeded or Failed"
Mar 16 10:18:47.093: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-projected-tnsk container test-container-subpath-projected-tnsk: <nil>
STEP: delete the pod 03/16/23 10:18:47.188
Mar 16 10:18:47.282: INFO: Waiting for pod pod-subpath-test-projected-tnsk to disappear
Mar 16 10:18:47.372: INFO: Pod pod-subpath-test-projected-tnsk no longer exists
STEP: Deleting pod pod-subpath-test-projected-tnsk 03/16/23 10:18:47.372
Mar 16 10:18:47.372: INFO: Deleting pod "pod-subpath-test-projected-tnsk" in namespace "subpath-6528"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:47.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6528" for this suite. 03/16/23 10:18:47.64
------------------------------
• [25.809 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:21.922
    Mar 16 10:18:21.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 03/16/23 10:18:21.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:22.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:22.369
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/16/23 10:18:22.547
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-tnsk 03/16/23 10:18:22.727
    STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:18:22.727
    Mar 16 10:18:22.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tnsk" in namespace "subpath-6528" to be "Succeeded or Failed"
    Mar 16 10:18:22.912: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Pending", Reason="", readiness=false. Elapsed: 89.500994ms
    Mar 16 10:18:25.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.179868528s
    Mar 16 10:18:27.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 4.180006874s
    Mar 16 10:18:29.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 6.180354169s
    Mar 16 10:18:31.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 8.180094609s
    Mar 16 10:18:33.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 10.180401546s
    Mar 16 10:18:35.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 12.180209232s
    Mar 16 10:18:37.001: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 14.179290136s
    Mar 16 10:18:39.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 16.180578962s
    Mar 16 10:18:41.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.180473326s
    Mar 16 10:18:43.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=true. Elapsed: 20.18062925s
    Mar 16 10:18:45.002: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Running", Reason="", readiness=false. Elapsed: 22.180290487s
    Mar 16 10:18:47.003: INFO: Pod "pod-subpath-test-projected-tnsk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180393254s
    STEP: Saw pod success 03/16/23 10:18:47.003
    Mar 16 10:18:47.003: INFO: Pod "pod-subpath-test-projected-tnsk" satisfied condition "Succeeded or Failed"
    Mar 16 10:18:47.093: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-projected-tnsk container test-container-subpath-projected-tnsk: <nil>
    STEP: delete the pod 03/16/23 10:18:47.188
    Mar 16 10:18:47.282: INFO: Waiting for pod pod-subpath-test-projected-tnsk to disappear
    Mar 16 10:18:47.372: INFO: Pod pod-subpath-test-projected-tnsk no longer exists
    STEP: Deleting pod pod-subpath-test-projected-tnsk 03/16/23 10:18:47.372
    Mar 16 10:18:47.372: INFO: Deleting pod "pod-subpath-test-projected-tnsk" in namespace "subpath-6528"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:47.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6528" for this suite. 03/16/23 10:18:47.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:47.732
Mar 16 10:18:47.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:18:47.733
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:48.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:48.18
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 03/16/23 10:18:48.359
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:48.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9693" for this suite. 03/16/23 10:18:48.54
------------------------------
• [0.905 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:47.732
    Mar 16 10:18:47.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:18:47.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:48.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:48.18
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 03/16/23 10:18:48.359
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:48.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9693" for this suite. 03/16/23 10:18:48.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:48.638
Mar 16 10:18:48.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 10:18:48.639
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:48.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:49.088
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 03/16/23 10:18:49.265
STEP: Ensure pods equal to parallelism count is attached to the job 03/16/23 10:18:49.355
STEP: patching /status 03/16/23 10:18:51.446
STEP: updating /status 03/16/23 10:18:51.537
STEP: get /status 03/16/23 10:18:51.718
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:51.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9448" for this suite. 03/16/23 10:18:51.985
------------------------------
• [3.437 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:48.638
    Mar 16 10:18:48.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 10:18:48.639
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:48.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:49.088
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 03/16/23 10:18:49.265
    STEP: Ensure pods equal to parallelism count is attached to the job 03/16/23 10:18:49.355
    STEP: patching /status 03/16/23 10:18:51.446
    STEP: updating /status 03/16/23 10:18:51.537
    STEP: get /status 03/16/23 10:18:51.718
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:51.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9448" for this suite. 03/16/23 10:18:51.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:52.077
Mar 16 10:18:52.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:18:52.078
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:52.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:52.523
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 03/16/23 10:18:52.701
Mar 16 10:18:52.798: INFO: Waiting up to 5m0s for pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7" in namespace "downward-api-8934" to be "Succeeded or Failed"
Mar 16 10:18:52.887: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.243654ms
Mar 16 10:18:54.976: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178841595s
Mar 16 10:18:56.978: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.17996252s
STEP: Saw pod success 03/16/23 10:18:56.978
Mar 16 10:18:56.978: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7" satisfied condition "Succeeded or Failed"
Mar 16 10:18:57.068: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 container dapi-container: <nil>
STEP: delete the pod 03/16/23 10:18:57.191
Mar 16 10:18:57.284: INFO: Waiting for pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 to disappear
Mar 16 10:18:57.373: INFO: Pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 16 10:18:57.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8934" for this suite. 03/16/23 10:18:57.551
------------------------------
• [5.564 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:52.077
    Mar 16 10:18:52.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:18:52.078
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:52.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:52.523
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 03/16/23 10:18:52.701
    Mar 16 10:18:52.798: INFO: Waiting up to 5m0s for pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7" in namespace "downward-api-8934" to be "Succeeded or Failed"
    Mar 16 10:18:52.887: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.243654ms
    Mar 16 10:18:54.976: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178841595s
    Mar 16 10:18:56.978: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.17996252s
    STEP: Saw pod success 03/16/23 10:18:56.978
    Mar 16 10:18:56.978: INFO: Pod "downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7" satisfied condition "Succeeded or Failed"
    Mar 16 10:18:57.068: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 container dapi-container: <nil>
    STEP: delete the pod 03/16/23 10:18:57.191
    Mar 16 10:18:57.284: INFO: Waiting for pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 to disappear
    Mar 16 10:18:57.373: INFO: Pod downward-api-53daed2b-a1b5-46c7-839c-f02dfa0452c7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:18:57.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8934" for this suite. 03/16/23 10:18:57.551
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:18:57.641
Mar 16 10:18:57.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 03/16/23 10:18:57.642
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:57.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:58.087
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 03/16/23 10:19:03.926
STEP: referencing matching pods with named port 03/16/23 10:19:09.106
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/16/23 10:19:14.287
STEP: recreating EndpointSlices after they've been deleted 03/16/23 10:19:19.467
Mar 16 10:19:19.828: INFO: EndpointSlice for Service endpointslice-1337/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:30.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1337" for this suite. 03/16/23 10:19:30.188
------------------------------
• [32.638 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:18:57.641
    Mar 16 10:18:57.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 03/16/23 10:18:57.642
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:18:57.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:18:58.087
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 03/16/23 10:19:03.926
    STEP: referencing matching pods with named port 03/16/23 10:19:09.106
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/16/23 10:19:14.287
    STEP: recreating EndpointSlices after they've been deleted 03/16/23 10:19:19.467
    Mar 16 10:19:19.828: INFO: EndpointSlice for Service endpointslice-1337/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:30.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1337" for this suite. 03/16/23 10:19:30.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:30.279
Mar 16 10:19:30.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 10:19:30.28
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:30.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:30.726
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/16/23 10:19:30.904
STEP: Wait for the Deployment to create new ReplicaSet 03/16/23 10:19:30.994
STEP: delete the deployment 03/16/23 10:19:31.084
STEP: wait for all rs to be garbage collected 03/16/23 10:19:31.175
STEP: Gathering metrics 03/16/23 10:19:31.442
W0316 10:19:31.539381    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 10:19:31.539: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:31.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5270" for this suite. 03/16/23 10:19:31.629
------------------------------
• [1.440 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:30.279
    Mar 16 10:19:30.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 10:19:30.28
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:30.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:30.726
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/16/23 10:19:30.904
    STEP: Wait for the Deployment to create new ReplicaSet 03/16/23 10:19:30.994
    STEP: delete the deployment 03/16/23 10:19:31.084
    STEP: wait for all rs to be garbage collected 03/16/23 10:19:31.175
    STEP: Gathering metrics 03/16/23 10:19:31.442
    W0316 10:19:31.539381    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 10:19:31.539: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:31.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5270" for this suite. 03/16/23 10:19:31.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:31.721
Mar 16 10:19:31.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:19:31.722
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:31.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:32.167
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:19:32.526
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:19:33.065
STEP: Deploying the webhook pod 03/16/23 10:19:33.157
STEP: Wait for the deployment to be ready 03/16/23 10:19:33.337
Mar 16 10:19:33.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:19:35.696
STEP: Verifying the service has paired with the endpoint 03/16/23 10:19:35.793
Mar 16 10:19:36.794: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 03/16/23 10:19:37.785
STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:19:38.124
STEP: Deleting the collection of validation webhooks 03/16/23 10:19:38.3
STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:19:38.406
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7089" for this suite. 03/16/23 10:19:39.04
STEP: Destroying namespace "webhook-7089-markers" for this suite. 03/16/23 10:19:39.13
------------------------------
• [7.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:31.721
    Mar 16 10:19:31.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:19:31.722
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:31.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:32.167
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:19:32.526
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:19:33.065
    STEP: Deploying the webhook pod 03/16/23 10:19:33.157
    STEP: Wait for the deployment to be ready 03/16/23 10:19:33.337
    Mar 16 10:19:33.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 19, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:19:35.696
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:19:35.793
    Mar 16 10:19:36.794: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 03/16/23 10:19:37.785
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:19:38.124
    STEP: Deleting the collection of validation webhooks 03/16/23 10:19:38.3
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:19:38.406
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7089" for this suite. 03/16/23 10:19:39.04
    STEP: Destroying namespace "webhook-7089-markers" for this suite. 03/16/23 10:19:39.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:39.222
Mar 16 10:19:39.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 10:19:39.223
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:39.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:39.668
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 03/16/23 10:19:39.846
Mar 16 10:19:39.940: INFO: Waiting up to 5m0s for pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5" in namespace "var-expansion-1416" to be "Succeeded or Failed"
Mar 16 10:19:40.030: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.708422ms
Mar 16 10:19:42.119: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179292749s
Mar 16 10:19:44.125: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18497632s
STEP: Saw pod success 03/16/23 10:19:44.125
Mar 16 10:19:44.125: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5" satisfied condition "Succeeded or Failed"
Mar 16 10:19:44.215: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 container dapi-container: <nil>
STEP: delete the pod 03/16/23 10:19:44.35
Mar 16 10:19:44.443: INFO: Waiting for pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 to disappear
Mar 16 10:19:44.532: INFO: Pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:44.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1416" for this suite. 03/16/23 10:19:44.71
------------------------------
• [5.578 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:39.222
    Mar 16 10:19:39.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 10:19:39.223
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:39.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:39.668
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 03/16/23 10:19:39.846
    Mar 16 10:19:39.940: INFO: Waiting up to 5m0s for pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5" in namespace "var-expansion-1416" to be "Succeeded or Failed"
    Mar 16 10:19:40.030: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.708422ms
    Mar 16 10:19:42.119: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179292749s
    Mar 16 10:19:44.125: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18497632s
    STEP: Saw pod success 03/16/23 10:19:44.125
    Mar 16 10:19:44.125: INFO: Pod "var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5" satisfied condition "Succeeded or Failed"
    Mar 16 10:19:44.215: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 container dapi-container: <nil>
    STEP: delete the pod 03/16/23 10:19:44.35
    Mar 16 10:19:44.443: INFO: Waiting for pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 to disappear
    Mar 16 10:19:44.532: INFO: Pod var-expansion-78da91b5-61c2-493e-93e8-149cbb9ba7c5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:44.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1416" for this suite. 03/16/23 10:19:44.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:44.804
Mar 16 10:19:44.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:19:44.805
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:45.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:45.251
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/16/23 10:19:45.429
STEP: getting /apis/node.k8s.io 03/16/23 10:19:45.606
STEP: getting /apis/node.k8s.io/v1 03/16/23 10:19:45.694
STEP: creating 03/16/23 10:19:45.782
STEP: watching 03/16/23 10:19:46.052
Mar 16 10:19:46.052: INFO: starting watch
STEP: getting 03/16/23 10:19:46.23
STEP: listing 03/16/23 10:19:46.319
STEP: patching 03/16/23 10:19:46.408
STEP: updating 03/16/23 10:19:46.498
Mar 16 10:19:46.588: INFO: waiting for watch events with expected annotations
STEP: deleting 03/16/23 10:19:46.588
STEP: deleting a collection 03/16/23 10:19:46.856
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8343" for this suite. 03/16/23 10:19:47.216
------------------------------
• [2.503 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:44.804
    Mar 16 10:19:44.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:19:44.805
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:45.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:45.251
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/16/23 10:19:45.429
    STEP: getting /apis/node.k8s.io 03/16/23 10:19:45.606
    STEP: getting /apis/node.k8s.io/v1 03/16/23 10:19:45.694
    STEP: creating 03/16/23 10:19:45.782
    STEP: watching 03/16/23 10:19:46.052
    Mar 16 10:19:46.052: INFO: starting watch
    STEP: getting 03/16/23 10:19:46.23
    STEP: listing 03/16/23 10:19:46.319
    STEP: patching 03/16/23 10:19:46.408
    STEP: updating 03/16/23 10:19:46.498
    Mar 16 10:19:46.588: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/16/23 10:19:46.588
    STEP: deleting a collection 03/16/23 10:19:46.856
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8343" for this suite. 03/16/23 10:19:47.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:47.307
Mar 16 10:19:47.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 03/16/23 10:19:47.309
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:47.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:47.755
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/16/23 10:19:47.932
STEP: creating a new configmap 03/16/23 10:19:48.021
STEP: modifying the configmap once 03/16/23 10:19:48.11
STEP: changing the label value of the configmap 03/16/23 10:19:48.289
STEP: Expecting to observe a delete notification for the watched object 03/16/23 10:19:48.468
Mar 16 10:19:48.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24376 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 10:19:48.469: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24379 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 10:19:48.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24380 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/16/23 10:19:48.469
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/16/23 10:19:48.648
STEP: changing the label value of the configmap back 03/16/23 10:19:58.649
STEP: modifying the configmap a third time 03/16/23 10:19:58.83
STEP: deleting the configmap 03/16/23 10:19:59.01
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/16/23 10:19:59.101
Mar 16 10:19:59.101: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24473 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 10:19:59.101: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24475 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 10:19:59.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24476 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 16 10:19:59.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2455" for this suite. 03/16/23 10:19:59.28
------------------------------
• [12.064 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:47.307
    Mar 16 10:19:47.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 03/16/23 10:19:47.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:47.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:47.755
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/16/23 10:19:47.932
    STEP: creating a new configmap 03/16/23 10:19:48.021
    STEP: modifying the configmap once 03/16/23 10:19:48.11
    STEP: changing the label value of the configmap 03/16/23 10:19:48.289
    STEP: Expecting to observe a delete notification for the watched object 03/16/23 10:19:48.468
    Mar 16 10:19:48.469: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24376 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 10:19:48.469: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24379 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 10:19:48.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24380 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/16/23 10:19:48.469
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/16/23 10:19:48.648
    STEP: changing the label value of the configmap back 03/16/23 10:19:58.649
    STEP: modifying the configmap a third time 03/16/23 10:19:58.83
    STEP: deleting the configmap 03/16/23 10:19:59.01
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/16/23 10:19:59.101
    Mar 16 10:19:59.101: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24473 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 10:19:59.101: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24475 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 10:19:59.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2455  d060a2f3-7fa0-457d-aec4-1098fd67e066 24476 0 2023-03-16 10:19:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-16 10:19:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:19:59.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2455" for this suite. 03/16/23 10:19:59.28
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:19:59.372
Mar 16 10:19:59.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 10:19:59.374
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:59.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:59.825
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 16 10:20:00.184: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 10:20:00.185
Mar 16 10:20:00.185: INFO: Waiting up to 5m0s for pod "test-rollover-controller-5jnz2" in namespace "deployment-8824" to be "running"
Mar 16 10:20:00.275: INFO: Pod "test-rollover-controller-5jnz2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.016356ms
Mar 16 10:20:02.365: INFO: Pod "test-rollover-controller-5jnz2": Phase="Running", Reason="", readiness=true. Elapsed: 2.18043975s
Mar 16 10:20:02.365: INFO: Pod "test-rollover-controller-5jnz2" satisfied condition "running"
Mar 16 10:20:02.365: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 16 10:20:04.456: INFO: Creating deployment "test-rollover-deployment"
Mar 16 10:20:04.637: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 16 10:20:04.727: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 16 10:20:04.906: INFO: Ensure that both replica sets have 1 created replica
Mar 16 10:20:05.086: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 16 10:20:05.268: INFO: Updating deployment test-rollover-deployment
Mar 16 10:20:05.268: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 16 10:20:05.358: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 16 10:20:05.539: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 16 10:20:05.719: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:05.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:07.900: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:07.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:09.901: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:09.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:11.901: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:11.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:13.899: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:13.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:15.900: INFO: all replica sets need to contain the pod-template-hash label
Mar 16 10:20:15.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:20:17.899: INFO: 
Mar 16 10:20:17.899: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 10:20:18.168: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8824  bb472039-1b9e-4f2a-ae1f-74737878a58d 24630 2 2023-03-16 10:20:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb8258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:20:04 +0000 UTC,LastTransitionTime:2023-03-16 10:20:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-16 10:20:16 +0000 UTC,LastTransitionTime:2023-03-16 10:20:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 16 10:20:18.258: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8824  6f31c35c-f532-43dd-950d-42f0207d4e7b 24623 2 2023-03-16 10:20:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc00523df77 0xc00523df78}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003840048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:20:18.258: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 16 10:20:18.258: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8824  0a492400-ac92-4ef1-8d50-9d122249e85e 24629 2 2023-03-16 10:20:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc00523de47 0xc00523de48}] [] [{e2e.test Update apps/v1 2023-03-16 10:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00523df08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:20:18.258: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8824  2f433753-19c1-4394-aa01-46d858ddc23f 24552 2 2023-03-16 10:20:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc003840217 0xc003840218}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038403b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:20:18.349: INFO: Pod "test-rollover-deployment-6c6df9974f-gskmc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-gskmc test-rollover-deployment-6c6df9974f- deployment-8824  45a67bf2-f835-4534-887f-74e68ea75068 24564 0 2023-03-16 10:20:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:4e6adb9116731338ba40b2bc6376f6f5cf0971ab73b60fe9f00adc0b489befd1 cni.projectcalico.org/podIP:100.64.1.4/32 cni.projectcalico.org/podIPs:100.64.1.4/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 6f31c35c-f532-43dd-950d-42f0207d4e7b 0xc003840ea7 0xc003840ea8}] [] [{calico Update v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f31c35c-f532-43dd-950d-42f0207d4e7b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:20:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvcnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvcnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.4,StartTime:2023-03-16 10:20:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:20:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fec7341eb8bb994190beeddb2888a123a5c34b36fb36be01ba6084d3714e674f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:18.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8824" for this suite. 03/16/23 10:20:18.527
------------------------------
• [19.246 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:19:59.372
    Mar 16 10:19:59.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 10:19:59.374
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:19:59.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:19:59.825
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 16 10:20:00.184: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 10:20:00.185
    Mar 16 10:20:00.185: INFO: Waiting up to 5m0s for pod "test-rollover-controller-5jnz2" in namespace "deployment-8824" to be "running"
    Mar 16 10:20:00.275: INFO: Pod "test-rollover-controller-5jnz2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.016356ms
    Mar 16 10:20:02.365: INFO: Pod "test-rollover-controller-5jnz2": Phase="Running", Reason="", readiness=true. Elapsed: 2.18043975s
    Mar 16 10:20:02.365: INFO: Pod "test-rollover-controller-5jnz2" satisfied condition "running"
    Mar 16 10:20:02.365: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 16 10:20:04.456: INFO: Creating deployment "test-rollover-deployment"
    Mar 16 10:20:04.637: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 16 10:20:04.727: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 16 10:20:04.906: INFO: Ensure that both replica sets have 1 created replica
    Mar 16 10:20:05.086: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 16 10:20:05.268: INFO: Updating deployment test-rollover-deployment
    Mar 16 10:20:05.268: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 16 10:20:05.358: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 16 10:20:05.539: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 16 10:20:05.719: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:05.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:07.900: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:07.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:09.901: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:09.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:11.901: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:11.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:13.899: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:13.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:15.900: INFO: all replica sets need to contain the pod-template-hash label
    Mar 16 10:20:15.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 20, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:20:17.899: INFO: 
    Mar 16 10:20:17.899: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 10:20:18.168: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8824  bb472039-1b9e-4f2a-ae1f-74737878a58d 24630 2 2023-03-16 10:20:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bb8258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:20:04 +0000 UTC,LastTransitionTime:2023-03-16 10:20:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-16 10:20:16 +0000 UTC,LastTransitionTime:2023-03-16 10:20:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 16 10:20:18.258: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8824  6f31c35c-f532-43dd-950d-42f0207d4e7b 24623 2 2023-03-16 10:20:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc00523df77 0xc00523df78}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003840048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:20:18.258: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 16 10:20:18.258: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8824  0a492400-ac92-4ef1-8d50-9d122249e85e 24629 2 2023-03-16 10:20:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc00523de47 0xc00523de48}] [] [{e2e.test Update apps/v1 2023-03-16 10:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00523df08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:20:18.258: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8824  2f433753-19c1-4394-aa01-46d858ddc23f 24552 2 2023-03-16 10:20:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bb472039-1b9e-4f2a-ae1f-74737878a58d 0xc003840217 0xc003840218}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb472039-1b9e-4f2a-ae1f-74737878a58d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038403b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:20:18.349: INFO: Pod "test-rollover-deployment-6c6df9974f-gskmc" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-gskmc test-rollover-deployment-6c6df9974f- deployment-8824  45a67bf2-f835-4534-887f-74e68ea75068 24564 0 2023-03-16 10:20:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:4e6adb9116731338ba40b2bc6376f6f5cf0971ab73b60fe9f00adc0b489befd1 cni.projectcalico.org/podIP:100.64.1.4/32 cni.projectcalico.org/podIPs:100.64.1.4/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 6f31c35c-f532-43dd-950d-42f0207d4e7b 0xc003840ea7 0xc003840ea8}] [] [{calico Update v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 10:20:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f31c35c-f532-43dd-950d-42f0207d4e7b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:20:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvcnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvcnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:20:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.4,StartTime:2023-03-16 10:20:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:20:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fec7341eb8bb994190beeddb2888a123a5c34b36fb36be01ba6084d3714e674f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:18.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8824" for this suite. 03/16/23 10:20:18.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:18.619
Mar 16 10:20:18.619: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:20:18.62
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:18.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:19.067
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:20:19.246
Mar 16 10:20:19.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671" in namespace "projected-8962" to be "Succeeded or Failed"
Mar 16 10:20:19.431: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671": Phase="Pending", Reason="", readiness=false. Elapsed: 89.739822ms
Mar 16 10:20:21.523: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181489098s
STEP: Saw pod success 03/16/23 10:20:21.523
Mar 16 10:20:21.523: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671" satisfied condition "Succeeded or Failed"
Mar 16 10:20:21.613: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 container client-container: <nil>
STEP: delete the pod 03/16/23 10:20:21.754
Mar 16 10:20:21.848: INFO: Waiting for pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 to disappear
Mar 16 10:20:21.937: INFO: Pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:21.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8962" for this suite. 03/16/23 10:20:22.116
------------------------------
• [3.587 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:18.619
    Mar 16 10:20:18.619: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:20:18.62
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:18.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:19.067
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:20:19.246
    Mar 16 10:20:19.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671" in namespace "projected-8962" to be "Succeeded or Failed"
    Mar 16 10:20:19.431: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671": Phase="Pending", Reason="", readiness=false. Elapsed: 89.739822ms
    Mar 16 10:20:21.523: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181489098s
    STEP: Saw pod success 03/16/23 10:20:21.523
    Mar 16 10:20:21.523: INFO: Pod "downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671" satisfied condition "Succeeded or Failed"
    Mar 16 10:20:21.613: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:20:21.754
    Mar 16 10:20:21.848: INFO: Waiting for pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 to disappear
    Mar 16 10:20:21.937: INFO: Pod downwardapi-volume-b25411ec-26d6-451e-b86b-0d0514767671 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:21.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8962" for this suite. 03/16/23 10:20:22.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:22.207
Mar 16 10:20:22.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:20:22.208
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:22.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:22.655
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-9f4b1db5-65d2-43cc-bc0c-e5b76fde80ff 03/16/23 10:20:22.833
STEP: Creating a pod to test consume secrets 03/16/23 10:20:22.923
Mar 16 10:20:23.019: INFO: Waiting up to 5m0s for pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c" in namespace "secrets-8479" to be "Succeeded or Failed"
Mar 16 10:20:23.108: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.612775ms
Mar 16 10:20:25.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179843541s
Mar 16 10:20:27.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179577353s
STEP: Saw pod success 03/16/23 10:20:27.198
Mar 16 10:20:27.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c" satisfied condition "Succeeded or Failed"
Mar 16 10:20:27.288: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:20:27.383
Mar 16 10:20:27.477: INFO: Waiting for pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c to disappear
Mar 16 10:20:27.566: INFO: Pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:27.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8479" for this suite. 03/16/23 10:20:27.744
------------------------------
• [5.628 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:22.207
    Mar 16 10:20:22.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:20:22.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:22.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:22.655
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-9f4b1db5-65d2-43cc-bc0c-e5b76fde80ff 03/16/23 10:20:22.833
    STEP: Creating a pod to test consume secrets 03/16/23 10:20:22.923
    Mar 16 10:20:23.019: INFO: Waiting up to 5m0s for pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c" in namespace "secrets-8479" to be "Succeeded or Failed"
    Mar 16 10:20:23.108: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.612775ms
    Mar 16 10:20:25.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179843541s
    Mar 16 10:20:27.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179577353s
    STEP: Saw pod success 03/16/23 10:20:27.198
    Mar 16 10:20:27.198: INFO: Pod "pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c" satisfied condition "Succeeded or Failed"
    Mar 16 10:20:27.288: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:20:27.383
    Mar 16 10:20:27.477: INFO: Waiting for pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c to disappear
    Mar 16 10:20:27.566: INFO: Pod pod-secrets-c87b2851-082a-4150-ba46-6ae610d8772c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:27.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8479" for this suite. 03/16/23 10:20:27.744
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:27.835
Mar 16 10:20:27.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 03/16/23 10:20:27.836
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:28.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:28.284
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Mar 16 10:20:28.556: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5" in namespace "security-context-test-9714" to be "Succeeded or Failed"
Mar 16 10:20:28.646: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.828902ms
Mar 16 10:20:30.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Running", Reason="", readiness=false. Elapsed: 2.180922097s
Mar 16 10:20:32.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180601717s
Mar 16 10:20:32.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:32.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9714" for this suite. 03/16/23 10:20:32.915
------------------------------
• [5.170 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:27.835
    Mar 16 10:20:27.835: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 03/16/23 10:20:27.836
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:28.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:28.284
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Mar 16 10:20:28.556: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5" in namespace "security-context-test-9714" to be "Succeeded or Failed"
    Mar 16 10:20:28.646: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.828902ms
    Mar 16 10:20:30.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Running", Reason="", readiness=false. Elapsed: 2.180922097s
    Mar 16 10:20:32.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180601717s
    Mar 16 10:20:32.737: INFO: Pod "busybox-readonly-false-c509477e-37ce-46a9-8f18-192063732ea5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:32.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9714" for this suite. 03/16/23 10:20:32.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:33.007
Mar 16 10:20:33.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:20:33.008
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:33.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:33.456
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 03/16/23 10:20:33.634
Mar 16 10:20:33.634: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 create -f -'
Mar 16 10:20:34.877: INFO: stderr: ""
Mar 16 10:20:34.877: INFO: stdout: "pod/pause created\n"
Mar 16 10:20:34.877: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 16 10:20:34.878: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2670" to be "running and ready"
Mar 16 10:20:34.967: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.496829ms
Mar 16 10:20:34.967: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-250-19-136.ec2.internal' to be 'Running' but was 'Pending'
Mar 16 10:20:37.065: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.187764327s
Mar 16 10:20:37.065: INFO: Pod "pause" satisfied condition "running and ready"
Mar 16 10:20:37.065: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 03/16/23 10:20:37.065
Mar 16 10:20:37.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 label pods pause testing-label=testing-label-value'
Mar 16 10:20:37.507: INFO: stderr: ""
Mar 16 10:20:37.507: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/16/23 10:20:37.507
Mar 16 10:20:37.508: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pod pause -L testing-label'
Mar 16 10:20:37.853: INFO: stderr: ""
Mar 16 10:20:37.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/16/23 10:20:37.853
Mar 16 10:20:37.853: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 label pods pause testing-label-'
Mar 16 10:20:38.292: INFO: stderr: ""
Mar 16 10:20:38.292: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/16/23 10:20:38.292
Mar 16 10:20:38.293: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pod pause -L testing-label'
Mar 16 10:20:38.633: INFO: stderr: ""
Mar 16 10:20:38.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 03/16/23 10:20:38.633
Mar 16 10:20:38.633: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 delete --grace-period=0 --force -f -'
Mar 16 10:20:39.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:20:39.070: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 16 10:20:39.070: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get rc,svc -l name=pause --no-headers'
Mar 16 10:20:39.512: INFO: stderr: "No resources found in kubectl-2670 namespace.\n"
Mar 16 10:20:39.512: INFO: stdout: ""
Mar 16 10:20:39.512: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 16 10:20:39.851: INFO: stderr: ""
Mar 16 10:20:39.851: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2670" for this suite. 03/16/23 10:20:40.029
------------------------------
• [7.113 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:33.007
    Mar 16 10:20:33.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:20:33.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:33.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:33.456
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 03/16/23 10:20:33.634
    Mar 16 10:20:33.634: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 create -f -'
    Mar 16 10:20:34.877: INFO: stderr: ""
    Mar 16 10:20:34.877: INFO: stdout: "pod/pause created\n"
    Mar 16 10:20:34.877: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 16 10:20:34.878: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2670" to be "running and ready"
    Mar 16 10:20:34.967: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.496829ms
    Mar 16 10:20:34.967: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-250-19-136.ec2.internal' to be 'Running' but was 'Pending'
    Mar 16 10:20:37.065: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.187764327s
    Mar 16 10:20:37.065: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 16 10:20:37.065: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 03/16/23 10:20:37.065
    Mar 16 10:20:37.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 label pods pause testing-label=testing-label-value'
    Mar 16 10:20:37.507: INFO: stderr: ""
    Mar 16 10:20:37.507: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/16/23 10:20:37.507
    Mar 16 10:20:37.508: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pod pause -L testing-label'
    Mar 16 10:20:37.853: INFO: stderr: ""
    Mar 16 10:20:37.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/16/23 10:20:37.853
    Mar 16 10:20:37.853: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 label pods pause testing-label-'
    Mar 16 10:20:38.292: INFO: stderr: ""
    Mar 16 10:20:38.292: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/16/23 10:20:38.292
    Mar 16 10:20:38.293: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pod pause -L testing-label'
    Mar 16 10:20:38.633: INFO: stderr: ""
    Mar 16 10:20:38.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 03/16/23 10:20:38.633
    Mar 16 10:20:38.633: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 delete --grace-period=0 --force -f -'
    Mar 16 10:20:39.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:20:39.070: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 16 10:20:39.070: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get rc,svc -l name=pause --no-headers'
    Mar 16 10:20:39.512: INFO: stderr: "No resources found in kubectl-2670 namespace.\n"
    Mar 16 10:20:39.512: INFO: stdout: ""
    Mar 16 10:20:39.512: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2670 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 16 10:20:39.851: INFO: stderr: ""
    Mar 16 10:20:39.851: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2670" for this suite. 03/16/23 10:20:40.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:40.121
Mar 16 10:20:40.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 10:20:40.122
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:40.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:40.569
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/16/23 10:20:40.747
STEP: delete the rc 03/16/23 10:20:45.929
STEP: wait for all pods to be garbage collected 03/16/23 10:20:46.019
STEP: Gathering metrics 03/16/23 10:20:51.199
W0316 10:20:51.384300    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 10:20:51.384: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 10:20:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1829" for this suite. 03/16/23 10:20:51.474
------------------------------
• [11.444 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:40.121
    Mar 16 10:20:40.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 10:20:40.122
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:40.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:40.569
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/16/23 10:20:40.747
    STEP: delete the rc 03/16/23 10:20:45.929
    STEP: wait for all pods to be garbage collected 03/16/23 10:20:46.019
    STEP: Gathering metrics 03/16/23 10:20:51.199
    W0316 10:20:51.384300    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 10:20:51.384: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:20:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1829" for this suite. 03/16/23 10:20:51.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:20:51.565
Mar 16 10:20:51.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:20:51.566
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:51.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:52.014
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9619 03/16/23 10:20:52.191
STEP: creating a selector 03/16/23 10:20:52.192
STEP: Creating the service pods in kubernetes 03/16/23 10:20:52.192
Mar 16 10:20:52.192: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 16 10:20:52.561: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9619" to be "running and ready"
Mar 16 10:20:52.651: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.663923ms
Mar 16 10:20:52.651: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:20:54.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180640403s
Mar 16 10:20:54.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:20:56.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.181000541s
Mar 16 10:20:56.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:20:58.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.181402942s
Mar 16 10:20:58.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:00.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.181078071s
Mar 16 10:21:00.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:02.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180153642s
Mar 16 10:21:02.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:04.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.181057074s
Mar 16 10:21:04.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:06.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.180942321s
Mar 16 10:21:06.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:08.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.18124394s
Mar 16 10:21:08.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:10.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.180935971s
Mar 16 10:21:10.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:12.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.180218091s
Mar 16 10:21:12.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:21:14.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.180861625s
Mar 16 10:21:14.742: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 16 10:21:14.742: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 16 10:21:14.832: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9619" to be "running and ready"
Mar 16 10:21:14.922: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.573776ms
Mar 16 10:21:14.922: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 16 10:21:14.922: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/16/23 10:21:15.012
Mar 16 10:21:15.198: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9619" to be "running"
Mar 16 10:21:15.288: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.890268ms
Mar 16 10:21:17.378: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179944311s
Mar 16 10:21:17.378: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 16 10:21:17.468: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9619" to be "running"
Mar 16 10:21:17.558: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.394884ms
Mar 16 10:21:17.558: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 16 10:21:17.647: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 16 10:21:17.648: INFO: Going to poll 100.64.1.14 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Mar 16 10:21:17.737: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.14:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9619 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:21:17.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:21:17.738: INFO: ExecWithOptions: Clientset creation
Mar 16 10:21:17.738: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9619/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.14%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 16 10:21:18.546: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 16 10:21:18.546: INFO: Going to poll 100.64.0.160 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Mar 16 10:21:18.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.160:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9619 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:21:18.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:21:18.638: INFO: ExecWithOptions: Clientset creation
Mar 16 10:21:18.638: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9619/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.160%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 16 10:21:19.425: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:19.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9619" for this suite. 03/16/23 10:21:19.604
------------------------------
• [28.128 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:20:51.565
    Mar 16 10:20:51.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:20:51.566
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:20:51.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:20:52.014
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9619 03/16/23 10:20:52.191
    STEP: creating a selector 03/16/23 10:20:52.192
    STEP: Creating the service pods in kubernetes 03/16/23 10:20:52.192
    Mar 16 10:20:52.192: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 16 10:20:52.561: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9619" to be "running and ready"
    Mar 16 10:20:52.651: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.663923ms
    Mar 16 10:20:52.651: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:20:54.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180640403s
    Mar 16 10:20:54.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:20:56.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.181000541s
    Mar 16 10:20:56.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:20:58.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.181402942s
    Mar 16 10:20:58.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:00.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.181078071s
    Mar 16 10:21:00.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:02.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180153642s
    Mar 16 10:21:02.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:04.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.181057074s
    Mar 16 10:21:04.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:06.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.180942321s
    Mar 16 10:21:06.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:08.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.18124394s
    Mar 16 10:21:08.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:10.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.180935971s
    Mar 16 10:21:10.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:12.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.180218091s
    Mar 16 10:21:12.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:21:14.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.180861625s
    Mar 16 10:21:14.742: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 16 10:21:14.742: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 16 10:21:14.832: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9619" to be "running and ready"
    Mar 16 10:21:14.922: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.573776ms
    Mar 16 10:21:14.922: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 16 10:21:14.922: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/16/23 10:21:15.012
    Mar 16 10:21:15.198: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9619" to be "running"
    Mar 16 10:21:15.288: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.890268ms
    Mar 16 10:21:17.378: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179944311s
    Mar 16 10:21:17.378: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 16 10:21:17.468: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9619" to be "running"
    Mar 16 10:21:17.558: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.394884ms
    Mar 16 10:21:17.558: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 16 10:21:17.647: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar 16 10:21:17.648: INFO: Going to poll 100.64.1.14 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Mar 16 10:21:17.737: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.14:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9619 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:21:17.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:21:17.738: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:21:17.738: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9619/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.1.14%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 16 10:21:18.546: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 16 10:21:18.546: INFO: Going to poll 100.64.0.160 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Mar 16 10:21:18.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.160:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9619 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:21:18.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:21:18.638: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:21:18.638: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9619/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.64.0.160%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 16 10:21:19.425: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:19.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9619" for this suite. 03/16/23 10:21:19.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:19.696
Mar 16 10:21:19.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:21:19.697
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:19.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:20.145
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Mar 16 10:21:20.418: INFO: Waiting up to 5m0s for pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712" in namespace "pods-5742" to be "running and ready"
Mar 16 10:21:20.510: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712": Phase="Pending", Reason="", readiness=false. Elapsed: 92.7718ms
Mar 16 10:21:20.510: INFO: The phase of Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:21:22.601: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712": Phase="Running", Reason="", readiness=true. Elapsed: 2.183377895s
Mar 16 10:21:22.601: INFO: The phase of Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 is Running (Ready = true)
Mar 16 10:21:22.601: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712" satisfied condition "running and ready"
Mar 16 10:21:22.879: INFO: Waiting up to 5m0s for pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d" in namespace "pods-5742" to be "Succeeded or Failed"
Mar 16 10:21:22.969: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.732731ms
Mar 16 10:21:25.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18051982s
Mar 16 10:21:27.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180676816s
STEP: Saw pod success 03/16/23 10:21:27.06
Mar 16 10:21:27.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d" satisfied condition "Succeeded or Failed"
Mar 16 10:21:27.150: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d container env3cont: <nil>
STEP: delete the pod 03/16/23 10:21:27.244
Mar 16 10:21:27.339: INFO: Waiting for pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d to disappear
Mar 16 10:21:27.428: INFO: Pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:27.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5742" for this suite. 03/16/23 10:21:27.606
------------------------------
• [8.001 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:19.696
    Mar 16 10:21:19.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:21:19.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:19.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:20.145
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Mar 16 10:21:20.418: INFO: Waiting up to 5m0s for pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712" in namespace "pods-5742" to be "running and ready"
    Mar 16 10:21:20.510: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712": Phase="Pending", Reason="", readiness=false. Elapsed: 92.7718ms
    Mar 16 10:21:20.510: INFO: The phase of Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:21:22.601: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712": Phase="Running", Reason="", readiness=true. Elapsed: 2.183377895s
    Mar 16 10:21:22.601: INFO: The phase of Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 is Running (Ready = true)
    Mar 16 10:21:22.601: INFO: Pod "server-envvars-97e55cf0-434f-4170-9fc8-c239514df712" satisfied condition "running and ready"
    Mar 16 10:21:22.879: INFO: Waiting up to 5m0s for pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d" in namespace "pods-5742" to be "Succeeded or Failed"
    Mar 16 10:21:22.969: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.732731ms
    Mar 16 10:21:25.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18051982s
    Mar 16 10:21:27.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180676816s
    STEP: Saw pod success 03/16/23 10:21:27.06
    Mar 16 10:21:27.060: INFO: Pod "client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d" satisfied condition "Succeeded or Failed"
    Mar 16 10:21:27.150: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d container env3cont: <nil>
    STEP: delete the pod 03/16/23 10:21:27.244
    Mar 16 10:21:27.339: INFO: Waiting for pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d to disappear
    Mar 16 10:21:27.428: INFO: Pod client-envvars-57a37d25-75a5-4781-a3d7-7eddb930cf7d no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:27.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5742" for this suite. 03/16/23 10:21:27.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:27.698
Mar 16 10:21:27.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 03/16/23 10:21:27.699
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:27.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:28.148
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 16 10:21:28.326: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 16 10:21:28.506: INFO: Waiting for terminating namespaces to be deleted...
Mar 16 10:21:28.596: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
Mar 16 10:21:28.778: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 16 10:21:28.778: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container proxy ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 10:21:28.778: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 10:21:28.778: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 10:21:28.778: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 10:21:28.778: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 10:21:28.778: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 10:21:28.778: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 10:21:28.778: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 10:21:28.778: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 10:21:28.778: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 10:21:28.778: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 10:21:28.778: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 10:21:28.778: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 10:21:28.778: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container vpn-shoot ready: true, restart count 0
Mar 16 10:21:28.778: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.778: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 16 10:21:28.778: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
Mar 16 10:21:28.874: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.874: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Mar 16 10:21:28.874: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.874: INFO: 	Container proxy ready: true, restart count 0
Mar 16 10:21:28.875: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 10:21:28.875: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 10:21:28.875: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 10:21:28.875: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 10:21:28.875: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 10:21:28.875: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 10:21:28.875: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 10:21:28.875: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 10:21:28.875: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container coredns ready: true, restart count 0
Mar 16 10:21:28.875: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container coredns ready: true, restart count 0
Mar 16 10:21:28.875: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 10:21:28.875: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 10:21:28.875: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 10:21:28.875: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 10:21:28.875: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 10:21:28.875: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 10:21:28.875: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 10:21:28.875: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 10:21:28.875: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 10:21:28.875: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 10:21:28.875: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 10:21:28.875: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 10:21:28.875: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 16 10:21:28.875: INFO: server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 from pods-5742 started at 2023-03-16 10:21:20 +0000 UTC (1 container statuses recorded)
Mar 16 10:21:28.875: INFO: 	Container srv ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-10-250-19-136.ec2.internal 03/16/23 10:21:29.071
STEP: verifying the node has the label node ip-10-250-19-246.ec2.internal 03/16/23 10:21:29.259
Mar 16 10:21:29.447: INFO: Pod addons-nginx-ingress-controller-dfbd87bb4-tpjwc requesting resource cpu=23m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c requesting resource cpu=20m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod apiserver-proxy-6595h requesting resource cpu=40m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod apiserver-proxy-lt25p requesting resource cpu=40m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod blackbox-exporter-5778995784-cxqf6 requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod blackbox-exporter-5778995784-z5cvh requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-node-9fxnl requesting resource cpu=260m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-node-z796q requesting resource cpu=260m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-typha-deploy-7c5596cb97-brtbb requesting resource cpu=320m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-typha-deploy-7c5596cb97-wzz4f requesting resource cpu=320m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-typha-horizontal-autoscaler-86db97cb8-fm2gn requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod calico-typha-vertical-autoscaler-8d8b46f5-pbqln requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod coredns-678f577694-8bdpf requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod coredns-678f577694-np4gr requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod csi-driver-node-fcrrs requesting resource cpu=33m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod csi-driver-node-mgpvm requesting resource cpu=33m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod egress-filter-applier-cj2st requesting resource cpu=50m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod egress-filter-applier-nwmq2 requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod kube-proxy-worker-1-v1.26.1-k2gbl requesting resource cpu=22m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod kube-proxy-worker-1-v1.26.1-rwknf requesting resource cpu=22m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod metrics-server-96cc55556-kzjrb requesting resource cpu=23m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod metrics-server-96cc55556-ltn74 requesting resource cpu=23m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod network-problem-detector-host-9gcfg requesting resource cpu=10m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod network-problem-detector-host-gj6px requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod network-problem-detector-pod-fbzrg requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod network-problem-detector-pod-tqj4w requesting resource cpu=10m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-exporter-nkxbw requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-exporter-pvzcs requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-local-dns-cks6x requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-local-dns-zssn4 requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-problem-detector-jv6z7 requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod node-problem-detector-s6t66 requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod vpn-shoot-55c8b48df6-6fhfb requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod dashboard-metrics-scraper-5cc9c9d874-588cq requesting resource cpu=0m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.447: INFO: Pod kubernetes-dashboard-5dd889f4f-zm2nt requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.447: INFO: Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 requesting resource cpu=0m on Node ip-10-250-19-246.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU. 03/16/23 10:21:29.447
Mar 16 10:21:29.447: INFO: Creating a pod which consumes cpu=744m on Node ip-10-250-19-136.ec2.internal
Mar 16 10:21:29.543: INFO: Creating a pod which consumes cpu=671m on Node ip-10-250-19-246.ec2.internal
Mar 16 10:21:29.637: INFO: Waiting up to 5m0s for pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962" in namespace "sched-pred-7077" to be "running"
Mar 16 10:21:29.727: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962": Phase="Pending", Reason="", readiness=false. Elapsed: 89.790929ms
Mar 16 10:21:31.818: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962": Phase="Running", Reason="", readiness=true. Elapsed: 2.180805578s
Mar 16 10:21:31.818: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962" satisfied condition "running"
Mar 16 10:21:31.818: INFO: Waiting up to 5m0s for pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa" in namespace "sched-pred-7077" to be "running"
Mar 16 10:21:31.908: INFO: Pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa": Phase="Running", Reason="", readiness=true. Elapsed: 89.824162ms
Mar 16 10:21:31.908: INFO: Pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/16/23 10:21:31.908
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf1ff81ed387], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7077/filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962 to ip-10-250-19-136.ec2.internal] 03/16/23 10:21:31.998
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf20187123db], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/16/23 10:21:31.998
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf20199e9691], Reason = [Created], Message = [Created container filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf201f5b7aaf], Reason = [Started], Message = [Started container filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf1ffdb48e87], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7077/filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa to ip-10-250-19-246.ec2.internal] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf201f4a7508], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf20205c4cbf], Reason = [Created], Message = [Created container filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf202689e8f9], Reason = [Started], Message = [Started container filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa] 03/16/23 10:21:31.999
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174cdf209523f2f5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 03/16/23 10:21:32.183
STEP: removing the label node off the node ip-10-250-19-136.ec2.internal 03/16/23 10:21:33.271
STEP: verifying the node doesn't have the label node 03/16/23 10:21:33.543
STEP: removing the label node off the node ip-10-250-19-246.ec2.internal 03/16/23 10:21:33.633
STEP: verifying the node doesn't have the label node 03/16/23 10:21:33.818
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7077" for this suite. 03/16/23 10:21:33.999
------------------------------
• [6.391 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:27.698
    Mar 16 10:21:27.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 03/16/23 10:21:27.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:27.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:28.148
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 16 10:21:28.326: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 16 10:21:28.506: INFO: Waiting for terminating namespaces to be deleted...
    Mar 16 10:21:28.596: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
    Mar 16 10:21:28.778: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container vpn-shoot ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.778: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 16 10:21:28.778: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
    Mar 16 10:21:28.874: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.874: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Mar 16 10:21:28.874: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.874: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Mar 16 10:21:28.875: INFO: server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 from pods-5742 started at 2023-03-16 10:21:20 +0000 UTC (1 container statuses recorded)
    Mar 16 10:21:28.875: INFO: 	Container srv ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-10-250-19-136.ec2.internal 03/16/23 10:21:29.071
    STEP: verifying the node has the label node ip-10-250-19-246.ec2.internal 03/16/23 10:21:29.259
    Mar 16 10:21:29.447: INFO: Pod addons-nginx-ingress-controller-dfbd87bb4-tpjwc requesting resource cpu=23m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c requesting resource cpu=20m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod apiserver-proxy-6595h requesting resource cpu=40m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod apiserver-proxy-lt25p requesting resource cpu=40m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod blackbox-exporter-5778995784-cxqf6 requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod blackbox-exporter-5778995784-z5cvh requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-node-9fxnl requesting resource cpu=260m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-node-z796q requesting resource cpu=260m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-typha-deploy-7c5596cb97-brtbb requesting resource cpu=320m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-typha-deploy-7c5596cb97-wzz4f requesting resource cpu=320m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-typha-horizontal-autoscaler-86db97cb8-fm2gn requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod calico-typha-vertical-autoscaler-8d8b46f5-pbqln requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod coredns-678f577694-8bdpf requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod coredns-678f577694-np4gr requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod csi-driver-node-fcrrs requesting resource cpu=33m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod csi-driver-node-mgpvm requesting resource cpu=33m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod egress-filter-applier-cj2st requesting resource cpu=50m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod egress-filter-applier-nwmq2 requesting resource cpu=50m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod kube-proxy-worker-1-v1.26.1-k2gbl requesting resource cpu=22m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod kube-proxy-worker-1-v1.26.1-rwknf requesting resource cpu=22m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod metrics-server-96cc55556-kzjrb requesting resource cpu=23m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod metrics-server-96cc55556-ltn74 requesting resource cpu=23m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod network-problem-detector-host-9gcfg requesting resource cpu=10m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod network-problem-detector-host-gj6px requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod network-problem-detector-pod-fbzrg requesting resource cpu=10m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod network-problem-detector-pod-tqj4w requesting resource cpu=10m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-exporter-nkxbw requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-exporter-pvzcs requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-local-dns-cks6x requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-local-dns-zssn4 requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-problem-detector-jv6z7 requesting resource cpu=11m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod node-problem-detector-s6t66 requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod vpn-shoot-55c8b48df6-6fhfb requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod dashboard-metrics-scraper-5cc9c9d874-588cq requesting resource cpu=0m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod kubernetes-dashboard-5dd889f4f-zm2nt requesting resource cpu=11m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.447: INFO: Pod server-envvars-97e55cf0-434f-4170-9fc8-c239514df712 requesting resource cpu=0m on Node ip-10-250-19-246.ec2.internal
    STEP: Starting Pods to consume most of the cluster CPU. 03/16/23 10:21:29.447
    Mar 16 10:21:29.447: INFO: Creating a pod which consumes cpu=744m on Node ip-10-250-19-136.ec2.internal
    Mar 16 10:21:29.543: INFO: Creating a pod which consumes cpu=671m on Node ip-10-250-19-246.ec2.internal
    Mar 16 10:21:29.637: INFO: Waiting up to 5m0s for pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962" in namespace "sched-pred-7077" to be "running"
    Mar 16 10:21:29.727: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962": Phase="Pending", Reason="", readiness=false. Elapsed: 89.790929ms
    Mar 16 10:21:31.818: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962": Phase="Running", Reason="", readiness=true. Elapsed: 2.180805578s
    Mar 16 10:21:31.818: INFO: Pod "filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962" satisfied condition "running"
    Mar 16 10:21:31.818: INFO: Waiting up to 5m0s for pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa" in namespace "sched-pred-7077" to be "running"
    Mar 16 10:21:31.908: INFO: Pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa": Phase="Running", Reason="", readiness=true. Elapsed: 89.824162ms
    Mar 16 10:21:31.908: INFO: Pod "filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/16/23 10:21:31.908
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf1ff81ed387], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7077/filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962 to ip-10-250-19-136.ec2.internal] 03/16/23 10:21:31.998
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf20187123db], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/16/23 10:21:31.998
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf20199e9691], Reason = [Created], Message = [Created container filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962.174cdf201f5b7aaf], Reason = [Started], Message = [Started container filler-pod-7b03aced-ba3d-4fdc-af29-2b710a420962] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf1ffdb48e87], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7077/filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa to ip-10-250-19-246.ec2.internal] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf201f4a7508], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf20205c4cbf], Reason = [Created], Message = [Created container filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa.174cdf202689e8f9], Reason = [Started], Message = [Started container filler-pod-a16e2a42-d911-4654-a483-f5c4448d6caa] 03/16/23 10:21:31.999
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174cdf209523f2f5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 03/16/23 10:21:32.183
    STEP: removing the label node off the node ip-10-250-19-136.ec2.internal 03/16/23 10:21:33.271
    STEP: verifying the node doesn't have the label node 03/16/23 10:21:33.543
    STEP: removing the label node off the node ip-10-250-19-246.ec2.internal 03/16/23 10:21:33.633
    STEP: verifying the node doesn't have the label node 03/16/23 10:21:33.818
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7077" for this suite. 03/16/23 10:21:33.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:34.091
Mar 16 10:21:34.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:21:34.092
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:34.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:34.539
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 03/16/23 10:21:34.717
Mar 16 10:21:34.717: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 create -f -'
Mar 16 10:21:35.914: INFO: stderr: ""
Mar 16 10:21:35.914: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/16/23 10:21:35.914
Mar 16 10:21:35.914: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 diff -f -'
Mar 16 10:21:36.572: INFO: rc: 1
Mar 16 10:21:36.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 delete -f -'
Mar 16 10:21:37.009: INFO: stderr: ""
Mar 16 10:21:37.009: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:37.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3524" for this suite. 03/16/23 10:21:37.19
------------------------------
• [3.189 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:34.091
    Mar 16 10:21:34.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:21:34.092
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:34.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:34.539
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 03/16/23 10:21:34.717
    Mar 16 10:21:34.717: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 create -f -'
    Mar 16 10:21:35.914: INFO: stderr: ""
    Mar 16 10:21:35.914: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/16/23 10:21:35.914
    Mar 16 10:21:35.914: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 diff -f -'
    Mar 16 10:21:36.572: INFO: rc: 1
    Mar 16 10:21:36.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3524 delete -f -'
    Mar 16 10:21:37.009: INFO: stderr: ""
    Mar 16 10:21:37.009: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:37.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3524" for this suite. 03/16/23 10:21:37.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:37.281
Mar 16 10:21:37.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:21:37.282
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:37.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:37.729
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 03/16/23 10:21:37.907
Mar 16 10:21:38.006: INFO: Waiting up to 5m0s for pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb" in namespace "downward-api-5845" to be "Succeeded or Failed"
Mar 16 10:21:38.096: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.693807ms
Mar 16 10:21:40.187: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180937247s
Mar 16 10:21:42.186: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180183271s
STEP: Saw pod success 03/16/23 10:21:42.186
Mar 16 10:21:42.186: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb" satisfied condition "Succeeded or Failed"
Mar 16 10:21:42.276: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb container dapi-container: <nil>
STEP: delete the pod 03/16/23 10:21:42.375
Mar 16 10:21:42.470: INFO: Waiting for pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb to disappear
Mar 16 10:21:42.559: INFO: Pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:42.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5845" for this suite. 03/16/23 10:21:42.738
------------------------------
• [5.547 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:37.281
    Mar 16 10:21:37.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:21:37.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:37.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:37.729
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 03/16/23 10:21:37.907
    Mar 16 10:21:38.006: INFO: Waiting up to 5m0s for pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb" in namespace "downward-api-5845" to be "Succeeded or Failed"
    Mar 16 10:21:38.096: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.693807ms
    Mar 16 10:21:40.187: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180937247s
    Mar 16 10:21:42.186: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180183271s
    STEP: Saw pod success 03/16/23 10:21:42.186
    Mar 16 10:21:42.186: INFO: Pod "downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb" satisfied condition "Succeeded or Failed"
    Mar 16 10:21:42.276: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb container dapi-container: <nil>
    STEP: delete the pod 03/16/23 10:21:42.375
    Mar 16 10:21:42.470: INFO: Waiting for pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb to disappear
    Mar 16 10:21:42.559: INFO: Pod downward-api-8c5d2d8a-aec3-4fe4-bdcb-7a5ac7463dcb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:42.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5845" for this suite. 03/16/23 10:21:42.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:42.829
Mar 16 10:21:42.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 10:21:42.83
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:43.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:43.278
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/16/23 10:21:43.546
STEP: waiting for Deployment to be created 03/16/23 10:21:43.636
STEP: waiting for all Replicas to be Ready 03/16/23 10:21:43.725
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.815: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:43.815: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 16 10:21:44.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 16 10:21:44.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 16 10:21:44.556: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/16/23 10:21:44.556
W0316 10:21:44.651229    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 16 10:21:44.740: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/16/23 10:21:44.74
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:45.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:45.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:45.512: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
STEP: listing Deployments 03/16/23 10:21:45.512
Mar 16 10:21:45.603: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/16/23 10:21:45.603
Mar 16 10:21:45.786: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/16/23 10:21:45.786
Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:46.509: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:46.517: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:46.519: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:46.526: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:46.542: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 16 10:21:47.590: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/16/23 10:21:47.604
STEP: fetching the DeploymentStatus 03/16/23 10:21:47.784
Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 3
STEP: deleting the Deployment 03/16/23 10:21:47.964
Mar 16 10:21:48.144: INFO: observed event type MODIFIED
Mar 16 10:21:48.144: INFO: observed event type MODIFIED
Mar 16 10:21:48.144: INFO: observed event type MODIFIED
Mar 16 10:21:48.144: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
Mar 16 10:21:48.145: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 10:21:48.234: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 10:21:48.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4061" for this suite. 03/16/23 10:21:48.414
------------------------------
• [5.676 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:42.829
    Mar 16 10:21:42.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 10:21:42.83
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:43.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:43.278
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/16/23 10:21:43.546
    STEP: waiting for Deployment to be created 03/16/23 10:21:43.636
    STEP: waiting for all Replicas to be Ready 03/16/23 10:21:43.725
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.814: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.815: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:43.815: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 16 10:21:44.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 16 10:21:44.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 16 10:21:44.556: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/16/23 10:21:44.556
    W0316 10:21:44.651229    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 16 10:21:44.740: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/16/23 10:21:44.74
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 0
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.829: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.915: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:44.916: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:45.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:45.502: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:45.512: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    STEP: listing Deployments 03/16/23 10:21:45.512
    Mar 16 10:21:45.603: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/16/23 10:21:45.603
    Mar 16 10:21:45.786: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/16/23 10:21:45.786
    Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:45.966: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:46.509: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:46.517: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:46.519: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:46.526: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:46.542: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 16 10:21:47.590: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/16/23 10:21:47.604
    STEP: fetching the DeploymentStatus 03/16/23 10:21:47.784
    Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:47.963: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 1
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 2
    Mar 16 10:21:47.964: INFO: observed Deployment test-deployment in namespace deployment-4061 with ReadyReplicas 3
    STEP: deleting the Deployment 03/16/23 10:21:47.964
    Mar 16 10:21:48.144: INFO: observed event type MODIFIED
    Mar 16 10:21:48.144: INFO: observed event type MODIFIED
    Mar 16 10:21:48.144: INFO: observed event type MODIFIED
    Mar 16 10:21:48.144: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    Mar 16 10:21:48.145: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 10:21:48.234: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:21:48.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4061" for this suite. 03/16/23 10:21:48.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:21:48.506
Mar 16 10:21:48.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 03/16/23 10:21:48.507
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:48.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:48.955
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/16/23 10:21:49.133
STEP: Ensuring a job is scheduled 03/16/23 10:21:49.223
STEP: Ensuring exactly one is scheduled 03/16/23 10:22:01.314
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/16/23 10:22:01.404
STEP: Ensuring no more jobs are scheduled 03/16/23 10:22:01.494
------------------------------
Automatically polling progress:
  [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] (Spec Runtime: 5m0.627s)
    test/e2e/apps/cronjob.go:124
    In [It] (Node Runtime: 5m0s)
      test/e2e/apps/cronjob.go:124
      At [By Step] Ensuring no more jobs are scheduled (Step Runtime: 4m47.64s)
        test/e2e/apps/cronjob.go:146

      Spec Goroutine
      goroutine 15079 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x7fe5ba8, 0xc000136000}, 0xc00523b0c8, 0x2fdd8ca?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fe5ba8, 0xc000136000}, 0x10?, 0x2fdc465?, 0x30?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x7fe5ba8, 0xc000136000}, 0x7672cca?, 0xc005279d60?, 0x262c967?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:460
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Poll(0x0?, 0xc0fcd9525d743253?, 0x20f4cb36201?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:445
      > k8s.io/kubernetes/test/e2e/apps.waitForActiveJobs({0x8022ee8?, 0xc005194d00}, {0xc004fd80e0, 0xc}, {0xc0051b8340, 0x6}, 0x2)
          test/e2e/apps/cronjob.go:593
      > k8s.io/kubernetes/test/e2e/apps.glob..func2.3()
          test/e2e/apps/cronjob.go:147
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc0037b6f30, 0xc004a269c0})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
STEP: Removing cronjob 03/16/23 10:27:01.675
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1585" for this suite. 03/16/23 10:27:01.943
• [SLOW TEST] [313.527 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:21:48.506
    Mar 16 10:21:48.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 03/16/23 10:21:48.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:21:48.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:21:48.955
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/16/23 10:21:49.133
    STEP: Ensuring a job is scheduled 03/16/23 10:21:49.223
    STEP: Ensuring exactly one is scheduled 03/16/23 10:22:01.314
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/16/23 10:22:01.404
    STEP: Ensuring no more jobs are scheduled 03/16/23 10:22:01.494
    STEP: Removing cronjob 03/16/23 10:27:01.675
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1585" for this suite. 03/16/23 10:27:01.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:02.034
Mar 16 10:27:02.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:27:02.036
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:02.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:02.486
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 03/16/23 10:27:02.778
STEP: watching for the Service to be added 03/16/23 10:27:02.875
Mar 16 10:27:02.964: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 16 10:27:02.964: INFO: Service test-service-ssv72 created
STEP: Getting /status 03/16/23 10:27:02.964
Mar 16 10:27:03.054: INFO: Service test-service-ssv72 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/16/23 10:27:03.054
STEP: watching for the Service to be patched 03/16/23 10:27:03.145
Mar 16 10:27:03.233: INFO: observed Service test-service-ssv72 in namespace services-9407 with annotations: map[] & LoadBalancer: {[]}
Mar 16 10:27:03.233: INFO: Found Service test-service-ssv72 in namespace services-9407 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 16 10:27:03.234: INFO: Service test-service-ssv72 has service status patched
STEP: updating the ServiceStatus 03/16/23 10:27:03.234
Mar 16 10:27:03.414: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/16/23 10:27:03.414
Mar 16 10:27:03.503: INFO: Observed Service test-service-ssv72 in namespace services-9407 with annotations: map[] & Conditions: {[]}
Mar 16 10:27:03.504: INFO: Observed event: &Service{ObjectMeta:{test-service-ssv72  services-9407  0f72b4ce-5fb0-45ea-94c3-87308ec22228 27480 0 2023-03-16 10:27:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-16 10:27:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-16 10:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.105.87.223,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.105.87.223],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 16 10:27:03.505: INFO: Found Service test-service-ssv72 in namespace services-9407 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 16 10:27:03.505: INFO: Service test-service-ssv72 has service status updated
STEP: patching the service 03/16/23 10:27:03.505
STEP: watching for the Service to be patched 03/16/23 10:27:03.6
Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
Mar 16 10:27:03.689: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service:patched test-service-static:true]
Mar 16 10:27:03.689: INFO: Service test-service-ssv72 patched
STEP: deleting the service 03/16/23 10:27:03.689
STEP: watching for the Service to be deleted 03/16/23 10:27:03.782
Mar 16 10:27:03.871: INFO: Observed event: ADDED
Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
Mar 16 10:27:03.871: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 16 10:27:03.871: INFO: Service test-service-ssv72 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:03.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9407" for this suite. 03/16/23 10:27:04.049
------------------------------
• [2.105 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:02.034
    Mar 16 10:27:02.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:27:02.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:02.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:02.486
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 03/16/23 10:27:02.778
    STEP: watching for the Service to be added 03/16/23 10:27:02.875
    Mar 16 10:27:02.964: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 16 10:27:02.964: INFO: Service test-service-ssv72 created
    STEP: Getting /status 03/16/23 10:27:02.964
    Mar 16 10:27:03.054: INFO: Service test-service-ssv72 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/16/23 10:27:03.054
    STEP: watching for the Service to be patched 03/16/23 10:27:03.145
    Mar 16 10:27:03.233: INFO: observed Service test-service-ssv72 in namespace services-9407 with annotations: map[] & LoadBalancer: {[]}
    Mar 16 10:27:03.233: INFO: Found Service test-service-ssv72 in namespace services-9407 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 16 10:27:03.234: INFO: Service test-service-ssv72 has service status patched
    STEP: updating the ServiceStatus 03/16/23 10:27:03.234
    Mar 16 10:27:03.414: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/16/23 10:27:03.414
    Mar 16 10:27:03.503: INFO: Observed Service test-service-ssv72 in namespace services-9407 with annotations: map[] & Conditions: {[]}
    Mar 16 10:27:03.504: INFO: Observed event: &Service{ObjectMeta:{test-service-ssv72  services-9407  0f72b4ce-5fb0-45ea-94c3-87308ec22228 27480 0 2023-03-16 10:27:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-16 10:27:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-16 10:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.105.87.223,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.105.87.223],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 16 10:27:03.505: INFO: Found Service test-service-ssv72 in namespace services-9407 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 16 10:27:03.505: INFO: Service test-service-ssv72 has service status updated
    STEP: patching the service 03/16/23 10:27:03.505
    STEP: watching for the Service to be patched 03/16/23 10:27:03.6
    Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
    Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
    Mar 16 10:27:03.689: INFO: observed Service test-service-ssv72 in namespace services-9407 with labels: map[test-service-static:true]
    Mar 16 10:27:03.689: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service:patched test-service-static:true]
    Mar 16 10:27:03.689: INFO: Service test-service-ssv72 patched
    STEP: deleting the service 03/16/23 10:27:03.689
    STEP: watching for the Service to be deleted 03/16/23 10:27:03.782
    Mar 16 10:27:03.871: INFO: Observed event: ADDED
    Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
    Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
    Mar 16 10:27:03.871: INFO: Observed event: MODIFIED
    Mar 16 10:27:03.871: INFO: Found Service test-service-ssv72 in namespace services-9407 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 16 10:27:03.871: INFO: Service test-service-ssv72 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:03.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9407" for this suite. 03/16/23 10:27:04.049
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:04.14
Mar 16 10:27:04.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper 03/16/23 10:27:04.141
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:04.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:04.588
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/16/23 10:27:04.766
STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:09.349
Mar 16 10:27:09.622: INFO: Pod name wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/16/23 10:27:09.622
Mar 16 10:27:09.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:09.712: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw": Phase="Pending", Reason="", readiness=false. Elapsed: 89.781419ms
Mar 16 10:27:11.890: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.268229916s
Mar 16 10:27:11.890: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw" satisfied condition "running"
Mar 16 10:27:11.890: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:11.980: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84": Phase="Running", Reason="", readiness=true. Elapsed: 89.652667ms
Mar 16 10:27:11.980: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84" satisfied condition "running"
Mar 16 10:27:11.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:12.069: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6": Phase="Running", Reason="", readiness=true. Elapsed: 89.608628ms
Mar 16 10:27:12.069: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6" satisfied condition "running"
Mar 16 10:27:12.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:12.159: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr": Phase="Running", Reason="", readiness=true. Elapsed: 89.731383ms
Mar 16 10:27:12.159: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr" satisfied condition "running"
Mar 16 10:27:12.159: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:12.249: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk": Phase="Running", Reason="", readiness=true. Elapsed: 89.772695ms
Mar 16 10:27:12.249: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:12.249
Mar 16 10:27:12.631: INFO: Deleting ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 took: 91.193381ms
Mar 16 10:27:12.733: INFO: Terminating ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 pods took: 101.157439ms
STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:15.527
Mar 16 10:27:15.714: INFO: Pod name wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/16/23 10:27:15.714
Mar 16 10:27:15.715: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:15.805: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.391726ms
Mar 16 10:27:17.984: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2": Phase="Running", Reason="", readiness=true. Elapsed: 2.268967387s
Mar 16 10:27:17.984: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2" satisfied condition "running"
Mar 16 10:27:17.984: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:18.074: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb": Phase="Running", Reason="", readiness=true. Elapsed: 89.917491ms
Mar 16 10:27:18.074: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb" satisfied condition "running"
Mar 16 10:27:18.074: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:18.164: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp": Phase="Running", Reason="", readiness=true. Elapsed: 90.091829ms
Mar 16 10:27:18.164: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp" satisfied condition "running"
Mar 16 10:27:18.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:18.254: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn": Phase="Running", Reason="", readiness=true. Elapsed: 89.964447ms
Mar 16 10:27:18.254: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn" satisfied condition "running"
Mar 16 10:27:18.254: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:18.344: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h": Phase="Running", Reason="", readiness=true. Elapsed: 90.16021ms
Mar 16 10:27:18.344: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:18.344
Mar 16 10:27:18.731: INFO: Deleting ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 took: 93.959357ms
Mar 16 10:27:18.831: INFO: Terminating ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 pods took: 100.248911ms
STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:20.623
Mar 16 10:27:20.809: INFO: Pod name wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/16/23 10:27:20.809
Mar 16 10:27:20.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:20.899: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9": Phase="Pending", Reason="", readiness=false. Elapsed: 90.14568ms
Mar 16 10:27:23.077: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.268463811s
Mar 16 10:27:23.078: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9" satisfied condition "running"
Mar 16 10:27:23.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:23.167: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch": Phase="Running", Reason="", readiness=true. Elapsed: 89.873175ms
Mar 16 10:27:23.167: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch" satisfied condition "running"
Mar 16 10:27:23.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:23.257: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc": Phase="Running", Reason="", readiness=true. Elapsed: 89.968842ms
Mar 16 10:27:23.257: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc" satisfied condition "running"
Mar 16 10:27:23.258: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:23.348: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k": Phase="Running", Reason="", readiness=true. Elapsed: 90.082979ms
Mar 16 10:27:23.348: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k" satisfied condition "running"
Mar 16 10:27:23.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf" in namespace "emptydir-wrapper-8231" to be "running"
Mar 16 10:27:23.438: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf": Phase="Running", Reason="", readiness=true. Elapsed: 90.107562ms
Mar 16 10:27:23.438: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:23.438
Mar 16 10:27:23.819: INFO: Deleting ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 took: 90.775215ms
Mar 16 10:27:23.920: INFO: Terminating ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 pods took: 100.862274ms
STEP: Cleaning up the configMaps 03/16/23 10:27:25.721
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:30.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8231" for this suite. 03/16/23 10:27:30.336
------------------------------
• [26.287 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:04.14
    Mar 16 10:27:04.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir-wrapper 03/16/23 10:27:04.141
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:04.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:04.588
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/16/23 10:27:04.766
    STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:09.349
    Mar 16 10:27:09.622: INFO: Pod name wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/16/23 10:27:09.622
    Mar 16 10:27:09.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:09.712: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw": Phase="Pending", Reason="", readiness=false. Elapsed: 89.781419ms
    Mar 16 10:27:11.890: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.268229916s
    Mar 16 10:27:11.890: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-4vxzw" satisfied condition "running"
    Mar 16 10:27:11.890: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:11.980: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84": Phase="Running", Reason="", readiness=true. Elapsed: 89.652667ms
    Mar 16 10:27:11.980: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9mr84" satisfied condition "running"
    Mar 16 10:27:11.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:12.069: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6": Phase="Running", Reason="", readiness=true. Elapsed: 89.608628ms
    Mar 16 10:27:12.069: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-9qvz6" satisfied condition "running"
    Mar 16 10:27:12.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:12.159: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr": Phase="Running", Reason="", readiness=true. Elapsed: 89.731383ms
    Mar 16 10:27:12.159: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-fddbr" satisfied condition "running"
    Mar 16 10:27:12.159: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:12.249: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk": Phase="Running", Reason="", readiness=true. Elapsed: 89.772695ms
    Mar 16 10:27:12.249: INFO: Pod "wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0-xcwpk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:12.249
    Mar 16 10:27:12.631: INFO: Deleting ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 took: 91.193381ms
    Mar 16 10:27:12.733: INFO: Terminating ReplicationController wrapped-volume-race-fa4be0f2-043e-4621-9ee3-664946c5f2e0 pods took: 101.157439ms
    STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:15.527
    Mar 16 10:27:15.714: INFO: Pod name wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/16/23 10:27:15.714
    Mar 16 10:27:15.715: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:15.805: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.391726ms
    Mar 16 10:27:17.984: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2": Phase="Running", Reason="", readiness=true. Elapsed: 2.268967387s
    Mar 16 10:27:17.984: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-dp6n2" satisfied condition "running"
    Mar 16 10:27:17.984: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:18.074: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb": Phase="Running", Reason="", readiness=true. Elapsed: 89.917491ms
    Mar 16 10:27:18.074: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-fkvmb" satisfied condition "running"
    Mar 16 10:27:18.074: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:18.164: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp": Phase="Running", Reason="", readiness=true. Elapsed: 90.091829ms
    Mar 16 10:27:18.164: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-hw8tp" satisfied condition "running"
    Mar 16 10:27:18.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:18.254: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn": Phase="Running", Reason="", readiness=true. Elapsed: 89.964447ms
    Mar 16 10:27:18.254: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-l25pn" satisfied condition "running"
    Mar 16 10:27:18.254: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:18.344: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h": Phase="Running", Reason="", readiness=true. Elapsed: 90.16021ms
    Mar 16 10:27:18.344: INFO: Pod "wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6-p6s2h" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:18.344
    Mar 16 10:27:18.731: INFO: Deleting ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 took: 93.959357ms
    Mar 16 10:27:18.831: INFO: Terminating ReplicationController wrapped-volume-race-cb2e9d53-b040-4036-9dc9-29a74a55b6e6 pods took: 100.248911ms
    STEP: Creating RC which spawns configmap-volume pods 03/16/23 10:27:20.623
    Mar 16 10:27:20.809: INFO: Pod name wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/16/23 10:27:20.809
    Mar 16 10:27:20.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:20.899: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9": Phase="Pending", Reason="", readiness=false. Elapsed: 90.14568ms
    Mar 16 10:27:23.077: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.268463811s
    Mar 16 10:27:23.078: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-d6jl9" satisfied condition "running"
    Mar 16 10:27:23.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:23.167: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch": Phase="Running", Reason="", readiness=true. Elapsed: 89.873175ms
    Mar 16 10:27:23.167: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-jb8ch" satisfied condition "running"
    Mar 16 10:27:23.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:23.257: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc": Phase="Running", Reason="", readiness=true. Elapsed: 89.968842ms
    Mar 16 10:27:23.257: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-mz5xc" satisfied condition "running"
    Mar 16 10:27:23.258: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:23.348: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k": Phase="Running", Reason="", readiness=true. Elapsed: 90.082979ms
    Mar 16 10:27:23.348: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-pzf2k" satisfied condition "running"
    Mar 16 10:27:23.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf" in namespace "emptydir-wrapper-8231" to be "running"
    Mar 16 10:27:23.438: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf": Phase="Running", Reason="", readiness=true. Elapsed: 90.107562ms
    Mar 16 10:27:23.438: INFO: Pod "wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4-s7ghf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 in namespace emptydir-wrapper-8231, will wait for the garbage collector to delete the pods 03/16/23 10:27:23.438
    Mar 16 10:27:23.819: INFO: Deleting ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 took: 90.775215ms
    Mar 16 10:27:23.920: INFO: Terminating ReplicationController wrapped-volume-race-9a9be177-9737-4225-9cf8-a0cfe7fa5ec4 pods took: 100.862274ms
    STEP: Cleaning up the configMaps 03/16/23 10:27:25.721
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:30.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8231" for this suite. 03/16/23 10:27:30.336
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:30.427
Mar 16 10:27:30.427: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:27:30.428
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:30.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:30.875
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:27:31.053
Mar 16 10:27:31.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764" in namespace "downward-api-5407" to be "Succeeded or Failed"
Mar 16 10:27:31.241: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Pending", Reason="", readiness=false. Elapsed: 89.938095ms
Mar 16 10:27:33.332: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181127294s
Mar 16 10:27:35.333: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182187714s
STEP: Saw pod success 03/16/23 10:27:35.333
Mar 16 10:27:35.333: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764" satisfied condition "Succeeded or Failed"
Mar 16 10:27:35.423: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 container client-container: <nil>
STEP: delete the pod 03/16/23 10:27:35.582
Mar 16 10:27:35.677: INFO: Waiting for pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 to disappear
Mar 16 10:27:35.767: INFO: Pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:35.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5407" for this suite. 03/16/23 10:27:35.945
------------------------------
• [5.609 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:30.427
    Mar 16 10:27:30.427: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:27:30.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:30.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:30.875
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:27:31.053
    Mar 16 10:27:31.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764" in namespace "downward-api-5407" to be "Succeeded or Failed"
    Mar 16 10:27:31.241: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Pending", Reason="", readiness=false. Elapsed: 89.938095ms
    Mar 16 10:27:33.332: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181127294s
    Mar 16 10:27:35.333: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182187714s
    STEP: Saw pod success 03/16/23 10:27:35.333
    Mar 16 10:27:35.333: INFO: Pod "downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764" satisfied condition "Succeeded or Failed"
    Mar 16 10:27:35.423: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:27:35.582
    Mar 16 10:27:35.677: INFO: Waiting for pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 to disappear
    Mar 16 10:27:35.767: INFO: Pod downwardapi-volume-a87fee00-de68-46aa-a15b-b265b1039764 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:35.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5407" for this suite. 03/16/23 10:27:35.945
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:36.036
Mar 16 10:27:36.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:27:36.037
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:36.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:36.483
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-039d298f-6178-40a3-9ec6-3302a9552653 03/16/23 10:27:37.023
STEP: Creating a pod to test consume secrets 03/16/23 10:27:37.113
Mar 16 10:27:37.210: INFO: Waiting up to 5m0s for pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f" in namespace "secrets-8542" to be "Succeeded or Failed"
Mar 16 10:27:37.300: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.973021ms
Mar 16 10:27:39.391: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180144914s
Mar 16 10:27:41.392: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181559535s
STEP: Saw pod success 03/16/23 10:27:41.392
Mar 16 10:27:41.392: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f" satisfied condition "Succeeded or Failed"
Mar 16 10:27:41.482: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-41c61585-1646-4559-9117-f126009f740f container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:27:41.616
Mar 16 10:27:41.710: INFO: Waiting for pod pod-secrets-41c61585-1646-4559-9117-f126009f740f to disappear
Mar 16 10:27:41.801: INFO: Pod pod-secrets-41c61585-1646-4559-9117-f126009f740f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8542" for this suite. 03/16/23 10:27:41.979
STEP: Destroying namespace "secret-namespace-967" for this suite. 03/16/23 10:27:42.07
------------------------------
• [6.124 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:36.036
    Mar 16 10:27:36.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:27:36.037
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:36.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:36.483
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-039d298f-6178-40a3-9ec6-3302a9552653 03/16/23 10:27:37.023
    STEP: Creating a pod to test consume secrets 03/16/23 10:27:37.113
    Mar 16 10:27:37.210: INFO: Waiting up to 5m0s for pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f" in namespace "secrets-8542" to be "Succeeded or Failed"
    Mar 16 10:27:37.300: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.973021ms
    Mar 16 10:27:39.391: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180144914s
    Mar 16 10:27:41.392: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181559535s
    STEP: Saw pod success 03/16/23 10:27:41.392
    Mar 16 10:27:41.392: INFO: Pod "pod-secrets-41c61585-1646-4559-9117-f126009f740f" satisfied condition "Succeeded or Failed"
    Mar 16 10:27:41.482: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-41c61585-1646-4559-9117-f126009f740f container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:27:41.616
    Mar 16 10:27:41.710: INFO: Waiting for pod pod-secrets-41c61585-1646-4559-9117-f126009f740f to disappear
    Mar 16 10:27:41.801: INFO: Pod pod-secrets-41c61585-1646-4559-9117-f126009f740f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8542" for this suite. 03/16/23 10:27:41.979
    STEP: Destroying namespace "secret-namespace-967" for this suite. 03/16/23 10:27:42.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:42.16
Mar 16 10:27:42.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables 03/16/23 10:27:42.162
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:42.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:42.608
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:42.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-5237" for this suite. 03/16/23 10:27:43.058
------------------------------
• [0.987 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:42.16
    Mar 16 10:27:42.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename tables 03/16/23 10:27:42.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:42.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:42.608
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:42.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-5237" for this suite. 03/16/23 10:27:43.058
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:43.148
Mar 16 10:27:43.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:27:43.149
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:43.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:43.596
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 03/16/23 10:27:43.774
STEP: setting up watch 03/16/23 10:27:43.774
STEP: submitting the pod to kubernetes 03/16/23 10:27:43.964
STEP: verifying the pod is in kubernetes 03/16/23 10:27:44.059
STEP: verifying pod creation was observed 03/16/23 10:27:44.149
Mar 16 10:27:44.149: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d" in namespace "pods-7616" to be "running"
Mar 16 10:27:44.238: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.474343ms
Mar 16 10:27:46.328: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d": Phase="Running", Reason="", readiness=true. Elapsed: 2.179726139s
Mar 16 10:27:46.329: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d" satisfied condition "running"
STEP: deleting the pod gracefully 03/16/23 10:27:46.418
STEP: verifying pod deletion was observed 03/16/23 10:27:46.509
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7616" for this suite. 03/16/23 10:27:48.648
------------------------------
• [5.590 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:43.148
    Mar 16 10:27:43.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:27:43.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:43.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:43.596
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 03/16/23 10:27:43.774
    STEP: setting up watch 03/16/23 10:27:43.774
    STEP: submitting the pod to kubernetes 03/16/23 10:27:43.964
    STEP: verifying the pod is in kubernetes 03/16/23 10:27:44.059
    STEP: verifying pod creation was observed 03/16/23 10:27:44.149
    Mar 16 10:27:44.149: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d" in namespace "pods-7616" to be "running"
    Mar 16 10:27:44.238: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.474343ms
    Mar 16 10:27:46.328: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d": Phase="Running", Reason="", readiness=true. Elapsed: 2.179726139s
    Mar 16 10:27:46.329: INFO: Pod "pod-submit-remove-2a1018be-2e11-43d2-85e1-1c05b5b7777d" satisfied condition "running"
    STEP: deleting the pod gracefully 03/16/23 10:27:46.418
    STEP: verifying pod deletion was observed 03/16/23 10:27:46.509
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7616" for this suite. 03/16/23 10:27:48.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:48.739
Mar 16 10:27:48.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 10:27:48.741
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:49.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:49.187
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 03/16/23 10:27:49.365
Mar 16 10:27:49.459: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7879" to be "running and ready"
Mar 16 10:27:49.549: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 89.384355ms
Mar 16 10:27:49.549: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:27:51.639: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.179366978s
Mar 16 10:27:51.639: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 16 10:27:51.639: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/16/23 10:27:51.729
STEP: Then the orphan pod is adopted 03/16/23 10:27:51.819
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7879" for this suite. 03/16/23 10:27:52.089
------------------------------
• [3.440 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:48.739
    Mar 16 10:27:48.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 10:27:48.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:49.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:49.187
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/16/23 10:27:49.365
    Mar 16 10:27:49.459: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7879" to be "running and ready"
    Mar 16 10:27:49.549: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 89.384355ms
    Mar 16 10:27:49.549: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:27:51.639: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.179366978s
    Mar 16 10:27:51.639: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 16 10:27:51.639: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/16/23 10:27:51.729
    STEP: Then the orphan pod is adopted 03/16/23 10:27:51.819
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7879" for this suite. 03/16/23 10:27:52.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:52.18
Mar 16 10:27:52.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csiinlinevolumes 03/16/23 10:27:52.181
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:52.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:52.627
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 03/16/23 10:27:52.805
STEP: getting 03/16/23 10:27:52.995
STEP: listing in namespace 03/16/23 10:27:53.085
STEP: patching 03/16/23 10:27:53.175
STEP: deleting 03/16/23 10:27:53.267
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:53.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-179" for this suite. 03/16/23 10:27:53.538
------------------------------
• [1.449 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:52.18
    Mar 16 10:27:52.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csiinlinevolumes 03/16/23 10:27:52.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:52.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:52.627
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 03/16/23 10:27:52.805
    STEP: getting 03/16/23 10:27:52.995
    STEP: listing in namespace 03/16/23 10:27:53.085
    STEP: patching 03/16/23 10:27:53.175
    STEP: deleting 03/16/23 10:27:53.267
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:53.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-179" for this suite. 03/16/23 10:27:53.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:53.631
Mar 16 10:27:53.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:27:53.633
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:53.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:54.081
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:27:54.259
Mar 16 10:27:54.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e" in namespace "projected-840" to be "Succeeded or Failed"
Mar 16 10:27:54.444: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.672315ms
Mar 16 10:27:56.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179811739s
Mar 16 10:27:58.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179549437s
STEP: Saw pod success 03/16/23 10:27:58.534
Mar 16 10:27:58.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e" satisfied condition "Succeeded or Failed"
Mar 16 10:27:58.624: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e container client-container: <nil>
STEP: delete the pod 03/16/23 10:27:58.762
Mar 16 10:27:58.857: INFO: Waiting for pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e to disappear
Mar 16 10:27:58.946: INFO: Pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:27:58.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-840" for this suite. 03/16/23 10:27:59.124
------------------------------
• [5.583 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:53.631
    Mar 16 10:27:53.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:27:53.633
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:53.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:54.081
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:27:54.259
    Mar 16 10:27:54.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e" in namespace "projected-840" to be "Succeeded or Failed"
    Mar 16 10:27:54.444: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.672315ms
    Mar 16 10:27:56.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179811739s
    Mar 16 10:27:58.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179549437s
    STEP: Saw pod success 03/16/23 10:27:58.534
    Mar 16 10:27:58.534: INFO: Pod "downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e" satisfied condition "Succeeded or Failed"
    Mar 16 10:27:58.624: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e container client-container: <nil>
    STEP: delete the pod 03/16/23 10:27:58.762
    Mar 16 10:27:58.857: INFO: Waiting for pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e to disappear
    Mar 16 10:27:58.946: INFO: Pod downwardapi-volume-eefaf247-6ceb-4e43-b43c-0014f8ab128e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:27:58.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-840" for this suite. 03/16/23 10:27:59.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:27:59.22
Mar 16 10:27:59.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 03/16/23 10:27:59.221
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:59.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:59.668
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Mar 16 10:27:59.939: INFO: Waiting up to 5m0s for pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02" in namespace "containers-4623" to be "running"
Mar 16 10:28:00.029: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02": Phase="Pending", Reason="", readiness=false. Elapsed: 89.288105ms
Mar 16 10:28:02.120: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02": Phase="Running", Reason="", readiness=true. Elapsed: 2.180278849s
Mar 16 10:28:02.120: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 16 10:28:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4623" for this suite. 03/16/23 10:28:02.598
------------------------------
• [3.468 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:27:59.22
    Mar 16 10:27:59.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 03/16/23 10:27:59.221
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:27:59.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:27:59.668
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Mar 16 10:27:59.939: INFO: Waiting up to 5m0s for pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02" in namespace "containers-4623" to be "running"
    Mar 16 10:28:00.029: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02": Phase="Pending", Reason="", readiness=false. Elapsed: 89.288105ms
    Mar 16 10:28:02.120: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02": Phase="Running", Reason="", readiness=true. Elapsed: 2.180278849s
    Mar 16 10:28:02.120: INFO: Pod "client-containers-297678c7-ad1e-41b3-a042-3c59b7c79f02" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:28:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4623" for this suite. 03/16/23 10:28:02.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:28:02.689
Mar 16 10:28:02.689: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 10:28:02.69
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:02.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:03.136
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:28:03.674
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:28:03.764
Mar 16 10:28:03.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:28:03.944: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:28:05.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:28:05.213: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:28:06.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:28:06.212: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 03/16/23 10:28:06.302
STEP: DeleteCollection of the DaemonSets 03/16/23 10:28:06.393
STEP: Verify that ReplicaSets have been deleted 03/16/23 10:28:06.485
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Mar 16 10:28:06.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28317"},"items":null}

Mar 16 10:28:06.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28318"},"items":[{"metadata":{"name":"daemon-set-87f4m","generateName":"daemon-set-","namespace":"daemonsets-5296","uid":"f92e1599-c215-4855-b070-d1287f1aeabe","resourceVersion":"28318","creationTimestamp":"2023-03-16T10:28:03Z","deletionTimestamp":"2023-03-16T10:28:36Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"444ce5b2e9800e588b4bdd28cf1257938f22bb773c0f0a9926034b709a356d2a","cni.projectcalico.org/podIP":"","cni.projectcalico.org/podIPs":""},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2b213e7d-9dea-43da-acdd-7b828b09d1ee","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b213e7d-9dea-43da-acdd-7b828b09d1ee\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-562lk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-562lk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-250-19-246.ec2.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-250-19-246.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"}],"hostIP":"10.250.19.246","podIP":"100.64.0.177","podIPs":[{"ip":"100.64.0.177"}],"startTime":"2023-03-16T10:28:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-16T10:28:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://158d88cacab7703a3c7e1c53062d347ba7200d9ec61c41d86ff7fe5a11d19114","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tm7bz","generateName":"daemon-set-","namespace":"daemonsets-5296","uid":"6517f225-fa0a-4903-8867-d12aaa5a691c","resourceVersion":"28316","creationTimestamp":"2023-03-16T10:28:03Z","deletionTimestamp":"2023-03-16T10:28:36Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"043ff3a559747985ad8368b4e2e8c2053922e8385d20886b399bc557d766e5db","cni.projectcalico.org/podIP":"100.64.1.34/32","cni.projectcalico.org/podIPs":"100.64.1.34/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2b213e7d-9dea-43da-acdd-7b828b09d1ee","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b213e7d-9dea-43da-acdd-7b828b09d1ee\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2fjnn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2fjnn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-250-19-136.ec2.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-250-19-136.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"}],"hostIP":"10.250.19.136","podIP":"100.64.1.34","podIPs":[{"ip":"100.64.1.34"}],"startTime":"2023-03-16T10:28:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-16T10:28:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://987d1dc02e8872d0ec81d8f79abf2bbbd1a165739a389d4a4c191db8cc2cea9f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:28:07.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5296" for this suite. 03/16/23 10:28:07.294
------------------------------
• [4.695 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:28:02.689
    Mar 16 10:28:02.689: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 10:28:02.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:02.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:03.136
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:28:03.674
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:28:03.764
    Mar 16 10:28:03.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:28:03.944: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:28:05.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:28:05.213: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:28:06.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:28:06.212: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 03/16/23 10:28:06.302
    STEP: DeleteCollection of the DaemonSets 03/16/23 10:28:06.393
    STEP: Verify that ReplicaSets have been deleted 03/16/23 10:28:06.485
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Mar 16 10:28:06.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28317"},"items":null}

    Mar 16 10:28:06.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28318"},"items":[{"metadata":{"name":"daemon-set-87f4m","generateName":"daemon-set-","namespace":"daemonsets-5296","uid":"f92e1599-c215-4855-b070-d1287f1aeabe","resourceVersion":"28318","creationTimestamp":"2023-03-16T10:28:03Z","deletionTimestamp":"2023-03-16T10:28:36Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"444ce5b2e9800e588b4bdd28cf1257938f22bb773c0f0a9926034b709a356d2a","cni.projectcalico.org/podIP":"","cni.projectcalico.org/podIPs":""},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2b213e7d-9dea-43da-acdd-7b828b09d1ee","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b213e7d-9dea-43da-acdd-7b828b09d1ee\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.0.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-562lk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-562lk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-250-19-246.ec2.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-250-19-246.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"}],"hostIP":"10.250.19.246","podIP":"100.64.0.177","podIPs":[{"ip":"100.64.0.177"}],"startTime":"2023-03-16T10:28:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-16T10:28:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://158d88cacab7703a3c7e1c53062d347ba7200d9ec61c41d86ff7fe5a11d19114","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tm7bz","generateName":"daemon-set-","namespace":"daemonsets-5296","uid":"6517f225-fa0a-4903-8867-d12aaa5a691c","resourceVersion":"28316","creationTimestamp":"2023-03-16T10:28:03Z","deletionTimestamp":"2023-03-16T10:28:36Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"043ff3a559747985ad8368b4e2e8c2053922e8385d20886b399bc557d766e5db","cni.projectcalico.org/podIP":"100.64.1.34/32","cni.projectcalico.org/podIPs":"100.64.1.34/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2b213e7d-9dea-43da-acdd-7b828b09d1ee","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b213e7d-9dea-43da-acdd-7b828b09d1ee\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-16T10:28:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2fjnn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2fjnn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-250-19-136.ec2.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-250-19-136.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-16T10:28:03Z"}],"hostIP":"10.250.19.136","podIP":"100.64.1.34","podIPs":[{"ip":"100.64.1.34"}],"startTime":"2023-03-16T10:28:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-16T10:28:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://987d1dc02e8872d0ec81d8f79abf2bbbd1a165739a389d4a4c191db8cc2cea9f","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:28:07.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5296" for this suite. 03/16/23 10:28:07.294
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:28:07.385
Mar 16 10:28:07.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:28:07.386
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:07.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:07.831
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 03/16/23 10:28:08.009
Mar 16 10:28:08.105: INFO: Waiting up to 5m0s for pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5" in namespace "pods-2359" to be "running and ready"
Mar 16 10:28:08.195: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.809323ms
Mar 16 10:28:08.195: INFO: The phase of Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:28:10.285: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.179839143s
Mar 16 10:28:10.285: INFO: The phase of Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 is Running (Ready = true)
Mar 16 10:28:10.285: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5" satisfied condition "running and ready"
Mar 16 10:28:10.464: INFO: Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 has hostIP: 10.250.19.246
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:28:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2359" for this suite. 03/16/23 10:28:10.642
------------------------------
• [3.347 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:28:07.385
    Mar 16 10:28:07.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:28:07.386
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:07.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:07.831
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 03/16/23 10:28:08.009
    Mar 16 10:28:08.105: INFO: Waiting up to 5m0s for pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5" in namespace "pods-2359" to be "running and ready"
    Mar 16 10:28:08.195: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.809323ms
    Mar 16 10:28:08.195: INFO: The phase of Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:28:10.285: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.179839143s
    Mar 16 10:28:10.285: INFO: The phase of Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 is Running (Ready = true)
    Mar 16 10:28:10.285: INFO: Pod "pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5" satisfied condition "running and ready"
    Mar 16 10:28:10.464: INFO: Pod pod-hostip-d9c6ecf0-b68c-48b7-8842-c881a16769b5 has hostIP: 10.250.19.246
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:28:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2359" for this suite. 03/16/23 10:28:10.642
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:28:10.732
Mar 16 10:28:10.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:28:10.733
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:11.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:11.18
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 03/16/23 10:28:11.358
Mar 16 10:28:11.358: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version 03/16/23 10:28:26.821
STEP: check the new version name is served 03/16/23 10:28:27.099
STEP: check the old version name is removed 03/16/23 10:28:32.346
STEP: check the other version is not changed 03/16/23 10:28:36.083
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:28:49.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4451" for this suite. 03/16/23 10:28:49.56
------------------------------
• [38.918 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:28:10.732
    Mar 16 10:28:10.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:28:10.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:11.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:11.18
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 03/16/23 10:28:11.358
    Mar 16 10:28:11.358: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: rename a version 03/16/23 10:28:26.821
    STEP: check the new version name is served 03/16/23 10:28:27.099
    STEP: check the old version name is removed 03/16/23 10:28:32.346
    STEP: check the other version is not changed 03/16/23 10:28:36.083
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:28:49.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4451" for this suite. 03/16/23 10:28:49.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:28:49.651
Mar 16 10:28:49.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:28:49.652
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:49.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:50.099
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Mar 16 10:28:50.461: INFO: Waiting up to 5m0s for pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f" in namespace "svcaccounts-6594" to be "running"
Mar 16 10:28:50.551: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f": Phase="Pending", Reason="", readiness=false. Elapsed: 90.025388ms
Mar 16 10:28:52.642: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f": Phase="Running", Reason="", readiness=true. Elapsed: 2.181044662s
Mar 16 10:28:52.642: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f" satisfied condition "running"
STEP: reading a file in the container 03/16/23 10:28:52.642
Mar 16 10:28:52.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/16/23 10:28:53.765
Mar 16 10:28:53.765: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/16/23 10:28:54.925
Mar 16 10:28:54.926: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 16 10:28:56.060: INFO: Got root ca configmap in namespace "svcaccounts-6594"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:28:56.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6594" for this suite. 03/16/23 10:28:56.328
------------------------------
• [6.767 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:28:49.651
    Mar 16 10:28:49.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:28:49.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:49.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:50.099
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Mar 16 10:28:50.461: INFO: Waiting up to 5m0s for pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f" in namespace "svcaccounts-6594" to be "running"
    Mar 16 10:28:50.551: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f": Phase="Pending", Reason="", readiness=false. Elapsed: 90.025388ms
    Mar 16 10:28:52.642: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f": Phase="Running", Reason="", readiness=true. Elapsed: 2.181044662s
    Mar 16 10:28:52.642: INFO: Pod "pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f" satisfied condition "running"
    STEP: reading a file in the container 03/16/23 10:28:52.642
    Mar 16 10:28:52.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/16/23 10:28:53.765
    Mar 16 10:28:53.765: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/16/23 10:28:54.925
    Mar 16 10:28:54.926: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6594 pod-service-account-1625e03e-4441-4ca2-92da-8a58af7da12f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 16 10:28:56.060: INFO: Got root ca configmap in namespace "svcaccounts-6594"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:28:56.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6594" for this suite. 03/16/23 10:28:56.328
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:28:56.419
Mar 16 10:28:56.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:28:56.42
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:56.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:56.874
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-143892c6-6cab-409e-923d-c3ade2ab02c3 03/16/23 10:28:57.052
STEP: Creating a pod to test consume secrets 03/16/23 10:28:57.142
Mar 16 10:28:57.237: INFO: Waiting up to 5m0s for pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b" in namespace "secrets-878" to be "Succeeded or Failed"
Mar 16 10:28:57.327: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.633368ms
Mar 16 10:28:59.418: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180641271s
Mar 16 10:29:01.417: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179935285s
STEP: Saw pod success 03/16/23 10:29:01.417
Mar 16 10:29:01.417: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b" satisfied condition "Succeeded or Failed"
Mar 16 10:29:01.507: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:29:01.751
Mar 16 10:29:01.844: INFO: Waiting for pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b to disappear
Mar 16 10:29:01.934: INFO: Pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-878" for this suite. 03/16/23 10:29:02.112
------------------------------
• [5.784 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:28:56.419
    Mar 16 10:28:56.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:28:56.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:28:56.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:28:56.874
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-143892c6-6cab-409e-923d-c3ade2ab02c3 03/16/23 10:28:57.052
    STEP: Creating a pod to test consume secrets 03/16/23 10:28:57.142
    Mar 16 10:28:57.237: INFO: Waiting up to 5m0s for pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b" in namespace "secrets-878" to be "Succeeded or Failed"
    Mar 16 10:28:57.327: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.633368ms
    Mar 16 10:28:59.418: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180641271s
    Mar 16 10:29:01.417: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179935285s
    STEP: Saw pod success 03/16/23 10:29:01.417
    Mar 16 10:29:01.417: INFO: Pod "pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b" satisfied condition "Succeeded or Failed"
    Mar 16 10:29:01.507: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:29:01.751
    Mar 16 10:29:01.844: INFO: Waiting for pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b to disappear
    Mar 16 10:29:01.934: INFO: Pod pod-secrets-7b897421-dd05-47d2-a68b-a38abebba54b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-878" for this suite. 03/16/23 10:29:02.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:02.203
Mar 16 10:29:02.204: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:29:02.206
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:02.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:02.653
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 03/16/23 10:29:02.831
STEP: Creating a ResourceQuota 03/16/23 10:29:07.922
STEP: Ensuring resource quota status is calculated 03/16/23 10:29:08.012
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:10.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1562" for this suite. 03/16/23 10:29:10.281
------------------------------
• [8.169 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:02.203
    Mar 16 10:29:02.204: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:29:02.206
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:02.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:02.653
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 03/16/23 10:29:02.831
    STEP: Creating a ResourceQuota 03/16/23 10:29:07.922
    STEP: Ensuring resource quota status is calculated 03/16/23 10:29:08.012
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:10.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1562" for this suite. 03/16/23 10:29:10.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:10.373
Mar 16 10:29:10.373: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:29:10.375
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:10.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:10.822
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 03/16/23 10:29:10.999
Mar 16 10:29:11.000: INFO: Creating e2e-svc-a-nzwwf
Mar 16 10:29:11.094: INFO: Creating e2e-svc-b-4658z
Mar 16 10:29:11.187: INFO: Creating e2e-svc-c-dkqq7
STEP: deleting service collection 03/16/23 10:29:11.371
Mar 16 10:29:11.561: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:11.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1527" for this suite. 03/16/23 10:29:11.651
------------------------------
• [1.369 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:10.373
    Mar 16 10:29:10.373: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:29:10.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:10.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:10.822
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 03/16/23 10:29:10.999
    Mar 16 10:29:11.000: INFO: Creating e2e-svc-a-nzwwf
    Mar 16 10:29:11.094: INFO: Creating e2e-svc-b-4658z
    Mar 16 10:29:11.187: INFO: Creating e2e-svc-c-dkqq7
    STEP: deleting service collection 03/16/23 10:29:11.371
    Mar 16 10:29:11.561: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:11.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1527" for this suite. 03/16/23 10:29:11.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:11.742
Mar 16 10:29:11.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:29:11.743
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:12.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:12.189
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 03/16/23 10:29:12.457
STEP: waiting for available Endpoint 03/16/23 10:29:12.547
STEP: listing all Endpoints 03/16/23 10:29:12.636
STEP: updating the Endpoint 03/16/23 10:29:12.726
STEP: fetching the Endpoint 03/16/23 10:29:12.905
STEP: patching the Endpoint 03/16/23 10:29:12.995
STEP: fetching the Endpoint 03/16/23 10:29:13.192
STEP: deleting the Endpoint by Collection 03/16/23 10:29:13.282
STEP: waiting for Endpoint deletion 03/16/23 10:29:13.373
STEP: fetching the Endpoint 03/16/23 10:29:13.462
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:13.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6493" for this suite. 03/16/23 10:29:13.642
------------------------------
• [1.991 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:11.742
    Mar 16 10:29:11.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:29:11.743
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:12.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:12.189
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 03/16/23 10:29:12.457
    STEP: waiting for available Endpoint 03/16/23 10:29:12.547
    STEP: listing all Endpoints 03/16/23 10:29:12.636
    STEP: updating the Endpoint 03/16/23 10:29:12.726
    STEP: fetching the Endpoint 03/16/23 10:29:12.905
    STEP: patching the Endpoint 03/16/23 10:29:12.995
    STEP: fetching the Endpoint 03/16/23 10:29:13.192
    STEP: deleting the Endpoint by Collection 03/16/23 10:29:13.282
    STEP: waiting for Endpoint deletion 03/16/23 10:29:13.373
    STEP: fetching the Endpoint 03/16/23 10:29:13.462
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:13.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6493" for this suite. 03/16/23 10:29:13.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:13.734
Mar 16 10:29:13.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:29:13.735
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:14.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:14.182
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:29:14.359
Mar 16 10:29:14.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173" in namespace "downward-api-6991" to be "Succeeded or Failed"
Mar 16 10:29:14.545: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Pending", Reason="", readiness=false. Elapsed: 89.635035ms
Mar 16 10:29:16.635: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180472541s
Mar 16 10:29:18.636: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180752001s
STEP: Saw pod success 03/16/23 10:29:18.636
Mar 16 10:29:18.636: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173" satisfied condition "Succeeded or Failed"
Mar 16 10:29:18.725: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 container client-container: <nil>
STEP: delete the pod 03/16/23 10:29:18.82
Mar 16 10:29:18.918: INFO: Waiting for pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 to disappear
Mar 16 10:29:19.007: INFO: Pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:19.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6991" for this suite. 03/16/23 10:29:19.185
------------------------------
• [5.542 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:13.734
    Mar 16 10:29:13.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:29:13.735
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:14.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:14.182
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:29:14.359
    Mar 16 10:29:14.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173" in namespace "downward-api-6991" to be "Succeeded or Failed"
    Mar 16 10:29:14.545: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Pending", Reason="", readiness=false. Elapsed: 89.635035ms
    Mar 16 10:29:16.635: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180472541s
    Mar 16 10:29:18.636: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180752001s
    STEP: Saw pod success 03/16/23 10:29:18.636
    Mar 16 10:29:18.636: INFO: Pod "downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173" satisfied condition "Succeeded or Failed"
    Mar 16 10:29:18.725: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:29:18.82
    Mar 16 10:29:18.918: INFO: Waiting for pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 to disappear
    Mar 16 10:29:19.007: INFO: Pod downwardapi-volume-e6aa2f21-de43-49cb-9a9e-d00225d49173 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:19.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6991" for this suite. 03/16/23 10:29:19.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:19.276
Mar 16 10:29:19.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:29:19.277
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:19.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:19.724
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6252 03/16/23 10:29:19.902
STEP: creating service affinity-clusterip in namespace services-6252 03/16/23 10:29:19.902
STEP: creating replication controller affinity-clusterip in namespace services-6252 03/16/23 10:29:19.996
I0316 10:29:20.086754    8588 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6252, replica count: 3
I0316 10:29:23.187680    8588 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:29:23.366: INFO: Creating new exec pod
Mar 16 10:29:23.459: INFO: Waiting up to 5m0s for pod "execpod-affinityjbtdb" in namespace "services-6252" to be "running"
Mar 16 10:29:23.549: INFO: Pod "execpod-affinityjbtdb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.309983ms
Mar 16 10:29:25.640: INFO: Pod "execpod-affinityjbtdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.180440615s
Mar 16 10:29:25.640: INFO: Pod "execpod-affinityjbtdb" satisfied condition "running"
Mar 16 10:29:26.640: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Mar 16 10:29:27.781: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 16 10:29:27.781: INFO: stdout: ""
Mar 16 10:29:27.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c nc -v -z -w 2 100.107.186.174 80'
Mar 16 10:29:28.908: INFO: stderr: "+ nc -v -z -w 2 100.107.186.174 80\nConnection to 100.107.186.174 80 port [tcp/http] succeeded!\n"
Mar 16 10:29:28.908: INFO: stdout: ""
Mar 16 10:29:28.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.186.174:80/ ; done'
Mar 16 10:29:30.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n"
Mar 16 10:29:30.097: INFO: stdout: "\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n"
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
Mar 16 10:29:30.097: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6252, will wait for the garbage collector to delete the pods 03/16/23 10:29:30.19
Mar 16 10:29:30.471: INFO: Deleting ReplicationController affinity-clusterip took: 90.692634ms
Mar 16 10:29:30.572: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.220846ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:29:32.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6252" for this suite. 03/16/23 10:29:33.059
------------------------------
• [13.873 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:19.276
    Mar 16 10:29:19.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:29:19.277
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:19.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:19.724
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6252 03/16/23 10:29:19.902
    STEP: creating service affinity-clusterip in namespace services-6252 03/16/23 10:29:19.902
    STEP: creating replication controller affinity-clusterip in namespace services-6252 03/16/23 10:29:19.996
    I0316 10:29:20.086754    8588 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6252, replica count: 3
    I0316 10:29:23.187680    8588 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:29:23.366: INFO: Creating new exec pod
    Mar 16 10:29:23.459: INFO: Waiting up to 5m0s for pod "execpod-affinityjbtdb" in namespace "services-6252" to be "running"
    Mar 16 10:29:23.549: INFO: Pod "execpod-affinityjbtdb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.309983ms
    Mar 16 10:29:25.640: INFO: Pod "execpod-affinityjbtdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.180440615s
    Mar 16 10:29:25.640: INFO: Pod "execpod-affinityjbtdb" satisfied condition "running"
    Mar 16 10:29:26.640: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Mar 16 10:29:27.781: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 16 10:29:27.781: INFO: stdout: ""
    Mar 16 10:29:27.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c nc -v -z -w 2 100.107.186.174 80'
    Mar 16 10:29:28.908: INFO: stderr: "+ nc -v -z -w 2 100.107.186.174 80\nConnection to 100.107.186.174 80 port [tcp/http] succeeded!\n"
    Mar 16 10:29:28.908: INFO: stdout: ""
    Mar 16 10:29:28.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6252 exec execpod-affinityjbtdb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.186.174:80/ ; done'
    Mar 16 10:29:30.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.186.174:80/\n"
    Mar 16 10:29:30.097: INFO: stdout: "\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n\naffinity-clusterip-j5q8n"
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Received response from host: affinity-clusterip-j5q8n
    Mar 16 10:29:30.097: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6252, will wait for the garbage collector to delete the pods 03/16/23 10:29:30.19
    Mar 16 10:29:30.471: INFO: Deleting ReplicationController affinity-clusterip took: 90.692634ms
    Mar 16 10:29:30.572: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.220846ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:29:32.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6252" for this suite. 03/16/23 10:29:33.059
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:29:33.149
Mar 16 10:29:33.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 03/16/23 10:29:33.151
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:33.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:33.596
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 16 10:29:34.042: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 10:30:34.854: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:30:34.943
Mar 16 10:30:34.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path 03/16/23 10:30:34.944
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:35.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:35.39
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Mar 16 10:30:35.837: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 16 10:30:35.926: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Mar 16 10:30:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:30:36.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8540" for this suite. 03/16/23 10:30:37.11
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-962" for this suite. 03/16/23 10:30:37.2
------------------------------
• [64.141 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:29:33.149
    Mar 16 10:29:33.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 03/16/23 10:29:33.151
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:29:33.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:29:33.596
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 16 10:29:34.042: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 10:30:34.854: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:30:34.943
    Mar 16 10:30:34.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption-path 03/16/23 10:30:34.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:35.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:35.39
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Mar 16 10:30:35.837: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 16 10:30:35.926: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:30:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:30:36.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8540" for this suite. 03/16/23 10:30:37.11
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-962" for this suite. 03/16/23 10:30:37.2
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:30:37.291
Mar 16 10:30:37.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 03/16/23 10:30:37.292
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:37.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:37.737
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/16/23 10:30:37.914
STEP: get a list of Events with a label in the current namespace 03/16/23 10:30:38.184
STEP: delete a list of events 03/16/23 10:30:38.274
Mar 16 10:30:38.274: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/16/23 10:30:38.37
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 16 10:30:38.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3021" for this suite. 03/16/23 10:30:38.549
------------------------------
• [1.348 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:30:37.291
    Mar 16 10:30:37.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 03/16/23 10:30:37.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:37.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:37.737
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/16/23 10:30:37.914
    STEP: get a list of Events with a label in the current namespace 03/16/23 10:30:38.184
    STEP: delete a list of events 03/16/23 10:30:38.274
    Mar 16 10:30:38.274: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/16/23 10:30:38.37
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:30:38.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3021" for this suite. 03/16/23 10:30:38.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:30:38.64
Mar 16 10:30:38.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 03/16/23 10:30:38.642
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:38.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:39.087
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 03/16/23 10:30:39.264
STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.366
STEP: updating the pdb 03/16/23 10:30:39.455
STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.635
STEP: patching the pdb 03/16/23 10:30:39.724
STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.904
STEP: Waiting for the pdb to be deleted 03/16/23 10:30:40.083
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:30:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4534" for this suite. 03/16/23 10:30:40.264
------------------------------
• [1.713 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:30:38.64
    Mar 16 10:30:38.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 03/16/23 10:30:38.642
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:38.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:39.087
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 03/16/23 10:30:39.264
    STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.366
    STEP: updating the pdb 03/16/23 10:30:39.455
    STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.635
    STEP: patching the pdb 03/16/23 10:30:39.724
    STEP: Waiting for the pdb to be processed 03/16/23 10:30:39.904
    STEP: Waiting for the pdb to be deleted 03/16/23 10:30:40.083
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:30:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4534" for this suite. 03/16/23 10:30:40.264
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:30:40.354
Mar 16 10:30:40.354: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:30:40.356
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:40.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:40.801
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 03/16/23 10:30:40.978
Mar 16 10:30:41.072: INFO: Waiting up to 5m0s for pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3" in namespace "emptydir-3418" to be "Succeeded or Failed"
Mar 16 10:30:41.161: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Pending", Reason="", readiness=false. Elapsed: 88.989935ms
Mar 16 10:30:43.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180009718s
Mar 16 10:30:45.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179653792s
STEP: Saw pod success 03/16/23 10:30:45.252
Mar 16 10:30:45.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3" satisfied condition "Succeeded or Failed"
Mar 16 10:30:45.342: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 container test-container: <nil>
STEP: delete the pod 03/16/23 10:30:45.435
Mar 16 10:30:45.528: INFO: Waiting for pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 to disappear
Mar 16 10:30:45.617: INFO: Pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:30:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3418" for this suite. 03/16/23 10:30:45.795
------------------------------
• [5.531 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:30:40.354
    Mar 16 10:30:40.354: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:30:40.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:40.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:40.801
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 03/16/23 10:30:40.978
    Mar 16 10:30:41.072: INFO: Waiting up to 5m0s for pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3" in namespace "emptydir-3418" to be "Succeeded or Failed"
    Mar 16 10:30:41.161: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Pending", Reason="", readiness=false. Elapsed: 88.989935ms
    Mar 16 10:30:43.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180009718s
    Mar 16 10:30:45.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179653792s
    STEP: Saw pod success 03/16/23 10:30:45.252
    Mar 16 10:30:45.252: INFO: Pod "pod-99fd1fab-8794-4073-b973-cabba52e42e3" satisfied condition "Succeeded or Failed"
    Mar 16 10:30:45.342: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 container test-container: <nil>
    STEP: delete the pod 03/16/23 10:30:45.435
    Mar 16 10:30:45.528: INFO: Waiting for pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 to disappear
    Mar 16 10:30:45.617: INFO: Pod pod-99fd1fab-8794-4073-b973-cabba52e42e3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:30:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3418" for this suite. 03/16/23 10:30:45.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:30:45.886
Mar 16 10:30:45.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:30:45.887
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:46.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:46.333
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8298 03/16/23 10:30:46.51
STEP: creating a selector 03/16/23 10:30:46.51
STEP: Creating the service pods in kubernetes 03/16/23 10:30:46.51
Mar 16 10:30:46.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 16 10:30:46.878: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8298" to be "running and ready"
Mar 16 10:30:46.967: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.209705ms
Mar 16 10:30:46.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:30:49.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.179818032s
Mar 16 10:30:49.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:30:51.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.179913893s
Mar 16 10:30:51.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:30:53.057: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.178990503s
Mar 16 10:30:53.057: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:30:55.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.17984768s
Mar 16 10:30:55.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:30:57.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.179457035s
Mar 16 10:30:57.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:30:59.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.180000993s
Mar 16 10:30:59.058: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 16 10:30:59.058: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 16 10:30:59.148: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8298" to be "running and ready"
Mar 16 10:30:59.237: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.172149ms
Mar 16 10:30:59.237: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 16 10:30:59.237: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/16/23 10:30:59.326
Mar 16 10:30:59.419: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8298" to be "running"
Mar 16 10:30:59.509: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.382474ms
Mar 16 10:31:01.599: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179733063s
Mar 16 10:31:01.599: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 16 10:31:01.688: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 16 10:31:01.688: INFO: Breadth first check of 100.64.1.40 on host 10.250.19.136...
Mar 16 10:31:01.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.41:9080/dial?request=hostname&protocol=udp&host=100.64.1.40&port=8081&tries=1'] Namespace:pod-network-test-8298 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:31:01.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:31:01.779: INFO: ExecWithOptions: Clientset creation
Mar 16 10:31:01.779: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8298/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 16 10:31:02.537: INFO: Waiting for responses: map[]
Mar 16 10:31:02.537: INFO: reached 100.64.1.40 after 0/1 tries
Mar 16 10:31:02.537: INFO: Breadth first check of 100.64.0.182 on host 10.250.19.246...
Mar 16 10:31:02.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.41:9080/dial?request=hostname&protocol=udp&host=100.64.0.182&port=8081&tries=1'] Namespace:pod-network-test-8298 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:31:02.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:31:02.627: INFO: ExecWithOptions: Clientset creation
Mar 16 10:31:02.627: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8298/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.182%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 16 10:31:03.382: INFO: Waiting for responses: map[]
Mar 16 10:31:03.382: INFO: reached 100.64.0.182 after 0/1 tries
Mar 16 10:31:03.382: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8298" for this suite. 03/16/23 10:31:03.56
------------------------------
• [17.765 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:30:45.886
    Mar 16 10:30:45.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:30:45.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:30:46.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:30:46.333
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8298 03/16/23 10:30:46.51
    STEP: creating a selector 03/16/23 10:30:46.51
    STEP: Creating the service pods in kubernetes 03/16/23 10:30:46.51
    Mar 16 10:30:46.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 16 10:30:46.878: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8298" to be "running and ready"
    Mar 16 10:30:46.967: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.209705ms
    Mar 16 10:30:46.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:30:49.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.179818032s
    Mar 16 10:30:49.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:30:51.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.179913893s
    Mar 16 10:30:51.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:30:53.057: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.178990503s
    Mar 16 10:30:53.057: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:30:55.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.17984768s
    Mar 16 10:30:55.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:30:57.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.179457035s
    Mar 16 10:30:57.058: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:30:59.058: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.180000993s
    Mar 16 10:30:59.058: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 16 10:30:59.058: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 16 10:30:59.148: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8298" to be "running and ready"
    Mar 16 10:30:59.237: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.172149ms
    Mar 16 10:30:59.237: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 16 10:30:59.237: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/16/23 10:30:59.326
    Mar 16 10:30:59.419: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8298" to be "running"
    Mar 16 10:30:59.509: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.382474ms
    Mar 16 10:31:01.599: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179733063s
    Mar 16 10:31:01.599: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 16 10:31:01.688: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar 16 10:31:01.688: INFO: Breadth first check of 100.64.1.40 on host 10.250.19.136...
    Mar 16 10:31:01.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.41:9080/dial?request=hostname&protocol=udp&host=100.64.1.40&port=8081&tries=1'] Namespace:pod-network-test-8298 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:31:01.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:31:01.779: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:31:01.779: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8298/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.1.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 16 10:31:02.537: INFO: Waiting for responses: map[]
    Mar 16 10:31:02.537: INFO: reached 100.64.1.40 after 0/1 tries
    Mar 16 10:31:02.537: INFO: Breadth first check of 100.64.0.182 on host 10.250.19.246...
    Mar 16 10:31:02.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.41:9080/dial?request=hostname&protocol=udp&host=100.64.0.182&port=8081&tries=1'] Namespace:pod-network-test-8298 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:31:02.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:31:02.627: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:31:02.627: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8298/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.41%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.64.0.182%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 16 10:31:03.382: INFO: Waiting for responses: map[]
    Mar 16 10:31:03.382: INFO: reached 100.64.0.182 after 0/1 tries
    Mar 16 10:31:03.382: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8298" for this suite. 03/16/23 10:31:03.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:03.652
Mar 16 10:31:03.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 03/16/23 10:31:03.653
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:03.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:04.099
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 03/16/23 10:31:04.276
Mar 16 10:31:04.276: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3586" for this suite. 03/16/23 10:31:10.156
------------------------------
• [6.594 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:03.652
    Mar 16 10:31:03.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 03/16/23 10:31:03.653
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:03.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:04.099
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 03/16/23 10:31:04.276
    Mar 16 10:31:04.276: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3586" for this suite. 03/16/23 10:31:10.156
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:10.246
Mar 16 10:31:10.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 10:31:10.247
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:10.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:10.693
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 16 10:31:10.870: INFO: Creating simple deployment test-new-deployment
Mar 16 10:31:11.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 03/16/23 10:31:13.407
STEP: updating a scale subresource 03/16/23 10:31:13.496
STEP: verifying the deployment Spec.Replicas was modified 03/16/23 10:31:13.588
STEP: Patch a scale subresource 03/16/23 10:31:13.677
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 10:31:13.947: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9376  b1b25bb9-1dca-4850-9665-6d72234ff826 29828 3 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-16 10:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ec6fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:4,UpdatedReplicas:4,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-16 10:31:11 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 10:31:13 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 16 10:31:14.037: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9376  174d9f65-15e4-4737-84dc-c79604590e57 29826 3 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b1b25bb9-1dca-4850-9665-6d72234ff826 0xc004abb287 0xc004abb288}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1b25bb9-1dca-4850-9665-6d72234ff826\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004abb318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:31:14.214: INFO: Pod "test-new-deployment-7f5969cbc7-4zj8j" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4zj8j test-new-deployment-7f5969cbc7- deployment-9376  e01a4d4d-6ee3-405f-9cae-009712a0ed49 29825 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7387 0xc005ec7388}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4czb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4czb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 10:31:14.214: INFO: Pod "test-new-deployment-7f5969cbc7-8lsfd" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8lsfd test-new-deployment-7f5969cbc7- deployment-9376  5f41d27a-1ab9-424a-bfe5-e3b7a57a26f7 29831 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cdffa8234dcf5053e908b9dca780d8bef27bd81c11e1003eeffc8b323e0d0a15 cni.projectcalico.org/podIP:100.64.0.183/32 cni.projectcalico.org/podIPs:100.64.0.183/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7577 0xc005ec7578}] [] [{calico Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lt5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lt5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 10:31:14.215: INFO: Pod "test-new-deployment-7f5969cbc7-tv2vd" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tv2vd test-new-deployment-7f5969cbc7- deployment-9376  fd41efda-8a38-453c-8295-0eaf2eca5d85 29795 0 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:50140a15181105768dec6176d3b19aecd40d5b3db14ea85e3cf380b4ae2918d8 cni.projectcalico.org/podIP:100.64.1.43/32 cni.projectcalico.org/podIPs:100.64.1.43/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7767 0xc005ec7768}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n86fs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n86fs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.43,StartTime:2023-03-16 10:31:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:31:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://08cc5261f38dbead2248a3e4e5b6bdd5e224c4e0c1bdbc3207ce3c56f11229dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 10:31:14.215: INFO: Pod "test-new-deployment-7f5969cbc7-tvgnh" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tvgnh test-new-deployment-7f5969cbc7- deployment-9376  b1039d90-183b-4556-921a-7268821b7246 29827 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7960 0xc005ec7961}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qg4v9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qg4v9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:14.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9376" for this suite. 03/16/23 10:31:14.306
------------------------------
• [4.151 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:10.246
    Mar 16 10:31:10.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 10:31:10.247
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:10.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:10.693
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 16 10:31:10.870: INFO: Creating simple deployment test-new-deployment
    Mar 16 10:31:11.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 03/16/23 10:31:13.407
    STEP: updating a scale subresource 03/16/23 10:31:13.496
    STEP: verifying the deployment Spec.Replicas was modified 03/16/23 10:31:13.588
    STEP: Patch a scale subresource 03/16/23 10:31:13.677
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 10:31:13.947: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9376  b1b25bb9-1dca-4850-9665-6d72234ff826 29828 3 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-16 10:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ec6fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:4,UpdatedReplicas:4,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-16 10:31:11 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-16 10:31:13 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 16 10:31:14.037: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9376  174d9f65-15e4-4737-84dc-c79604590e57 29826 3 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b1b25bb9-1dca-4850-9665-6d72234ff826 0xc004abb287 0xc004abb288}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1b25bb9-1dca-4850-9665-6d72234ff826\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004abb318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:31:14.214: INFO: Pod "test-new-deployment-7f5969cbc7-4zj8j" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4zj8j test-new-deployment-7f5969cbc7- deployment-9376  e01a4d4d-6ee3-405f-9cae-009712a0ed49 29825 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7387 0xc005ec7388}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4czb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4czb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 10:31:14.214: INFO: Pod "test-new-deployment-7f5969cbc7-8lsfd" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8lsfd test-new-deployment-7f5969cbc7- deployment-9376  5f41d27a-1ab9-424a-bfe5-e3b7a57a26f7 29831 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cdffa8234dcf5053e908b9dca780d8bef27bd81c11e1003eeffc8b323e0d0a15 cni.projectcalico.org/podIP:100.64.0.183/32 cni.projectcalico.org/podIPs:100.64.0.183/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7577 0xc005ec7578}] [] [{calico Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lt5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lt5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 10:31:14.215: INFO: Pod "test-new-deployment-7f5969cbc7-tv2vd" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tv2vd test-new-deployment-7f5969cbc7- deployment-9376  fd41efda-8a38-453c-8295-0eaf2eca5d85 29795 0 2023-03-16 10:31:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:50140a15181105768dec6176d3b19aecd40d5b3db14ea85e3cf380b4ae2918d8 cni.projectcalico.org/podIP:100.64.1.43/32 cni.projectcalico.org/podIPs:100.64.1.43/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7767 0xc005ec7768}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n86fs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n86fs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.43,StartTime:2023-03-16 10:31:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:31:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://08cc5261f38dbead2248a3e4e5b6bdd5e224c4e0c1bdbc3207ce3c56f11229dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 10:31:14.215: INFO: Pod "test-new-deployment-7f5969cbc7-tvgnh" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tvgnh test-new-deployment-7f5969cbc7- deployment-9376  b1039d90-183b-4556-921a-7268821b7246 29827 0 2023-03-16 10:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 174d9f65-15e4-4737-84dc-c79604590e57 0xc005ec7960 0xc005ec7961}] [] [{kube-controller-manager Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174d9f65-15e4-4737-84dc-c79604590e57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-16 10:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qg4v9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qg4v9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-246.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.246,PodIP:,StartTime:2023-03-16 10:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:14.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9376" for this suite. 03/16/23 10:31:14.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:14.399
Mar 16 10:31:14.399: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:31:14.401
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:14.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:14.847
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Mar 16 10:31:15.211: INFO: created pod
Mar 16 10:31:15.211: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3341" to be "Succeeded or Failed"
Mar 16 10:31:15.301: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.572134ms
Mar 16 10:31:17.391: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.180113843s
Mar 16 10:31:19.391: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180309309s
STEP: Saw pod success 03/16/23 10:31:19.391
Mar 16 10:31:19.392: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 16 10:31:49.393: INFO: polling logs
Mar 16 10:31:49.494: INFO: Pod logs: 
I0316 10:31:15.833531       1 log.go:198] OK: Got token
I0316 10:31:15.833567       1 log.go:198] validating with in-cluster discovery
I0316 10:31:15.833952       1 log.go:198] OK: got issuer https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com
I0316 10:31:15.834038       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3341:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678963275, NotBefore:1678962675, IssuedAt:1678962675, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3341", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30ac4743-debc-4e7d-a26d-d61b092042ea"}}}
I0316 10:31:15.848682       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com
I0316 10:31:15.851499       1 log.go:198] OK: Validated signature on JWT
I0316 10:31:15.851584       1 log.go:198] OK: Got valid claims from token!
I0316 10:31:15.851614       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3341:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678963275, NotBefore:1678962675, IssuedAt:1678962675, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3341", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30ac4743-debc-4e7d-a26d-d61b092042ea"}}}

Mar 16 10:31:49.494: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:49.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3341" for this suite. 03/16/23 10:31:49.762
------------------------------
• [35.453 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:14.399
    Mar 16 10:31:14.399: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:31:14.401
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:14.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:14.847
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Mar 16 10:31:15.211: INFO: created pod
    Mar 16 10:31:15.211: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3341" to be "Succeeded or Failed"
    Mar 16 10:31:15.301: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.572134ms
    Mar 16 10:31:17.391: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.180113843s
    Mar 16 10:31:19.391: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180309309s
    STEP: Saw pod success 03/16/23 10:31:19.391
    Mar 16 10:31:19.392: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 16 10:31:49.393: INFO: polling logs
    Mar 16 10:31:49.494: INFO: Pod logs: 
    I0316 10:31:15.833531       1 log.go:198] OK: Got token
    I0316 10:31:15.833567       1 log.go:198] validating with in-cluster discovery
    I0316 10:31:15.833952       1 log.go:198] OK: got issuer https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com
    I0316 10:31:15.834038       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3341:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678963275, NotBefore:1678962675, IssuedAt:1678962675, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3341", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30ac4743-debc-4e7d-a26d-d61b092042ea"}}}
    I0316 10:31:15.848682       1 log.go:198] OK: Constructed OIDC provider for issuer https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com
    I0316 10:31:15.851499       1 log.go:198] OK: Validated signature on JWT
    I0316 10:31:15.851584       1 log.go:198] OK: Got valid claims from token!
    I0316 10:31:15.851614       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-3341:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678963275, NotBefore:1678962675, IssuedAt:1678962675, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3341", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30ac4743-debc-4e7d-a26d-d61b092042ea"}}}

    Mar 16 10:31:49.494: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:49.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3341" for this suite. 03/16/23 10:31:49.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:49.852
Mar 16 10:31:49.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:31:49.853
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:50.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:50.299
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:51.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4454" for this suite. 03/16/23 10:31:51.374
------------------------------
• [1.611 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:49.852
    Mar 16 10:31:49.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:31:49.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:50.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:50.299
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:51.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4454" for this suite. 03/16/23 10:31:51.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:51.465
Mar 16 10:31:51.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:31:51.466
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:51.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:51.913
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1054-delete-me 03/16/23 10:31:52.18
STEP: Waiting for the RuntimeClass to disappear 03/16/23 10:31:52.27
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 16 10:31:52.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1054" for this suite. 03/16/23 10:31:52.546
------------------------------
• [1.174 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:51.465
    Mar 16 10:31:51.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:31:51.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:51.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:51.913
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1054-delete-me 03/16/23 10:31:52.18
    STEP: Waiting for the RuntimeClass to disappear 03/16/23 10:31:52.27
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:31:52.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1054" for this suite. 03/16/23 10:31:52.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:31:52.639
Mar 16 10:31:52.639: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:31:52.64
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:52.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:53.093
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:31:53.453
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:31:53.853
STEP: Deploying the webhook pod 03/16/23 10:31:53.944
STEP: Wait for the deployment to be ready 03/16/23 10:31:54.124
Mar 16 10:31:54.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:31:56.485
STEP: Verifying the service has paired with the endpoint 03/16/23 10:31:56.58
Mar 16 10:31:57.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Mar 16 10:31:57.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3907-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 10:31:57.937
STEP: Creating a custom resource that should be mutated by the webhook 03/16/23 10:31:58.207
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:32:00.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3355" for this suite. 03/16/23 10:32:01.372
STEP: Destroying namespace "webhook-3355-markers" for this suite. 03/16/23 10:32:01.463
------------------------------
• [8.917 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:31:52.639
    Mar 16 10:31:52.639: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:31:52.64
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:31:52.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:31:53.093
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:31:53.453
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:31:53.853
    STEP: Deploying the webhook pod 03/16/23 10:31:53.944
    STEP: Wait for the deployment to be ready 03/16/23 10:31:54.124
    Mar 16 10:31:54.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 31, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:31:56.485
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:31:56.58
    Mar 16 10:31:57.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Mar 16 10:31:57.669: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3907-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 10:31:57.937
    STEP: Creating a custom resource that should be mutated by the webhook 03/16/23 10:31:58.207
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:32:00.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3355" for this suite. 03/16/23 10:32:01.372
    STEP: Destroying namespace "webhook-3355-markers" for this suite. 03/16/23 10:32:01.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:32:01.557
Mar 16 10:32:01.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:32:01.558
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:01.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:02.004
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9650 03/16/23 10:32:02.181
STEP: changing the ExternalName service to type=ClusterIP 03/16/23 10:32:02.271
STEP: creating replication controller externalname-service in namespace services-9650 03/16/23 10:32:02.455
I0316 10:32:02.545944    8588 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9650, replica count: 2
I0316 10:32:05.647984    8588 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:32:05.648: INFO: Creating new exec pod
Mar 16 10:32:05.740: INFO: Waiting up to 5m0s for pod "execpodj7z6n" in namespace "services-9650" to be "running"
Mar 16 10:32:05.829: INFO: Pod "execpodj7z6n": Phase="Pending", Reason="", readiness=false. Elapsed: 89.055361ms
Mar 16 10:32:07.919: INFO: Pod "execpodj7z6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.17872884s
Mar 16 10:32:07.919: INFO: Pod "execpodj7z6n" satisfied condition "running"
Mar 16 10:32:08.920: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9650 exec execpodj7z6n -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 16 10:32:10.043: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 16 10:32:10.044: INFO: stdout: ""
Mar 16 10:32:10.044: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9650 exec execpodj7z6n -- /bin/sh -x -c nc -v -z -w 2 100.106.70.103 80'
Mar 16 10:32:11.165: INFO: stderr: "+ nc -v -z -w 2 100.106.70.103 80\nConnection to 100.106.70.103 80 port [tcp/http] succeeded!\n"
Mar 16 10:32:11.165: INFO: stdout: ""
Mar 16 10:32:11.165: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:32:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9650" for this suite. 03/16/23 10:32:11.436
------------------------------
• [9.969 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:32:01.557
    Mar 16 10:32:01.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:32:01.558
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:01.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:02.004
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9650 03/16/23 10:32:02.181
    STEP: changing the ExternalName service to type=ClusterIP 03/16/23 10:32:02.271
    STEP: creating replication controller externalname-service in namespace services-9650 03/16/23 10:32:02.455
    I0316 10:32:02.545944    8588 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9650, replica count: 2
    I0316 10:32:05.647984    8588 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:32:05.648: INFO: Creating new exec pod
    Mar 16 10:32:05.740: INFO: Waiting up to 5m0s for pod "execpodj7z6n" in namespace "services-9650" to be "running"
    Mar 16 10:32:05.829: INFO: Pod "execpodj7z6n": Phase="Pending", Reason="", readiness=false. Elapsed: 89.055361ms
    Mar 16 10:32:07.919: INFO: Pod "execpodj7z6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.17872884s
    Mar 16 10:32:07.919: INFO: Pod "execpodj7z6n" satisfied condition "running"
    Mar 16 10:32:08.920: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9650 exec execpodj7z6n -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 16 10:32:10.043: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 16 10:32:10.044: INFO: stdout: ""
    Mar 16 10:32:10.044: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9650 exec execpodj7z6n -- /bin/sh -x -c nc -v -z -w 2 100.106.70.103 80'
    Mar 16 10:32:11.165: INFO: stderr: "+ nc -v -z -w 2 100.106.70.103 80\nConnection to 100.106.70.103 80 port [tcp/http] succeeded!\n"
    Mar 16 10:32:11.165: INFO: stdout: ""
    Mar 16 10:32:11.165: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:32:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9650" for this suite. 03/16/23 10:32:11.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:32:11.527
Mar 16 10:32:11.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl 03/16/23 10:32:11.528
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:11.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:11.974
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/16/23 10:32:12.151
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:32:12.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9936" for this suite. 03/16/23 10:32:12.334
------------------------------
• [0.897 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:32:11.527
    Mar 16 10:32:11.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sysctl 03/16/23 10:32:11.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:11.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:11.974
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/16/23 10:32:12.151
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:32:12.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9936" for this suite. 03/16/23 10:32:12.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:32:12.425
Mar 16 10:32:12.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 03/16/23 10:32:12.426
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:12.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:12.873
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/16/23 10:32:13.05
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-86x7 03/16/23 10:32:13.229
STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:32:13.229
Mar 16 10:32:13.323: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-86x7" in namespace "subpath-4906" to be "Succeeded or Failed"
Mar 16 10:32:13.413: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.612227ms
Mar 16 10:32:15.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179512444s
Mar 16 10:32:17.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 4.179246659s
Mar 16 10:32:19.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 6.179513574s
Mar 16 10:32:21.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 8.179502321s
Mar 16 10:32:23.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 10.179351377s
Mar 16 10:32:25.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 12.180943662s
Mar 16 10:32:27.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 14.179604241s
Mar 16 10:32:29.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 16.180345155s
Mar 16 10:32:31.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 18.179599958s
Mar 16 10:32:33.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 20.179779558s
Mar 16 10:32:35.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=false. Elapsed: 22.179438835s
Mar 16 10:32:37.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180815773s
STEP: Saw pod success 03/16/23 10:32:37.504
Mar 16 10:32:37.505: INFO: Pod "pod-subpath-test-configmap-86x7" satisfied condition "Succeeded or Failed"
Mar 16 10:32:37.594: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-configmap-86x7 container test-container-subpath-configmap-86x7: <nil>
STEP: delete the pod 03/16/23 10:32:37.693
Mar 16 10:32:37.786: INFO: Waiting for pod pod-subpath-test-configmap-86x7 to disappear
Mar 16 10:32:37.875: INFO: Pod pod-subpath-test-configmap-86x7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-86x7 03/16/23 10:32:37.875
Mar 16 10:32:37.875: INFO: Deleting pod "pod-subpath-test-configmap-86x7" in namespace "subpath-4906"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 16 10:32:37.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4906" for this suite. 03/16/23 10:32:38.142
------------------------------
• [25.808 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:32:12.425
    Mar 16 10:32:12.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 03/16/23 10:32:12.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:12.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:12.873
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/16/23 10:32:13.05
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-86x7 03/16/23 10:32:13.229
    STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:32:13.229
    Mar 16 10:32:13.323: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-86x7" in namespace "subpath-4906" to be "Succeeded or Failed"
    Mar 16 10:32:13.413: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.612227ms
    Mar 16 10:32:15.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179512444s
    Mar 16 10:32:17.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 4.179246659s
    Mar 16 10:32:19.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 6.179513574s
    Mar 16 10:32:21.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 8.179502321s
    Mar 16 10:32:23.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 10.179351377s
    Mar 16 10:32:25.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 12.180943662s
    Mar 16 10:32:27.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 14.179604241s
    Mar 16 10:32:29.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 16.180345155s
    Mar 16 10:32:31.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 18.179599958s
    Mar 16 10:32:33.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=true. Elapsed: 20.179779558s
    Mar 16 10:32:35.503: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Running", Reason="", readiness=false. Elapsed: 22.179438835s
    Mar 16 10:32:37.504: INFO: Pod "pod-subpath-test-configmap-86x7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180815773s
    STEP: Saw pod success 03/16/23 10:32:37.504
    Mar 16 10:32:37.505: INFO: Pod "pod-subpath-test-configmap-86x7" satisfied condition "Succeeded or Failed"
    Mar 16 10:32:37.594: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-configmap-86x7 container test-container-subpath-configmap-86x7: <nil>
    STEP: delete the pod 03/16/23 10:32:37.693
    Mar 16 10:32:37.786: INFO: Waiting for pod pod-subpath-test-configmap-86x7 to disappear
    Mar 16 10:32:37.875: INFO: Pod pod-subpath-test-configmap-86x7 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-86x7 03/16/23 10:32:37.875
    Mar 16 10:32:37.875: INFO: Deleting pod "pod-subpath-test-configmap-86x7" in namespace "subpath-4906"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:32:37.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4906" for this suite. 03/16/23 10:32:38.142
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:32:38.233
Mar 16 10:32:38.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 10:32:38.234
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:38.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:38.68
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9084 03/16/23 10:32:38.858
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 03/16/23 10:32:38.948
STEP: Creating pod with conflicting port in namespace statefulset-9084 03/16/23 10:32:39.038
STEP: Waiting until pod test-pod will start running in namespace statefulset-9084 03/16/23 10:32:39.132
Mar 16 10:32:39.133: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9084" to be "running"
Mar 16 10:32:39.222: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.16742ms
Mar 16 10:32:41.312: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179128226s
Mar 16 10:32:41.312: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9084 03/16/23 10:32:41.312
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9084 03/16/23 10:32:41.402
Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Pending. Waiting for statefulset controller to delete.
Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Failed. Waiting for statefulset controller to delete.
Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Failed. Waiting for statefulset controller to delete.
Mar 16 10:32:41.580: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9084
STEP: Removing pod with conflicting port in namespace statefulset-9084 03/16/23 10:32:41.58
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9084 and will be in running state 03/16/23 10:32:41.673
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 10:32:43.852: INFO: Deleting all statefulset in ns statefulset-9084
Mar 16 10:32:43.941: INFO: Scaling statefulset ss to 0
Mar 16 10:32:54.300: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:32:54.391: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:32:54.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9084" for this suite. 03/16/23 10:32:54.839
------------------------------
• [16.696 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:32:38.233
    Mar 16 10:32:38.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 10:32:38.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:38.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:38.68
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9084 03/16/23 10:32:38.858
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 03/16/23 10:32:38.948
    STEP: Creating pod with conflicting port in namespace statefulset-9084 03/16/23 10:32:39.038
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9084 03/16/23 10:32:39.132
    Mar 16 10:32:39.133: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9084" to be "running"
    Mar 16 10:32:39.222: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.16742ms
    Mar 16 10:32:41.312: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.179128226s
    Mar 16 10:32:41.312: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9084 03/16/23 10:32:41.312
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9084 03/16/23 10:32:41.402
    Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 16 10:32:41.580: INFO: Observed stateful pod in namespace: statefulset-9084, name: ss-0, uid: 12ff318a-b7af-475c-b44c-cae978cdda41, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 16 10:32:41.580: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9084
    STEP: Removing pod with conflicting port in namespace statefulset-9084 03/16/23 10:32:41.58
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9084 and will be in running state 03/16/23 10:32:41.673
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 10:32:43.852: INFO: Deleting all statefulset in ns statefulset-9084
    Mar 16 10:32:43.941: INFO: Scaling statefulset ss to 0
    Mar 16 10:32:54.300: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:32:54.391: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:32:54.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9084" for this suite. 03/16/23 10:32:54.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:32:54.93
Mar 16 10:32:54.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:32:54.931
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:55.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:55.378
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-769 03/16/23 10:32:55.555
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/16/23 10:32:55.649
STEP: creating service externalsvc in namespace services-769 03/16/23 10:32:55.649
STEP: creating replication controller externalsvc in namespace services-769 03/16/23 10:32:55.743
I0316 10:32:55.833237    8588 runners.go:193] Created replication controller with name: externalsvc, namespace: services-769, replica count: 2
I0316 10:32:58.934849    8588 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/16/23 10:32:59.024
Mar 16 10:32:59.207: INFO: Creating new exec pod
Mar 16 10:32:59.301: INFO: Waiting up to 5m0s for pod "execpodb4nb7" in namespace "services-769" to be "running"
Mar 16 10:32:59.390: INFO: Pod "execpodb4nb7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.381191ms
Mar 16 10:33:01.481: INFO: Pod "execpodb4nb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179929973s
Mar 16 10:33:01.481: INFO: Pod "execpodb4nb7" satisfied condition "running"
Mar 16 10:33:01.481: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-769 exec execpodb4nb7 -- /bin/sh -x -c nslookup clusterip-service.services-769.svc.cluster.local'
Mar 16 10:33:02.657: INFO: stderr: "+ nslookup clusterip-service.services-769.svc.cluster.local\n"
Mar 16 10:33:02.657: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-769.svc.cluster.local\tcanonical name = externalsvc.services-769.svc.cluster.local.\nName:\texternalsvc.services-769.svc.cluster.local\nAddress: 100.109.65.235\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-769, will wait for the garbage collector to delete the pods 03/16/23 10:33:02.657
Mar 16 10:33:02.938: INFO: Deleting ReplicationController externalsvc took: 89.862781ms
Mar 16 10:33:03.038: INFO: Terminating ReplicationController externalsvc pods took: 100.233116ms
Mar 16 10:33:04.732: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-769" for this suite. 03/16/23 10:33:05.003
------------------------------
• [10.163 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:32:54.93
    Mar 16 10:32:54.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:32:54.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:32:55.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:32:55.378
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-769 03/16/23 10:32:55.555
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/16/23 10:32:55.649
    STEP: creating service externalsvc in namespace services-769 03/16/23 10:32:55.649
    STEP: creating replication controller externalsvc in namespace services-769 03/16/23 10:32:55.743
    I0316 10:32:55.833237    8588 runners.go:193] Created replication controller with name: externalsvc, namespace: services-769, replica count: 2
    I0316 10:32:58.934849    8588 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/16/23 10:32:59.024
    Mar 16 10:32:59.207: INFO: Creating new exec pod
    Mar 16 10:32:59.301: INFO: Waiting up to 5m0s for pod "execpodb4nb7" in namespace "services-769" to be "running"
    Mar 16 10:32:59.390: INFO: Pod "execpodb4nb7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.381191ms
    Mar 16 10:33:01.481: INFO: Pod "execpodb4nb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179929973s
    Mar 16 10:33:01.481: INFO: Pod "execpodb4nb7" satisfied condition "running"
    Mar 16 10:33:01.481: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-769 exec execpodb4nb7 -- /bin/sh -x -c nslookup clusterip-service.services-769.svc.cluster.local'
    Mar 16 10:33:02.657: INFO: stderr: "+ nslookup clusterip-service.services-769.svc.cluster.local\n"
    Mar 16 10:33:02.657: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-769.svc.cluster.local\tcanonical name = externalsvc.services-769.svc.cluster.local.\nName:\texternalsvc.services-769.svc.cluster.local\nAddress: 100.109.65.235\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-769, will wait for the garbage collector to delete the pods 03/16/23 10:33:02.657
    Mar 16 10:33:02.938: INFO: Deleting ReplicationController externalsvc took: 89.862781ms
    Mar 16 10:33:03.038: INFO: Terminating ReplicationController externalsvc pods took: 100.233116ms
    Mar 16 10:33:04.732: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-769" for this suite. 03/16/23 10:33:05.003
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:05.093
Mar 16 10:33:05.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:33:05.095
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:05.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:05.546
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 16 10:33:05.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8612" for this suite. 03/16/23 10:33:06.449
------------------------------
• [1.446 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:05.093
    Mar 16 10:33:05.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:33:05.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:05.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:05.546
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 16 10:33:05.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8612" for this suite. 03/16/23 10:33:06.449
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:06.539
Mar 16 10:33:06.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:33:06.541
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:06.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:06.987
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/16/23 10:33:07.165
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1132;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1132;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +notcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_tcp@PTR;sleep 1; done
 03/16/23 10:33:07.35
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1132;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1132;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +notcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_tcp@PTR;sleep 1; done
 03/16/23 10:33:07.35
STEP: creating a pod to probe DNS 03/16/23 10:33:07.351
STEP: submitting the pod to kubernetes 03/16/23 10:33:07.351
Mar 16 10:33:07.448: INFO: Waiting up to 15m0s for pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9" in namespace "dns-1132" to be "running"
Mar 16 10:33:07.543: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 94.750024ms
Mar 16 10:33:09.633: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.18560631s
Mar 16 10:33:09.633: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9" satisfied condition "running"
STEP: retrieving the pod 03/16/23 10:33:09.633
STEP: looking for the results for each expected name from probers 03/16/23 10:33:09.723
Mar 16 10:33:09.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.050: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.194: INFO: Unable to read wheezy_udp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.286: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.379: INFO: Unable to read wheezy_udp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.564: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:10.656: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.116: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.208: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.301: INFO: Unable to read jessie_udp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.393: INFO: Unable to read jessie_tcp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.485: INFO: Unable to read jessie_udp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.596: INFO: Unable to read jessie_tcp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:11.779: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
Mar 16 10:33:12.159: INFO: Lookups using dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1132 wheezy_tcp@dns-test-service.dns-1132 wheezy_udp@dns-test-service.dns-1132.svc wheezy_tcp@dns-test-service.dns-1132.svc wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1132 jessie_tcp@dns-test-service.dns-1132 jessie_udp@dns-test-service.dns-1132.svc jessie_tcp@dns-test-service.dns-1132.svc jessie_udp@_http._tcp.dns-test-service.dns-1132.svc jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc]

Mar 16 10:33:19.412: INFO: DNS probes using dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9 succeeded

STEP: deleting the pod 03/16/23 10:33:19.412
STEP: deleting the test service 03/16/23 10:33:19.506
STEP: deleting the test headless service 03/16/23 10:33:19.6
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:19.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1132" for this suite. 03/16/23 10:33:19.869
------------------------------
• [13.420 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:06.539
    Mar 16 10:33:06.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:33:06.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:06.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:06.987
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/16/23 10:33:07.165
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1132;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1132;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +notcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_tcp@PTR;sleep 1; done
     03/16/23 10:33:07.35
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1132;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1132;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1132.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1132.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1132.svc;check="$$(dig +notcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.7.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.7.123_tcp@PTR;sleep 1; done
     03/16/23 10:33:07.35
    STEP: creating a pod to probe DNS 03/16/23 10:33:07.351
    STEP: submitting the pod to kubernetes 03/16/23 10:33:07.351
    Mar 16 10:33:07.448: INFO: Waiting up to 15m0s for pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9" in namespace "dns-1132" to be "running"
    Mar 16 10:33:07.543: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 94.750024ms
    Mar 16 10:33:09.633: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.18560631s
    Mar 16 10:33:09.633: INFO: Pod "dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 10:33:09.633
    STEP: looking for the results for each expected name from probers 03/16/23 10:33:09.723
    Mar 16 10:33:09.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.050: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.194: INFO: Unable to read wheezy_udp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.286: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.379: INFO: Unable to read wheezy_udp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.564: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:10.656: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.116: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.208: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.301: INFO: Unable to read jessie_udp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.393: INFO: Unable to read jessie_tcp@dns-test-service.dns-1132 from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.485: INFO: Unable to read jessie_udp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.596: INFO: Unable to read jessie_tcp@dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:11.779: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc from pod dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9: the server could not find the requested resource (get pods dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9)
    Mar 16 10:33:12.159: INFO: Lookups using dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1132 wheezy_tcp@dns-test-service.dns-1132 wheezy_udp@dns-test-service.dns-1132.svc wheezy_tcp@dns-test-service.dns-1132.svc wheezy_udp@_http._tcp.dns-test-service.dns-1132.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1132.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1132 jessie_tcp@dns-test-service.dns-1132 jessie_udp@dns-test-service.dns-1132.svc jessie_tcp@dns-test-service.dns-1132.svc jessie_udp@_http._tcp.dns-test-service.dns-1132.svc jessie_tcp@_http._tcp.dns-test-service.dns-1132.svc]

    Mar 16 10:33:19.412: INFO: DNS probes using dns-1132/dns-test-400b4aa4-2d43-414e-9930-d209b1c7b6a9 succeeded

    STEP: deleting the pod 03/16/23 10:33:19.412
    STEP: deleting the test service 03/16/23 10:33:19.506
    STEP: deleting the test headless service 03/16/23 10:33:19.6
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:19.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1132" for this suite. 03/16/23 10:33:19.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:19.96
Mar 16 10:33:19.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 03/16/23 10:33:19.962
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:20.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:20.407
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 16 10:33:20.584: INFO: Creating pod...
Mar 16 10:33:20.678: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6265" to be "running"
Mar 16 10:33:20.768: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 89.737829ms
Mar 16 10:33:22.859: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.180331013s
Mar 16 10:33:22.859: INFO: Pod "agnhost" satisfied condition "running"
Mar 16 10:33:22.859: INFO: Creating service...
Mar 16 10:33:22.953: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=DELETE
Mar 16 10:33:23.138: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 16 10:33:23.138: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=OPTIONS
Mar 16 10:33:23.274: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 16 10:33:23.274: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=PATCH
Mar 16 10:33:23.417: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 16 10:33:23.417: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=POST
Mar 16 10:33:23.508: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 16 10:33:23.508: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=PUT
Mar 16 10:33:23.600: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 16 10:33:23.600: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 16 10:33:23.695: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 16 10:33:23.695: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 16 10:33:23.794: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 16 10:33:23.794: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 16 10:33:23.887: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 16 10:33:23.887: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=POST
Mar 16 10:33:23.982: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 16 10:33:23.982: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=PUT
Mar 16 10:33:24.075: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 16 10:33:24.075: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=GET
Mar 16 10:33:24.164: INFO: http.Client request:GET StatusCode:301
Mar 16 10:33:24.164: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=GET
Mar 16 10:33:24.253: INFO: http.Client request:GET StatusCode:301
Mar 16 10:33:24.253: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=HEAD
Mar 16 10:33:24.342: INFO: http.Client request:HEAD StatusCode:301
Mar 16 10:33:24.342: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 16 10:33:24.432: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:24.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6265" for this suite. 03/16/23 10:33:24.611
------------------------------
• [4.741 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:19.96
    Mar 16 10:33:19.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 03/16/23 10:33:19.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:20.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:20.407
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 16 10:33:20.584: INFO: Creating pod...
    Mar 16 10:33:20.678: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6265" to be "running"
    Mar 16 10:33:20.768: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 89.737829ms
    Mar 16 10:33:22.859: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.180331013s
    Mar 16 10:33:22.859: INFO: Pod "agnhost" satisfied condition "running"
    Mar 16 10:33:22.859: INFO: Creating service...
    Mar 16 10:33:22.953: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=DELETE
    Mar 16 10:33:23.138: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 16 10:33:23.138: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=OPTIONS
    Mar 16 10:33:23.274: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 16 10:33:23.274: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=PATCH
    Mar 16 10:33:23.417: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 16 10:33:23.417: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=POST
    Mar 16 10:33:23.508: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 16 10:33:23.508: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=PUT
    Mar 16 10:33:23.600: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 16 10:33:23.600: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 16 10:33:23.695: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 16 10:33:23.695: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 16 10:33:23.794: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 16 10:33:23.794: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 16 10:33:23.887: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 16 10:33:23.887: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=POST
    Mar 16 10:33:23.982: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 16 10:33:23.982: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 16 10:33:24.075: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 16 10:33:24.075: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=GET
    Mar 16 10:33:24.164: INFO: http.Client request:GET StatusCode:301
    Mar 16 10:33:24.164: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=GET
    Mar 16 10:33:24.253: INFO: http.Client request:GET StatusCode:301
    Mar 16 10:33:24.253: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/pods/agnhost/proxy?method=HEAD
    Mar 16 10:33:24.342: INFO: http.Client request:HEAD StatusCode:301
    Mar 16 10:33:24.342: INFO: Starting http.Client for https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6265/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 16 10:33:24.432: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:24.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6265" for this suite. 03/16/23 10:33:24.611
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:24.702
Mar 16 10:33:24.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:33:24.703
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:24.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:25.149
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Mar 16 10:33:25.327: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4105 version'
Mar 16 10:33:25.666: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 16 10:33:25.666: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4105" for this suite. 03/16/23 10:33:25.844
------------------------------
• [1.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:24.702
    Mar 16 10:33:24.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:33:24.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:24.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:25.149
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Mar 16 10:33:25.327: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4105 version'
    Mar 16 10:33:25.666: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 16 10:33:25.666: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4105" for this suite. 03/16/23 10:33:25.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:25.935
Mar 16 10:33:25.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:33:25.936
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:26.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:26.381
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-8a594857-0d21-46ed-b187-8a344c458a64 03/16/23 10:33:26.558
STEP: Creating a pod to test consume configMaps 03/16/23 10:33:26.648
Mar 16 10:33:26.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a" in namespace "configmap-9414" to be "Succeeded or Failed"
Mar 16 10:33:26.832: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.243458ms
Mar 16 10:33:28.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179707966s
Mar 16 10:33:30.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179732491s
STEP: Saw pod success 03/16/23 10:33:30.922
Mar 16 10:33:30.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a" satisfied condition "Succeeded or Failed"
Mar 16 10:33:31.011: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:33:31.107
Mar 16 10:33:31.200: INFO: Waiting for pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a to disappear
Mar 16 10:33:31.289: INFO: Pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:31.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9414" for this suite. 03/16/23 10:33:31.467
------------------------------
• [5.622 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:25.935
    Mar 16 10:33:25.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:33:25.936
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:26.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:26.381
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-8a594857-0d21-46ed-b187-8a344c458a64 03/16/23 10:33:26.558
    STEP: Creating a pod to test consume configMaps 03/16/23 10:33:26.648
    Mar 16 10:33:26.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a" in namespace "configmap-9414" to be "Succeeded or Failed"
    Mar 16 10:33:26.832: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.243458ms
    Mar 16 10:33:28.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179707966s
    Mar 16 10:33:30.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179732491s
    STEP: Saw pod success 03/16/23 10:33:30.922
    Mar 16 10:33:30.922: INFO: Pod "pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a" satisfied condition "Succeeded or Failed"
    Mar 16 10:33:31.011: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:33:31.107
    Mar 16 10:33:31.200: INFO: Waiting for pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a to disappear
    Mar 16 10:33:31.289: INFO: Pod pod-configmaps-96eaa190-0efd-4136-aa9d-249784c5187a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:31.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9414" for this suite. 03/16/23 10:33:31.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:31.558
Mar 16 10:33:31.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 03/16/23 10:33:31.559
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:31.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:32.005
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/16/23 10:33:32.182
Mar 16 10:33:32.277: INFO: Waiting up to 5m0s for pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60" in namespace "security-context-1452" to be "Succeeded or Failed"
Mar 16 10:33:32.366: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Pending", Reason="", readiness=false. Elapsed: 89.23039ms
Mar 16 10:33:34.456: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179079235s
Mar 16 10:33:36.457: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180333879s
STEP: Saw pod success 03/16/23 10:33:36.457
Mar 16 10:33:36.457: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60" satisfied condition "Succeeded or Failed"
Mar 16 10:33:36.547: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 container test-container: <nil>
STEP: delete the pod 03/16/23 10:33:36.64
Mar 16 10:33:36.734: INFO: Waiting for pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 to disappear
Mar 16 10:33:36.823: INFO: Pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 10:33:36.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1452" for this suite. 03/16/23 10:33:37.001
------------------------------
• [5.533 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:31.558
    Mar 16 10:33:31.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 03/16/23 10:33:31.559
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:31.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:32.005
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/16/23 10:33:32.182
    Mar 16 10:33:32.277: INFO: Waiting up to 5m0s for pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60" in namespace "security-context-1452" to be "Succeeded or Failed"
    Mar 16 10:33:32.366: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Pending", Reason="", readiness=false. Elapsed: 89.23039ms
    Mar 16 10:33:34.456: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179079235s
    Mar 16 10:33:36.457: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180333879s
    STEP: Saw pod success 03/16/23 10:33:36.457
    Mar 16 10:33:36.457: INFO: Pod "security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60" satisfied condition "Succeeded or Failed"
    Mar 16 10:33:36.547: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 container test-container: <nil>
    STEP: delete the pod 03/16/23 10:33:36.64
    Mar 16 10:33:36.734: INFO: Waiting for pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 to disappear
    Mar 16 10:33:36.823: INFO: Pod security-context-c68309e8-8bd3-4fcc-83f8-83252f5d4b60 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:33:36.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1452" for this suite. 03/16/23 10:33:37.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:33:37.092
Mar 16 10:33:37.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 10:33:37.093
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:37.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:37.538
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7990 03/16/23 10:33:37.716
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7990 03/16/23 10:33:37.805
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7990 03/16/23 10:33:37.895
Mar 16 10:33:37.985: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 10:33:48.076: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/16/23 10:33:48.076
Mar 16 10:33:48.166: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:33:49.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:33:49.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:33:49.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:33:49.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 16 10:33:59.482: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:33:59.482: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:33:59.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999584s
Mar 16 10:34:00.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909437044s
Mar 16 10:34:02.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.818861472s
Mar 16 10:34:03.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.728615036s
Mar 16 10:34:04.204: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.637567865s
Mar 16 10:34:05.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.547427516s
Mar 16 10:34:06.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.453549289s
Mar 16 10:34:07.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.362648933s
Mar 16 10:34:08.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.271429665s
Mar 16 10:34:09.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 180.508227ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7990 03/16/23 10:34:10.661
Mar 16 10:34:10.751: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:34:11.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 10:34:11.877: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:34:11.877: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:34:11.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:34:13.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 16 10:34:13.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:34:13.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:34:13.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 10:34:14.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 16 10:34:14.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 10:34:14.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 16 10:34:14.253: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:34:14.253: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:34:14.253: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/16/23 10:34:14.253
Mar 16 10:34:14.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:34:15.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:34:15.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:34:15.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:34:15.489: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:34:16.603: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:34:16.603: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:34:16.603: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:34:16.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 10:34:17.754: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 10:34:17.754: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 10:34:17.754: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 10:34:17.754: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:34:17.844: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 16 10:34:28.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:34:28.024: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:34:28.024: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 16 10:34:28.295: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar 16 10:34:28.295: INFO: ss-0  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  }]
Mar 16 10:34:28.296: INFO: ss-1  ip-10-250-19-246.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
Mar 16 10:34:28.296: INFO: ss-2  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
Mar 16 10:34:28.296: INFO: 
Mar 16 10:34:28.296: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 16 10:34:29.386: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar 16 10:34:29.386: INFO: ss-0  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  }]
Mar 16 10:34:29.386: INFO: ss-2  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
Mar 16 10:34:29.386: INFO: 
Mar 16 10:34:29.386: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 16 10:34:30.476: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.818753835s
Mar 16 10:34:31.566: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.729086397s
Mar 16 10:34:32.656: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.639307836s
Mar 16 10:34:33.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.549294993s
Mar 16 10:34:34.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.459314445s
Mar 16 10:34:35.926: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.36956111s
Mar 16 10:34:37.017: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.278325536s
Mar 16 10:34:38.107: INFO: Verifying statefulset ss doesn't scale past 0 for another 187.455855ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7990 03/16/23 10:34:39.108
Mar 16 10:34:39.198: INFO: Scaling statefulset ss to 0
Mar 16 10:34:39.468: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 10:34:39.566: INFO: Deleting all statefulset in ns statefulset-7990
Mar 16 10:34:39.656: INFO: Scaling statefulset ss to 0
Mar 16 10:34:39.925: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 10:34:40.014: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:34:40.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7990" for this suite. 03/16/23 10:34:40.462
------------------------------
• [63.461 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:33:37.092
    Mar 16 10:33:37.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 10:33:37.093
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:33:37.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:33:37.538
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7990 03/16/23 10:33:37.716
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7990 03/16/23 10:33:37.805
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7990 03/16/23 10:33:37.895
    Mar 16 10:33:37.985: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 10:33:48.076: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/16/23 10:33:48.076
    Mar 16 10:33:48.166: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:33:49.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:33:49.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:33:49.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:33:49.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 16 10:33:59.482: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:33:59.482: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:33:59.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999584s
    Mar 16 10:34:00.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909437044s
    Mar 16 10:34:02.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.818861472s
    Mar 16 10:34:03.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.728615036s
    Mar 16 10:34:04.204: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.637567865s
    Mar 16 10:34:05.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.547427516s
    Mar 16 10:34:06.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.453549289s
    Mar 16 10:34:07.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.362648933s
    Mar 16 10:34:08.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.271429665s
    Mar 16 10:34:09.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 180.508227ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7990 03/16/23 10:34:10.661
    Mar 16 10:34:10.751: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:34:11.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 10:34:11.877: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:34:11.877: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:34:11.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:34:13.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 16 10:34:13.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:34:13.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:34:13.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 10:34:14.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 16 10:34:14.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 10:34:14.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 16 10:34:14.253: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:34:14.253: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:34:14.253: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/16/23 10:34:14.253
    Mar 16 10:34:14.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:34:15.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:34:15.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:34:15.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:34:15.489: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:34:16.603: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:34:16.603: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:34:16.603: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:34:16.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7990 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 10:34:17.754: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 10:34:17.754: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 10:34:17.754: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 10:34:17.754: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:34:17.844: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Mar 16 10:34:28.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:34:28.024: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:34:28.024: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 16 10:34:28.295: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Mar 16 10:34:28.295: INFO: ss-0  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  }]
    Mar 16 10:34:28.296: INFO: ss-1  ip-10-250-19-246.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
    Mar 16 10:34:28.296: INFO: ss-2  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
    Mar 16 10:34:28.296: INFO: 
    Mar 16 10:34:28.296: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 16 10:34:29.386: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Mar 16 10:34:29.386: INFO: ss-0  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:37 +0000 UTC  }]
    Mar 16 10:34:29.386: INFO: ss-2  ip-10-250-19-136.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:33:59 +0000 UTC  }]
    Mar 16 10:34:29.386: INFO: 
    Mar 16 10:34:29.386: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar 16 10:34:30.476: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.818753835s
    Mar 16 10:34:31.566: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.729086397s
    Mar 16 10:34:32.656: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.639307836s
    Mar 16 10:34:33.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.549294993s
    Mar 16 10:34:34.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.459314445s
    Mar 16 10:34:35.926: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.36956111s
    Mar 16 10:34:37.017: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.278325536s
    Mar 16 10:34:38.107: INFO: Verifying statefulset ss doesn't scale past 0 for another 187.455855ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7990 03/16/23 10:34:39.108
    Mar 16 10:34:39.198: INFO: Scaling statefulset ss to 0
    Mar 16 10:34:39.468: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 10:34:39.566: INFO: Deleting all statefulset in ns statefulset-7990
    Mar 16 10:34:39.656: INFO: Scaling statefulset ss to 0
    Mar 16 10:34:39.925: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 10:34:40.014: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:34:40.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7990" for this suite. 03/16/23 10:34:40.462
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:34:40.553
Mar 16 10:34:40.553: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 10:34:40.554
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:34:40.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:34:41.002
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 in namespace container-probe-8863 03/16/23 10:34:41.18
Mar 16 10:34:41.275: INFO: Waiting up to 5m0s for pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8" in namespace "container-probe-8863" to be "not pending"
Mar 16 10:34:41.364: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.769591ms
Mar 16 10:34:43.454: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.179699285s
Mar 16 10:34:43.454: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8" satisfied condition "not pending"
Mar 16 10:34:43.454: INFO: Started pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 in namespace container-probe-8863
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:34:43.454
Mar 16 10:34:43.544: INFO: Initial restart count of pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 is 0
Mar 16 10:35:33.806: INFO: Restart count of pod container-probe-8863/busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 is now 1 (50.261517225s elapsed)
STEP: deleting the pod 03/16/23 10:35:33.806
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 10:35:33.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8863" for this suite. 03/16/23 10:35:34.077
------------------------------
• [53.615 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:34:40.553
    Mar 16 10:34:40.553: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 10:34:40.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:34:40.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:34:41.002
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 in namespace container-probe-8863 03/16/23 10:34:41.18
    Mar 16 10:34:41.275: INFO: Waiting up to 5m0s for pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8" in namespace "container-probe-8863" to be "not pending"
    Mar 16 10:34:41.364: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.769591ms
    Mar 16 10:34:43.454: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.179699285s
    Mar 16 10:34:43.454: INFO: Pod "busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8" satisfied condition "not pending"
    Mar 16 10:34:43.454: INFO: Started pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 in namespace container-probe-8863
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:34:43.454
    Mar 16 10:34:43.544: INFO: Initial restart count of pod busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 is 0
    Mar 16 10:35:33.806: INFO: Restart count of pod container-probe-8863/busybox-fe981d25-ef89-4ba7-88ec-caa169d713e8 is now 1 (50.261517225s elapsed)
    STEP: deleting the pod 03/16/23 10:35:33.806
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:35:33.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8863" for this suite. 03/16/23 10:35:34.077
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:35:34.168
Mar 16 10:35:34.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:35:34.169
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:34.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:34.614
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 03/16/23 10:35:34.792
Mar 16 10:35:34.887: INFO: created test-pod-1
Mar 16 10:35:34.980: INFO: created test-pod-2
Mar 16 10:35:35.073: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/16/23 10:35:35.073
Mar 16 10:35:35.073: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7395' to be running and ready
Mar 16 10:35:35.341: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 16 10:35:35.341: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 16 10:35:35.341: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 16 10:35:35.341: INFO: 0 / 3 pods in namespace 'pods-7395' are running and ready (0 seconds elapsed)
Mar 16 10:35:35.341: INFO: expected 0 pod replicas in namespace 'pods-7395', 0 are Running and Ready.
Mar 16 10:35:35.341: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
Mar 16 10:35:35.341: INFO: test-pod-1  ip-10-250-19-136.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  }]
Mar 16 10:35:35.341: INFO: test-pod-2  ip-10-250-19-136.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  }]
Mar 16 10:35:35.341: INFO: test-pod-3  ip-10-250-19-246.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC  }]
Mar 16 10:35:35.341: INFO: 
Mar 16 10:35:37.609: INFO: 3 / 3 pods in namespace 'pods-7395' are running and ready (2 seconds elapsed)
Mar 16 10:35:37.609: INFO: expected 0 pod replicas in namespace 'pods-7395', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/16/23 10:35:37.714
Mar 16 10:35:37.803: INFO: Pod quantity 3 is different from expected quantity 0
Mar 16 10:35:38.893: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:35:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7395" for this suite. 03/16/23 10:35:40.07
------------------------------
• [5.993 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:35:34.168
    Mar 16 10:35:34.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:35:34.169
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:34.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:34.614
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 03/16/23 10:35:34.792
    Mar 16 10:35:34.887: INFO: created test-pod-1
    Mar 16 10:35:34.980: INFO: created test-pod-2
    Mar 16 10:35:35.073: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/16/23 10:35:35.073
    Mar 16 10:35:35.073: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7395' to be running and ready
    Mar 16 10:35:35.341: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 16 10:35:35.341: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 16 10:35:35.341: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 16 10:35:35.341: INFO: 0 / 3 pods in namespace 'pods-7395' are running and ready (0 seconds elapsed)
    Mar 16 10:35:35.341: INFO: expected 0 pod replicas in namespace 'pods-7395', 0 are Running and Ready.
    Mar 16 10:35:35.341: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
    Mar 16 10:35:35.341: INFO: test-pod-1  ip-10-250-19-136.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  }]
    Mar 16 10:35:35.341: INFO: test-pod-2  ip-10-250-19-136.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:34 +0000 UTC  }]
    Mar 16 10:35:35.341: INFO: test-pod-3  ip-10-250-19-246.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-16 10:35:35 +0000 UTC  }]
    Mar 16 10:35:35.341: INFO: 
    Mar 16 10:35:37.609: INFO: 3 / 3 pods in namespace 'pods-7395' are running and ready (2 seconds elapsed)
    Mar 16 10:35:37.609: INFO: expected 0 pod replicas in namespace 'pods-7395', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/16/23 10:35:37.714
    Mar 16 10:35:37.803: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 16 10:35:38.893: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:35:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7395" for this suite. 03/16/23 10:35:40.07
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:35:40.161
Mar 16 10:35:40.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 10:35:40.162
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:40.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:40.607
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Mar 16 10:35:41.147: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/16/23 10:35:41.237
Mar 16 10:35:41.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:41.326: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/16/23 10:35:41.326
Mar 16 10:35:41.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:41.774: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:35:42.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:35:42.865: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/16/23 10:35:42.961
Mar 16 10:35:43.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:43.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/16/23 10:35:43.408
Mar 16 10:35:43.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:43.591: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:35:44.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:44.682: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:35:45.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:45.681: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:35:46.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:46.681: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:35:47.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:35:47.681: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:35:47.859
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7288, will wait for the garbage collector to delete the pods 03/16/23 10:35:47.859
Mar 16 10:35:48.139: INFO: Deleting DaemonSet.extensions daemon-set took: 89.971902ms
Mar 16 10:35:48.240: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.980632ms
Mar 16 10:35:49.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:35:49.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 10:35:50.019: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32163"},"items":null}

Mar 16 10:35:50.108: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32163"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:35:50.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7288" for this suite. 03/16/23 10:35:50.734
------------------------------
• [10.663 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:35:40.161
    Mar 16 10:35:40.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 10:35:40.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:40.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:40.607
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Mar 16 10:35:41.147: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/16/23 10:35:41.237
    Mar 16 10:35:41.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:41.326: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/16/23 10:35:41.326
    Mar 16 10:35:41.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:41.774: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:35:42.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:35:42.865: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/16/23 10:35:42.961
    Mar 16 10:35:43.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:43.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/16/23 10:35:43.408
    Mar 16 10:35:43.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:43.591: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:35:44.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:44.682: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:35:45.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:45.681: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:35:46.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:46.681: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:35:47.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:35:47.681: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:35:47.859
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7288, will wait for the garbage collector to delete the pods 03/16/23 10:35:47.859
    Mar 16 10:35:48.139: INFO: Deleting DaemonSet.extensions daemon-set took: 89.971902ms
    Mar 16 10:35:48.240: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.980632ms
    Mar 16 10:35:49.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:35:49.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 10:35:50.019: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32163"},"items":null}

    Mar 16 10:35:50.108: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32163"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:35:50.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7288" for this suite. 03/16/23 10:35:50.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:35:50.824
Mar 16 10:35:50.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:35:50.826
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:51.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:51.27
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:35:51.628
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:35:52.204
STEP: Deploying the webhook pod 03/16/23 10:35:52.295
STEP: Wait for the deployment to be ready 03/16/23 10:35:52.475
Mar 16 10:35:52.751: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:35:54.84
STEP: Verifying the service has paired with the endpoint 03/16/23 10:35:54.935
Mar 16 10:35:55.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 03/16/23 10:35:56.027
STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.295
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/16/23 10:35:56.467
STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.647
STEP: Patching a validating webhook configuration's rules to include the create operation 03/16/23 10:35:56.826
STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:35:57.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6660" for this suite. 03/16/23 10:35:57.691
STEP: Destroying namespace "webhook-6660-markers" for this suite. 03/16/23 10:35:57.781
------------------------------
• [7.047 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:35:50.824
    Mar 16 10:35:50.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:35:50.826
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:51.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:51.27
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:35:51.628
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:35:52.204
    STEP: Deploying the webhook pod 03/16/23 10:35:52.295
    STEP: Wait for the deployment to be ready 03/16/23 10:35:52.475
    Mar 16 10:35:52.751: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 35, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:35:54.84
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:35:54.935
    Mar 16 10:35:55.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 03/16/23 10:35:56.027
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.295
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/16/23 10:35:56.467
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.647
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/16/23 10:35:56.826
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/16/23 10:35:56.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:35:57.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6660" for this suite. 03/16/23 10:35:57.691
    STEP: Destroying namespace "webhook-6660-markers" for this suite. 03/16/23 10:35:57.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:35:57.871
Mar 16 10:35:57.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:35:57.873
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:58.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:58.317
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-70eeca81-1446-4acc-8c57-079c66407a00 03/16/23 10:35:58.583
STEP: Creating secret with name s-test-opt-upd-555ec628-1e89-4855-98c3-602367a1fcb5 03/16/23 10:35:58.673
STEP: Creating the pod 03/16/23 10:35:58.762
Mar 16 10:35:58.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad" in namespace "projected-2461" to be "running and ready"
Mar 16 10:35:58.951: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 93.190641ms
Mar 16 10:35:58.951: INFO: The phase of Pod pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:36:01.040: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.182614932s
Mar 16 10:36:01.040: INFO: The phase of Pod pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad is Running (Ready = true)
Mar 16 10:36:01.040: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-70eeca81-1446-4acc-8c57-079c66407a00 03/16/23 10:36:01.414
STEP: Updating secret s-test-opt-upd-555ec628-1e89-4855-98c3-602367a1fcb5 03/16/23 10:36:01.504
STEP: Creating secret with name s-test-opt-create-9e9b9ae1-1715-4bbe-900a-0fefb24e9ff0 03/16/23 10:36:01.593
STEP: waiting to observe update in volume 03/16/23 10:36:01.683
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2461" for this suite. 03/16/23 10:37:19.74
------------------------------
• [81.959 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:35:57.871
    Mar 16 10:35:57.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:35:57.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:35:58.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:35:58.317
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-70eeca81-1446-4acc-8c57-079c66407a00 03/16/23 10:35:58.583
    STEP: Creating secret with name s-test-opt-upd-555ec628-1e89-4855-98c3-602367a1fcb5 03/16/23 10:35:58.673
    STEP: Creating the pod 03/16/23 10:35:58.762
    Mar 16 10:35:58.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad" in namespace "projected-2461" to be "running and ready"
    Mar 16 10:35:58.951: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 93.190641ms
    Mar 16 10:35:58.951: INFO: The phase of Pod pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:36:01.040: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.182614932s
    Mar 16 10:36:01.040: INFO: The phase of Pod pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad is Running (Ready = true)
    Mar 16 10:36:01.040: INFO: Pod "pod-projected-secrets-c1f9b9fe-ddcd-4a35-9139-ba288ff4f4ad" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-70eeca81-1446-4acc-8c57-079c66407a00 03/16/23 10:36:01.414
    STEP: Updating secret s-test-opt-upd-555ec628-1e89-4855-98c3-602367a1fcb5 03/16/23 10:36:01.504
    STEP: Creating secret with name s-test-opt-create-9e9b9ae1-1715-4bbe-900a-0fefb24e9ff0 03/16/23 10:36:01.593
    STEP: waiting to observe update in volume 03/16/23 10:36:01.683
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2461" for this suite. 03/16/23 10:37:19.74
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:19.831
Mar 16 10:37:19.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:37:19.832
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:20.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:20.277
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 03/16/23 10:37:20.454
Mar 16 10:37:20.548: INFO: Waiting up to 5m0s for pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31" in namespace "emptydir-2627" to be "Succeeded or Failed"
Mar 16 10:37:20.638: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Pending", Reason="", readiness=false. Elapsed: 89.390227ms
Mar 16 10:37:22.727: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179037821s
Mar 16 10:37:24.728: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179858661s
STEP: Saw pod success 03/16/23 10:37:24.728
Mar 16 10:37:24.728: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31" satisfied condition "Succeeded or Failed"
Mar 16 10:37:24.817: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 container test-container: <nil>
STEP: delete the pod 03/16/23 10:37:25.062
Mar 16 10:37:25.155: INFO: Waiting for pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 to disappear
Mar 16 10:37:25.244: INFO: Pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2627" for this suite. 03/16/23 10:37:25.421
------------------------------
• [5.681 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:19.831
    Mar 16 10:37:19.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:37:19.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:20.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:20.277
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/16/23 10:37:20.454
    Mar 16 10:37:20.548: INFO: Waiting up to 5m0s for pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31" in namespace "emptydir-2627" to be "Succeeded or Failed"
    Mar 16 10:37:20.638: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Pending", Reason="", readiness=false. Elapsed: 89.390227ms
    Mar 16 10:37:22.727: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179037821s
    Mar 16 10:37:24.728: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179858661s
    STEP: Saw pod success 03/16/23 10:37:24.728
    Mar 16 10:37:24.728: INFO: Pod "pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31" satisfied condition "Succeeded or Failed"
    Mar 16 10:37:24.817: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 container test-container: <nil>
    STEP: delete the pod 03/16/23 10:37:25.062
    Mar 16 10:37:25.155: INFO: Waiting for pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 to disappear
    Mar 16 10:37:25.244: INFO: Pod pod-e572e0a8-1886-4fec-b1b3-ac091f8f4f31 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2627" for this suite. 03/16/23 10:37:25.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:25.512
Mar 16 10:37:25.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:37:25.514
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:25.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:25.961
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:37:26.138
Mar 16 10:37:26.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7" in namespace "projected-9955" to be "Succeeded or Failed"
Mar 16 10:37:26.326: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.081083ms
Mar 16 10:37:28.417: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Running", Reason="", readiness=false. Elapsed: 2.179568231s
Mar 16 10:37:30.418: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181368852s
STEP: Saw pod success 03/16/23 10:37:30.418
Mar 16 10:37:30.419: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7" satisfied condition "Succeeded or Failed"
Mar 16 10:37:30.508: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 container client-container: <nil>
STEP: delete the pod 03/16/23 10:37:30.602
Mar 16 10:37:30.695: INFO: Waiting for pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 to disappear
Mar 16 10:37:30.784: INFO: Pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:30.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9955" for this suite. 03/16/23 10:37:30.961
------------------------------
• [5.539 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:25.512
    Mar 16 10:37:25.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:37:25.514
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:25.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:25.961
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:37:26.138
    Mar 16 10:37:26.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7" in namespace "projected-9955" to be "Succeeded or Failed"
    Mar 16 10:37:26.326: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.081083ms
    Mar 16 10:37:28.417: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Running", Reason="", readiness=false. Elapsed: 2.179568231s
    Mar 16 10:37:30.418: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181368852s
    STEP: Saw pod success 03/16/23 10:37:30.418
    Mar 16 10:37:30.419: INFO: Pod "downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7" satisfied condition "Succeeded or Failed"
    Mar 16 10:37:30.508: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:37:30.602
    Mar 16 10:37:30.695: INFO: Waiting for pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 to disappear
    Mar 16 10:37:30.784: INFO: Pod downwardapi-volume-83fff48a-40ee-41d9-b059-3eae6baf72c7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:30.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9955" for this suite. 03/16/23 10:37:30.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:31.051
Mar 16 10:37:31.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename conformance-tests 03/16/23 10:37:31.052
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:31.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:31.497
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/16/23 10:37:31.674
Mar 16 10:37:31.674: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:31.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-6204" for this suite. 03/16/23 10:37:32.03
------------------------------
• [1.069 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:31.051
    Mar 16 10:37:31.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename conformance-tests 03/16/23 10:37:31.052
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:31.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:31.497
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/16/23 10:37:31.674
    Mar 16 10:37:31.674: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:31.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-6204" for this suite. 03/16/23 10:37:32.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:32.121
Mar 16 10:37:32.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:37:32.122
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:32.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:32.567
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 03/16/23 10:37:32.744
Mar 16 10:37:32.840: INFO: Waiting up to 5m0s for pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191" in namespace "projected-5007" to be "running and ready"
Mar 16 10:37:32.929: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191": Phase="Pending", Reason="", readiness=false. Elapsed: 88.965808ms
Mar 16 10:37:32.929: INFO: The phase of Pod labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:37:35.019: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191": Phase="Running", Reason="", readiness=true. Elapsed: 2.179097318s
Mar 16 10:37:35.019: INFO: The phase of Pod labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191 is Running (Ready = true)
Mar 16 10:37:35.019: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191" satisfied condition "running and ready"
Mar 16 10:37:35.884: INFO: Successfully updated pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5007" for this suite. 03/16/23 10:37:38.252
------------------------------
• [6.222 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:32.121
    Mar 16 10:37:32.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:37:32.122
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:32.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:32.567
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 03/16/23 10:37:32.744
    Mar 16 10:37:32.840: INFO: Waiting up to 5m0s for pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191" in namespace "projected-5007" to be "running and ready"
    Mar 16 10:37:32.929: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191": Phase="Pending", Reason="", readiness=false. Elapsed: 88.965808ms
    Mar 16 10:37:32.929: INFO: The phase of Pod labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:37:35.019: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191": Phase="Running", Reason="", readiness=true. Elapsed: 2.179097318s
    Mar 16 10:37:35.019: INFO: The phase of Pod labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191 is Running (Ready = true)
    Mar 16 10:37:35.019: INFO: Pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191" satisfied condition "running and ready"
    Mar 16 10:37:35.884: INFO: Successfully updated pod "labelsupdatee822ffe0-3822-45cc-af44-1be5c9906191"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5007" for this suite. 03/16/23 10:37:38.252
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:38.343
Mar 16 10:37:38.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 10:37:38.345
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:38.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:38.789
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 16 10:37:39.336: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b9b5dedb-851b-4fd6-bd5e-3f0b7116ea95", Controller:(*bool)(0xc004ba10be), BlockOwnerDeletion:(*bool)(0xc004ba10bf)}}
Mar 16 10:37:39.427: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a3795b6c-0408-400c-ba19-c9ad2dc29a85", Controller:(*bool)(0xc0067d7cf6), BlockOwnerDeletion:(*bool)(0xc0067d7cf7)}}
Mar 16 10:37:39.519: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a15de6d4-f300-41a2-9033-c9ecc6c619d8", Controller:(*bool)(0xc004de6fa6), BlockOwnerDeletion:(*bool)(0xc004de6fa7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:44.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7539" for this suite. 03/16/23 10:37:44.886
------------------------------
• [6.633 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:38.343
    Mar 16 10:37:38.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 10:37:38.345
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:38.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:38.789
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 16 10:37:39.336: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b9b5dedb-851b-4fd6-bd5e-3f0b7116ea95", Controller:(*bool)(0xc004ba10be), BlockOwnerDeletion:(*bool)(0xc004ba10bf)}}
    Mar 16 10:37:39.427: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a3795b6c-0408-400c-ba19-c9ad2dc29a85", Controller:(*bool)(0xc0067d7cf6), BlockOwnerDeletion:(*bool)(0xc0067d7cf7)}}
    Mar 16 10:37:39.519: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a15de6d4-f300-41a2-9033-c9ecc6c619d8", Controller:(*bool)(0xc004de6fa6), BlockOwnerDeletion:(*bool)(0xc004de6fa7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:44.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7539" for this suite. 03/16/23 10:37:44.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:44.977
Mar 16 10:37:44.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:37:44.978
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:45.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:45.422
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:37:45.599
Mar 16 10:37:45.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907" in namespace "projected-9620" to be "Succeeded or Failed"
Mar 16 10:37:45.783: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Pending", Reason="", readiness=false. Elapsed: 89.229412ms
Mar 16 10:37:47.873: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179246677s
Mar 16 10:37:49.874: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18005384s
STEP: Saw pod success 03/16/23 10:37:49.874
Mar 16 10:37:49.874: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907" satisfied condition "Succeeded or Failed"
Mar 16 10:37:49.963: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 container client-container: <nil>
STEP: delete the pod 03/16/23 10:37:50.058
Mar 16 10:37:50.151: INFO: Waiting for pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 to disappear
Mar 16 10:37:50.240: INFO: Pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:50.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9620" for this suite. 03/16/23 10:37:50.417
------------------------------
• [5.530 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:44.977
    Mar 16 10:37:44.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:37:44.978
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:45.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:45.422
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:37:45.599
    Mar 16 10:37:45.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907" in namespace "projected-9620" to be "Succeeded or Failed"
    Mar 16 10:37:45.783: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Pending", Reason="", readiness=false. Elapsed: 89.229412ms
    Mar 16 10:37:47.873: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179246677s
    Mar 16 10:37:49.874: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18005384s
    STEP: Saw pod success 03/16/23 10:37:49.874
    Mar 16 10:37:49.874: INFO: Pod "downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907" satisfied condition "Succeeded or Failed"
    Mar 16 10:37:49.963: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:37:50.058
    Mar 16 10:37:50.151: INFO: Waiting for pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 to disappear
    Mar 16 10:37:50.240: INFO: Pod downwardapi-volume-a328507c-2788-46a2-b91a-c6f8c22b4907 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:50.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9620" for this suite. 03/16/23 10:37:50.417
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:50.508
Mar 16 10:37:50.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:37:50.509
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:50.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:50.954
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Mar 16 10:37:51.219: INFO: Got root ca configmap in namespace "svcaccounts-6980"
Mar 16 10:37:51.309: INFO: Deleted root ca configmap in namespace "svcaccounts-6980"
STEP: waiting for a new root ca configmap created 03/16/23 10:37:51.81
Mar 16 10:37:51.899: INFO: Recreated root ca configmap in namespace "svcaccounts-6980"
Mar 16 10:37:51.989: INFO: Updated root ca configmap in namespace "svcaccounts-6980"
STEP: waiting for the root ca configmap reconciled 03/16/23 10:37:52.489
Mar 16 10:37:52.578: INFO: Reconciled root ca configmap in namespace "svcaccounts-6980"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:52.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6980" for this suite. 03/16/23 10:37:52.755
------------------------------
• [2.338 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:50.508
    Mar 16 10:37:50.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:37:50.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:50.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:50.954
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Mar 16 10:37:51.219: INFO: Got root ca configmap in namespace "svcaccounts-6980"
    Mar 16 10:37:51.309: INFO: Deleted root ca configmap in namespace "svcaccounts-6980"
    STEP: waiting for a new root ca configmap created 03/16/23 10:37:51.81
    Mar 16 10:37:51.899: INFO: Recreated root ca configmap in namespace "svcaccounts-6980"
    Mar 16 10:37:51.989: INFO: Updated root ca configmap in namespace "svcaccounts-6980"
    STEP: waiting for the root ca configmap reconciled 03/16/23 10:37:52.489
    Mar 16 10:37:52.578: INFO: Reconciled root ca configmap in namespace "svcaccounts-6980"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:52.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6980" for this suite. 03/16/23 10:37:52.755
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:37:52.846
Mar 16 10:37:52.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:37:52.847
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:53.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:53.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:37:53.649
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:37:54.358
STEP: Deploying the webhook pod 03/16/23 10:37:54.449
STEP: Wait for the deployment to be ready 03/16/23 10:37:54.637
Mar 16 10:37:54.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:37:57.003
STEP: Verifying the service has paired with the endpoint 03/16/23 10:37:57.097
Mar 16 10:37:58.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/16/23 10:37:58.188
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/16/23 10:37:58.455
STEP: Creating a dummy validating-webhook-configuration object 03/16/23 10:37:58.719
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/16/23 10:37:58.951
STEP: Creating a dummy mutating-webhook-configuration object 03/16/23 10:37:59.041
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/16/23 10:37:59.271
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:37:59.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4181" for this suite. 03/16/23 10:38:00.084
STEP: Destroying namespace "webhook-4181-markers" for this suite. 03/16/23 10:38:00.174
------------------------------
• [7.418 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:37:52.846
    Mar 16 10:37:52.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:37:52.847
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:37:53.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:37:53.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:37:53.649
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:37:54.358
    STEP: Deploying the webhook pod 03/16/23 10:37:54.449
    STEP: Wait for the deployment to be ready 03/16/23 10:37:54.637
    Mar 16 10:37:54.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 37, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:37:57.003
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:37:57.097
    Mar 16 10:37:58.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/16/23 10:37:58.188
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/16/23 10:37:58.455
    STEP: Creating a dummy validating-webhook-configuration object 03/16/23 10:37:58.719
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/16/23 10:37:58.951
    STEP: Creating a dummy mutating-webhook-configuration object 03/16/23 10:37:59.041
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/16/23 10:37:59.271
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:37:59.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4181" for this suite. 03/16/23 10:38:00.084
    STEP: Destroying namespace "webhook-4181-markers" for this suite. 03/16/23 10:38:00.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:38:00.265
Mar 16 10:38:00.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:38:00.266
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:00.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:00.711
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/16/23 10:38:00.888
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_tcp@PTR;sleep 1; done
 03/16/23 10:38:01.072
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_tcp@PTR;sleep 1; done
 03/16/23 10:38:01.072
STEP: creating a pod to probe DNS 03/16/23 10:38:01.072
STEP: submitting the pod to kubernetes 03/16/23 10:38:01.073
Mar 16 10:38:01.171: INFO: Waiting up to 15m0s for pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f" in namespace "dns-5909" to be "running"
Mar 16 10:38:01.261: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.242378ms
Mar 16 10:38:03.351: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.179319583s
Mar 16 10:38:03.351: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f" satisfied condition "running"
STEP: retrieving the pod 03/16/23 10:38:03.351
STEP: looking for the results for each expected name from probers 03/16/23 10:38:03.44
Mar 16 10:38:03.626: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:03.762: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:03.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:03.990: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:04.450: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:04.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:04.635: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:04.727: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:05.094: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

Mar 16 10:38:10.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:10.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:10.374: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:10.465: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:10.930: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:11.022: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:11.113: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:11.205: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:11.575: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

Mar 16 10:38:15.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:15.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:15.390: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:15.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:15.942: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:16.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:16.126: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:16.266: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
Mar 16 10:38:16.635: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

Mar 16 10:38:21.572: INFO: DNS probes using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f succeeded

STEP: deleting the pod 03/16/23 10:38:21.572
STEP: deleting the test service 03/16/23 10:38:21.665
STEP: deleting the test headless service 03/16/23 10:38:21.759
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:38:21.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5909" for this suite. 03/16/23 10:38:22.028
------------------------------
• [21.853 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:38:00.265
    Mar 16 10:38:00.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:38:00.266
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:00.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:00.711
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/16/23 10:38:00.888
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_tcp@PTR;sleep 1; done
     03/16/23 10:38:01.072
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.72.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.72.209_tcp@PTR;sleep 1; done
     03/16/23 10:38:01.072
    STEP: creating a pod to probe DNS 03/16/23 10:38:01.072
    STEP: submitting the pod to kubernetes 03/16/23 10:38:01.073
    Mar 16 10:38:01.171: INFO: Waiting up to 15m0s for pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f" in namespace "dns-5909" to be "running"
    Mar 16 10:38:01.261: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.242378ms
    Mar 16 10:38:03.351: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.179319583s
    Mar 16 10:38:03.351: INFO: Pod "dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 10:38:03.351
    STEP: looking for the results for each expected name from probers 03/16/23 10:38:03.44
    Mar 16 10:38:03.626: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:03.762: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:03.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:03.990: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:04.450: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:04.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:04.635: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:04.727: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:05.094: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

    Mar 16 10:38:10.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:10.280: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:10.374: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:10.465: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:10.930: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:11.022: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:11.113: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:11.205: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:11.575: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

    Mar 16 10:38:15.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:15.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:15.390: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:15.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:15.942: INFO: Unable to read jessie_udp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:16.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:16.126: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:16.266: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local from pod dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f: the server could not find the requested resource (get pods dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f)
    Mar 16 10:38:16.635: INFO: Lookups using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f failed for: [wheezy_udp@dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@dns-test-service.dns-5909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_udp@dns-test-service.dns-5909.svc.cluster.local jessie_tcp@dns-test-service.dns-5909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5909.svc.cluster.local]

    Mar 16 10:38:21.572: INFO: DNS probes using dns-5909/dns-test-ae3ca9fe-6bdd-46e6-a14e-788503ec4e2f succeeded

    STEP: deleting the pod 03/16/23 10:38:21.572
    STEP: deleting the test service 03/16/23 10:38:21.665
    STEP: deleting the test headless service 03/16/23 10:38:21.759
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:38:21.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5909" for this suite. 03/16/23 10:38:22.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:38:22.119
Mar 16 10:38:22.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates 03/16/23 10:38:22.121
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:22.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:22.565
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/16/23 10:38:23.379
STEP: getting /apis/certificates.k8s.io 03/16/23 10:38:23.555
STEP: getting /apis/certificates.k8s.io/v1 03/16/23 10:38:23.643
STEP: creating 03/16/23 10:38:23.731
STEP: getting 03/16/23 10:38:24.002
STEP: listing 03/16/23 10:38:24.091
STEP: watching 03/16/23 10:38:24.18
Mar 16 10:38:24.180: INFO: starting watch
STEP: patching 03/16/23 10:38:24.268
STEP: updating 03/16/23 10:38:24.36
Mar 16 10:38:24.450: INFO: waiting for watch events with expected annotations
Mar 16 10:38:24.450: INFO: saw patched and updated annotations
STEP: getting /approval 03/16/23 10:38:24.45
STEP: patching /approval 03/16/23 10:38:24.542
STEP: updating /approval 03/16/23 10:38:24.633
STEP: getting /status 03/16/23 10:38:24.723
STEP: patching /status 03/16/23 10:38:24.813
STEP: updating /status 03/16/23 10:38:24.903
STEP: deleting 03/16/23 10:38:24.997
STEP: deleting a collection 03/16/23 10:38:25.265
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:38:25.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9046" for this suite. 03/16/23 10:38:25.624
------------------------------
• [3.595 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:38:22.119
    Mar 16 10:38:22.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename certificates 03/16/23 10:38:22.121
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:22.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:22.565
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/16/23 10:38:23.379
    STEP: getting /apis/certificates.k8s.io 03/16/23 10:38:23.555
    STEP: getting /apis/certificates.k8s.io/v1 03/16/23 10:38:23.643
    STEP: creating 03/16/23 10:38:23.731
    STEP: getting 03/16/23 10:38:24.002
    STEP: listing 03/16/23 10:38:24.091
    STEP: watching 03/16/23 10:38:24.18
    Mar 16 10:38:24.180: INFO: starting watch
    STEP: patching 03/16/23 10:38:24.268
    STEP: updating 03/16/23 10:38:24.36
    Mar 16 10:38:24.450: INFO: waiting for watch events with expected annotations
    Mar 16 10:38:24.450: INFO: saw patched and updated annotations
    STEP: getting /approval 03/16/23 10:38:24.45
    STEP: patching /approval 03/16/23 10:38:24.542
    STEP: updating /approval 03/16/23 10:38:24.633
    STEP: getting /status 03/16/23 10:38:24.723
    STEP: patching /status 03/16/23 10:38:24.813
    STEP: updating /status 03/16/23 10:38:24.903
    STEP: deleting 03/16/23 10:38:24.997
    STEP: deleting a collection 03/16/23 10:38:25.265
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:38:25.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9046" for this suite. 03/16/23 10:38:25.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:38:25.715
Mar 16 10:38:25.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 10:38:25.716
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:25.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:26.161
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 03/16/23 10:38:26.338
STEP: Ensuring active pods == parallelism 03/16/23 10:38:26.429
STEP: Orphaning one of the Job's Pods 03/16/23 10:38:28.52
Mar 16 10:38:29.297: INFO: Successfully updated pod "adopt-release-872bt"
STEP: Checking that the Job readopts the Pod 03/16/23 10:38:29.297
Mar 16 10:38:29.297: INFO: Waiting up to 15m0s for pod "adopt-release-872bt" in namespace "job-666" to be "adopted"
Mar 16 10:38:29.386: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 89.26832ms
Mar 16 10:38:31.477: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.179552179s
Mar 16 10:38:31.477: INFO: Pod "adopt-release-872bt" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/16/23 10:38:31.477
Mar 16 10:38:32.158: INFO: Successfully updated pod "adopt-release-872bt"
STEP: Checking that the Job releases the Pod 03/16/23 10:38:32.158
Mar 16 10:38:32.158: INFO: Waiting up to 15m0s for pod "adopt-release-872bt" in namespace "job-666" to be "released"
Mar 16 10:38:32.250: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 91.980228ms
Mar 16 10:38:34.341: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.182881532s
Mar 16 10:38:34.341: INFO: Pod "adopt-release-872bt" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 10:38:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-666" for this suite. 03/16/23 10:38:34.521
------------------------------
• [8.897 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:38:25.715
    Mar 16 10:38:25.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 10:38:25.716
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:25.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:26.161
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 03/16/23 10:38:26.338
    STEP: Ensuring active pods == parallelism 03/16/23 10:38:26.429
    STEP: Orphaning one of the Job's Pods 03/16/23 10:38:28.52
    Mar 16 10:38:29.297: INFO: Successfully updated pod "adopt-release-872bt"
    STEP: Checking that the Job readopts the Pod 03/16/23 10:38:29.297
    Mar 16 10:38:29.297: INFO: Waiting up to 15m0s for pod "adopt-release-872bt" in namespace "job-666" to be "adopted"
    Mar 16 10:38:29.386: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 89.26832ms
    Mar 16 10:38:31.477: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.179552179s
    Mar 16 10:38:31.477: INFO: Pod "adopt-release-872bt" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/16/23 10:38:31.477
    Mar 16 10:38:32.158: INFO: Successfully updated pod "adopt-release-872bt"
    STEP: Checking that the Job releases the Pod 03/16/23 10:38:32.158
    Mar 16 10:38:32.158: INFO: Waiting up to 15m0s for pod "adopt-release-872bt" in namespace "job-666" to be "released"
    Mar 16 10:38:32.250: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 91.980228ms
    Mar 16 10:38:34.341: INFO: Pod "adopt-release-872bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.182881532s
    Mar 16 10:38:34.341: INFO: Pod "adopt-release-872bt" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:38:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-666" for this suite. 03/16/23 10:38:34.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:38:34.612
Mar 16 10:38:34.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 03/16/23 10:38:34.613
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:34.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:35.058
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/16/23 10:38:35.241
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-kskz 03/16/23 10:38:35.42
STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:38:35.421
Mar 16 10:38:35.515: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kskz" in namespace "subpath-5726" to be "Succeeded or Failed"
Mar 16 10:38:35.604: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Pending", Reason="", readiness=false. Elapsed: 89.235756ms
Mar 16 10:38:37.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 2.17939948s
Mar 16 10:38:39.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 4.178698074s
Mar 16 10:38:41.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 6.178750313s
Mar 16 10:38:43.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 8.17864373s
Mar 16 10:38:45.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 10.178809898s
Mar 16 10:38:47.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 12.178437454s
Mar 16 10:38:49.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 14.178768808s
Mar 16 10:38:51.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 16.179897235s
Mar 16 10:38:53.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 18.178754597s
Mar 16 10:38:55.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 20.17865369s
Mar 16 10:38:57.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=false. Elapsed: 22.179711464s
Mar 16 10:38:59.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.178666863s
STEP: Saw pod success 03/16/23 10:38:59.694
Mar 16 10:38:59.694: INFO: Pod "pod-subpath-test-downwardapi-kskz" satisfied condition "Succeeded or Failed"
Mar 16 10:38:59.783: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-subpath-test-downwardapi-kskz container test-container-subpath-downwardapi-kskz: <nil>
STEP: delete the pod 03/16/23 10:38:59.978
Mar 16 10:39:00.071: INFO: Waiting for pod pod-subpath-test-downwardapi-kskz to disappear
Mar 16 10:39:00.160: INFO: Pod pod-subpath-test-downwardapi-kskz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kskz 03/16/23 10:39:00.16
Mar 16 10:39:00.160: INFO: Deleting pod "pod-subpath-test-downwardapi-kskz" in namespace "subpath-5726"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:00.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5726" for this suite. 03/16/23 10:39:00.427
------------------------------
• [25.905 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:38:34.612
    Mar 16 10:38:34.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 03/16/23 10:38:34.613
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:38:34.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:38:35.058
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/16/23 10:38:35.241
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-kskz 03/16/23 10:38:35.42
    STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:38:35.421
    Mar 16 10:38:35.515: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kskz" in namespace "subpath-5726" to be "Succeeded or Failed"
    Mar 16 10:38:35.604: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Pending", Reason="", readiness=false. Elapsed: 89.235756ms
    Mar 16 10:38:37.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 2.17939948s
    Mar 16 10:38:39.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 4.178698074s
    Mar 16 10:38:41.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 6.178750313s
    Mar 16 10:38:43.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 8.17864373s
    Mar 16 10:38:45.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 10.178809898s
    Mar 16 10:38:47.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 12.178437454s
    Mar 16 10:38:49.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 14.178768808s
    Mar 16 10:38:51.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 16.179897235s
    Mar 16 10:38:53.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 18.178754597s
    Mar 16 10:38:55.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=true. Elapsed: 20.17865369s
    Mar 16 10:38:57.695: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Running", Reason="", readiness=false. Elapsed: 22.179711464s
    Mar 16 10:38:59.694: INFO: Pod "pod-subpath-test-downwardapi-kskz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.178666863s
    STEP: Saw pod success 03/16/23 10:38:59.694
    Mar 16 10:38:59.694: INFO: Pod "pod-subpath-test-downwardapi-kskz" satisfied condition "Succeeded or Failed"
    Mar 16 10:38:59.783: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-subpath-test-downwardapi-kskz container test-container-subpath-downwardapi-kskz: <nil>
    STEP: delete the pod 03/16/23 10:38:59.978
    Mar 16 10:39:00.071: INFO: Waiting for pod pod-subpath-test-downwardapi-kskz to disappear
    Mar 16 10:39:00.160: INFO: Pod pod-subpath-test-downwardapi-kskz no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-kskz 03/16/23 10:39:00.16
    Mar 16 10:39:00.160: INFO: Deleting pod "pod-subpath-test-downwardapi-kskz" in namespace "subpath-5726"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:00.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5726" for this suite. 03/16/23 10:39:00.427
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:00.517
Mar 16 10:39:00.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:39:00.519
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:00.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:00.962
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:39:01.139
Mar 16 10:39:01.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf" in namespace "downward-api-4753" to be "Succeeded or Failed"
Mar 16 10:39:01.323: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 88.95876ms
Mar 16 10:39:03.413: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179353112s
Mar 16 10:39:05.414: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.17974878s
STEP: Saw pod success 03/16/23 10:39:05.414
Mar 16 10:39:05.414: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf" satisfied condition "Succeeded or Failed"
Mar 16 10:39:05.503: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf container client-container: <nil>
STEP: delete the pod 03/16/23 10:39:05.638
Mar 16 10:39:05.730: INFO: Waiting for pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf to disappear
Mar 16 10:39:05.819: INFO: Pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:05.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4753" for this suite. 03/16/23 10:39:05.996
------------------------------
• [5.570 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:00.517
    Mar 16 10:39:00.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:39:00.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:00.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:00.962
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:39:01.139
    Mar 16 10:39:01.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf" in namespace "downward-api-4753" to be "Succeeded or Failed"
    Mar 16 10:39:01.323: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 88.95876ms
    Mar 16 10:39:03.413: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179353112s
    Mar 16 10:39:05.414: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.17974878s
    STEP: Saw pod success 03/16/23 10:39:05.414
    Mar 16 10:39:05.414: INFO: Pod "downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf" satisfied condition "Succeeded or Failed"
    Mar 16 10:39:05.503: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf container client-container: <nil>
    STEP: delete the pod 03/16/23 10:39:05.638
    Mar 16 10:39:05.730: INFO: Waiting for pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf to disappear
    Mar 16 10:39:05.819: INFO: Pod downwardapi-volume-c2cfe3f1-120a-4544-bf11-29a7c2ec6bbf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:05.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4753" for this suite. 03/16/23 10:39:05.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:06.088
Mar 16 10:39:06.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:39:06.089
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:06.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:06.534
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:07.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8030" for this suite. 03/16/23 10:39:07.606
------------------------------
• [1.608 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:06.088
    Mar 16 10:39:06.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:39:06.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:06.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:06.534
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:07.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8030" for this suite. 03/16/23 10:39:07.606
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:07.696
Mar 16 10:39:07.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 10:39:07.697
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:07.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:08.141
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 16 10:39:08.497: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 10:39:08.497
Mar 16 10:39:08.497: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-qn779" in namespace "deployment-4694" to be "running"
Mar 16 10:39:08.587: INFO: Pod "test-cleanup-controller-qn779": Phase="Pending", Reason="", readiness=false. Elapsed: 89.118349ms
Mar 16 10:39:10.676: INFO: Pod "test-cleanup-controller-qn779": Phase="Running", Reason="", readiness=true. Elapsed: 2.178951685s
Mar 16 10:39:10.676: INFO: Pod "test-cleanup-controller-qn779" satisfied condition "running"
Mar 16 10:39:10.677: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/16/23 10:39:10.944
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 10:39:13.391: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4694  5b3d5e4c-2efa-467f-97b6-a6aa0f3be573 33834 1 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068049b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:39:10 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-16 10:39:12 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 16 10:39:13.481: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4694  f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe 33827 1 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5b3d5e4c-2efa-467f-97b6-a6aa0f3be573 0xc006804d87 0xc006804d88}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b3d5e4c-2efa-467f-97b6-a6aa0f3be573\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006804e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:39:13.570: INFO: Pod "test-cleanup-deployment-7698ff6f6b-vpmvd" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-vpmvd test-cleanup-deployment-7698ff6f6b- deployment-4694  0912d50a-530c-437c-9234-38df4779cb5f 33826 0 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:3305715635a8dbc003daf7557d373930fd46b192359a1ccb5351b171c032365d cni.projectcalico.org/podIP:100.64.1.75/32 cni.projectcalico.org/podIPs:100.64.1.75/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe 0xc0068051d7 0xc0068051d8}] [] [{kube-controller-manager Update v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:39:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58rlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58rlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.75,StartTime:2023-03-16 10:39:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:39:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://629fe3f044940930d0e5b9e6a1f6a9417ab4bede5c6fb97e134fc55988d1889f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:13.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4694" for this suite. 03/16/23 10:39:13.748
------------------------------
• [6.142 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:07.696
    Mar 16 10:39:07.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 10:39:07.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:07.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:08.141
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 16 10:39:08.497: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 10:39:08.497
    Mar 16 10:39:08.497: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-qn779" in namespace "deployment-4694" to be "running"
    Mar 16 10:39:08.587: INFO: Pod "test-cleanup-controller-qn779": Phase="Pending", Reason="", readiness=false. Elapsed: 89.118349ms
    Mar 16 10:39:10.676: INFO: Pod "test-cleanup-controller-qn779": Phase="Running", Reason="", readiness=true. Elapsed: 2.178951685s
    Mar 16 10:39:10.676: INFO: Pod "test-cleanup-controller-qn779" satisfied condition "running"
    Mar 16 10:39:10.677: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/16/23 10:39:10.944
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 10:39:13.391: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4694  5b3d5e4c-2efa-467f-97b6-a6aa0f3be573 33834 1 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068049b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:39:10 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-16 10:39:12 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 16 10:39:13.481: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4694  f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe 33827 1 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5b3d5e4c-2efa-467f-97b6-a6aa0f3be573 0xc006804d87 0xc006804d88}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b3d5e4c-2efa-467f-97b6-a6aa0f3be573\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006804e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:39:13.570: INFO: Pod "test-cleanup-deployment-7698ff6f6b-vpmvd" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-vpmvd test-cleanup-deployment-7698ff6f6b- deployment-4694  0912d50a-530c-437c-9234-38df4779cb5f 33826 0 2023-03-16 10:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:3305715635a8dbc003daf7557d373930fd46b192359a1ccb5351b171c032365d cni.projectcalico.org/podIP:100.64.1.75/32 cni.projectcalico.org/podIPs:100.64.1.75/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe 0xc0068051d7 0xc0068051d8}] [] [{kube-controller-manager Update v1 2023-03-16 10:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7b75aec-37c2-4e8a-a3a0-7ad7cd8f4dfe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:39:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58rlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58rlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.75,StartTime:2023-03-16 10:39:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:39:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://629fe3f044940930d0e5b9e6a1f6a9417ab4bede5c6fb97e134fc55988d1889f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:13.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4694" for this suite. 03/16/23 10:39:13.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:13.839
Mar 16 10:39:13.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment 03/16/23 10:39:13.84
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:14.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:14.284
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 16 10:39:14.460: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 16 10:39:14.639: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 10:39:14.639
Mar 16 10:39:14.639: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-wjttd" in namespace "deployment-7660" to be "running"
Mar 16 10:39:14.728: INFO: Pod "test-rolling-update-controller-wjttd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.023287ms
Mar 16 10:39:16.818: INFO: Pod "test-rolling-update-controller-wjttd": Phase="Running", Reason="", readiness=true. Elapsed: 2.17846287s
Mar 16 10:39:16.818: INFO: Pod "test-rolling-update-controller-wjttd" satisfied condition "running"
Mar 16 10:39:16.818: INFO: Creating deployment "test-rolling-update-deployment"
Mar 16 10:39:16.908: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 16 10:39:17.086: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 16 10:39:17.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:39:19.265: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 16 10:39:19.533: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7660  e05a700b-3737-45c8-862e-3c30126ea4ab 33910 1 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c11d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:39:16 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-16 10:39:18 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 16 10:39:19.622: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7660  986b2791-aff1-49da-bb0e-6f26e34a1534 33903 1 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e05a700b-3737-45c8-862e-3c30126ea4ab 0xc0051084f7 0xc0051084f8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e05a700b-3737-45c8-862e-3c30126ea4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005108678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:39:19.622: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 16 10:39:19.622: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7660  9a051cc8-a7cf-47d3-a90f-dc6ae639d104 33909 2 2023-03-16 10:39:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e05a700b-3737-45c8-862e-3c30126ea4ab 0xc0051083c7 0xc0051083c8}] [] [{e2e.test Update apps/v1 2023-03-16 10:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e05a700b-3737-45c8-862e-3c30126ea4ab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005108488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 16 10:39:19.713: INFO: Pod "test-rolling-update-deployment-7549d9f46d-lhdrs" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-lhdrs test-rolling-update-deployment-7549d9f46d- deployment-7660  09cd9c4c-9dc1-40cc-8be6-39be8ac20c93 33902 0 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3a54ff54de121e13a8d46a0cf0bb386a3c8ec81541d67267577e2ab79dbcc0b0 cni.projectcalico.org/podIP:100.64.1.77/32 cni.projectcalico.org/podIPs:100.64.1.77/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 986b2791-aff1-49da-bb0e-6f26e34a1534 0xc005108ae7 0xc005108ae8}] [] [{kube-controller-manager Update v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986b2791-aff1-49da-bb0e-6f26e34a1534\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:39:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlr6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlr6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.77,StartTime:2023-03-16 10:39:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:39:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fca3c8400c92f482d70bea552978d6118725734e7c21f069932ed687be753502,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7660" for this suite. 03/16/23 10:39:19.892
------------------------------
• [6.144 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:13.839
    Mar 16 10:39:13.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename deployment 03/16/23 10:39:13.84
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:14.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:14.284
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 16 10:39:14.460: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 16 10:39:14.639: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 10:39:14.639
    Mar 16 10:39:14.639: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-wjttd" in namespace "deployment-7660" to be "running"
    Mar 16 10:39:14.728: INFO: Pod "test-rolling-update-controller-wjttd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.023287ms
    Mar 16 10:39:16.818: INFO: Pod "test-rolling-update-controller-wjttd": Phase="Running", Reason="", readiness=true. Elapsed: 2.17846287s
    Mar 16 10:39:16.818: INFO: Pod "test-rolling-update-controller-wjttd" satisfied condition "running"
    Mar 16 10:39:16.818: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 16 10:39:16.908: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 16 10:39:17.086: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 16 10:39:17.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 39, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:39:19.265: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 16 10:39:19.533: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7660  e05a700b-3737-45c8-862e-3c30126ea4ab 33910 1 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c11d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-16 10:39:16 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-16 10:39:18 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 16 10:39:19.622: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7660  986b2791-aff1-49da-bb0e-6f26e34a1534 33903 1 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e05a700b-3737-45c8-862e-3c30126ea4ab 0xc0051084f7 0xc0051084f8}] [] [{kube-controller-manager Update apps/v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e05a700b-3737-45c8-862e-3c30126ea4ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005108678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:39:19.622: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 16 10:39:19.622: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7660  9a051cc8-a7cf-47d3-a90f-dc6ae639d104 33909 2 2023-03-16 10:39:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e05a700b-3737-45c8-862e-3c30126ea4ab 0xc0051083c7 0xc0051083c8}] [] [{e2e.test Update apps/v1 2023-03-16 10:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e05a700b-3737-45c8-862e-3c30126ea4ab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005108488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 16 10:39:19.713: INFO: Pod "test-rolling-update-deployment-7549d9f46d-lhdrs" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-lhdrs test-rolling-update-deployment-7549d9f46d- deployment-7660  09cd9c4c-9dc1-40cc-8be6-39be8ac20c93 33902 0 2023-03-16 10:39:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3a54ff54de121e13a8d46a0cf0bb386a3c8ec81541d67267577e2ab79dbcc0b0 cni.projectcalico.org/podIP:100.64.1.77/32 cni.projectcalico.org/podIPs:100.64.1.77/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 986b2791-aff1-49da-bb0e-6f26e34a1534 0xc005108ae7 0xc005108ae8}] [] [{kube-controller-manager Update v1 2023-03-16 10:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986b2791-aff1-49da-bb0e-6f26e34a1534\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-16 10:39:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-16 10:39:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.64.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlr6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlr6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-19-136.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-16 10:39:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.19.136,PodIP:100.64.1.77,StartTime:2023-03-16 10:39:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-16 10:39:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fca3c8400c92f482d70bea552978d6118725734e7c21f069932ed687be753502,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7660" for this suite. 03/16/23 10:39:19.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:19.984
Mar 16 10:39:19.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 10:39:19.985
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:20.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:20.429
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:39:20.606
Mar 16 10:39:20.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29" in namespace "downward-api-8888" to be "Succeeded or Failed"
Mar 16 10:39:20.790: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Pending", Reason="", readiness=false. Elapsed: 89.19285ms
Mar 16 10:39:22.879: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178595739s
Mar 16 10:39:24.880: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179576226s
STEP: Saw pod success 03/16/23 10:39:24.88
Mar 16 10:39:24.881: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29" satisfied condition "Succeeded or Failed"
Mar 16 10:39:24.970: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 container client-container: <nil>
STEP: delete the pod 03/16/23 10:39:25.194
Mar 16 10:39:25.288: INFO: Waiting for pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 to disappear
Mar 16 10:39:25.376: INFO: Pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:25.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8888" for this suite. 03/16/23 10:39:25.553
------------------------------
• [5.660 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:19.984
    Mar 16 10:39:19.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 10:39:19.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:20.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:20.429
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:39:20.606
    Mar 16 10:39:20.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29" in namespace "downward-api-8888" to be "Succeeded or Failed"
    Mar 16 10:39:20.790: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Pending", Reason="", readiness=false. Elapsed: 89.19285ms
    Mar 16 10:39:22.879: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178595739s
    Mar 16 10:39:24.880: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179576226s
    STEP: Saw pod success 03/16/23 10:39:24.88
    Mar 16 10:39:24.881: INFO: Pod "downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29" satisfied condition "Succeeded or Failed"
    Mar 16 10:39:24.970: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:39:25.194
    Mar 16 10:39:25.288: INFO: Waiting for pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 to disappear
    Mar 16 10:39:25.376: INFO: Pod downwardapi-volume-9142dde5-1a9b-4d88-941a-f72c52154e29 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:25.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8888" for this suite. 03/16/23 10:39:25.553
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:25.644
Mar 16 10:39:25.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:39:25.645
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:25.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:26.089
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 16 10:39:26.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:29.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-117" for this suite. 03/16/23 10:39:29.402
------------------------------
• [3.848 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:25.644
    Mar 16 10:39:25.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:39:25.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:25.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:26.089
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 16 10:39:26.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:29.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-117" for this suite. 03/16/23 10:39:29.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:29.492
Mar 16 10:39:29.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 03/16/23 10:39:29.494
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:29.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:29.939
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 03/16/23 10:39:30.205
STEP: Waiting for all pods to be running 03/16/23 10:39:30.573
Mar 16 10:39:30.663: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-495" for this suite. 03/16/23 10:39:33.019
------------------------------
• [3.617 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:29.492
    Mar 16 10:39:29.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 03/16/23 10:39:29.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:29.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:29.939
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 03/16/23 10:39:30.205
    STEP: Waiting for all pods to be running 03/16/23 10:39:30.573
    Mar 16 10:39:30.663: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-495" for this suite. 03/16/23 10:39:33.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:33.11
Mar 16 10:39:33.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:39:33.112
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:33.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:33.556
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-3669/configmap-test-21d32bfd-d480-40f1-b900-d838bcccb968 03/16/23 10:39:33.733
STEP: Creating a pod to test consume configMaps 03/16/23 10:39:33.823
Mar 16 10:39:33.916: INFO: Waiting up to 5m0s for pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa" in namespace "configmap-3669" to be "Succeeded or Failed"
Mar 16 10:39:34.008: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Pending", Reason="", readiness=false. Elapsed: 91.703007ms
Mar 16 10:39:36.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181286422s
Mar 16 10:39:38.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181361555s
STEP: Saw pod success 03/16/23 10:39:38.098
Mar 16 10:39:38.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa" satisfied condition "Succeeded or Failed"
Mar 16 10:39:38.187: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa container env-test: <nil>
STEP: delete the pod 03/16/23 10:39:38.284
Mar 16 10:39:38.376: INFO: Waiting for pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa to disappear
Mar 16 10:39:38.465: INFO: Pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:38.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3669" for this suite. 03/16/23 10:39:38.642
------------------------------
• [5.623 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:33.11
    Mar 16 10:39:33.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:39:33.112
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:33.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:33.556
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-3669/configmap-test-21d32bfd-d480-40f1-b900-d838bcccb968 03/16/23 10:39:33.733
    STEP: Creating a pod to test consume configMaps 03/16/23 10:39:33.823
    Mar 16 10:39:33.916: INFO: Waiting up to 5m0s for pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa" in namespace "configmap-3669" to be "Succeeded or Failed"
    Mar 16 10:39:34.008: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Pending", Reason="", readiness=false. Elapsed: 91.703007ms
    Mar 16 10:39:36.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181286422s
    Mar 16 10:39:38.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181361555s
    STEP: Saw pod success 03/16/23 10:39:38.098
    Mar 16 10:39:38.098: INFO: Pod "pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa" satisfied condition "Succeeded or Failed"
    Mar 16 10:39:38.187: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa container env-test: <nil>
    STEP: delete the pod 03/16/23 10:39:38.284
    Mar 16 10:39:38.376: INFO: Waiting for pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa to disappear
    Mar 16 10:39:38.465: INFO: Pod pod-configmaps-d05a7eea-bfb6-4d77-a643-7b69ac5a21aa no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:38.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3669" for this suite. 03/16/23 10:39:38.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:38.734
Mar 16 10:39:38.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:39:38.735
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:39.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:39.179
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-7d598ba5-88fd-4ab5-8b45-1e62384594d4 03/16/23 10:39:39.356
STEP: Creating a pod to test consume configMaps 03/16/23 10:39:39.446
Mar 16 10:39:39.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f" in namespace "configmap-9233" to be "Succeeded or Failed"
Mar 16 10:39:39.630: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.107628ms
Mar 16 10:39:41.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179723714s
Mar 16 10:39:43.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180055444s
STEP: Saw pod success 03/16/23 10:39:43.721
Mar 16 10:39:43.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f" satisfied condition "Succeeded or Failed"
Mar 16 10:39:43.810: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:39:43.905
Mar 16 10:39:43.997: INFO: Waiting for pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f to disappear
Mar 16 10:39:44.086: INFO: Pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:44.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9233" for this suite. 03/16/23 10:39:44.264
------------------------------
• [5.621 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:38.734
    Mar 16 10:39:38.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:39:38.735
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:39.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:39.179
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-7d598ba5-88fd-4ab5-8b45-1e62384594d4 03/16/23 10:39:39.356
    STEP: Creating a pod to test consume configMaps 03/16/23 10:39:39.446
    Mar 16 10:39:39.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f" in namespace "configmap-9233" to be "Succeeded or Failed"
    Mar 16 10:39:39.630: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.107628ms
    Mar 16 10:39:41.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179723714s
    Mar 16 10:39:43.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180055444s
    STEP: Saw pod success 03/16/23 10:39:43.721
    Mar 16 10:39:43.721: INFO: Pod "pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f" satisfied condition "Succeeded or Failed"
    Mar 16 10:39:43.810: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:39:43.905
    Mar 16 10:39:43.997: INFO: Waiting for pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f to disappear
    Mar 16 10:39:44.086: INFO: Pod pod-configmaps-df5db6e2-b1ba-4349-bbee-4c3b862efd5f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:44.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9233" for this suite. 03/16/23 10:39:44.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:44.356
Mar 16 10:39:44.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:39:44.357
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:44.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:44.801
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1005 03/16/23 10:39:44.978
STEP: creating service affinity-clusterip-transition in namespace services-1005 03/16/23 10:39:44.978
STEP: creating replication controller affinity-clusterip-transition in namespace services-1005 03/16/23 10:39:45.074
I0316 10:39:45.164220    8588 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1005, replica count: 3
I0316 10:39:48.265843    8588 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:39:48.443: INFO: Creating new exec pod
Mar 16 10:39:48.535: INFO: Waiting up to 5m0s for pod "execpod-affinitynndxp" in namespace "services-1005" to be "running"
Mar 16 10:39:48.624: INFO: Pod "execpod-affinitynndxp": Phase="Pending", Reason="", readiness=false. Elapsed: 89.087877ms
Mar 16 10:39:50.715: INFO: Pod "execpod-affinitynndxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.17984192s
Mar 16 10:39:50.715: INFO: Pod "execpod-affinitynndxp" satisfied condition "running"
Mar 16 10:39:51.716: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Mar 16 10:39:52.861: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 16 10:39:52.862: INFO: stdout: ""
Mar 16 10:39:52.862: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c nc -v -z -w 2 100.107.238.154 80'
Mar 16 10:39:54.013: INFO: stderr: "+ nc -v -z -w 2 100.107.238.154 80\nConnection to 100.107.238.154 80 port [tcp/http] succeeded!\n"
Mar 16 10:39:54.013: INFO: stdout: ""
Mar 16 10:39:54.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.238.154:80/ ; done'
Mar 16 10:39:55.374: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n"
Mar 16 10:39:55.374: INFO: stdout: "\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5"
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
Mar 16 10:39:55.553: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.238.154:80/ ; done'
Mar 16 10:39:56.774: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n"
Mar 16 10:39:56.774: INFO: stdout: "\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp"
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
Mar 16 10:39:56.774: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1005, will wait for the garbage collector to delete the pods 03/16/23 10:39:56.868
Mar 16 10:39:57.148: INFO: Deleting ReplicationController affinity-clusterip-transition took: 89.701476ms
Mar 16 10:39:57.249: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.013081ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:39:59.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1005" for this suite. 03/16/23 10:39:59.737
------------------------------
• [15.471 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:44.356
    Mar 16 10:39:44.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:39:44.357
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:39:44.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:39:44.801
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1005 03/16/23 10:39:44.978
    STEP: creating service affinity-clusterip-transition in namespace services-1005 03/16/23 10:39:44.978
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1005 03/16/23 10:39:45.074
    I0316 10:39:45.164220    8588 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1005, replica count: 3
    I0316 10:39:48.265843    8588 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:39:48.443: INFO: Creating new exec pod
    Mar 16 10:39:48.535: INFO: Waiting up to 5m0s for pod "execpod-affinitynndxp" in namespace "services-1005" to be "running"
    Mar 16 10:39:48.624: INFO: Pod "execpod-affinitynndxp": Phase="Pending", Reason="", readiness=false. Elapsed: 89.087877ms
    Mar 16 10:39:50.715: INFO: Pod "execpod-affinitynndxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.17984192s
    Mar 16 10:39:50.715: INFO: Pod "execpod-affinitynndxp" satisfied condition "running"
    Mar 16 10:39:51.716: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Mar 16 10:39:52.861: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 16 10:39:52.862: INFO: stdout: ""
    Mar 16 10:39:52.862: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c nc -v -z -w 2 100.107.238.154 80'
    Mar 16 10:39:54.013: INFO: stderr: "+ nc -v -z -w 2 100.107.238.154 80\nConnection to 100.107.238.154 80 port [tcp/http] succeeded!\n"
    Mar 16 10:39:54.013: INFO: stdout: ""
    Mar 16 10:39:54.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.238.154:80/ ; done'
    Mar 16 10:39:55.374: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n"
    Mar 16 10:39:55.374: INFO: stdout: "\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-9xwz5\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-7q6b5\naffinity-clusterip-transition-7q6b5"
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-9xwz5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.374: INFO: Received response from host: affinity-clusterip-transition-7q6b5
    Mar 16 10:39:55.553: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1005 exec execpod-affinitynndxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.107.238.154:80/ ; done'
    Mar 16 10:39:56.774: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.107.238.154:80/\n"
    Mar 16 10:39:56.774: INFO: stdout: "\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp\naffinity-clusterip-transition-ztclp"
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Received response from host: affinity-clusterip-transition-ztclp
    Mar 16 10:39:56.774: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1005, will wait for the garbage collector to delete the pods 03/16/23 10:39:56.868
    Mar 16 10:39:57.148: INFO: Deleting ReplicationController affinity-clusterip-transition took: 89.701476ms
    Mar 16 10:39:57.249: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.013081ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:39:59.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1005" for this suite. 03/16/23 10:39:59.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:39:59.828
Mar 16 10:39:59.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy 03/16/23 10:39:59.829
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:00.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:00.274
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/16/23 10:40:00.547
STEP: creating replication controller proxy-service-tvbk9 in namespace proxy-9475 03/16/23 10:40:00.547
I0316 10:40:00.638133    8588 runners.go:193] Created replication controller with name: proxy-service-tvbk9, namespace: proxy-9475, replica count: 1
I0316 10:40:01.739109    8588 runners.go:193] proxy-service-tvbk9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0316 10:40:02.739891    8588 runners.go:193] proxy-service-tvbk9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:40:02.828: INFO: setup took 2.377138567s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/16/23 10:40:02.828
Mar 16 10:40:02.931: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 102.506969ms)
Mar 16 10:40:02.933: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 104.814901ms)
Mar 16 10:40:02.933: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 104.78951ms)
Mar 16 10:40:02.934: INFO: (0) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 105.312967ms)
Mar 16 10:40:02.934: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 105.269133ms)
Mar 16 10:40:02.935: INFO: (0) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 106.821276ms)
Mar 16 10:40:02.935: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 106.842367ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 191.89255ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 191.898915ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 191.850186ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 191.878084ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 191.854777ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 191.905853ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 191.942874ms)
Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 191.881808ms)
Mar 16 10:40:03.026: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 197.512303ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.163871ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.239496ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.25114ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.292479ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.350924ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.381242ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.392381ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.490178ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.57487ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.466346ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.509008ms)
Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.588246ms)
Mar 16 10:40:03.127: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.140089ms)
Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 101.256949ms)
Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.151485ms)
Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.163735ms)
Mar 16 10:40:03.223: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.606408ms)
Mar 16 10:40:03.223: INFO: (2) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.589973ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.638897ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.661482ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.712745ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.714672ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.835395ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.950998ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.860825ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.716845ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.737417ms)
Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.884057ms)
Mar 16 10:40:03.225: INFO: (2) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.61881ms)
Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 100.27392ms)
Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.338087ms)
Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.36178ms)
Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.006322ms)
Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.991404ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.095578ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.167052ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.16607ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.120389ms)
Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.272929ms)
Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.104852ms)
Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.06125ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.090698ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.098226ms)
Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.186492ms)
Mar 16 10:40:03.326: INFO: (3) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.720769ms)
Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.414658ms)
Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.496548ms)
Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.658884ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.432108ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.595503ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.456799ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.530547ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.713494ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.530795ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.670069ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.562365ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.632757ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.574186ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.511671ms)
Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.605833ms)
Mar 16 10:40:03.427: INFO: (4) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 97.43664ms)
Mar 16 10:40:03.428: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 98.971156ms)
Mar 16 10:40:03.428: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.988042ms)
Mar 16 10:40:03.430: INFO: (4) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 100.649491ms)
Mar 16 10:40:03.523: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 93.327693ms)
Mar 16 10:40:03.527: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 97.275555ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 97.462774ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 97.534295ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 97.67689ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 97.704522ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 97.783873ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 97.692258ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 97.791955ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 97.946891ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 97.799197ms)
Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 98.037754ms)
Mar 16 10:40:03.530: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.673361ms)
Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 101.887719ms)
Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.803728ms)
Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.896149ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.087346ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.178579ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.173283ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.148952ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.084959ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.101411ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.212786ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.185189ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.178339ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.2803ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.413181ms)
Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.354328ms)
Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 99.065763ms)
Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 99.202895ms)
Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 99.090438ms)
Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 99.141639ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.726514ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.749684ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.80217ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.811537ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.843887ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 94.85676ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.765278ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.907601ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.851353ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.986323ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.982357ms)
Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.914664ms)
Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 98.668944ms)
Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.670906ms)
Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.658273ms)
Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.736366ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.306648ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.510394ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.45647ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.485688ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.590251ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.654078ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.501071ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.558155ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.613162ms)
Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.618586ms)
Mar 16 10:40:03.827: INFO: (8) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.375505ms)
Mar 16 10:40:03.827: INFO: (8) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.45139ms)
Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 101.163466ms)
Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 101.162544ms)
Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.32801ms)
Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.258114ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.069369ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.130729ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.11621ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.124524ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.162366ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.211776ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.229539ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.226716ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.246197ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.397619ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.471674ms)
Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.346858ms)
Mar 16 10:40:03.929: INFO: (9) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.191239ms)
Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 99.313602ms)
Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 99.245831ms)
Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.262414ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.364019ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.50185ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.487207ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.563855ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.584596ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.544621ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.578693ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.712677ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.690564ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.627497ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.670547ms)
Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.818435ms)
Mar 16 10:40:04.028: INFO: (10) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 96.359376ms)
Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 98.447408ms)
Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.504331ms)
Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.42332ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.336524ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.447388ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.278625ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.310449ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.350625ms)
Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.39015ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.60472ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.583678ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.62177ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.550043ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.550192ms)
Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.660758ms)
Mar 16 10:40:04.129: INFO: (11) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.822455ms)
Mar 16 10:40:04.129: INFO: (11) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.910248ms)
Mar 16 10:40:04.130: INFO: (11) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.346532ms)
Mar 16 10:40:04.131: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.397935ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.920313ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.027671ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.966815ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.162388ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.042512ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.151439ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.098437ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.991528ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.087546ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.984077ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.084061ms)
Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.190436ms)
Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.664037ms)
Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.672539ms)
Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.627138ms)
Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 98.663788ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 96.491982ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 96.517494ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 96.571674ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 96.62517ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 96.634369ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.695946ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 96.722229ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 96.821914ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.943106ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 96.795241ms)
Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.918518ms)
Mar 16 10:40:04.327: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 96.929735ms)
Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.006383ms)
Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.979681ms)
Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.914696ms)
Mar 16 10:40:04.331: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.023362ms)
Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.577139ms)
Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.544699ms)
Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.637347ms)
Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.549087ms)
Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.607738ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.938706ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 96.015197ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.045659ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 96.08714ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.219095ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 96.239196ms)
Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.174908ms)
Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 99.70045ms)
Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 99.588179ms)
Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 99.64487ms)
Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 99.614581ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.373928ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.364518ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.408652ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.386237ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.417276ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.410196ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.398056ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.43587ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.489931ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.542033ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.450718ms)
Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.552278ms)
Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.006796ms)
Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.965784ms)
Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.061899ms)
Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 100.112296ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.198315ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.233018ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.247332ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.248283ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.331443ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.365285ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.295997ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.334456ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.49467ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.287755ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.432601ms)
Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.478307ms)
Mar 16 10:40:04.629: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 97.928837ms)
Mar 16 10:40:04.630: INFO: (16) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.769904ms)
Mar 16 10:40:04.630: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.800922ms)
Mar 16 10:40:04.631: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.319132ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 94.988595ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.007765ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.029433ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.992041ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.117097ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.094389ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.157822ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.178015ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.15313ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.1667ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.251414ms)
Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.228006ms)
Mar 16 10:40:04.728: INFO: (17) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 96.841467ms)
Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.366598ms)
Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.405889ms)
Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.552958ms)
Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.856417ms)
Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.966053ms)
Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.953632ms)
Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.02285ms)
Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.965989ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 96.963577ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.977565ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 96.992076ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.996719ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 97.049837ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 97.095166ms)
Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 96.971568ms)
Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.95144ms)
Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.970764ms)
Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 101.114434ms)
Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.200101ms)
Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 92.290474ms)
Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 92.332327ms)
Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 92.361117ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.378661ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.388824ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.42515ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.453978ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.517736ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.573734ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.691018ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.694584ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.639139ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.829305ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.88394ms)
Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 94.79608ms)
Mar 16 10:40:04.976: INFO: (19) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 143.191102ms)
STEP: deleting ReplicationController proxy-service-tvbk9 in namespace proxy-9475, will wait for the garbage collector to delete the pods 03/16/23 10:40:04.976
Mar 16 10:40:05.257: INFO: Deleting ReplicationController proxy-service-tvbk9 took: 91.048174ms
Mar 16 10:40:05.358: INFO: Terminating ReplicationController proxy-service-tvbk9 pods took: 100.414799ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9475" for this suite. 03/16/23 10:40:06.748
------------------------------
• [7.012 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:39:59.828
    Mar 16 10:39:59.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename proxy 03/16/23 10:39:59.829
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:00.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:00.274
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/16/23 10:40:00.547
    STEP: creating replication controller proxy-service-tvbk9 in namespace proxy-9475 03/16/23 10:40:00.547
    I0316 10:40:00.638133    8588 runners.go:193] Created replication controller with name: proxy-service-tvbk9, namespace: proxy-9475, replica count: 1
    I0316 10:40:01.739109    8588 runners.go:193] proxy-service-tvbk9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0316 10:40:02.739891    8588 runners.go:193] proxy-service-tvbk9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:40:02.828: INFO: setup took 2.377138567s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/16/23 10:40:02.828
    Mar 16 10:40:02.931: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 102.506969ms)
    Mar 16 10:40:02.933: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 104.814901ms)
    Mar 16 10:40:02.933: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 104.78951ms)
    Mar 16 10:40:02.934: INFO: (0) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 105.312967ms)
    Mar 16 10:40:02.934: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 105.269133ms)
    Mar 16 10:40:02.935: INFO: (0) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 106.821276ms)
    Mar 16 10:40:02.935: INFO: (0) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 106.842367ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 191.89255ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 191.898915ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 191.850186ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 191.878084ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 191.854777ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 191.905853ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 191.942874ms)
    Mar 16 10:40:03.020: INFO: (0) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 191.881808ms)
    Mar 16 10:40:03.026: INFO: (0) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 197.512303ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.163871ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.239496ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.25114ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.292479ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.350924ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.381242ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.392381ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.490178ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.57487ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.466346ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.509008ms)
    Mar 16 10:40:03.122: INFO: (1) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.588246ms)
    Mar 16 10:40:03.127: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.140089ms)
    Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 101.256949ms)
    Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.151485ms)
    Mar 16 10:40:03.128: INFO: (1) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.163735ms)
    Mar 16 10:40:03.223: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.606408ms)
    Mar 16 10:40:03.223: INFO: (2) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.589973ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.638897ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.661482ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.712745ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.714672ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.835395ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.950998ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.860825ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.716845ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.737417ms)
    Mar 16 10:40:03.224: INFO: (2) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.884057ms)
    Mar 16 10:40:03.225: INFO: (2) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.61881ms)
    Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 100.27392ms)
    Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.338087ms)
    Mar 16 10:40:03.228: INFO: (2) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.36178ms)
    Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.006322ms)
    Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.991404ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.095578ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.167052ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.16607ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.120389ms)
    Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.272929ms)
    Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.104852ms)
    Mar 16 10:40:03.323: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.06125ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.090698ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.098226ms)
    Mar 16 10:40:03.324: INFO: (3) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.186492ms)
    Mar 16 10:40:03.326: INFO: (3) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.720769ms)
    Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.414658ms)
    Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.496548ms)
    Mar 16 10:40:03.329: INFO: (3) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.658884ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.432108ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.595503ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.456799ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.530547ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.713494ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.530795ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.670069ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.562365ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.632757ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.574186ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.511671ms)
    Mar 16 10:40:03.425: INFO: (4) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.605833ms)
    Mar 16 10:40:03.427: INFO: (4) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 97.43664ms)
    Mar 16 10:40:03.428: INFO: (4) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 98.971156ms)
    Mar 16 10:40:03.428: INFO: (4) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.988042ms)
    Mar 16 10:40:03.430: INFO: (4) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 100.649491ms)
    Mar 16 10:40:03.523: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 93.327693ms)
    Mar 16 10:40:03.527: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 97.275555ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 97.462774ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 97.534295ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 97.67689ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 97.704522ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 97.783873ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 97.692258ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 97.791955ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 97.946891ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 97.799197ms)
    Mar 16 10:40:03.528: INFO: (5) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 98.037754ms)
    Mar 16 10:40:03.530: INFO: (5) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.673361ms)
    Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 101.887719ms)
    Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.803728ms)
    Mar 16 10:40:03.532: INFO: (5) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.896149ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.087346ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.178579ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.173283ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.148952ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.084959ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.101411ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.212786ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.185189ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.178339ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.2803ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.413181ms)
    Mar 16 10:40:03.627: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.354328ms)
    Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 99.065763ms)
    Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 99.202895ms)
    Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 99.090438ms)
    Mar 16 10:40:03.631: INFO: (6) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 99.141639ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.726514ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.749684ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.80217ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.811537ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.843887ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 94.85676ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.765278ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.907601ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.851353ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.986323ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.982357ms)
    Mar 16 10:40:03.726: INFO: (7) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.914664ms)
    Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 98.668944ms)
    Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.670906ms)
    Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.658273ms)
    Mar 16 10:40:03.730: INFO: (7) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.736366ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.306648ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.510394ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.45647ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.485688ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.590251ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.654078ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.501071ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.558155ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.613162ms)
    Mar 16 10:40:03.826: INFO: (8) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.618586ms)
    Mar 16 10:40:03.827: INFO: (8) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.375505ms)
    Mar 16 10:40:03.827: INFO: (8) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.45139ms)
    Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 101.163466ms)
    Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 101.162544ms)
    Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.32801ms)
    Mar 16 10:40:03.832: INFO: (8) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 101.258114ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.069369ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.130729ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.11621ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.124524ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.162366ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.211776ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.229539ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.226716ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.246197ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.397619ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.471674ms)
    Mar 16 10:40:03.927: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.346858ms)
    Mar 16 10:40:03.929: INFO: (9) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 97.191239ms)
    Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 99.313602ms)
    Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 99.245831ms)
    Mar 16 10:40:03.931: INFO: (9) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.262414ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.364019ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.50185ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.487207ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.563855ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.584596ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.544621ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.578693ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.712677ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.690564ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.627497ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.670547ms)
    Mar 16 10:40:04.026: INFO: (10) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.818435ms)
    Mar 16 10:40:04.028: INFO: (10) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 96.359376ms)
    Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 98.447408ms)
    Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.504331ms)
    Mar 16 10:40:04.030: INFO: (10) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.42332ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.336524ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.447388ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.278625ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.310449ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.350625ms)
    Mar 16 10:40:04.125: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.39015ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.60472ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.583678ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.62177ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.550043ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.550192ms)
    Mar 16 10:40:04.126: INFO: (11) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.660758ms)
    Mar 16 10:40:04.129: INFO: (11) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.822455ms)
    Mar 16 10:40:04.129: INFO: (11) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.910248ms)
    Mar 16 10:40:04.130: INFO: (11) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.346532ms)
    Mar 16 10:40:04.131: INFO: (11) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.397935ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.920313ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.027671ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.966815ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.162388ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.042512ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.151439ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.098437ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.991528ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.087546ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.984077ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.084061ms)
    Mar 16 10:40:04.226: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.190436ms)
    Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.664037ms)
    Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 98.672539ms)
    Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 98.627138ms)
    Mar 16 10:40:04.229: INFO: (12) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 98.663788ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 96.491982ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 96.517494ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 96.571674ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 96.62517ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 96.634369ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.695946ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 96.722229ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 96.821914ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.943106ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 96.795241ms)
    Mar 16 10:40:04.326: INFO: (13) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.918518ms)
    Mar 16 10:40:04.327: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 96.929735ms)
    Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.006383ms)
    Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.979681ms)
    Mar 16 10:40:04.330: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.914696ms)
    Mar 16 10:40:04.331: INFO: (13) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 101.023362ms)
    Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.577139ms)
    Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.544699ms)
    Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.637347ms)
    Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.549087ms)
    Mar 16 10:40:04.426: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.607738ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.938706ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 96.015197ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.045659ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 96.08714ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 96.219095ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 96.239196ms)
    Mar 16 10:40:04.427: INFO: (14) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.174908ms)
    Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 99.70045ms)
    Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 99.588179ms)
    Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 99.64487ms)
    Mar 16 10:40:04.430: INFO: (14) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 99.614581ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.373928ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.364518ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.408652ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.386237ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.417276ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.410196ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.398056ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 95.43587ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.489931ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.542033ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.450718ms)
    Mar 16 10:40:04.526: INFO: (15) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.552278ms)
    Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.006796ms)
    Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 99.965784ms)
    Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.061899ms)
    Mar 16 10:40:04.531: INFO: (15) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 100.112296ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 95.198315ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 95.233018ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 95.247332ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.248283ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.331443ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 95.365285ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.295997ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.334456ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.49467ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.287755ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.432601ms)
    Mar 16 10:40:04.626: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.478307ms)
    Mar 16 10:40:04.629: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 97.928837ms)
    Mar 16 10:40:04.630: INFO: (16) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 98.769904ms)
    Mar 16 10:40:04.630: INFO: (16) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 98.800922ms)
    Mar 16 10:40:04.631: INFO: (16) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.319132ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 94.988595ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 95.007765ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.029433ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.992041ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 95.117097ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.094389ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 95.157822ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 95.178015ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 95.15313ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 95.1667ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 95.251414ms)
    Mar 16 10:40:04.726: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 95.228006ms)
    Mar 16 10:40:04.728: INFO: (17) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 96.841467ms)
    Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 100.366598ms)
    Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.405889ms)
    Mar 16 10:40:04.732: INFO: (17) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 100.552958ms)
    Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.856417ms)
    Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.966053ms)
    Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.953632ms)
    Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 95.02285ms)
    Mar 16 10:40:04.827: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.965989ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 96.963577ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 96.977565ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 96.992076ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 96.996719ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 97.049837ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 97.095166ms)
    Mar 16 10:40:04.829: INFO: (18) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 96.971568ms)
    Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 100.95144ms)
    Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 100.970764ms)
    Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 101.114434ms)
    Mar 16 10:40:04.833: INFO: (18) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 101.200101ms)
    Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 92.290474ms)
    Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname1/proxy/: foo (200; 92.332327ms)
    Mar 16 10:40:04.926: INFO: (19) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname2/proxy/: bar (200; 92.361117ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.378661ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:462/proxy/: tls qux (200; 94.388824ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:443/proxy/tlsrewritem... (200; 94.42515ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/http:proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">... (200; 94.453978ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname1/proxy/: tls baz (200; 94.517736ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn/proxy/rewriteme">test</a> (200; 94.573734ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/https:proxy-service-tvbk9:tlsportname2/proxy/: tls qux (200; 94.691018ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/https:proxy-service-tvbk9-gw5tn:460/proxy/: tls baz (200; 94.694584ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:1080/proxy/rewriteme">test<... (200; 94.639139ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:160/proxy/: foo (200; 94.829305ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/pods/proxy-service-tvbk9-gw5tn:162/proxy/: bar (200; 94.88394ms)
    Mar 16 10:40:04.928: INFO: (19) /api/v1/namespaces/proxy-9475/services/http:proxy-service-tvbk9:portname2/proxy/: bar (200; 94.79608ms)
    Mar 16 10:40:04.976: INFO: (19) /api/v1/namespaces/proxy-9475/services/proxy-service-tvbk9:portname1/proxy/: foo (200; 143.191102ms)
    STEP: deleting ReplicationController proxy-service-tvbk9 in namespace proxy-9475, will wait for the garbage collector to delete the pods 03/16/23 10:40:04.976
    Mar 16 10:40:05.257: INFO: Deleting ReplicationController proxy-service-tvbk9 took: 91.048174ms
    Mar 16 10:40:05.358: INFO: Terminating ReplicationController proxy-service-tvbk9 pods took: 100.414799ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9475" for this suite. 03/16/23 10:40:06.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:06.84
Mar 16 10:40:06.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:40:06.841
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:07.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:07.286
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/16/23 10:40:07.463
Mar 16 10:40:07.557: INFO: Waiting up to 5m0s for pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b" in namespace "emptydir-8285" to be "Succeeded or Failed"
Mar 16 10:40:07.646: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.001301ms
Mar 16 10:40:09.736: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179446192s
Mar 16 10:40:11.735: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.178449515s
STEP: Saw pod success 03/16/23 10:40:11.735
Mar 16 10:40:11.736: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b" satisfied condition "Succeeded or Failed"
Mar 16 10:40:11.825: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b container test-container: <nil>
STEP: delete the pod 03/16/23 10:40:11.94
Mar 16 10:40:12.035: INFO: Waiting for pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b to disappear
Mar 16 10:40:12.124: INFO: Pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:12.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8285" for this suite. 03/16/23 10:40:12.306
------------------------------
• [5.556 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:06.84
    Mar 16 10:40:06.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:40:06.841
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:07.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:07.286
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/16/23 10:40:07.463
    Mar 16 10:40:07.557: INFO: Waiting up to 5m0s for pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b" in namespace "emptydir-8285" to be "Succeeded or Failed"
    Mar 16 10:40:07.646: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.001301ms
    Mar 16 10:40:09.736: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179446192s
    Mar 16 10:40:11.735: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.178449515s
    STEP: Saw pod success 03/16/23 10:40:11.735
    Mar 16 10:40:11.736: INFO: Pod "pod-be6af3d8-694f-4e81-ac33-fa85951cf80b" satisfied condition "Succeeded or Failed"
    Mar 16 10:40:11.825: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b container test-container: <nil>
    STEP: delete the pod 03/16/23 10:40:11.94
    Mar 16 10:40:12.035: INFO: Waiting for pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b to disappear
    Mar 16 10:40:12.124: INFO: Pod pod-be6af3d8-694f-4e81-ac33-fa85951cf80b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:12.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8285" for this suite. 03/16/23 10:40:12.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:12.397
Mar 16 10:40:12.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:40:12.399
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:12.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:12.844
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 03/16/23 10:40:13.021
Mar 16 10:40:13.116: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe" in namespace "emptydir-9482" to be "running"
Mar 16 10:40:13.205: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 89.086533ms
Mar 16 10:40:15.296: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe": Phase="Running", Reason="", readiness=false. Elapsed: 2.180070483s
Mar 16 10:40:15.296: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/16/23 10:40:15.296
Mar 16 10:40:15.297: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9482 PodName:pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:40:15.297: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:40:15.297: INFO: ExecWithOptions: Clientset creation
Mar 16 10:40:15.298: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-9482/pods/pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 16 10:40:16.058: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:16.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9482" for this suite. 03/16/23 10:40:16.236
------------------------------
• [3.928 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:12.397
    Mar 16 10:40:12.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:40:12.399
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:12.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:12.844
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 03/16/23 10:40:13.021
    Mar 16 10:40:13.116: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe" in namespace "emptydir-9482" to be "running"
    Mar 16 10:40:13.205: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 89.086533ms
    Mar 16 10:40:15.296: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe": Phase="Running", Reason="", readiness=false. Elapsed: 2.180070483s
    Mar 16 10:40:15.296: INFO: Pod "pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/16/23 10:40:15.296
    Mar 16 10:40:15.297: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9482 PodName:pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:40:15.297: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:40:15.297: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:40:15.298: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-9482/pods/pod-sharedvolume-2f56170b-ad26-40e1-9168-18d2da90d6fe/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 16 10:40:16.058: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:16.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9482" for this suite. 03/16/23 10:40:16.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:16.327
Mar 16 10:40:16.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:40:16.329
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:16.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:16.774
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 10:40:16.952
Mar 16 10:40:16.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4354 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Mar 16 10:40:17.303: INFO: stderr: ""
Mar 16 10:40:17.303: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/16/23 10:40:17.303
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Mar 16 10:40:17.393: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4354 delete pods e2e-test-httpd-pod'
Mar 16 10:40:19.601: INFO: stderr: ""
Mar 16 10:40:19.601: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:19.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4354" for this suite. 03/16/23 10:40:19.779
------------------------------
• [3.542 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:16.327
    Mar 16 10:40:16.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:40:16.329
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:16.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:16.774
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 10:40:16.952
    Mar 16 10:40:16.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4354 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Mar 16 10:40:17.303: INFO: stderr: ""
    Mar 16 10:40:17.303: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/16/23 10:40:17.303
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Mar 16 10:40:17.393: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4354 delete pods e2e-test-httpd-pod'
    Mar 16 10:40:19.601: INFO: stderr: ""
    Mar 16 10:40:19.601: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:19.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4354" for this suite. 03/16/23 10:40:19.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:19.875
Mar 16 10:40:19.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename controllerrevisions 03/16/23 10:40:19.876
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:20.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:20.32
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-7vfj8-daemon-set" 03/16/23 10:40:20.855
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:40:20.945
Mar 16 10:40:21.123: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
Mar 16 10:40:21.124: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:40:22.390: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
Mar 16 10:40:22.390: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:40:23.390: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 2
Mar 16 10:40:23.390: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-7vfj8-daemon-set
STEP: Confirm DaemonSet "e2e-7vfj8-daemon-set" successfully created with "daemonset-name=e2e-7vfj8-daemon-set" label 03/16/23 10:40:23.479
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7vfj8-daemon-set" 03/16/23 10:40:23.658
Mar 16 10:40:23.748: INFO: Located ControllerRevision: "e2e-7vfj8-daemon-set-6b74fdd896"
STEP: Patching ControllerRevision "e2e-7vfj8-daemon-set-6b74fdd896" 03/16/23 10:40:23.837
Mar 16 10:40:23.927: INFO: e2e-7vfj8-daemon-set-6b74fdd896 has been patched
STEP: Create a new ControllerRevision 03/16/23 10:40:23.927
Mar 16 10:40:24.017: INFO: Created ControllerRevision: e2e-7vfj8-daemon-set-745f5fb45d
STEP: Confirm that there are two ControllerRevisions 03/16/23 10:40:24.018
Mar 16 10:40:24.018: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 16 10:40:24.107: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-7vfj8-daemon-set-6b74fdd896" 03/16/23 10:40:24.107
STEP: Confirm that there is only one ControllerRevision 03/16/23 10:40:24.197
Mar 16 10:40:24.197: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 16 10:40:24.286: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-7vfj8-daemon-set-745f5fb45d" 03/16/23 10:40:24.375
Mar 16 10:40:24.554: INFO: e2e-7vfj8-daemon-set-745f5fb45d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/16/23 10:40:24.554
W0316 10:40:24.645236    8588 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/16/23 10:40:24.645
Mar 16 10:40:24.645: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 16 10:40:24.734: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7vfj8-daemon-set-745f5fb45d=updated" 03/16/23 10:40:24.734
STEP: Confirm that there is only one ControllerRevision 03/16/23 10:40:24.825
Mar 16 10:40:24.825: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 16 10:40:24.914: INFO: Found 1 ControllerRevisions
Mar 16 10:40:25.003: INFO: ControllerRevision "e2e-7vfj8-daemon-set-6f9d8545f9" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-7vfj8-daemon-set" 03/16/23 10:40:25.092
STEP: deleting DaemonSet.extensions e2e-7vfj8-daemon-set in namespace controllerrevisions-4561, will wait for the garbage collector to delete the pods 03/16/23 10:40:25.092
Mar 16 10:40:25.372: INFO: Deleting DaemonSet.extensions e2e-7vfj8-daemon-set took: 90.103622ms
Mar 16 10:40:25.472: INFO: Terminating DaemonSet.extensions e2e-7vfj8-daemon-set pods took: 100.136818ms
Mar 16 10:40:26.661: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
Mar 16 10:40:26.662: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7vfj8-daemon-set
Mar 16 10:40:26.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34664"},"items":null}

Mar 16 10:40:26.840: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34665"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:27.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4561" for this suite. 03/16/23 10:40:27.286
------------------------------
• [7.501 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:19.875
    Mar 16 10:40:19.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename controllerrevisions 03/16/23 10:40:19.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:20.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:20.32
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-7vfj8-daemon-set" 03/16/23 10:40:20.855
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:40:20.945
    Mar 16 10:40:21.123: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
    Mar 16 10:40:21.124: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:40:22.390: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
    Mar 16 10:40:22.390: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:40:23.390: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 2
    Mar 16 10:40:23.390: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-7vfj8-daemon-set
    STEP: Confirm DaemonSet "e2e-7vfj8-daemon-set" successfully created with "daemonset-name=e2e-7vfj8-daemon-set" label 03/16/23 10:40:23.479
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7vfj8-daemon-set" 03/16/23 10:40:23.658
    Mar 16 10:40:23.748: INFO: Located ControllerRevision: "e2e-7vfj8-daemon-set-6b74fdd896"
    STEP: Patching ControllerRevision "e2e-7vfj8-daemon-set-6b74fdd896" 03/16/23 10:40:23.837
    Mar 16 10:40:23.927: INFO: e2e-7vfj8-daemon-set-6b74fdd896 has been patched
    STEP: Create a new ControllerRevision 03/16/23 10:40:23.927
    Mar 16 10:40:24.017: INFO: Created ControllerRevision: e2e-7vfj8-daemon-set-745f5fb45d
    STEP: Confirm that there are two ControllerRevisions 03/16/23 10:40:24.018
    Mar 16 10:40:24.018: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 16 10:40:24.107: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-7vfj8-daemon-set-6b74fdd896" 03/16/23 10:40:24.107
    STEP: Confirm that there is only one ControllerRevision 03/16/23 10:40:24.197
    Mar 16 10:40:24.197: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 16 10:40:24.286: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-7vfj8-daemon-set-745f5fb45d" 03/16/23 10:40:24.375
    Mar 16 10:40:24.554: INFO: e2e-7vfj8-daemon-set-745f5fb45d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/16/23 10:40:24.554
    W0316 10:40:24.645236    8588 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/16/23 10:40:24.645
    Mar 16 10:40:24.645: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 16 10:40:24.734: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7vfj8-daemon-set-745f5fb45d=updated" 03/16/23 10:40:24.734
    STEP: Confirm that there is only one ControllerRevision 03/16/23 10:40:24.825
    Mar 16 10:40:24.825: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 16 10:40:24.914: INFO: Found 1 ControllerRevisions
    Mar 16 10:40:25.003: INFO: ControllerRevision "e2e-7vfj8-daemon-set-6f9d8545f9" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-7vfj8-daemon-set" 03/16/23 10:40:25.092
    STEP: deleting DaemonSet.extensions e2e-7vfj8-daemon-set in namespace controllerrevisions-4561, will wait for the garbage collector to delete the pods 03/16/23 10:40:25.092
    Mar 16 10:40:25.372: INFO: Deleting DaemonSet.extensions e2e-7vfj8-daemon-set took: 90.103622ms
    Mar 16 10:40:25.472: INFO: Terminating DaemonSet.extensions e2e-7vfj8-daemon-set pods took: 100.136818ms
    Mar 16 10:40:26.661: INFO: Number of nodes with available pods controlled by daemonset e2e-7vfj8-daemon-set: 0
    Mar 16 10:40:26.662: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7vfj8-daemon-set
    Mar 16 10:40:26.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34664"},"items":null}

    Mar 16 10:40:26.840: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34665"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:27.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4561" for this suite. 03/16/23 10:40:27.286
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:27.376
Mar 16 10:40:27.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:40:27.378
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:27.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:27.822
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 16 10:40:27.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:31.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2104" for this suite. 03/16/23 10:40:31.526
------------------------------
• [4.240 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:27.376
    Mar 16 10:40:27.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:40:27.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:27.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:27.822
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 16 10:40:27.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:31.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2104" for this suite. 03/16/23 10:40:31.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:31.618
Mar 16 10:40:31.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:40:31.619
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:31.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:32.063
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 03/16/23 10:40:32.239
Mar 16 10:40:32.239: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 16 10:40:32.239: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:33.400: INFO: stderr: ""
Mar 16 10:40:33.400: INFO: stdout: "service/agnhost-replica created\n"
Mar 16 10:40:33.400: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 16 10:40:33.401: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:34.555: INFO: stderr: ""
Mar 16 10:40:34.555: INFO: stdout: "service/agnhost-primary created\n"
Mar 16 10:40:34.556: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 16 10:40:34.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:35.148: INFO: stderr: ""
Mar 16 10:40:35.148: INFO: stdout: "service/frontend created\n"
Mar 16 10:40:35.148: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 16 10:40:35.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:35.725: INFO: stderr: ""
Mar 16 10:40:35.725: INFO: stdout: "deployment.apps/frontend created\n"
Mar 16 10:40:35.726: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 16 10:40:35.726: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:36.290: INFO: stderr: ""
Mar 16 10:40:36.290: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 16 10:40:36.290: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 16 10:40:36.290: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
Mar 16 10:40:36.859: INFO: stderr: ""
Mar 16 10:40:36.859: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/16/23 10:40:36.859
Mar 16 10:40:36.859: INFO: Waiting for all frontend pods to be Running.
Mar 16 10:40:41.963: INFO: Waiting for frontend to serve content.
Mar 16 10:40:42.170: INFO: Trying to add a new entry to the guestbook.
Mar 16 10:40:42.265: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/16/23 10:40:42.45
Mar 16 10:40:42.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:42.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:42.892: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/16/23 10:40:42.892
Mar 16 10:40:42.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:43.334: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:43.334: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/16/23 10:40:43.334
Mar 16 10:40:43.335: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:43.774: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:43.774: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/16/23 10:40:43.774
Mar 16 10:40:43.774: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:44.222: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:44.222: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/16/23 10:40:44.223
Mar 16 10:40:44.223: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:44.677: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:44.677: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/16/23 10:40:44.677
Mar 16 10:40:44.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
Mar 16 10:40:45.114: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 10:40:45.114: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:40:45.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3498" for this suite. 03/16/23 10:40:45.29
------------------------------
• [13.763 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:31.618
    Mar 16 10:40:31.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:40:31.619
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:31.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:32.063
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 03/16/23 10:40:32.239
    Mar 16 10:40:32.239: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 16 10:40:32.239: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:33.400: INFO: stderr: ""
    Mar 16 10:40:33.400: INFO: stdout: "service/agnhost-replica created\n"
    Mar 16 10:40:33.400: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 16 10:40:33.401: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:34.555: INFO: stderr: ""
    Mar 16 10:40:34.555: INFO: stdout: "service/agnhost-primary created\n"
    Mar 16 10:40:34.556: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 16 10:40:34.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:35.148: INFO: stderr: ""
    Mar 16 10:40:35.148: INFO: stdout: "service/frontend created\n"
    Mar 16 10:40:35.148: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 16 10:40:35.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:35.725: INFO: stderr: ""
    Mar 16 10:40:35.725: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 16 10:40:35.726: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 16 10:40:35.726: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:36.290: INFO: stderr: ""
    Mar 16 10:40:36.290: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 16 10:40:36.290: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 16 10:40:36.290: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 create -f -'
    Mar 16 10:40:36.859: INFO: stderr: ""
    Mar 16 10:40:36.859: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/16/23 10:40:36.859
    Mar 16 10:40:36.859: INFO: Waiting for all frontend pods to be Running.
    Mar 16 10:40:41.963: INFO: Waiting for frontend to serve content.
    Mar 16 10:40:42.170: INFO: Trying to add a new entry to the guestbook.
    Mar 16 10:40:42.265: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/16/23 10:40:42.45
    Mar 16 10:40:42.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:42.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:42.892: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/16/23 10:40:42.892
    Mar 16 10:40:42.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:43.334: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:43.334: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/16/23 10:40:43.334
    Mar 16 10:40:43.335: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:43.774: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:43.774: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/16/23 10:40:43.774
    Mar 16 10:40:43.774: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:44.222: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:44.222: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/16/23 10:40:44.223
    Mar 16 10:40:44.223: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:44.677: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:44.677: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/16/23 10:40:44.677
    Mar 16 10:40:44.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3498 delete --grace-period=0 --force -f -'
    Mar 16 10:40:45.114: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 10:40:45.114: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:40:45.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3498" for this suite. 03/16/23 10:40:45.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:40:45.381
Mar 16 10:40:45.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:40:45.382
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:45.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:45.827
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/16/23 10:40:46.003
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local;sleep 1; done
 03/16/23 10:40:46.093
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local;sleep 1; done
 03/16/23 10:40:46.093
STEP: creating a pod to probe DNS 03/16/23 10:40:46.093
STEP: submitting the pod to kubernetes 03/16/23 10:40:46.093
Mar 16 10:40:46.188: INFO: Waiting up to 15m0s for pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355" in namespace "dns-1850" to be "running"
Mar 16 10:40:46.277: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355": Phase="Pending", Reason="", readiness=false. Elapsed: 88.975134ms
Mar 16 10:40:48.367: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355": Phase="Running", Reason="", readiness=true. Elapsed: 2.179175588s
Mar 16 10:40:48.368: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355" satisfied condition "running"
STEP: retrieving the pod 03/16/23 10:40:48.368
STEP: looking for the results for each expected name from probers 03/16/23 10:40:48.457
Mar 16 10:40:48.642: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:48.778: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:48.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.010: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.102: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.194: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.287: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.379: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:49.380: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:40:54.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:54.565: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:54.656: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:54.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:54.840: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:54.931: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:55.023: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:55.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:55.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:40:59.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:59.564: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:59.655: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:59.747: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:59.838: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:40:59.930: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:00.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:00.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:00.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:41:04.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:04.564: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:04.656: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:04.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:04.839: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:04.931: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:05.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:05.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:05.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:41:09.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:09.565: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:09.657: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:09.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:09.840: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:09.937: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:10.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:10.121: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:10.121: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:41:14.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:14.566: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:14.657: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:14.749: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:14.841: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:14.932: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:15.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:15.116: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
Mar 16 10:41:15.116: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

Mar 16 10:41:20.113: INFO: DNS probes using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 succeeded

STEP: deleting the pod 03/16/23 10:41:20.113
STEP: deleting the test headless service 03/16/23 10:41:20.206
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:41:20.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1850" for this suite. 03/16/23 10:41:20.474
------------------------------
• [35.183 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:40:45.381
    Mar 16 10:40:45.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:40:45.382
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:40:45.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:40:45.827
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/16/23 10:40:46.003
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local;sleep 1; done
     03/16/23 10:40:46.093
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1850.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local;sleep 1; done
     03/16/23 10:40:46.093
    STEP: creating a pod to probe DNS 03/16/23 10:40:46.093
    STEP: submitting the pod to kubernetes 03/16/23 10:40:46.093
    Mar 16 10:40:46.188: INFO: Waiting up to 15m0s for pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355" in namespace "dns-1850" to be "running"
    Mar 16 10:40:46.277: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355": Phase="Pending", Reason="", readiness=false. Elapsed: 88.975134ms
    Mar 16 10:40:48.367: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355": Phase="Running", Reason="", readiness=true. Elapsed: 2.179175588s
    Mar 16 10:40:48.368: INFO: Pod "dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 10:40:48.368
    STEP: looking for the results for each expected name from probers 03/16/23 10:40:48.457
    Mar 16 10:40:48.642: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:48.778: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:48.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.010: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.102: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.194: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.287: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.379: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:49.380: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:40:54.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:54.565: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:54.656: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:54.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:54.840: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:54.931: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:55.023: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:55.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:55.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:40:59.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:59.564: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:59.655: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:59.747: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:59.838: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:40:59.930: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:00.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:00.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:00.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:41:04.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:04.564: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:04.656: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:04.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:04.839: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:04.931: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:05.022: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:05.114: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:05.114: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:41:09.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:09.565: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:09.657: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:09.748: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:09.840: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:09.937: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:10.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:10.121: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:10.121: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:41:14.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:14.566: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:14.657: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:14.749: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:14.841: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:14.932: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:15.024: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:15.116: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local from pod dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355: the server could not find the requested resource (get pods dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355)
    Mar 16 10:41:15.116: INFO: Lookups using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1850.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1850.svc.cluster.local jessie_udp@dns-test-service-2.dns-1850.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1850.svc.cluster.local]

    Mar 16 10:41:20.113: INFO: DNS probes using dns-1850/dns-test-0fbe0f40-2047-412b-ae38-28b3c946c355 succeeded

    STEP: deleting the pod 03/16/23 10:41:20.113
    STEP: deleting the test headless service 03/16/23 10:41:20.206
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:41:20.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1850" for this suite. 03/16/23 10:41:20.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:41:20.566
Mar 16 10:41:20.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:41:20.567
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:20.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:21.011
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/16/23 10:41:21.188
Mar 16 10:41:21.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:41:25.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:41:48.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3301" for this suite. 03/16/23 10:41:48.631
------------------------------
• [28.155 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:41:20.566
    Mar 16 10:41:20.566: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 10:41:20.567
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:20.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:21.011
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/16/23 10:41:21.188
    Mar 16 10:41:21.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:41:25.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:41:48.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3301" for this suite. 03/16/23 10:41:48.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:41:48.722
Mar 16 10:41:48.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:41:48.723
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:48.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:49.168
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-2e7afbb5-a62f-4494-a701-00d6887e409e 03/16/23 10:41:49.346
STEP: Creating a pod to test consume configMaps 03/16/23 10:41:49.436
Mar 16 10:41:49.531: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e" in namespace "projected-5121" to be "Succeeded or Failed"
Mar 16 10:41:49.621: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.475604ms
Mar 16 10:41:51.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179964953s
Mar 16 10:41:53.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179772516s
STEP: Saw pod success 03/16/23 10:41:53.711
Mar 16 10:41:53.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e" satisfied condition "Succeeded or Failed"
Mar 16 10:41:53.800: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:41:53.92
Mar 16 10:41:54.013: INFO: Waiting for pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e to disappear
Mar 16 10:41:54.103: INFO: Pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:41:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5121" for this suite. 03/16/23 10:41:54.28
------------------------------
• [5.649 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:41:48.722
    Mar 16 10:41:48.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:41:48.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:48.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:49.168
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-2e7afbb5-a62f-4494-a701-00d6887e409e 03/16/23 10:41:49.346
    STEP: Creating a pod to test consume configMaps 03/16/23 10:41:49.436
    Mar 16 10:41:49.531: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e" in namespace "projected-5121" to be "Succeeded or Failed"
    Mar 16 10:41:49.621: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.475604ms
    Mar 16 10:41:51.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179964953s
    Mar 16 10:41:53.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179772516s
    STEP: Saw pod success 03/16/23 10:41:53.711
    Mar 16 10:41:53.711: INFO: Pod "pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e" satisfied condition "Succeeded or Failed"
    Mar 16 10:41:53.800: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:41:53.92
    Mar 16 10:41:54.013: INFO: Waiting for pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e to disappear
    Mar 16 10:41:54.103: INFO: Pod pod-projected-configmaps-935e728f-a832-4f78-89c0-7ff97c1c924e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:41:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5121" for this suite. 03/16/23 10:41:54.28
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:41:54.371
Mar 16 10:41:54.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:41:54.373
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:54.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:54.819
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 03/16/23 10:41:54.996
STEP: Creating a ResourceQuota 03/16/23 10:42:00.087
STEP: Ensuring resource quota status is calculated 03/16/23 10:42:00.177
STEP: Creating a ReplicaSet 03/16/23 10:42:02.27
STEP: Ensuring resource quota status captures replicaset creation 03/16/23 10:42:02.364
STEP: Deleting a ReplicaSet 03/16/23 10:42:04.455
STEP: Ensuring resource quota status released usage 03/16/23 10:42:04.545
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:42:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5832" for this suite. 03/16/23 10:42:06.813
------------------------------
• [12.533 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:41:54.371
    Mar 16 10:41:54.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:41:54.373
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:41:54.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:41:54.819
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 03/16/23 10:41:54.996
    STEP: Creating a ResourceQuota 03/16/23 10:42:00.087
    STEP: Ensuring resource quota status is calculated 03/16/23 10:42:00.177
    STEP: Creating a ReplicaSet 03/16/23 10:42:02.27
    STEP: Ensuring resource quota status captures replicaset creation 03/16/23 10:42:02.364
    STEP: Deleting a ReplicaSet 03/16/23 10:42:04.455
    STEP: Ensuring resource quota status released usage 03/16/23 10:42:04.545
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:42:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5832" for this suite. 03/16/23 10:42:06.813
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:42:06.905
Mar 16 10:42:06.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:42:06.906
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:07.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:07.353
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 16 10:42:07.625: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8" in namespace "kubelet-test-5240" to be "running and ready"
Mar 16 10:42:07.717: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8": Phase="Pending", Reason="", readiness=false. Elapsed: 91.813042ms
Mar 16 10:42:07.717: INFO: The phase of Pod busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:42:09.807: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.182215491s
Mar 16 10:42:09.807: INFO: The phase of Pod busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8 is Running (Ready = true)
Mar 16 10:42:09.807: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:42:09.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5240" for this suite. 03/16/23 10:42:10.169
------------------------------
• [3.355 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:42:06.905
    Mar 16 10:42:06.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:42:06.906
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:07.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:07.353
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 16 10:42:07.625: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8" in namespace "kubelet-test-5240" to be "running and ready"
    Mar 16 10:42:07.717: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8": Phase="Pending", Reason="", readiness=false. Elapsed: 91.813042ms
    Mar 16 10:42:07.717: INFO: The phase of Pod busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:42:09.807: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.182215491s
    Mar 16 10:42:09.807: INFO: The phase of Pod busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8 is Running (Ready = true)
    Mar 16 10:42:09.807: INFO: Pod "busybox-readonly-fs8f6231ca-8a0d-4685-b856-b2b371c107a8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:42:09.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5240" for this suite. 03/16/23 10:42:10.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:42:10.26
Mar 16 10:42:10.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:42:10.261
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:10.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:10.706
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-37d8d061-76d3-4bf4-ba94-3debd9544221 03/16/23 10:42:10.884
STEP: Creating a pod to test consume secrets 03/16/23 10:42:10.974
Mar 16 10:42:11.068: INFO: Waiting up to 5m0s for pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597" in namespace "secrets-2832" to be "Succeeded or Failed"
Mar 16 10:42:11.158: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Pending", Reason="", readiness=false. Elapsed: 89.552886ms
Mar 16 10:42:13.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180623522s
Mar 16 10:42:15.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181150396s
STEP: Saw pod success 03/16/23 10:42:15.249
Mar 16 10:42:15.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597" satisfied condition "Succeeded or Failed"
Mar 16 10:42:15.339: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:42:15.433
Mar 16 10:42:15.526: INFO: Waiting for pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 to disappear
Mar 16 10:42:15.615: INFO: Pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:42:15.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2832" for this suite. 03/16/23 10:42:15.793
------------------------------
• [5.624 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:42:10.26
    Mar 16 10:42:10.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:42:10.261
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:10.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:10.706
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-37d8d061-76d3-4bf4-ba94-3debd9544221 03/16/23 10:42:10.884
    STEP: Creating a pod to test consume secrets 03/16/23 10:42:10.974
    Mar 16 10:42:11.068: INFO: Waiting up to 5m0s for pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597" in namespace "secrets-2832" to be "Succeeded or Failed"
    Mar 16 10:42:11.158: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Pending", Reason="", readiness=false. Elapsed: 89.552886ms
    Mar 16 10:42:13.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180623522s
    Mar 16 10:42:15.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181150396s
    STEP: Saw pod success 03/16/23 10:42:15.249
    Mar 16 10:42:15.249: INFO: Pod "pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597" satisfied condition "Succeeded or Failed"
    Mar 16 10:42:15.339: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:42:15.433
    Mar 16 10:42:15.526: INFO: Waiting for pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 to disappear
    Mar 16 10:42:15.615: INFO: Pod pod-secrets-fb55ac95-3e62-4cdd-8e00-e30f28739597 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:42:15.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2832" for this suite. 03/16/23 10:42:15.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:42:15.884
Mar 16 10:42:15.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 10:42:15.886
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:16.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:16.334
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 03/16/23 10:42:16.516
STEP: Ensuring job reaches completions 03/16/23 10:42:16.607
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 10:42:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4886" for this suite. 03/16/23 10:42:28.875
------------------------------
• [13.081 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:42:15.884
    Mar 16 10:42:15.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 10:42:15.886
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:16.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:16.334
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 03/16/23 10:42:16.516
    STEP: Ensuring job reaches completions 03/16/23 10:42:16.607
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:42:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4886" for this suite. 03/16/23 10:42:28.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:42:28.966
Mar 16 10:42:28.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath 03/16/23 10:42:28.967
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:29.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:29.414
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/16/23 10:42:29.591
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-w98c 03/16/23 10:42:29.771
STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:42:29.771
Mar 16 10:42:29.866: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w98c" in namespace "subpath-1875" to be "Succeeded or Failed"
Mar 16 10:42:29.955: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.271444ms
Mar 16 10:42:32.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 2.179842788s
Mar 16 10:42:34.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 4.179884602s
Mar 16 10:42:36.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 6.18048679s
Mar 16 10:42:38.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 8.179674724s
Mar 16 10:42:40.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 10.180192909s
Mar 16 10:42:42.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 12.180218086s
Mar 16 10:42:44.047: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 14.180644621s
Mar 16 10:42:46.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 16.179990425s
Mar 16 10:42:48.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 18.179890604s
Mar 16 10:42:50.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 20.180359585s
Mar 16 10:42:52.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=false. Elapsed: 22.179768696s
Mar 16 10:42:54.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180160911s
STEP: Saw pod success 03/16/23 10:42:54.046
Mar 16 10:42:54.046: INFO: Pod "pod-subpath-test-configmap-w98c" satisfied condition "Succeeded or Failed"
Mar 16 10:42:54.136: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-configmap-w98c container test-container-subpath-configmap-w98c: <nil>
STEP: delete the pod 03/16/23 10:42:54.235
Mar 16 10:42:54.329: INFO: Waiting for pod pod-subpath-test-configmap-w98c to disappear
Mar 16 10:42:54.418: INFO: Pod pod-subpath-test-configmap-w98c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w98c 03/16/23 10:42:54.419
Mar 16 10:42:54.419: INFO: Deleting pod "pod-subpath-test-configmap-w98c" in namespace "subpath-1875"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 16 10:42:54.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1875" for this suite. 03/16/23 10:42:54.686
------------------------------
• [25.810 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:42:28.966
    Mar 16 10:42:28.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename subpath 03/16/23 10:42:28.967
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:29.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:29.414
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/16/23 10:42:29.591
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-w98c 03/16/23 10:42:29.771
    STEP: Creating a pod to test atomic-volume-subpath 03/16/23 10:42:29.771
    Mar 16 10:42:29.866: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w98c" in namespace "subpath-1875" to be "Succeeded or Failed"
    Mar 16 10:42:29.955: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.271444ms
    Mar 16 10:42:32.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 2.179842788s
    Mar 16 10:42:34.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 4.179884602s
    Mar 16 10:42:36.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 6.18048679s
    Mar 16 10:42:38.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 8.179674724s
    Mar 16 10:42:40.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 10.180192909s
    Mar 16 10:42:42.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 12.180218086s
    Mar 16 10:42:44.047: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 14.180644621s
    Mar 16 10:42:46.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 16.179990425s
    Mar 16 10:42:48.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 18.179890604s
    Mar 16 10:42:50.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=true. Elapsed: 20.180359585s
    Mar 16 10:42:52.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Running", Reason="", readiness=false. Elapsed: 22.179768696s
    Mar 16 10:42:54.046: INFO: Pod "pod-subpath-test-configmap-w98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.180160911s
    STEP: Saw pod success 03/16/23 10:42:54.046
    Mar 16 10:42:54.046: INFO: Pod "pod-subpath-test-configmap-w98c" satisfied condition "Succeeded or Failed"
    Mar 16 10:42:54.136: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-subpath-test-configmap-w98c container test-container-subpath-configmap-w98c: <nil>
    STEP: delete the pod 03/16/23 10:42:54.235
    Mar 16 10:42:54.329: INFO: Waiting for pod pod-subpath-test-configmap-w98c to disappear
    Mar 16 10:42:54.418: INFO: Pod pod-subpath-test-configmap-w98c no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-w98c 03/16/23 10:42:54.419
    Mar 16 10:42:54.419: INFO: Deleting pod "pod-subpath-test-configmap-w98c" in namespace "subpath-1875"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:42:54.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1875" for this suite. 03/16/23 10:42:54.686
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:42:54.777
Mar 16 10:42:54.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 10:42:54.778
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:55.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:55.225
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:42:55.858
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:42:55.949
Mar 16 10:42:56.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:42:56.129: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:42:57.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:42:57.397: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 03/16/23 10:42:57.486
Mar 16 10:42:57.576: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/16/23 10:42:57.576
Mar 16 10:42:57.756: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/16/23 10:42:57.756
Mar 16 10:42:57.845: INFO: Observed &DaemonSet event: ADDED
Mar 16 10:42:57.845: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:57.846: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:57.846: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:57.846: INFO: Found daemon set daemon-set in namespace daemonsets-2604 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 16 10:42:57.846: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/16/23 10:42:57.846
STEP: watching for the daemon set status to be patched 03/16/23 10:42:57.938
Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: ADDED
Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:58.027: INFO: Observed daemon set daemon-set in namespace daemonsets-2604 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 16 10:42:58.028: INFO: Observed &DaemonSet event: MODIFIED
Mar 16 10:42:58.028: INFO: Found daemon set daemon-set in namespace daemonsets-2604 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 16 10:42:58.028: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:42:58.117
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2604, will wait for the garbage collector to delete the pods 03/16/23 10:42:58.117
Mar 16 10:42:58.397: INFO: Deleting DaemonSet.extensions daemon-set took: 90.666335ms
Mar 16 10:42:58.498: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.551178ms
Mar 16 10:43:00.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:43:00.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 10:43:00.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36032"},"items":null}

Mar 16 10:43:00.366: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36032"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:43:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2604" for this suite. 03/16/23 10:43:00.813
------------------------------
• [6.127 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:42:54.777
    Mar 16 10:42:54.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 10:42:54.778
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:42:55.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:42:55.225
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:42:55.858
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:42:55.949
    Mar 16 10:42:56.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:42:56.129: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:42:57.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:42:57.397: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 03/16/23 10:42:57.486
    Mar 16 10:42:57.576: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/16/23 10:42:57.576
    Mar 16 10:42:57.756: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/16/23 10:42:57.756
    Mar 16 10:42:57.845: INFO: Observed &DaemonSet event: ADDED
    Mar 16 10:42:57.845: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:57.846: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:57.846: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:57.846: INFO: Found daemon set daemon-set in namespace daemonsets-2604 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 16 10:42:57.846: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/16/23 10:42:57.846
    STEP: watching for the daemon set status to be patched 03/16/23 10:42:57.938
    Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: ADDED
    Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:58.027: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:58.027: INFO: Observed daemon set daemon-set in namespace daemonsets-2604 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 16 10:42:58.028: INFO: Observed &DaemonSet event: MODIFIED
    Mar 16 10:42:58.028: INFO: Found daemon set daemon-set in namespace daemonsets-2604 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 16 10:42:58.028: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:42:58.117
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2604, will wait for the garbage collector to delete the pods 03/16/23 10:42:58.117
    Mar 16 10:42:58.397: INFO: Deleting DaemonSet.extensions daemon-set took: 90.666335ms
    Mar 16 10:42:58.498: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.551178ms
    Mar 16 10:43:00.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:43:00.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 10:43:00.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36032"},"items":null}

    Mar 16 10:43:00.366: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36032"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:43:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2604" for this suite. 03/16/23 10:43:00.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:43:00.905
Mar 16 10:43:00.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 10:43:00.906
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:43:01.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:43:01.352
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 in namespace container-probe-6028 03/16/23 10:43:01.529
Mar 16 10:43:01.624: INFO: Waiting up to 5m0s for pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941" in namespace "container-probe-6028" to be "not pending"
Mar 16 10:43:01.713: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941": Phase="Pending", Reason="", readiness=false. Elapsed: 89.425165ms
Mar 16 10:43:03.804: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941": Phase="Running", Reason="", readiness=true. Elapsed: 2.180398119s
Mar 16 10:43:03.805: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941" satisfied condition "not pending"
Mar 16 10:43:03.805: INFO: Started pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 in namespace container-probe-6028
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:43:03.805
Mar 16 10:43:03.894: INFO: Initial restart count of pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is 0
Mar 16 10:43:22.804: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 1 (18.909689516s elapsed)
Mar 16 10:43:43.710: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 2 (39.815446699s elapsed)
Mar 16 10:44:02.523: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 3 (58.628710919s elapsed)
Mar 16 10:44:23.426: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 4 (1m19.532220163s elapsed)
Mar 16 10:45:32.412: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 5 (2m28.518286091s elapsed)
STEP: deleting the pod 03/16/23 10:45:32.412
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 10:45:32.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6028" for this suite. 03/16/23 10:45:32.683
------------------------------
• [151.869 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:43:00.905
    Mar 16 10:43:00.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 10:43:00.906
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:43:01.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:43:01.352
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 in namespace container-probe-6028 03/16/23 10:43:01.529
    Mar 16 10:43:01.624: INFO: Waiting up to 5m0s for pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941" in namespace "container-probe-6028" to be "not pending"
    Mar 16 10:43:01.713: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941": Phase="Pending", Reason="", readiness=false. Elapsed: 89.425165ms
    Mar 16 10:43:03.804: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941": Phase="Running", Reason="", readiness=true. Elapsed: 2.180398119s
    Mar 16 10:43:03.805: INFO: Pod "liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941" satisfied condition "not pending"
    Mar 16 10:43:03.805: INFO: Started pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 in namespace container-probe-6028
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:43:03.805
    Mar 16 10:43:03.894: INFO: Initial restart count of pod liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is 0
    Mar 16 10:43:22.804: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 1 (18.909689516s elapsed)
    Mar 16 10:43:43.710: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 2 (39.815446699s elapsed)
    Mar 16 10:44:02.523: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 3 (58.628710919s elapsed)
    Mar 16 10:44:23.426: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 4 (1m19.532220163s elapsed)
    Mar 16 10:45:32.412: INFO: Restart count of pod container-probe-6028/liveness-efb3b14c-db7f-4a6d-8dd9-1f83439e0941 is now 5 (2m28.518286091s elapsed)
    STEP: deleting the pod 03/16/23 10:45:32.412
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:45:32.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6028" for this suite. 03/16/23 10:45:32.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:45:32.775
Mar 16 10:45:32.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 10:45:32.776
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:33.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:33.222
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 03/16/23 10:45:33.4
Mar 16 10:45:33.504: INFO: Waiting up to 5m0s for pod "pod-26d94b80-f157-4086-b773-e2e028af795c" in namespace "emptydir-4128" to be "Succeeded or Failed"
Mar 16 10:45:33.594: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.825635ms
Mar 16 10:45:35.690: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186255913s
Mar 16 10:45:37.684: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179987774s
STEP: Saw pod success 03/16/23 10:45:37.684
Mar 16 10:45:37.684: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c" satisfied condition "Succeeded or Failed"
Mar 16 10:45:37.773: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-26d94b80-f157-4086-b773-e2e028af795c container test-container: <nil>
STEP: delete the pod 03/16/23 10:45:37.884
Mar 16 10:45:37.979: INFO: Waiting for pod pod-26d94b80-f157-4086-b773-e2e028af795c to disappear
Mar 16 10:45:38.068: INFO: Pod pod-26d94b80-f157-4086-b773-e2e028af795c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 10:45:38.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4128" for this suite. 03/16/23 10:45:38.246
------------------------------
• [5.562 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:45:32.775
    Mar 16 10:45:32.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 10:45:32.776
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:33.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:33.222
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/16/23 10:45:33.4
    Mar 16 10:45:33.504: INFO: Waiting up to 5m0s for pod "pod-26d94b80-f157-4086-b773-e2e028af795c" in namespace "emptydir-4128" to be "Succeeded or Failed"
    Mar 16 10:45:33.594: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.825635ms
    Mar 16 10:45:35.690: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186255913s
    Mar 16 10:45:37.684: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179987774s
    STEP: Saw pod success 03/16/23 10:45:37.684
    Mar 16 10:45:37.684: INFO: Pod "pod-26d94b80-f157-4086-b773-e2e028af795c" satisfied condition "Succeeded or Failed"
    Mar 16 10:45:37.773: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-26d94b80-f157-4086-b773-e2e028af795c container test-container: <nil>
    STEP: delete the pod 03/16/23 10:45:37.884
    Mar 16 10:45:37.979: INFO: Waiting for pod pod-26d94b80-f157-4086-b773-e2e028af795c to disappear
    Mar 16 10:45:38.068: INFO: Pod pod-26d94b80-f157-4086-b773-e2e028af795c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:45:38.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4128" for this suite. 03/16/23 10:45:38.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:45:38.337
Mar 16 10:45:38.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:45:38.338
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:38.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:38.784
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-d823d809-a9fe-42b8-898f-96b393a2d0fc 03/16/23 10:45:38.962
STEP: Creating a pod to test consume configMaps 03/16/23 10:45:39.052
Mar 16 10:45:39.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb" in namespace "projected-6518" to be "Succeeded or Failed"
Mar 16 10:45:39.237: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.952246ms
Mar 16 10:45:41.327: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179841772s
Mar 16 10:45:43.329: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181913723s
STEP: Saw pod success 03/16/23 10:45:43.329
Mar 16 10:45:43.329: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb" satisfied condition "Succeeded or Failed"
Mar 16 10:45:43.419: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:45:43.514
Mar 16 10:45:43.607: INFO: Waiting for pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb to disappear
Mar 16 10:45:43.696: INFO: Pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:45:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6518" for this suite. 03/16/23 10:45:43.874
------------------------------
• [5.627 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:45:38.337
    Mar 16 10:45:38.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:45:38.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:38.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:38.784
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-d823d809-a9fe-42b8-898f-96b393a2d0fc 03/16/23 10:45:38.962
    STEP: Creating a pod to test consume configMaps 03/16/23 10:45:39.052
    Mar 16 10:45:39.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb" in namespace "projected-6518" to be "Succeeded or Failed"
    Mar 16 10:45:39.237: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.952246ms
    Mar 16 10:45:41.327: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179841772s
    Mar 16 10:45:43.329: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181913723s
    STEP: Saw pod success 03/16/23 10:45:43.329
    Mar 16 10:45:43.329: INFO: Pod "pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb" satisfied condition "Succeeded or Failed"
    Mar 16 10:45:43.419: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:45:43.514
    Mar 16 10:45:43.607: INFO: Waiting for pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb to disappear
    Mar 16 10:45:43.696: INFO: Pod pod-projected-configmaps-94e7d0a2-3b90-46b9-ac32-bb06faf77acb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:45:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6518" for this suite. 03/16/23 10:45:43.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:45:43.965
Mar 16 10:45:43.965: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption 03/16/23 10:45:43.966
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:44.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:44.413
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 03/16/23 10:45:44.591
STEP: Waiting for the pdb to be processed 03/16/23 10:45:44.68
STEP: First trying to evict a pod which shouldn't be evictable 03/16/23 10:45:44.864
STEP: Waiting for all pods to be running 03/16/23 10:45:44.865
Mar 16 10:45:44.954: INFO: running pods: 0 < 3
STEP: locating a running pod 03/16/23 10:45:47.045
STEP: Updating the pdb to allow a pod to be evicted 03/16/23 10:45:47.225
STEP: Waiting for the pdb to be processed 03/16/23 10:45:47.405
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/16/23 10:45:47.495
STEP: Waiting for all pods to be running 03/16/23 10:45:47.495
STEP: Waiting for the pdb to observed all healthy pods 03/16/23 10:45:47.585
STEP: Patching the pdb to disallow a pod to be evicted 03/16/23 10:45:47.774
STEP: Waiting for the pdb to be processed 03/16/23 10:45:47.955
STEP: Waiting for all pods to be running 03/16/23 10:45:48.044
Mar 16 10:45:48.134: INFO: running pods: 2 < 3
STEP: locating a running pod 03/16/23 10:45:50.225
STEP: Deleting the pdb to allow a pod to be evicted 03/16/23 10:45:50.406
STEP: Waiting for the pdb to be deleted 03/16/23 10:45:50.497
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/16/23 10:45:50.586
STEP: Waiting for all pods to be running 03/16/23 10:45:50.586
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:45:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5763" for this suite. 03/16/23 10:45:50.951
------------------------------
• [7.076 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:45:43.965
    Mar 16 10:45:43.965: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename disruption 03/16/23 10:45:43.966
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:44.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:44.413
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 03/16/23 10:45:44.591
    STEP: Waiting for the pdb to be processed 03/16/23 10:45:44.68
    STEP: First trying to evict a pod which shouldn't be evictable 03/16/23 10:45:44.864
    STEP: Waiting for all pods to be running 03/16/23 10:45:44.865
    Mar 16 10:45:44.954: INFO: running pods: 0 < 3
    STEP: locating a running pod 03/16/23 10:45:47.045
    STEP: Updating the pdb to allow a pod to be evicted 03/16/23 10:45:47.225
    STEP: Waiting for the pdb to be processed 03/16/23 10:45:47.405
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/16/23 10:45:47.495
    STEP: Waiting for all pods to be running 03/16/23 10:45:47.495
    STEP: Waiting for the pdb to observed all healthy pods 03/16/23 10:45:47.585
    STEP: Patching the pdb to disallow a pod to be evicted 03/16/23 10:45:47.774
    STEP: Waiting for the pdb to be processed 03/16/23 10:45:47.955
    STEP: Waiting for all pods to be running 03/16/23 10:45:48.044
    Mar 16 10:45:48.134: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/16/23 10:45:50.225
    STEP: Deleting the pdb to allow a pod to be evicted 03/16/23 10:45:50.406
    STEP: Waiting for the pdb to be deleted 03/16/23 10:45:50.497
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/16/23 10:45:50.586
    STEP: Waiting for all pods to be running 03/16/23 10:45:50.586
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:45:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5763" for this suite. 03/16/23 10:45:50.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:45:51.042
Mar 16 10:45:51.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:45:51.043
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:51.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:51.491
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-3c2c0c2d-401b-42dc-9216-fef4c2a35b43 03/16/23 10:45:51.669
STEP: Creating a pod to test consume secrets 03/16/23 10:45:51.759
Mar 16 10:45:51.853: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33" in namespace "projected-2657" to be "Succeeded or Failed"
Mar 16 10:45:51.943: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Pending", Reason="", readiness=false. Elapsed: 89.527956ms
Mar 16 10:45:54.046: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.192932679s
Mar 16 10:45:56.033: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180250569s
STEP: Saw pod success 03/16/23 10:45:56.033
Mar 16 10:45:56.033: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33" satisfied condition "Succeeded or Failed"
Mar 16 10:45:56.122: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:45:56.216
Mar 16 10:45:56.309: INFO: Waiting for pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 to disappear
Mar 16 10:45:56.400: INFO: Pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 10:45:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2657" for this suite. 03/16/23 10:45:56.577
------------------------------
• [5.626 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:45:51.042
    Mar 16 10:45:51.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:45:51.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:51.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:51.491
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-3c2c0c2d-401b-42dc-9216-fef4c2a35b43 03/16/23 10:45:51.669
    STEP: Creating a pod to test consume secrets 03/16/23 10:45:51.759
    Mar 16 10:45:51.853: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33" in namespace "projected-2657" to be "Succeeded or Failed"
    Mar 16 10:45:51.943: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Pending", Reason="", readiness=false. Elapsed: 89.527956ms
    Mar 16 10:45:54.046: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.192932679s
    Mar 16 10:45:56.033: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180250569s
    STEP: Saw pod success 03/16/23 10:45:56.033
    Mar 16 10:45:56.033: INFO: Pod "pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33" satisfied condition "Succeeded or Failed"
    Mar 16 10:45:56.122: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:45:56.216
    Mar 16 10:45:56.309: INFO: Waiting for pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 to disappear
    Mar 16 10:45:56.400: INFO: Pod pod-projected-secrets-40949e37-4b93-47ef-a137-2151def1ff33 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:45:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2657" for this suite. 03/16/23 10:45:56.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:45:56.668
Mar 16 10:45:56.668: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:45:56.669
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:56.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:57.115
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-2a92b4ff-ba81-4a25-9f65-9f6c7b6795ba 03/16/23 10:45:57.293
STEP: Creating a pod to test consume configMaps 03/16/23 10:45:57.382
Mar 16 10:45:57.477: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5" in namespace "projected-6822" to be "Succeeded or Failed"
Mar 16 10:45:57.566: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.232214ms
Mar 16 10:45:59.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180209284s
Mar 16 10:46:01.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180207494s
STEP: Saw pod success 03/16/23 10:46:01.657
Mar 16 10:46:01.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5" satisfied condition "Succeeded or Failed"
Mar 16 10:46:01.746: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:46:01.84
Mar 16 10:46:01.934: INFO: Waiting for pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 to disappear
Mar 16 10:46:02.023: INFO: Pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:02.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6822" for this suite. 03/16/23 10:46:02.201
------------------------------
• [5.624 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:45:56.668
    Mar 16 10:45:56.668: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:45:56.669
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:45:56.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:45:57.115
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-2a92b4ff-ba81-4a25-9f65-9f6c7b6795ba 03/16/23 10:45:57.293
    STEP: Creating a pod to test consume configMaps 03/16/23 10:45:57.382
    Mar 16 10:45:57.477: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5" in namespace "projected-6822" to be "Succeeded or Failed"
    Mar 16 10:45:57.566: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.232214ms
    Mar 16 10:45:59.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180209284s
    Mar 16 10:46:01.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180207494s
    STEP: Saw pod success 03/16/23 10:46:01.657
    Mar 16 10:46:01.657: INFO: Pod "pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5" satisfied condition "Succeeded or Failed"
    Mar 16 10:46:01.746: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:46:01.84
    Mar 16 10:46:01.934: INFO: Waiting for pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 to disappear
    Mar 16 10:46:02.023: INFO: Pod pod-projected-configmaps-d77403fe-2c3a-43eb-99fb-6efa536c34f5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:02.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6822" for this suite. 03/16/23 10:46:02.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:02.292
Mar 16 10:46:02.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:46:02.294
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:02.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:02.74
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  03/16/23 10:46:02.917
Mar 16 10:46:03.011: INFO: Waiting up to 5m0s for pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12" in namespace "svcaccounts-1383" to be "Succeeded or Failed"
Mar 16 10:46:03.101: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 89.477701ms
Mar 16 10:46:05.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18005284s
Mar 16 10:46:07.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179663484s
STEP: Saw pod success 03/16/23 10:46:07.191
Mar 16 10:46:07.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12" satisfied condition "Succeeded or Failed"
Mar 16 10:46:07.281: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:46:07.375
Mar 16 10:46:07.468: INFO: Waiting for pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 to disappear
Mar 16 10:46:07.557: INFO: Pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:07.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1383" for this suite. 03/16/23 10:46:07.735
------------------------------
• [5.532 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:02.292
    Mar 16 10:46:02.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:46:02.294
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:02.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:02.74
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  03/16/23 10:46:02.917
    Mar 16 10:46:03.011: INFO: Waiting up to 5m0s for pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12" in namespace "svcaccounts-1383" to be "Succeeded or Failed"
    Mar 16 10:46:03.101: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 89.477701ms
    Mar 16 10:46:05.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18005284s
    Mar 16 10:46:07.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179663484s
    STEP: Saw pod success 03/16/23 10:46:07.191
    Mar 16 10:46:07.191: INFO: Pod "test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12" satisfied condition "Succeeded or Failed"
    Mar 16 10:46:07.281: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:46:07.375
    Mar 16 10:46:07.468: INFO: Waiting for pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 to disappear
    Mar 16 10:46:07.557: INFO: Pod test-pod-b598de4c-e4d9-4e1b-8c7b-07d19924ac12 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:07.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1383" for this suite. 03/16/23 10:46:07.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:07.825
Mar 16 10:46:07.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport 03/16/23 10:46:07.826
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:08.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:08.273
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/16/23 10:46:08.541
Mar 16 10:46:08.636: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9250" to be "running and ready"
Mar 16 10:46:08.726: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.778865ms
Mar 16 10:46:08.726: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:46:10.816: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.180508992s
Mar 16 10:46:10.816: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 16 10:46:10.816: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.19.136 on the node which pod1 resides and expect scheduled 03/16/23 10:46:10.816
Mar 16 10:46:10.910: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9250" to be "running and ready"
Mar 16 10:46:11.000: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.520139ms
Mar 16 10:46:11.000: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:46:13.090: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.180237729s
Mar 16 10:46:13.090: INFO: The phase of Pod pod2 is Running (Ready = false)
Mar 16 10:46:15.090: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.180417552s
Mar 16 10:46:15.091: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 16 10:46:15.091: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.19.136 but use UDP protocol on the node which pod2 resides 03/16/23 10:46:15.091
Mar 16 10:46:15.183: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9250" to be "running and ready"
Mar 16 10:46:15.273: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 89.543302ms
Mar 16 10:46:15.273: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:46:17.363: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.179465279s
Mar 16 10:46:17.363: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 16 10:46:17.363: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 16 10:46:17.456: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9250" to be "running and ready"
Mar 16 10:46:17.545: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 89.468656ms
Mar 16 10:46:17.545: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:46:19.637: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.181339164s
Mar 16 10:46:19.637: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 16 10:46:19.637: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/16/23 10:46:19.728
Mar 16 10:46:19.728: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.19.136 http://127.0.0.1:54323/hostname] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:46:19.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:46:19.729: INFO: ExecWithOptions: Clientset creation
Mar 16 10:46:19.729: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.19.136+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.19.136, port: 54323 03/16/23 10:46:20.489
Mar 16 10:46:20.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.19.136:54323/hostname] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:46:20.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:46:20.491: INFO: ExecWithOptions: Clientset creation
Mar 16 10:46:20.491: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.19.136%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.19.136, port: 54323 UDP 03/16/23 10:46:21.266
Mar 16 10:46:21.266: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.19.136 54323] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:46:21.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:46:21.267: INFO: ExecWithOptions: Clientset creation
Mar 16 10:46:21.267: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.19.136+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:26.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9250" for this suite. 03/16/23 10:46:27.137
------------------------------
• [19.403 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:07.825
    Mar 16 10:46:07.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename hostport 03/16/23 10:46:07.826
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:08.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:08.273
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/16/23 10:46:08.541
    Mar 16 10:46:08.636: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9250" to be "running and ready"
    Mar 16 10:46:08.726: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.778865ms
    Mar 16 10:46:08.726: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:46:10.816: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.180508992s
    Mar 16 10:46:10.816: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 16 10:46:10.816: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.19.136 on the node which pod1 resides and expect scheduled 03/16/23 10:46:10.816
    Mar 16 10:46:10.910: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9250" to be "running and ready"
    Mar 16 10:46:11.000: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.520139ms
    Mar 16 10:46:11.000: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:46:13.090: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.180237729s
    Mar 16 10:46:13.090: INFO: The phase of Pod pod2 is Running (Ready = false)
    Mar 16 10:46:15.090: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.180417552s
    Mar 16 10:46:15.091: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 16 10:46:15.091: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.19.136 but use UDP protocol on the node which pod2 resides 03/16/23 10:46:15.091
    Mar 16 10:46:15.183: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9250" to be "running and ready"
    Mar 16 10:46:15.273: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 89.543302ms
    Mar 16 10:46:15.273: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:46:17.363: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.179465279s
    Mar 16 10:46:17.363: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 16 10:46:17.363: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 16 10:46:17.456: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9250" to be "running and ready"
    Mar 16 10:46:17.545: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 89.468656ms
    Mar 16 10:46:17.545: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:46:19.637: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.181339164s
    Mar 16 10:46:19.637: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 16 10:46:19.637: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/16/23 10:46:19.728
    Mar 16 10:46:19.728: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.19.136 http://127.0.0.1:54323/hostname] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:46:19.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:46:19.729: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:46:19.729: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.19.136+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.19.136, port: 54323 03/16/23 10:46:20.489
    Mar 16 10:46:20.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.19.136:54323/hostname] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:46:20.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:46:20.491: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:46:20.491: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.19.136%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.19.136, port: 54323 UDP 03/16/23 10:46:21.266
    Mar 16 10:46:21.266: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.19.136 54323] Namespace:hostport-9250 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:46:21.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:46:21.267: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:46:21.267: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-9250/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.19.136+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:26.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9250" for this suite. 03/16/23 10:46:27.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:27.229
Mar 16 10:46:27.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 10:46:27.23
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:27.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:27.676
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 10:46:28.034
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:46:28.384
STEP: Deploying the webhook pod 03/16/23 10:46:28.475
STEP: Wait for the deployment to be ready 03/16/23 10:46:28.655
Mar 16 10:46:28.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:46:31.016
STEP: Verifying the service has paired with the endpoint 03/16/23 10:46:31.11
Mar 16 10:46:32.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Mar 16 10:46:32.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-924-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 10:46:32.379
STEP: Creating a custom resource that should be mutated by the webhook 03/16/23 10:46:32.671
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:35.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2776" for this suite. 03/16/23 10:46:35.755
STEP: Destroying namespace "webhook-2776-markers" for this suite. 03/16/23 10:46:35.845
------------------------------
• [8.707 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:27.229
    Mar 16 10:46:27.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 10:46:27.23
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:27.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:27.676
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 10:46:28.034
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 10:46:28.384
    STEP: Deploying the webhook pod 03/16/23 10:46:28.475
    STEP: Wait for the deployment to be ready 03/16/23 10:46:28.655
    Mar 16 10:46:28.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:46:31.016
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:46:31.11
    Mar 16 10:46:32.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Mar 16 10:46:32.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-924-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 10:46:32.379
    STEP: Creating a custom resource that should be mutated by the webhook 03/16/23 10:46:32.671
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:35.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2776" for this suite. 03/16/23 10:46:35.755
    STEP: Destroying namespace "webhook-2776-markers" for this suite. 03/16/23 10:46:35.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:35.936
Mar 16 10:46:35.936: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 03/16/23 10:46:35.937
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:36.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:36.384
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/16/23 10:46:36.561
STEP: starting a background goroutine to produce watch events 03/16/23 10:46:36.651
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/16/23 10:46:36.651
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:45.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9286" for this suite. 03/16/23 10:46:45.915
------------------------------
• [10.069 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:35.936
    Mar 16 10:46:35.936: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 03/16/23 10:46:35.937
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:36.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:36.384
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/16/23 10:46:36.561
    STEP: starting a background goroutine to produce watch events 03/16/23 10:46:36.651
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/16/23 10:46:36.651
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:45.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9286" for this suite. 03/16/23 10:46:45.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:46.007
Mar 16 10:46:46.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:46:46.008
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:46.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:46.454
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1860.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/16/23 10:46:46.631
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1860.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/16/23 10:46:46.631
STEP: creating a pod to probe /etc/hosts 03/16/23 10:46:46.632
STEP: submitting the pod to kubernetes 03/16/23 10:46:46.632
Mar 16 10:46:46.726: INFO: Waiting up to 15m0s for pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e" in namespace "dns-1860" to be "running"
Mar 16 10:46:46.816: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.705033ms
Mar 16 10:46:48.907: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e": Phase="Running", Reason="", readiness=true. Elapsed: 2.18015763s
Mar 16 10:46:48.907: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e" satisfied condition "running"
STEP: retrieving the pod 03/16/23 10:46:48.907
STEP: looking for the results for each expected name from probers 03/16/23 10:46:48.996
Mar 16 10:46:49.551: INFO: DNS probes using dns-1860/dns-test-63354880-b324-4b83-8582-8d9b8b33044e succeeded

STEP: deleting the pod 03/16/23 10:46:49.551
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:49.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1860" for this suite. 03/16/23 10:46:49.822
------------------------------
• [3.906 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:46.007
    Mar 16 10:46:46.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:46:46.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:46.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:46.454
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1860.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/16/23 10:46:46.631
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1860.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1860.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/16/23 10:46:46.631
    STEP: creating a pod to probe /etc/hosts 03/16/23 10:46:46.632
    STEP: submitting the pod to kubernetes 03/16/23 10:46:46.632
    Mar 16 10:46:46.726: INFO: Waiting up to 15m0s for pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e" in namespace "dns-1860" to be "running"
    Mar 16 10:46:46.816: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.705033ms
    Mar 16 10:46:48.907: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e": Phase="Running", Reason="", readiness=true. Elapsed: 2.18015763s
    Mar 16 10:46:48.907: INFO: Pod "dns-test-63354880-b324-4b83-8582-8d9b8b33044e" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 10:46:48.907
    STEP: looking for the results for each expected name from probers 03/16/23 10:46:48.996
    Mar 16 10:46:49.551: INFO: DNS probes using dns-1860/dns-test-63354880-b324-4b83-8582-8d9b8b33044e succeeded

    STEP: deleting the pod 03/16/23 10:46:49.551
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:49.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1860" for this suite. 03/16/23 10:46:49.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:49.913
Mar 16 10:46:49.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 10:46:49.915
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:50.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:50.361
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 03/16/23 10:46:50.539
STEP: patching the Namespace 03/16/23 10:46:50.807
STEP: get the Namespace and ensuring it has the label 03/16/23 10:46:50.898
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:50.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8293" for this suite. 03/16/23 10:46:51.078
STEP: Destroying namespace "nspatchtest-17e586a7-3d90-42be-a4f6-c574ab566437-6877" for this suite. 03/16/23 10:46:51.168
------------------------------
• [1.345 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:49.913
    Mar 16 10:46:49.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 10:46:49.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:50.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:50.361
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 03/16/23 10:46:50.539
    STEP: patching the Namespace 03/16/23 10:46:50.807
    STEP: get the Namespace and ensuring it has the label 03/16/23 10:46:50.898
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:50.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8293" for this suite. 03/16/23 10:46:51.078
    STEP: Destroying namespace "nspatchtest-17e586a7-3d90-42be-a4f6-c574ab566437-6877" for this suite. 03/16/23 10:46:51.168
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:51.259
Mar 16 10:46:51.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:46:51.26
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:51.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:51.707
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:46:56.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-214" for this suite. 03/16/23 10:46:56.335
------------------------------
• [5.167 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:51.259
    Mar 16 10:46:51.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:46:51.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:51.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:51.707
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:46:56.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-214" for this suite. 03/16/23 10:46:56.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:46:56.426
Mar 16 10:46:56.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 10:46:56.427
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:56.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:56.873
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:46:57.411
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:46:57.501
Mar 16 10:46:57.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:46:57.681: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:46:58.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:46:58.949: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/16/23 10:46:59.039
Mar 16 10:46:59.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:46:59.399: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:47:00.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:47:00.668: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:47:01.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:47:01.667: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:47:02.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:47:02.667: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:47:02.756
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3818, will wait for the garbage collector to delete the pods 03/16/23 10:47:02.757
Mar 16 10:47:03.037: INFO: Deleting DaemonSet.extensions daemon-set took: 90.099298ms
Mar 16 10:47:03.138: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.394778ms
Mar 16 10:47:05.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:47:05.627: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 10:47:05.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37989"},"items":null}

Mar 16 10:47:05.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37989"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:06.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3818" for this suite. 03/16/23 10:47:06.254
------------------------------
• [9.918 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:46:56.426
    Mar 16 10:46:56.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 10:46:56.427
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:46:56.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:46:56.873
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 03/16/23 10:46:57.411
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:46:57.501
    Mar 16 10:46:57.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:46:57.681: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:46:58.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:46:58.949: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/16/23 10:46:59.039
    Mar 16 10:46:59.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:46:59.399: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:47:00.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:47:00.668: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:47:01.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:47:01.667: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:47:02.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:47:02.667: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:47:02.756
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3818, will wait for the garbage collector to delete the pods 03/16/23 10:47:02.757
    Mar 16 10:47:03.037: INFO: Deleting DaemonSet.extensions daemon-set took: 90.099298ms
    Mar 16 10:47:03.138: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.394778ms
    Mar 16 10:47:05.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:47:05.627: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 10:47:05.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37989"},"items":null}

    Mar 16 10:47:05.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37989"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:06.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3818" for this suite. 03/16/23 10:47:06.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:06.345
Mar 16 10:47:06.345: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version 03/16/23 10:47:06.346
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:06.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:06.791
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/16/23 10:47:06.969
STEP: Confirm major version 03/16/23 10:47:07.057
Mar 16 10:47:07.057: INFO: Major version: 1
STEP: Confirm minor version 03/16/23 10:47:07.057
Mar 16 10:47:07.057: INFO: cleanMinorVersion: 26
Mar 16 10:47:07.057: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-4084" for this suite. 03/16/23 10:47:07.148
------------------------------
• [0.893 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:06.345
    Mar 16 10:47:06.345: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename server-version 03/16/23 10:47:06.346
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:06.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:06.791
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/16/23 10:47:06.969
    STEP: Confirm major version 03/16/23 10:47:07.057
    Mar 16 10:47:07.057: INFO: Major version: 1
    STEP: Confirm minor version 03/16/23 10:47:07.057
    Mar 16 10:47:07.057: INFO: cleanMinorVersion: 26
    Mar 16 10:47:07.057: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-4084" for this suite. 03/16/23 10:47:07.148
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:07.238
Mar 16 10:47:07.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:47:07.239
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:07.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:07.685
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-a4047ef3-7105-4f13-b283-f7e58858783f 03/16/23 10:47:07.863
STEP: Creating a pod to test consume secrets 03/16/23 10:47:07.953
Mar 16 10:47:08.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65" in namespace "projected-9165" to be "Succeeded or Failed"
Mar 16 10:47:08.137: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Pending", Reason="", readiness=false. Elapsed: 89.551218ms
Mar 16 10:47:10.228: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180164532s
Mar 16 10:47:12.227: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179579687s
STEP: Saw pod success 03/16/23 10:47:12.228
Mar 16 10:47:12.228: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65" satisfied condition "Succeeded or Failed"
Mar 16 10:47:12.317: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:47:12.454
Mar 16 10:47:12.549: INFO: Waiting for pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 to disappear
Mar 16 10:47:12.638: INFO: Pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:12.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9165" for this suite. 03/16/23 10:47:12.816
------------------------------
• [5.669 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:07.238
    Mar 16 10:47:07.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:47:07.239
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:07.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:07.685
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-a4047ef3-7105-4f13-b283-f7e58858783f 03/16/23 10:47:07.863
    STEP: Creating a pod to test consume secrets 03/16/23 10:47:07.953
    Mar 16 10:47:08.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65" in namespace "projected-9165" to be "Succeeded or Failed"
    Mar 16 10:47:08.137: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Pending", Reason="", readiness=false. Elapsed: 89.551218ms
    Mar 16 10:47:10.228: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180164532s
    Mar 16 10:47:12.227: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179579687s
    STEP: Saw pod success 03/16/23 10:47:12.228
    Mar 16 10:47:12.228: INFO: Pod "pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65" satisfied condition "Succeeded or Failed"
    Mar 16 10:47:12.317: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:47:12.454
    Mar 16 10:47:12.549: INFO: Waiting for pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 to disappear
    Mar 16 10:47:12.638: INFO: Pod pod-projected-secrets-bcf7f623-ce08-409d-a966-d41eb989fe65 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:12.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9165" for this suite. 03/16/23 10:47:12.816
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:12.907
Mar 16 10:47:12.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:47:12.909
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:13.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:13.356
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-9h8xb"  03/16/23 10:47:13.534
Mar 16 10:47:13.623: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-9h8xb"  03/16/23 10:47:13.623
Mar 16 10:47:13.803: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1339" for this suite. 03/16/23 10:47:13.894
------------------------------
• [1.076 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:12.907
    Mar 16 10:47:12.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 10:47:12.909
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:13.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:13.356
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-9h8xb"  03/16/23 10:47:13.534
    Mar 16 10:47:13.623: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-9h8xb"  03/16/23 10:47:13.623
    Mar 16 10:47:13.803: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1339" for this suite. 03/16/23 10:47:13.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:13.985
Mar 16 10:47:13.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:47:13.986
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:14.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:14.432
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-5ce4a19f-8c99-4836-92a7-f111015325df 03/16/23 10:47:14.61
STEP: Creating a pod to test consume configMaps 03/16/23 10:47:14.7
Mar 16 10:47:14.794: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24" in namespace "configmap-2786" to be "Succeeded or Failed"
Mar 16 10:47:14.883: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Pending", Reason="", readiness=false. Elapsed: 89.405182ms
Mar 16 10:47:16.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179897561s
Mar 16 10:47:18.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18044545s
STEP: Saw pod success 03/16/23 10:47:18.974
Mar 16 10:47:18.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24" satisfied condition "Succeeded or Failed"
Mar 16 10:47:19.063: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:47:19.198
Mar 16 10:47:19.291: INFO: Waiting for pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 to disappear
Mar 16 10:47:19.380: INFO: Pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2786" for this suite. 03/16/23 10:47:19.558
------------------------------
• [5.664 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:13.985
    Mar 16 10:47:13.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:47:13.986
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:14.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:14.432
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-5ce4a19f-8c99-4836-92a7-f111015325df 03/16/23 10:47:14.61
    STEP: Creating a pod to test consume configMaps 03/16/23 10:47:14.7
    Mar 16 10:47:14.794: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24" in namespace "configmap-2786" to be "Succeeded or Failed"
    Mar 16 10:47:14.883: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Pending", Reason="", readiness=false. Elapsed: 89.405182ms
    Mar 16 10:47:16.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179897561s
    Mar 16 10:47:18.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18044545s
    STEP: Saw pod success 03/16/23 10:47:18.974
    Mar 16 10:47:18.974: INFO: Pod "pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24" satisfied condition "Succeeded or Failed"
    Mar 16 10:47:19.063: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:47:19.198
    Mar 16 10:47:19.291: INFO: Waiting for pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 to disappear
    Mar 16 10:47:19.380: INFO: Pod pod-configmaps-8fe69467-adc2-4cdb-85f3-e770230f6c24 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2786" for this suite. 03/16/23 10:47:19.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:19.65
Mar 16 10:47:19.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 10:47:19.651
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:19.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:20.098
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 03/16/23 10:47:20.275
STEP: submitting the pod to kubernetes 03/16/23 10:47:20.275
Mar 16 10:47:20.370: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" in namespace "pods-6964" to be "running and ready"
Mar 16 10:47:20.459: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Pending", Reason="", readiness=false. Elapsed: 89.515358ms
Mar 16 10:47:20.459: INFO: The phase of Pod pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:47:22.549: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=true. Elapsed: 2.179255285s
Mar 16 10:47:22.549: INFO: The phase of Pod pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630 is Running (Ready = true)
Mar 16 10:47:22.549: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/16/23 10:47:22.639
STEP: updating the pod 03/16/23 10:47:22.728
Mar 16 10:47:23.411: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630"
Mar 16 10:47:23.411: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" in namespace "pods-6964" to be "terminated with reason DeadlineExceeded"
Mar 16 10:47:23.500: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=true. Elapsed: 89.612981ms
Mar 16 10:47:25.591: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=false. Elapsed: 2.179889566s
Mar 16 10:47:27.590: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.179545887s
Mar 16 10:47:27.590: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:27.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6964" for this suite. 03/16/23 10:47:27.768
------------------------------
• [8.208 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:19.65
    Mar 16 10:47:19.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 10:47:19.651
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:19.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:20.098
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 03/16/23 10:47:20.275
    STEP: submitting the pod to kubernetes 03/16/23 10:47:20.275
    Mar 16 10:47:20.370: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" in namespace "pods-6964" to be "running and ready"
    Mar 16 10:47:20.459: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Pending", Reason="", readiness=false. Elapsed: 89.515358ms
    Mar 16 10:47:20.459: INFO: The phase of Pod pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:47:22.549: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=true. Elapsed: 2.179255285s
    Mar 16 10:47:22.549: INFO: The phase of Pod pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630 is Running (Ready = true)
    Mar 16 10:47:22.549: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/16/23 10:47:22.639
    STEP: updating the pod 03/16/23 10:47:22.728
    Mar 16 10:47:23.411: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630"
    Mar 16 10:47:23.411: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" in namespace "pods-6964" to be "terminated with reason DeadlineExceeded"
    Mar 16 10:47:23.500: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=true. Elapsed: 89.612981ms
    Mar 16 10:47:25.591: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Running", Reason="", readiness=false. Elapsed: 2.179889566s
    Mar 16 10:47:27.590: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.179545887s
    Mar 16 10:47:27.590: INFO: Pod "pod-update-activedeadlineseconds-5412d6d7-33de-40d1-980e-244b98dda630" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:27.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6964" for this suite. 03/16/23 10:47:27.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:27.859
Mar 16 10:47:27.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:47:27.86
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:28.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:28.306
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/16/23 10:47:28.577
Mar 16 10:47:28.577: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86" in namespace "kubelet-test-5753" to be "completed"
Mar 16 10:47:28.667: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Pending", Reason="", readiness=false. Elapsed: 89.68458ms
Mar 16 10:47:30.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180555893s
Mar 16 10:47:32.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180296272s
Mar 16 10:47:32.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:47:32.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5753" for this suite. 03/16/23 10:47:33.029
------------------------------
• [5.260 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:27.859
    Mar 16 10:47:27.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubelet-test 03/16/23 10:47:27.86
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:28.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:28.306
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/16/23 10:47:28.577
    Mar 16 10:47:28.577: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86" in namespace "kubelet-test-5753" to be "completed"
    Mar 16 10:47:28.667: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Pending", Reason="", readiness=false. Elapsed: 89.68458ms
    Mar 16 10:47:30.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180555893s
    Mar 16 10:47:32.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180296272s
    Mar 16 10:47:32.757: INFO: Pod "agnhost-host-aliases82da7ee6-dd39-46ba-b4b9-eb1a2e6d0d86" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:47:32.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5753" for this suite. 03/16/23 10:47:33.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:47:33.121
Mar 16 10:47:33.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 10:47:33.122
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:33.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:33.568
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 in namespace container-probe-8075 03/16/23 10:47:33.745
Mar 16 10:47:33.839: INFO: Waiting up to 5m0s for pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6" in namespace "container-probe-8075" to be "not pending"
Mar 16 10:47:33.929: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.424221ms
Mar 16 10:47:36.020: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.180204561s
Mar 16 10:47:36.020: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6" satisfied condition "not pending"
Mar 16 10:47:36.020: INFO: Started pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 in namespace container-probe-8075
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:47:36.02
Mar 16 10:47:36.109: INFO: Initial restart count of pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 is 0
STEP: deleting the pod 03/16/23 10:51:36.514
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 10:51:36.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8075" for this suite. 03/16/23 10:51:36.786
------------------------------
• [243.755 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:47:33.121
    Mar 16 10:47:33.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 10:47:33.122
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:47:33.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:47:33.568
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 in namespace container-probe-8075 03/16/23 10:47:33.745
    Mar 16 10:47:33.839: INFO: Waiting up to 5m0s for pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6" in namespace "container-probe-8075" to be "not pending"
    Mar 16 10:47:33.929: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.424221ms
    Mar 16 10:47:36.020: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.180204561s
    Mar 16 10:47:36.020: INFO: Pod "liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6" satisfied condition "not pending"
    Mar 16 10:47:36.020: INFO: Started pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 in namespace container-probe-8075
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 10:47:36.02
    Mar 16 10:47:36.109: INFO: Initial restart count of pod liveness-1a0df9bb-7d65-4133-a4d2-55fd3edbb9d6 is 0
    STEP: deleting the pod 03/16/23 10:51:36.514
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:51:36.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8075" for this suite. 03/16/23 10:51:36.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:51:36.877
Mar 16 10:51:36.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 10:51:36.879
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:51:37.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:51:37.325
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 03/16/23 10:51:37.502
STEP: Ensuring ResourceQuota status is calculated 03/16/23 10:51:37.597
STEP: Creating a ResourceQuota with not terminating scope 03/16/23 10:51:39.687
STEP: Ensuring ResourceQuota status is calculated 03/16/23 10:51:39.777
STEP: Creating a long running pod 03/16/23 10:51:41.868
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/16/23 10:51:41.972
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/16/23 10:51:44.063
STEP: Deleting the pod 03/16/23 10:51:46.154
STEP: Ensuring resource quota status released the pod usage 03/16/23 10:51:46.249
STEP: Creating a terminating pod 03/16/23 10:51:48.339
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/16/23 10:51:48.437
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/16/23 10:51:50.528
STEP: Deleting the pod 03/16/23 10:51:52.618
STEP: Ensuring resource quota status released the pod usage 03/16/23 10:51:52.713
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 10:51:54.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4398" for this suite. 03/16/23 10:51:54.986
------------------------------
• [18.202 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:51:36.877
    Mar 16 10:51:36.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 10:51:36.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:51:37.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:51:37.325
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 03/16/23 10:51:37.502
    STEP: Ensuring ResourceQuota status is calculated 03/16/23 10:51:37.597
    STEP: Creating a ResourceQuota with not terminating scope 03/16/23 10:51:39.687
    STEP: Ensuring ResourceQuota status is calculated 03/16/23 10:51:39.777
    STEP: Creating a long running pod 03/16/23 10:51:41.868
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/16/23 10:51:41.972
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/16/23 10:51:44.063
    STEP: Deleting the pod 03/16/23 10:51:46.154
    STEP: Ensuring resource quota status released the pod usage 03/16/23 10:51:46.249
    STEP: Creating a terminating pod 03/16/23 10:51:48.339
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/16/23 10:51:48.437
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/16/23 10:51:50.528
    STEP: Deleting the pod 03/16/23 10:51:52.618
    STEP: Ensuring resource quota status released the pod usage 03/16/23 10:51:52.713
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:51:54.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4398" for this suite. 03/16/23 10:51:54.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:51:55.079
Mar 16 10:51:55.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency 03/16/23 10:51:55.08
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:51:55.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:51:55.527
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 16 10:51:55.704: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7070 03/16/23 10:51:55.706
I0316 10:51:55.797060    8588 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7070, replica count: 1
I0316 10:51:56.901321    8588 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0316 10:51:57.901799    8588 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:51:58.096: INFO: Created: latency-svc-ltj8w
Mar 16 10:51:58.099: INFO: Got endpoints: latency-svc-ltj8w [96.852341ms]
Mar 16 10:51:58.194: INFO: Created: latency-svc-ch5rp
Mar 16 10:51:58.198: INFO: Got endpoints: latency-svc-ch5rp [97.888672ms]
Mar 16 10:51:58.198: INFO: Created: latency-svc-pw5ns
Mar 16 10:51:58.202: INFO: Created: latency-svc-c2jtb
Mar 16 10:51:58.275: INFO: Created: latency-svc-hgrnw
Mar 16 10:51:58.275: INFO: Created: latency-svc-59mxh
Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-pw5ns [176.221145ms]
Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-hgrnw [175.649094ms]
Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-59mxh [175.565329ms]
Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-c2jtb [176.036423ms]
Mar 16 10:51:58.281: INFO: Created: latency-svc-jhfwq
Mar 16 10:51:58.286: INFO: Got endpoints: latency-svc-jhfwq [184.931872ms]
Mar 16 10:51:58.286: INFO: Created: latency-svc-tcp7v
Mar 16 10:51:58.290: INFO: Got endpoints: latency-svc-tcp7v [188.610111ms]
Mar 16 10:51:58.290: INFO: Created: latency-svc-49fsb
Mar 16 10:51:58.295: INFO: Created: latency-svc-p4q9w
Mar 16 10:51:58.295: INFO: Got endpoints: latency-svc-49fsb [193.916078ms]
Mar 16 10:51:58.296: INFO: Got endpoints: latency-svc-p4q9w [194.906339ms]
Mar 16 10:51:58.298: INFO: Created: latency-svc-xlk6t
Mar 16 10:51:58.305: INFO: Got endpoints: latency-svc-xlk6t [204.045586ms]
Mar 16 10:51:58.332: INFO: Created: latency-svc-62kbs
Mar 16 10:51:58.332: INFO: Got endpoints: latency-svc-62kbs [231.976263ms]
Mar 16 10:51:58.363: INFO: Created: latency-svc-lcflh
Mar 16 10:51:58.363: INFO: Created: latency-svc-9cqp5
Mar 16 10:51:58.363: INFO: Created: latency-svc-wb86x
Mar 16 10:51:58.363: INFO: Created: latency-svc-z776s
Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-lcflh [261.528389ms]
Mar 16 10:51:58.363: INFO: Created: latency-svc-45cwr
Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-z776s [261.90946ms]
Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-45cwr [261.51524ms]
Mar 16 10:51:58.364: INFO: Got endpoints: latency-svc-9cqp5 [261.84715ms]
Mar 16 10:51:58.364: INFO: Got endpoints: latency-svc-wb86x [165.877605ms]
Mar 16 10:51:58.372: INFO: Created: latency-svc-cksmg
Mar 16 10:51:58.377: INFO: Got endpoints: latency-svc-cksmg [101.00761ms]
Mar 16 10:51:58.377: INFO: Created: latency-svc-c6fjr
Mar 16 10:51:58.379: INFO: Got endpoints: latency-svc-c6fjr [102.741485ms]
Mar 16 10:51:58.382: INFO: Created: latency-svc-sv6mb
Mar 16 10:51:58.384: INFO: Got endpoints: latency-svc-sv6mb [108.346069ms]
Mar 16 10:51:58.387: INFO: Created: latency-svc-jb25s
Mar 16 10:51:58.389: INFO: Got endpoints: latency-svc-jb25s [112.672392ms]
Mar 16 10:51:58.396: INFO: Created: latency-svc-g77ch
Mar 16 10:51:58.396: INFO: Got endpoints: latency-svc-g77ch [110.102393ms]
Mar 16 10:51:58.399: INFO: Created: latency-svc-cpclj
Mar 16 10:51:58.404: INFO: Got endpoints: latency-svc-cpclj [113.946363ms]
Mar 16 10:51:58.404: INFO: Created: latency-svc-2g7fp
Mar 16 10:51:58.407: INFO: Got endpoints: latency-svc-2g7fp [112.379786ms]
Mar 16 10:51:58.408: INFO: Created: latency-svc-klb5t
Mar 16 10:51:58.409: INFO: Got endpoints: latency-svc-klb5t [113.756586ms]
Mar 16 10:51:58.414: INFO: Created: latency-svc-6fz9g
Mar 16 10:51:58.416: INFO: Got endpoints: latency-svc-6fz9g [110.740518ms]
Mar 16 10:51:58.426: INFO: Created: latency-svc-9zlpc
Mar 16 10:51:58.428: INFO: Got endpoints: latency-svc-9zlpc [95.552768ms]
Mar 16 10:51:58.457: INFO: Created: latency-svc-cmpfv
Mar 16 10:51:58.459: INFO: Got endpoints: latency-svc-cmpfv [95.712395ms]
Mar 16 10:51:58.465: INFO: Created: latency-svc-4d4pg
Mar 16 10:51:58.469: INFO: Got endpoints: latency-svc-4d4pg [106.065159ms]
Mar 16 10:51:58.470: INFO: Created: latency-svc-mbbm8
Mar 16 10:51:58.477: INFO: Got endpoints: latency-svc-mbbm8 [113.701123ms]
Mar 16 10:51:58.477: INFO: Created: latency-svc-sfqrm
Mar 16 10:51:58.481: INFO: Got endpoints: latency-svc-sfqrm [117.480707ms]
Mar 16 10:51:58.481: INFO: Created: latency-svc-dx8hm
Mar 16 10:51:58.483: INFO: Got endpoints: latency-svc-dx8hm [119.538ms]
Mar 16 10:51:58.492: INFO: Created: latency-svc-q4wkc
Mar 16 10:51:58.501: INFO: Got endpoints: latency-svc-q4wkc [124.051488ms]
Mar 16 10:51:58.501: INFO: Created: latency-svc-zm5zp
Mar 16 10:51:58.506: INFO: Got endpoints: latency-svc-zm5zp [127.52321ms]
Mar 16 10:51:58.506: INFO: Created: latency-svc-j9mhb
Mar 16 10:51:58.513: INFO: Got endpoints: latency-svc-j9mhb [129.038921ms]
Mar 16 10:51:58.513: INFO: Created: latency-svc-nbmld
Mar 16 10:51:58.514: INFO: Got endpoints: latency-svc-nbmld [125.357534ms]
Mar 16 10:51:58.524: INFO: Created: latency-svc-9gd56
Mar 16 10:51:58.524: INFO: Created: latency-svc-5v2z9
Mar 16 10:51:58.525: INFO: Got endpoints: latency-svc-9gd56 [97.25874ms]
Mar 16 10:51:58.530: INFO: Created: latency-svc-c4ts7
Mar 16 10:51:58.534: INFO: Created: latency-svc-nrgqx
Mar 16 10:51:58.538: INFO: Created: latency-svc-wpdrj
Mar 16 10:51:58.542: INFO: Created: latency-svc-kfncv
Mar 16 10:51:58.546: INFO: Got endpoints: latency-svc-5v2z9 [150.585825ms]
Mar 16 10:51:58.553: INFO: Created: latency-svc-6qf6w
Mar 16 10:51:58.564: INFO: Created: latency-svc-nwpkx
Mar 16 10:51:58.571: INFO: Created: latency-svc-bwwtm
Mar 16 10:51:58.575: INFO: Created: latency-svc-7h6tn
Mar 16 10:51:58.584: INFO: Created: latency-svc-qj6hs
Mar 16 10:51:58.595: INFO: Created: latency-svc-n9647
Mar 16 10:51:58.601: INFO: Got endpoints: latency-svc-c4ts7 [193.477738ms]
Mar 16 10:51:58.607: INFO: Created: latency-svc-74x2f
Mar 16 10:51:58.612: INFO: Created: latency-svc-x6m4x
Mar 16 10:51:58.617: INFO: Created: latency-svc-w8hm9
Mar 16 10:51:58.623: INFO: Created: latency-svc-8nzgf
Mar 16 10:51:58.641: INFO: Created: latency-svc-jwggl
Mar 16 10:51:58.647: INFO: Got endpoints: latency-svc-nrgqx [237.477285ms]
Mar 16 10:51:58.698: INFO: Got endpoints: latency-svc-wpdrj [282.265395ms]
Mar 16 10:51:58.699: INFO: Created: latency-svc-d957m
Mar 16 10:51:58.741: INFO: Created: latency-svc-skmz8
Mar 16 10:51:58.746: INFO: Got endpoints: latency-svc-kfncv [342.654478ms]
Mar 16 10:51:58.798: INFO: Created: latency-svc-lk487
Mar 16 10:51:58.799: INFO: Got endpoints: latency-svc-6qf6w [339.601935ms]
Mar 16 10:51:58.841: INFO: Created: latency-svc-mpzf6
Mar 16 10:51:58.847: INFO: Got endpoints: latency-svc-nwpkx [377.759663ms]
Mar 16 10:51:58.894: INFO: Created: latency-svc-g6jnt
Mar 16 10:51:58.897: INFO: Got endpoints: latency-svc-bwwtm [420.14615ms]
Mar 16 10:51:58.942: INFO: Created: latency-svc-s7jrd
Mar 16 10:51:58.947: INFO: Got endpoints: latency-svc-7h6tn [465.401584ms]
Mar 16 10:51:58.991: INFO: Created: latency-svc-7nbzx
Mar 16 10:51:58.997: INFO: Got endpoints: latency-svc-qj6hs [513.766965ms]
Mar 16 10:51:59.041: INFO: Created: latency-svc-ndkk6
Mar 16 10:51:59.046: INFO: Got endpoints: latency-svc-n9647 [545.42069ms]
Mar 16 10:51:59.091: INFO: Created: latency-svc-gnh4n
Mar 16 10:51:59.098: INFO: Got endpoints: latency-svc-74x2f [585.13248ms]
Mar 16 10:51:59.143: INFO: Created: latency-svc-pgjtq
Mar 16 10:51:59.147: INFO: Got endpoints: latency-svc-x6m4x [633.716101ms]
Mar 16 10:51:59.192: INFO: Created: latency-svc-x66ms
Mar 16 10:51:59.202: INFO: Got endpoints: latency-svc-w8hm9 [687.407206ms]
Mar 16 10:51:59.242: INFO: Created: latency-svc-x9gn6
Mar 16 10:51:59.247: INFO: Got endpoints: latency-svc-8nzgf [721.807773ms]
Mar 16 10:51:59.296: INFO: Created: latency-svc-58xmb
Mar 16 10:51:59.297: INFO: Got endpoints: latency-svc-jwggl [750.403756ms]
Mar 16 10:51:59.343: INFO: Created: latency-svc-g5vbm
Mar 16 10:51:59.347: INFO: Got endpoints: latency-svc-d957m [746.104827ms]
Mar 16 10:51:59.392: INFO: Created: latency-svc-km9s5
Mar 16 10:51:59.398: INFO: Got endpoints: latency-svc-skmz8 [750.977514ms]
Mar 16 10:51:59.450: INFO: Got endpoints: latency-svc-lk487 [752.053654ms]
Mar 16 10:51:59.451: INFO: Created: latency-svc-bkf27
Mar 16 10:51:59.493: INFO: Created: latency-svc-rsfhr
Mar 16 10:51:59.498: INFO: Got endpoints: latency-svc-mpzf6 [751.889707ms]
Mar 16 10:51:59.545: INFO: Created: latency-svc-vbcps
Mar 16 10:51:59.547: INFO: Got endpoints: latency-svc-g6jnt [748.176972ms]
Mar 16 10:51:59.594: INFO: Created: latency-svc-qhdmt
Mar 16 10:51:59.596: INFO: Got endpoints: latency-svc-s7jrd [749.00607ms]
Mar 16 10:51:59.642: INFO: Created: latency-svc-mn924
Mar 16 10:51:59.646: INFO: Got endpoints: latency-svc-7nbzx [749.240537ms]
Mar 16 10:51:59.691: INFO: Created: latency-svc-rk59h
Mar 16 10:51:59.696: INFO: Got endpoints: latency-svc-ndkk6 [749.877898ms]
Mar 16 10:51:59.742: INFO: Created: latency-svc-z2t5j
Mar 16 10:51:59.750: INFO: Got endpoints: latency-svc-gnh4n [752.530971ms]
Mar 16 10:51:59.791: INFO: Created: latency-svc-nlgkp
Mar 16 10:51:59.797: INFO: Got endpoints: latency-svc-pgjtq [750.121138ms]
Mar 16 10:51:59.844: INFO: Created: latency-svc-qvkmr
Mar 16 10:51:59.846: INFO: Got endpoints: latency-svc-x66ms [748.194661ms]
Mar 16 10:51:59.891: INFO: Created: latency-svc-pbpbv
Mar 16 10:51:59.897: INFO: Got endpoints: latency-svc-x9gn6 [750.576617ms]
Mar 16 10:51:59.940: INFO: Created: latency-svc-br68t
Mar 16 10:51:59.947: INFO: Got endpoints: latency-svc-58xmb [744.986778ms]
Mar 16 10:51:59.991: INFO: Created: latency-svc-zwhfs
Mar 16 10:51:59.997: INFO: Got endpoints: latency-svc-g5vbm [750.105026ms]
Mar 16 10:52:00.041: INFO: Created: latency-svc-jnm7f
Mar 16 10:52:00.046: INFO: Got endpoints: latency-svc-km9s5 [749.698315ms]
Mar 16 10:52:00.093: INFO: Created: latency-svc-k2b5l
Mar 16 10:52:00.097: INFO: Got endpoints: latency-svc-bkf27 [749.414885ms]
Mar 16 10:52:00.141: INFO: Created: latency-svc-mzrct
Mar 16 10:52:00.147: INFO: Got endpoints: latency-svc-rsfhr [748.816075ms]
Mar 16 10:52:00.191: INFO: Created: latency-svc-wfr5f
Mar 16 10:52:00.197: INFO: Got endpoints: latency-svc-vbcps [746.424755ms]
Mar 16 10:52:00.241: INFO: Created: latency-svc-44ccr
Mar 16 10:52:00.247: INFO: Got endpoints: latency-svc-qhdmt [748.774281ms]
Mar 16 10:52:00.292: INFO: Created: latency-svc-zlwc7
Mar 16 10:52:00.305: INFO: Got endpoints: latency-svc-mn924 [758.080336ms]
Mar 16 10:52:00.341: INFO: Created: latency-svc-7jckz
Mar 16 10:52:00.346: INFO: Got endpoints: latency-svc-rk59h [750.268638ms]
Mar 16 10:52:00.399: INFO: Got endpoints: latency-svc-z2t5j [751.996637ms]
Mar 16 10:52:00.402: INFO: Created: latency-svc-qcglb
Mar 16 10:52:00.444: INFO: Created: latency-svc-nv7kg
Mar 16 10:52:00.447: INFO: Got endpoints: latency-svc-nlgkp [750.236376ms]
Mar 16 10:52:00.495: INFO: Created: latency-svc-hvp49
Mar 16 10:52:00.496: INFO: Got endpoints: latency-svc-qvkmr [746.348447ms]
Mar 16 10:52:00.545: INFO: Created: latency-svc-xl28m
Mar 16 10:52:00.546: INFO: Got endpoints: latency-svc-pbpbv [749.487123ms]
Mar 16 10:52:00.590: INFO: Created: latency-svc-wgzfd
Mar 16 10:52:00.597: INFO: Got endpoints: latency-svc-br68t [750.792412ms]
Mar 16 10:52:00.641: INFO: Created: latency-svc-tds4w
Mar 16 10:52:00.648: INFO: Got endpoints: latency-svc-zwhfs [750.985865ms]
Mar 16 10:52:00.692: INFO: Created: latency-svc-rt45x
Mar 16 10:52:00.697: INFO: Got endpoints: latency-svc-jnm7f [750.423141ms]
Mar 16 10:52:00.743: INFO: Created: latency-svc-wsbsz
Mar 16 10:52:00.753: INFO: Got endpoints: latency-svc-k2b5l [755.625184ms]
Mar 16 10:52:00.792: INFO: Created: latency-svc-h9bjp
Mar 16 10:52:00.797: INFO: Got endpoints: latency-svc-mzrct [750.55403ms]
Mar 16 10:52:00.849: INFO: Got endpoints: latency-svc-wfr5f [752.081372ms]
Mar 16 10:52:00.849: INFO: Created: latency-svc-9hdgc
Mar 16 10:52:00.892: INFO: Created: latency-svc-sgm8q
Mar 16 10:52:00.897: INFO: Got endpoints: latency-svc-44ccr [749.884604ms]
Mar 16 10:52:00.943: INFO: Created: latency-svc-8fhks
Mar 16 10:52:00.948: INFO: Got endpoints: latency-svc-zlwc7 [750.681259ms]
Mar 16 10:52:00.994: INFO: Created: latency-svc-8clx8
Mar 16 10:52:00.998: INFO: Got endpoints: latency-svc-7jckz [751.159857ms]
Mar 16 10:52:01.042: INFO: Created: latency-svc-qslfk
Mar 16 10:52:01.047: INFO: Got endpoints: latency-svc-qcglb [741.799246ms]
Mar 16 10:52:01.097: INFO: Got endpoints: latency-svc-nv7kg [750.696596ms]
Mar 16 10:52:01.097: INFO: Created: latency-svc-prknt
Mar 16 10:52:01.145: INFO: Created: latency-svc-6hctp
Mar 16 10:52:01.147: INFO: Got endpoints: latency-svc-hvp49 [748.90081ms]
Mar 16 10:52:01.192: INFO: Created: latency-svc-qh9s8
Mar 16 10:52:01.199: INFO: Got endpoints: latency-svc-xl28m [751.928225ms]
Mar 16 10:52:01.243: INFO: Created: latency-svc-x7xm4
Mar 16 10:52:01.247: INFO: Got endpoints: latency-svc-wgzfd [751.079098ms]
Mar 16 10:52:01.293: INFO: Created: latency-svc-mg7t4
Mar 16 10:52:01.296: INFO: Got endpoints: latency-svc-tds4w [749.748134ms]
Mar 16 10:52:01.343: INFO: Created: latency-svc-9zlbx
Mar 16 10:52:01.347: INFO: Got endpoints: latency-svc-rt45x [749.733884ms]
Mar 16 10:52:01.390: INFO: Created: latency-svc-v92wx
Mar 16 10:52:01.397: INFO: Got endpoints: latency-svc-wsbsz [748.309136ms]
Mar 16 10:52:01.445: INFO: Created: latency-svc-zsdx9
Mar 16 10:52:01.448: INFO: Got endpoints: latency-svc-h9bjp [751.336501ms]
Mar 16 10:52:01.491: INFO: Created: latency-svc-htbkt
Mar 16 10:52:01.497: INFO: Got endpoints: latency-svc-9hdgc [744.289389ms]
Mar 16 10:52:01.544: INFO: Created: latency-svc-bxmc2
Mar 16 10:52:01.547: INFO: Got endpoints: latency-svc-sgm8q [749.406569ms]
Mar 16 10:52:01.593: INFO: Created: latency-svc-2zrr9
Mar 16 10:52:01.598: INFO: Got endpoints: latency-svc-8fhks [749.214831ms]
Mar 16 10:52:01.642: INFO: Created: latency-svc-lhr9d
Mar 16 10:52:01.647: INFO: Got endpoints: latency-svc-8clx8 [750.455709ms]
Mar 16 10:52:01.697: INFO: Got endpoints: latency-svc-qslfk [749.324013ms]
Mar 16 10:52:01.698: INFO: Created: latency-svc-7d7h9
Mar 16 10:52:01.743: INFO: Created: latency-svc-chnq6
Mar 16 10:52:01.746: INFO: Got endpoints: latency-svc-prknt [748.1135ms]
Mar 16 10:52:01.792: INFO: Created: latency-svc-95wjg
Mar 16 10:52:01.797: INFO: Got endpoints: latency-svc-6hctp [750.30414ms]
Mar 16 10:52:01.843: INFO: Created: latency-svc-c5x4x
Mar 16 10:52:01.846: INFO: Got endpoints: latency-svc-qh9s8 [749.324328ms]
Mar 16 10:52:01.892: INFO: Created: latency-svc-m2fkn
Mar 16 10:52:01.896: INFO: Got endpoints: latency-svc-x7xm4 [748.486221ms]
Mar 16 10:52:01.942: INFO: Created: latency-svc-nsw2f
Mar 16 10:52:01.947: INFO: Got endpoints: latency-svc-mg7t4 [748.539182ms]
Mar 16 10:52:01.991: INFO: Created: latency-svc-2mqzf
Mar 16 10:52:01.997: INFO: Got endpoints: latency-svc-9zlbx [749.913925ms]
Mar 16 10:52:02.042: INFO: Created: latency-svc-rrp84
Mar 16 10:52:02.048: INFO: Got endpoints: latency-svc-v92wx [752.274356ms]
Mar 16 10:52:02.091: INFO: Created: latency-svc-tnwx8
Mar 16 10:52:02.099: INFO: Got endpoints: latency-svc-zsdx9 [752.185137ms]
Mar 16 10:52:02.143: INFO: Created: latency-svc-zxdmr
Mar 16 10:52:02.146: INFO: Got endpoints: latency-svc-htbkt [749.405143ms]
Mar 16 10:52:02.194: INFO: Created: latency-svc-lhhmp
Mar 16 10:52:02.196: INFO: Got endpoints: latency-svc-bxmc2 [747.957836ms]
Mar 16 10:52:02.241: INFO: Created: latency-svc-f7llj
Mar 16 10:52:02.247: INFO: Got endpoints: latency-svc-2zrr9 [749.454251ms]
Mar 16 10:52:02.292: INFO: Created: latency-svc-79xtp
Mar 16 10:52:02.299: INFO: Got endpoints: latency-svc-lhr9d [752.945191ms]
Mar 16 10:52:02.342: INFO: Created: latency-svc-9ztcl
Mar 16 10:52:02.346: INFO: Got endpoints: latency-svc-7d7h9 [748.121692ms]
Mar 16 10:52:02.398: INFO: Got endpoints: latency-svc-chnq6 [750.963984ms]
Mar 16 10:52:02.399: INFO: Created: latency-svc-vhwdn
Mar 16 10:52:02.441: INFO: Created: latency-svc-7rbdh
Mar 16 10:52:02.446: INFO: Got endpoints: latency-svc-95wjg [749.081724ms]
Mar 16 10:52:02.493: INFO: Created: latency-svc-tnkjp
Mar 16 10:52:02.497: INFO: Got endpoints: latency-svc-c5x4x [750.27571ms]
Mar 16 10:52:02.541: INFO: Created: latency-svc-xql9s
Mar 16 10:52:02.549: INFO: Got endpoints: latency-svc-m2fkn [751.698839ms]
Mar 16 10:52:02.591: INFO: Created: latency-svc-flkh4
Mar 16 10:52:02.597: INFO: Got endpoints: latency-svc-nsw2f [750.301529ms]
Mar 16 10:52:02.644: INFO: Created: latency-svc-46wpz
Mar 16 10:52:02.647: INFO: Got endpoints: latency-svc-2mqzf [750.723804ms]
Mar 16 10:52:02.691: INFO: Created: latency-svc-mpg9m
Mar 16 10:52:02.697: INFO: Got endpoints: latency-svc-rrp84 [749.255295ms]
Mar 16 10:52:02.741: INFO: Created: latency-svc-xjlhr
Mar 16 10:52:02.747: INFO: Got endpoints: latency-svc-tnwx8 [749.699031ms]
Mar 16 10:52:02.792: INFO: Created: latency-svc-qkzdp
Mar 16 10:52:02.798: INFO: Got endpoints: latency-svc-zxdmr [749.404818ms]
Mar 16 10:52:02.844: INFO: Created: latency-svc-8z5hn
Mar 16 10:52:02.846: INFO: Got endpoints: latency-svc-lhhmp [747.30539ms]
Mar 16 10:52:02.892: INFO: Created: latency-svc-glcsj
Mar 16 10:52:02.896: INFO: Got endpoints: latency-svc-f7llj [750.220589ms]
Mar 16 10:52:02.941: INFO: Created: latency-svc-dkrbm
Mar 16 10:52:02.949: INFO: Got endpoints: latency-svc-79xtp [752.150253ms]
Mar 16 10:52:02.992: INFO: Created: latency-svc-4v9f5
Mar 16 10:52:02.997: INFO: Got endpoints: latency-svc-9ztcl [750.247598ms]
Mar 16 10:52:03.043: INFO: Created: latency-svc-prtsv
Mar 16 10:52:03.047: INFO: Got endpoints: latency-svc-vhwdn [747.35138ms]
Mar 16 10:52:03.091: INFO: Created: latency-svc-gbd8b
Mar 16 10:52:03.098: INFO: Got endpoints: latency-svc-7rbdh [751.534162ms]
Mar 16 10:52:03.143: INFO: Created: latency-svc-ft79l
Mar 16 10:52:03.147: INFO: Got endpoints: latency-svc-tnkjp [748.918044ms]
Mar 16 10:52:03.192: INFO: Created: latency-svc-kdd55
Mar 16 10:52:03.197: INFO: Got endpoints: latency-svc-xql9s [750.859474ms]
Mar 16 10:52:03.242: INFO: Created: latency-svc-ln6pl
Mar 16 10:52:03.247: INFO: Got endpoints: latency-svc-flkh4 [750.080348ms]
Mar 16 10:52:03.291: INFO: Created: latency-svc-fgr99
Mar 16 10:52:03.297: INFO: Got endpoints: latency-svc-46wpz [747.634623ms]
Mar 16 10:52:03.341: INFO: Created: latency-svc-qvfmk
Mar 16 10:52:03.346: INFO: Got endpoints: latency-svc-mpg9m [749.289772ms]
Mar 16 10:52:03.391: INFO: Created: latency-svc-g64cb
Mar 16 10:52:03.396: INFO: Got endpoints: latency-svc-xjlhr [749.304404ms]
Mar 16 10:52:03.441: INFO: Created: latency-svc-wqgd9
Mar 16 10:52:03.447: INFO: Got endpoints: latency-svc-qkzdp [749.874109ms]
Mar 16 10:52:03.491: INFO: Created: latency-svc-pb6bz
Mar 16 10:52:03.497: INFO: Got endpoints: latency-svc-8z5hn [749.726512ms]
Mar 16 10:52:03.542: INFO: Created: latency-svc-t2wg6
Mar 16 10:52:03.547: INFO: Got endpoints: latency-svc-glcsj [748.862188ms]
Mar 16 10:52:03.591: INFO: Created: latency-svc-772fp
Mar 16 10:52:03.597: INFO: Got endpoints: latency-svc-dkrbm [750.880457ms]
Mar 16 10:52:03.642: INFO: Created: latency-svc-twgdp
Mar 16 10:52:03.646: INFO: Got endpoints: latency-svc-4v9f5 [749.97399ms]
Mar 16 10:52:03.692: INFO: Created: latency-svc-5tlnx
Mar 16 10:52:03.697: INFO: Got endpoints: latency-svc-prtsv [747.972606ms]
Mar 16 10:52:03.746: INFO: Created: latency-svc-5tk5q
Mar 16 10:52:03.753: INFO: Got endpoints: latency-svc-gbd8b [755.425372ms]
Mar 16 10:52:03.791: INFO: Created: latency-svc-lkm8c
Mar 16 10:52:03.796: INFO: Got endpoints: latency-svc-ft79l [749.283629ms]
Mar 16 10:52:03.847: INFO: Got endpoints: latency-svc-kdd55 [749.440774ms]
Mar 16 10:52:03.848: INFO: Created: latency-svc-v7pjz
Mar 16 10:52:03.892: INFO: Created: latency-svc-t62m5
Mar 16 10:52:03.898: INFO: Got endpoints: latency-svc-ln6pl [750.622062ms]
Mar 16 10:52:03.943: INFO: Created: latency-svc-qghdr
Mar 16 10:52:03.947: INFO: Got endpoints: latency-svc-fgr99 [749.665037ms]
Mar 16 10:52:03.996: INFO: Created: latency-svc-bttct
Mar 16 10:52:03.998: INFO: Got endpoints: latency-svc-qvfmk [751.201021ms]
Mar 16 10:52:04.041: INFO: Created: latency-svc-zvmhc
Mar 16 10:52:04.047: INFO: Got endpoints: latency-svc-g64cb [750.382287ms]
Mar 16 10:52:04.096: INFO: Created: latency-svc-9wqcd
Mar 16 10:52:04.102: INFO: Got endpoints: latency-svc-wqgd9 [756.086051ms]
Mar 16 10:52:04.143: INFO: Created: latency-svc-6p77s
Mar 16 10:52:04.147: INFO: Got endpoints: latency-svc-pb6bz [750.635418ms]
Mar 16 10:52:04.201: INFO: Got endpoints: latency-svc-t2wg6 [754.569484ms]
Mar 16 10:52:04.202: INFO: Created: latency-svc-b4dvn
Mar 16 10:52:04.241: INFO: Created: latency-svc-klgrr
Mar 16 10:52:04.246: INFO: Got endpoints: latency-svc-772fp [749.860396ms]
Mar 16 10:52:04.297: INFO: Created: latency-svc-pbvnr
Mar 16 10:52:04.298: INFO: Got endpoints: latency-svc-twgdp [751.675025ms]
Mar 16 10:52:04.342: INFO: Created: latency-svc-n7rjf
Mar 16 10:52:04.347: INFO: Got endpoints: latency-svc-5tlnx [749.472546ms]
Mar 16 10:52:04.393: INFO: Created: latency-svc-n5ppt
Mar 16 10:52:04.396: INFO: Got endpoints: latency-svc-5tk5q [749.708911ms]
Mar 16 10:52:04.445: INFO: Created: latency-svc-g9mv2
Mar 16 10:52:04.446: INFO: Got endpoints: latency-svc-lkm8c [749.396379ms]
Mar 16 10:52:04.491: INFO: Created: latency-svc-q7xlh
Mar 16 10:52:04.496: INFO: Got endpoints: latency-svc-v7pjz [743.368841ms]
Mar 16 10:52:04.541: INFO: Created: latency-svc-kn6cl
Mar 16 10:52:04.546: INFO: Got endpoints: latency-svc-t62m5 [749.834591ms]
Mar 16 10:52:04.592: INFO: Created: latency-svc-pxjcv
Mar 16 10:52:04.596: INFO: Got endpoints: latency-svc-qghdr [748.885172ms]
Mar 16 10:52:04.646: INFO: Created: latency-svc-2jhjb
Mar 16 10:52:04.647: INFO: Got endpoints: latency-svc-bttct [748.528115ms]
Mar 16 10:52:04.697: INFO: Created: latency-svc-68vwt
Mar 16 10:52:04.699: INFO: Got endpoints: latency-svc-zvmhc [752.319182ms]
Mar 16 10:52:04.742: INFO: Created: latency-svc-6h522
Mar 16 10:52:04.747: INFO: Got endpoints: latency-svc-9wqcd [748.645009ms]
Mar 16 10:52:04.794: INFO: Created: latency-svc-6qznb
Mar 16 10:52:04.796: INFO: Got endpoints: latency-svc-6p77s [748.897544ms]
Mar 16 10:52:04.842: INFO: Created: latency-svc-2zjtx
Mar 16 10:52:04.847: INFO: Got endpoints: latency-svc-b4dvn [744.244552ms]
Mar 16 10:52:04.891: INFO: Created: latency-svc-rq9qc
Mar 16 10:52:04.899: INFO: Got endpoints: latency-svc-klgrr [752.301138ms]
Mar 16 10:52:04.942: INFO: Created: latency-svc-2cvdr
Mar 16 10:52:04.947: INFO: Got endpoints: latency-svc-pbvnr [745.328717ms]
Mar 16 10:52:04.999: INFO: Created: latency-svc-dz72h
Mar 16 10:52:04.999: INFO: Got endpoints: latency-svc-n7rjf [752.090967ms]
Mar 16 10:52:05.045: INFO: Created: latency-svc-mkctf
Mar 16 10:52:05.046: INFO: Got endpoints: latency-svc-n5ppt [748.031668ms]
Mar 16 10:52:05.095: INFO: Created: latency-svc-vfhpb
Mar 16 10:52:05.096: INFO: Got endpoints: latency-svc-g9mv2 [749.606569ms]
Mar 16 10:52:05.144: INFO: Created: latency-svc-2lvf8
Mar 16 10:52:05.147: INFO: Got endpoints: latency-svc-q7xlh [750.285045ms]
Mar 16 10:52:05.191: INFO: Created: latency-svc-gfchv
Mar 16 10:52:05.197: INFO: Got endpoints: latency-svc-kn6cl [750.719912ms]
Mar 16 10:52:05.242: INFO: Created: latency-svc-cpths
Mar 16 10:52:05.248: INFO: Got endpoints: latency-svc-pxjcv [751.641869ms]
Mar 16 10:52:05.291: INFO: Created: latency-svc-hbvg5
Mar 16 10:52:05.297: INFO: Got endpoints: latency-svc-2jhjb [750.683968ms]
Mar 16 10:52:05.343: INFO: Created: latency-svc-rhlft
Mar 16 10:52:05.352: INFO: Got endpoints: latency-svc-68vwt [755.73852ms]
Mar 16 10:52:05.397: INFO: Created: latency-svc-q4zwb
Mar 16 10:52:05.400: INFO: Got endpoints: latency-svc-6h522 [753.387246ms]
Mar 16 10:52:05.449: INFO: Got endpoints: latency-svc-6qznb [749.426374ms]
Mar 16 10:52:05.449: INFO: Created: latency-svc-9pvh6
Mar 16 10:52:05.494: INFO: Created: latency-svc-76vzx
Mar 16 10:52:05.498: INFO: Got endpoints: latency-svc-2zjtx [751.600465ms]
Mar 16 10:52:05.543: INFO: Created: latency-svc-7fdqz
Mar 16 10:52:05.549: INFO: Got endpoints: latency-svc-rq9qc [752.886911ms]
Mar 16 10:52:05.593: INFO: Created: latency-svc-llrfv
Mar 16 10:52:05.597: INFO: Got endpoints: latency-svc-2cvdr [750.068491ms]
Mar 16 10:52:05.644: INFO: Created: latency-svc-q28kz
Mar 16 10:52:05.648: INFO: Got endpoints: latency-svc-dz72h [748.581035ms]
Mar 16 10:52:05.691: INFO: Created: latency-svc-txpjj
Mar 16 10:52:05.697: INFO: Got endpoints: latency-svc-mkctf [750.381046ms]
Mar 16 10:52:05.742: INFO: Created: latency-svc-bl4jz
Mar 16 10:52:05.746: INFO: Got endpoints: latency-svc-vfhpb [747.725561ms]
Mar 16 10:52:05.792: INFO: Created: latency-svc-fb2zq
Mar 16 10:52:05.796: INFO: Got endpoints: latency-svc-2lvf8 [749.506109ms]
Mar 16 10:52:05.841: INFO: Created: latency-svc-7pw9v
Mar 16 10:52:05.847: INFO: Got endpoints: latency-svc-gfchv [750.447628ms]
Mar 16 10:52:05.890: INFO: Created: latency-svc-qws2d
Mar 16 10:52:05.897: INFO: Got endpoints: latency-svc-cpths [750.542212ms]
Mar 16 10:52:05.941: INFO: Created: latency-svc-cdk7l
Mar 16 10:52:05.947: INFO: Got endpoints: latency-svc-hbvg5 [749.577223ms]
Mar 16 10:52:05.993: INFO: Created: latency-svc-tgnb5
Mar 16 10:52:05.996: INFO: Got endpoints: latency-svc-rhlft [748.427638ms]
Mar 16 10:52:06.040: INFO: Created: latency-svc-6wpxm
Mar 16 10:52:06.047: INFO: Got endpoints: latency-svc-q4zwb [749.781666ms]
Mar 16 10:52:06.097: INFO: Got endpoints: latency-svc-9pvh6 [744.862665ms]
Mar 16 10:52:06.147: INFO: Got endpoints: latency-svc-76vzx [746.684735ms]
Mar 16 10:52:06.197: INFO: Got endpoints: latency-svc-7fdqz [748.284261ms]
Mar 16 10:52:06.247: INFO: Got endpoints: latency-svc-llrfv [748.919625ms]
Mar 16 10:52:06.297: INFO: Got endpoints: latency-svc-q28kz [747.938381ms]
Mar 16 10:52:06.346: INFO: Got endpoints: latency-svc-txpjj [749.580572ms]
Mar 16 10:52:06.397: INFO: Got endpoints: latency-svc-bl4jz [749.170479ms]
Mar 16 10:52:06.447: INFO: Got endpoints: latency-svc-fb2zq [750.030224ms]
Mar 16 10:52:06.497: INFO: Got endpoints: latency-svc-7pw9v [750.3424ms]
Mar 16 10:52:06.547: INFO: Got endpoints: latency-svc-qws2d [751.284387ms]
Mar 16 10:52:06.597: INFO: Got endpoints: latency-svc-cdk7l [749.4971ms]
Mar 16 10:52:06.649: INFO: Got endpoints: latency-svc-tgnb5 [751.638165ms]
Mar 16 10:52:06.701: INFO: Got endpoints: latency-svc-6wpxm [754.444697ms]
Mar 16 10:52:06.701: INFO: Latencies: [95.552768ms 95.712395ms 97.25874ms 97.888672ms 101.00761ms 102.741485ms 106.065159ms 108.346069ms 110.102393ms 110.740518ms 112.379786ms 112.672392ms 113.701123ms 113.756586ms 113.946363ms 117.480707ms 119.538ms 124.051488ms 125.357534ms 127.52321ms 129.038921ms 150.585825ms 165.877605ms 175.565329ms 175.649094ms 176.036423ms 176.221145ms 184.931872ms 188.610111ms 193.477738ms 193.916078ms 194.906339ms 204.045586ms 231.976263ms 237.477285ms 261.51524ms 261.528389ms 261.84715ms 261.90946ms 282.265395ms 339.601935ms 342.654478ms 377.759663ms 420.14615ms 465.401584ms 513.766965ms 545.42069ms 585.13248ms 633.716101ms 687.407206ms 721.807773ms 741.799246ms 743.368841ms 744.244552ms 744.289389ms 744.862665ms 744.986778ms 745.328717ms 746.104827ms 746.348447ms 746.424755ms 746.684735ms 747.30539ms 747.35138ms 747.634623ms 747.725561ms 747.938381ms 747.957836ms 747.972606ms 748.031668ms 748.1135ms 748.121692ms 748.176972ms 748.194661ms 748.284261ms 748.309136ms 748.427638ms 748.486221ms 748.528115ms 748.539182ms 748.581035ms 748.645009ms 748.774281ms 748.816075ms 748.862188ms 748.885172ms 748.897544ms 748.90081ms 748.918044ms 748.919625ms 749.00607ms 749.081724ms 749.170479ms 749.214831ms 749.240537ms 749.255295ms 749.283629ms 749.289772ms 749.304404ms 749.324013ms 749.324328ms 749.396379ms 749.404818ms 749.405143ms 749.406569ms 749.414885ms 749.426374ms 749.440774ms 749.454251ms 749.472546ms 749.487123ms 749.4971ms 749.506109ms 749.577223ms 749.580572ms 749.606569ms 749.665037ms 749.698315ms 749.699031ms 749.708911ms 749.726512ms 749.733884ms 749.748134ms 749.781666ms 749.834591ms 749.860396ms 749.874109ms 749.877898ms 749.884604ms 749.913925ms 749.97399ms 750.030224ms 750.068491ms 750.080348ms 750.105026ms 750.121138ms 750.220589ms 750.236376ms 750.247598ms 750.268638ms 750.27571ms 750.285045ms 750.301529ms 750.30414ms 750.3424ms 750.381046ms 750.382287ms 750.403756ms 750.423141ms 750.447628ms 750.455709ms 750.542212ms 750.55403ms 750.576617ms 750.622062ms 750.635418ms 750.681259ms 750.683968ms 750.696596ms 750.719912ms 750.723804ms 750.792412ms 750.859474ms 750.880457ms 750.963984ms 750.977514ms 750.985865ms 751.079098ms 751.159857ms 751.201021ms 751.284387ms 751.336501ms 751.534162ms 751.600465ms 751.638165ms 751.641869ms 751.675025ms 751.698839ms 751.889707ms 751.928225ms 751.996637ms 752.053654ms 752.081372ms 752.090967ms 752.150253ms 752.185137ms 752.274356ms 752.301138ms 752.319182ms 752.530971ms 752.886911ms 752.945191ms 753.387246ms 754.444697ms 754.569484ms 755.425372ms 755.625184ms 755.73852ms 756.086051ms 758.080336ms]
Mar 16 10:52:06.701: INFO: 50 %ile: 749.324328ms
Mar 16 10:52:06.701: INFO: 90 %ile: 751.996637ms
Mar 16 10:52:06.701: INFO: 99 %ile: 756.086051ms
Mar 16 10:52:06.701: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Mar 16 10:52:06.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7070" for this suite. 03/16/23 10:52:06.794
------------------------------
• [11.805 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:51:55.079
    Mar 16 10:51:55.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svc-latency 03/16/23 10:51:55.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:51:55.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:51:55.527
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 16 10:51:55.704: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7070 03/16/23 10:51:55.706
    I0316 10:51:55.797060    8588 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7070, replica count: 1
    I0316 10:51:56.901321    8588 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0316 10:51:57.901799    8588 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:51:58.096: INFO: Created: latency-svc-ltj8w
    Mar 16 10:51:58.099: INFO: Got endpoints: latency-svc-ltj8w [96.852341ms]
    Mar 16 10:51:58.194: INFO: Created: latency-svc-ch5rp
    Mar 16 10:51:58.198: INFO: Got endpoints: latency-svc-ch5rp [97.888672ms]
    Mar 16 10:51:58.198: INFO: Created: latency-svc-pw5ns
    Mar 16 10:51:58.202: INFO: Created: latency-svc-c2jtb
    Mar 16 10:51:58.275: INFO: Created: latency-svc-hgrnw
    Mar 16 10:51:58.275: INFO: Created: latency-svc-59mxh
    Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-pw5ns [176.221145ms]
    Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-hgrnw [175.649094ms]
    Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-59mxh [175.565329ms]
    Mar 16 10:51:58.276: INFO: Got endpoints: latency-svc-c2jtb [176.036423ms]
    Mar 16 10:51:58.281: INFO: Created: latency-svc-jhfwq
    Mar 16 10:51:58.286: INFO: Got endpoints: latency-svc-jhfwq [184.931872ms]
    Mar 16 10:51:58.286: INFO: Created: latency-svc-tcp7v
    Mar 16 10:51:58.290: INFO: Got endpoints: latency-svc-tcp7v [188.610111ms]
    Mar 16 10:51:58.290: INFO: Created: latency-svc-49fsb
    Mar 16 10:51:58.295: INFO: Created: latency-svc-p4q9w
    Mar 16 10:51:58.295: INFO: Got endpoints: latency-svc-49fsb [193.916078ms]
    Mar 16 10:51:58.296: INFO: Got endpoints: latency-svc-p4q9w [194.906339ms]
    Mar 16 10:51:58.298: INFO: Created: latency-svc-xlk6t
    Mar 16 10:51:58.305: INFO: Got endpoints: latency-svc-xlk6t [204.045586ms]
    Mar 16 10:51:58.332: INFO: Created: latency-svc-62kbs
    Mar 16 10:51:58.332: INFO: Got endpoints: latency-svc-62kbs [231.976263ms]
    Mar 16 10:51:58.363: INFO: Created: latency-svc-lcflh
    Mar 16 10:51:58.363: INFO: Created: latency-svc-9cqp5
    Mar 16 10:51:58.363: INFO: Created: latency-svc-wb86x
    Mar 16 10:51:58.363: INFO: Created: latency-svc-z776s
    Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-lcflh [261.528389ms]
    Mar 16 10:51:58.363: INFO: Created: latency-svc-45cwr
    Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-z776s [261.90946ms]
    Mar 16 10:51:58.363: INFO: Got endpoints: latency-svc-45cwr [261.51524ms]
    Mar 16 10:51:58.364: INFO: Got endpoints: latency-svc-9cqp5 [261.84715ms]
    Mar 16 10:51:58.364: INFO: Got endpoints: latency-svc-wb86x [165.877605ms]
    Mar 16 10:51:58.372: INFO: Created: latency-svc-cksmg
    Mar 16 10:51:58.377: INFO: Got endpoints: latency-svc-cksmg [101.00761ms]
    Mar 16 10:51:58.377: INFO: Created: latency-svc-c6fjr
    Mar 16 10:51:58.379: INFO: Got endpoints: latency-svc-c6fjr [102.741485ms]
    Mar 16 10:51:58.382: INFO: Created: latency-svc-sv6mb
    Mar 16 10:51:58.384: INFO: Got endpoints: latency-svc-sv6mb [108.346069ms]
    Mar 16 10:51:58.387: INFO: Created: latency-svc-jb25s
    Mar 16 10:51:58.389: INFO: Got endpoints: latency-svc-jb25s [112.672392ms]
    Mar 16 10:51:58.396: INFO: Created: latency-svc-g77ch
    Mar 16 10:51:58.396: INFO: Got endpoints: latency-svc-g77ch [110.102393ms]
    Mar 16 10:51:58.399: INFO: Created: latency-svc-cpclj
    Mar 16 10:51:58.404: INFO: Got endpoints: latency-svc-cpclj [113.946363ms]
    Mar 16 10:51:58.404: INFO: Created: latency-svc-2g7fp
    Mar 16 10:51:58.407: INFO: Got endpoints: latency-svc-2g7fp [112.379786ms]
    Mar 16 10:51:58.408: INFO: Created: latency-svc-klb5t
    Mar 16 10:51:58.409: INFO: Got endpoints: latency-svc-klb5t [113.756586ms]
    Mar 16 10:51:58.414: INFO: Created: latency-svc-6fz9g
    Mar 16 10:51:58.416: INFO: Got endpoints: latency-svc-6fz9g [110.740518ms]
    Mar 16 10:51:58.426: INFO: Created: latency-svc-9zlpc
    Mar 16 10:51:58.428: INFO: Got endpoints: latency-svc-9zlpc [95.552768ms]
    Mar 16 10:51:58.457: INFO: Created: latency-svc-cmpfv
    Mar 16 10:51:58.459: INFO: Got endpoints: latency-svc-cmpfv [95.712395ms]
    Mar 16 10:51:58.465: INFO: Created: latency-svc-4d4pg
    Mar 16 10:51:58.469: INFO: Got endpoints: latency-svc-4d4pg [106.065159ms]
    Mar 16 10:51:58.470: INFO: Created: latency-svc-mbbm8
    Mar 16 10:51:58.477: INFO: Got endpoints: latency-svc-mbbm8 [113.701123ms]
    Mar 16 10:51:58.477: INFO: Created: latency-svc-sfqrm
    Mar 16 10:51:58.481: INFO: Got endpoints: latency-svc-sfqrm [117.480707ms]
    Mar 16 10:51:58.481: INFO: Created: latency-svc-dx8hm
    Mar 16 10:51:58.483: INFO: Got endpoints: latency-svc-dx8hm [119.538ms]
    Mar 16 10:51:58.492: INFO: Created: latency-svc-q4wkc
    Mar 16 10:51:58.501: INFO: Got endpoints: latency-svc-q4wkc [124.051488ms]
    Mar 16 10:51:58.501: INFO: Created: latency-svc-zm5zp
    Mar 16 10:51:58.506: INFO: Got endpoints: latency-svc-zm5zp [127.52321ms]
    Mar 16 10:51:58.506: INFO: Created: latency-svc-j9mhb
    Mar 16 10:51:58.513: INFO: Got endpoints: latency-svc-j9mhb [129.038921ms]
    Mar 16 10:51:58.513: INFO: Created: latency-svc-nbmld
    Mar 16 10:51:58.514: INFO: Got endpoints: latency-svc-nbmld [125.357534ms]
    Mar 16 10:51:58.524: INFO: Created: latency-svc-9gd56
    Mar 16 10:51:58.524: INFO: Created: latency-svc-5v2z9
    Mar 16 10:51:58.525: INFO: Got endpoints: latency-svc-9gd56 [97.25874ms]
    Mar 16 10:51:58.530: INFO: Created: latency-svc-c4ts7
    Mar 16 10:51:58.534: INFO: Created: latency-svc-nrgqx
    Mar 16 10:51:58.538: INFO: Created: latency-svc-wpdrj
    Mar 16 10:51:58.542: INFO: Created: latency-svc-kfncv
    Mar 16 10:51:58.546: INFO: Got endpoints: latency-svc-5v2z9 [150.585825ms]
    Mar 16 10:51:58.553: INFO: Created: latency-svc-6qf6w
    Mar 16 10:51:58.564: INFO: Created: latency-svc-nwpkx
    Mar 16 10:51:58.571: INFO: Created: latency-svc-bwwtm
    Mar 16 10:51:58.575: INFO: Created: latency-svc-7h6tn
    Mar 16 10:51:58.584: INFO: Created: latency-svc-qj6hs
    Mar 16 10:51:58.595: INFO: Created: latency-svc-n9647
    Mar 16 10:51:58.601: INFO: Got endpoints: latency-svc-c4ts7 [193.477738ms]
    Mar 16 10:51:58.607: INFO: Created: latency-svc-74x2f
    Mar 16 10:51:58.612: INFO: Created: latency-svc-x6m4x
    Mar 16 10:51:58.617: INFO: Created: latency-svc-w8hm9
    Mar 16 10:51:58.623: INFO: Created: latency-svc-8nzgf
    Mar 16 10:51:58.641: INFO: Created: latency-svc-jwggl
    Mar 16 10:51:58.647: INFO: Got endpoints: latency-svc-nrgqx [237.477285ms]
    Mar 16 10:51:58.698: INFO: Got endpoints: latency-svc-wpdrj [282.265395ms]
    Mar 16 10:51:58.699: INFO: Created: latency-svc-d957m
    Mar 16 10:51:58.741: INFO: Created: latency-svc-skmz8
    Mar 16 10:51:58.746: INFO: Got endpoints: latency-svc-kfncv [342.654478ms]
    Mar 16 10:51:58.798: INFO: Created: latency-svc-lk487
    Mar 16 10:51:58.799: INFO: Got endpoints: latency-svc-6qf6w [339.601935ms]
    Mar 16 10:51:58.841: INFO: Created: latency-svc-mpzf6
    Mar 16 10:51:58.847: INFO: Got endpoints: latency-svc-nwpkx [377.759663ms]
    Mar 16 10:51:58.894: INFO: Created: latency-svc-g6jnt
    Mar 16 10:51:58.897: INFO: Got endpoints: latency-svc-bwwtm [420.14615ms]
    Mar 16 10:51:58.942: INFO: Created: latency-svc-s7jrd
    Mar 16 10:51:58.947: INFO: Got endpoints: latency-svc-7h6tn [465.401584ms]
    Mar 16 10:51:58.991: INFO: Created: latency-svc-7nbzx
    Mar 16 10:51:58.997: INFO: Got endpoints: latency-svc-qj6hs [513.766965ms]
    Mar 16 10:51:59.041: INFO: Created: latency-svc-ndkk6
    Mar 16 10:51:59.046: INFO: Got endpoints: latency-svc-n9647 [545.42069ms]
    Mar 16 10:51:59.091: INFO: Created: latency-svc-gnh4n
    Mar 16 10:51:59.098: INFO: Got endpoints: latency-svc-74x2f [585.13248ms]
    Mar 16 10:51:59.143: INFO: Created: latency-svc-pgjtq
    Mar 16 10:51:59.147: INFO: Got endpoints: latency-svc-x6m4x [633.716101ms]
    Mar 16 10:51:59.192: INFO: Created: latency-svc-x66ms
    Mar 16 10:51:59.202: INFO: Got endpoints: latency-svc-w8hm9 [687.407206ms]
    Mar 16 10:51:59.242: INFO: Created: latency-svc-x9gn6
    Mar 16 10:51:59.247: INFO: Got endpoints: latency-svc-8nzgf [721.807773ms]
    Mar 16 10:51:59.296: INFO: Created: latency-svc-58xmb
    Mar 16 10:51:59.297: INFO: Got endpoints: latency-svc-jwggl [750.403756ms]
    Mar 16 10:51:59.343: INFO: Created: latency-svc-g5vbm
    Mar 16 10:51:59.347: INFO: Got endpoints: latency-svc-d957m [746.104827ms]
    Mar 16 10:51:59.392: INFO: Created: latency-svc-km9s5
    Mar 16 10:51:59.398: INFO: Got endpoints: latency-svc-skmz8 [750.977514ms]
    Mar 16 10:51:59.450: INFO: Got endpoints: latency-svc-lk487 [752.053654ms]
    Mar 16 10:51:59.451: INFO: Created: latency-svc-bkf27
    Mar 16 10:51:59.493: INFO: Created: latency-svc-rsfhr
    Mar 16 10:51:59.498: INFO: Got endpoints: latency-svc-mpzf6 [751.889707ms]
    Mar 16 10:51:59.545: INFO: Created: latency-svc-vbcps
    Mar 16 10:51:59.547: INFO: Got endpoints: latency-svc-g6jnt [748.176972ms]
    Mar 16 10:51:59.594: INFO: Created: latency-svc-qhdmt
    Mar 16 10:51:59.596: INFO: Got endpoints: latency-svc-s7jrd [749.00607ms]
    Mar 16 10:51:59.642: INFO: Created: latency-svc-mn924
    Mar 16 10:51:59.646: INFO: Got endpoints: latency-svc-7nbzx [749.240537ms]
    Mar 16 10:51:59.691: INFO: Created: latency-svc-rk59h
    Mar 16 10:51:59.696: INFO: Got endpoints: latency-svc-ndkk6 [749.877898ms]
    Mar 16 10:51:59.742: INFO: Created: latency-svc-z2t5j
    Mar 16 10:51:59.750: INFO: Got endpoints: latency-svc-gnh4n [752.530971ms]
    Mar 16 10:51:59.791: INFO: Created: latency-svc-nlgkp
    Mar 16 10:51:59.797: INFO: Got endpoints: latency-svc-pgjtq [750.121138ms]
    Mar 16 10:51:59.844: INFO: Created: latency-svc-qvkmr
    Mar 16 10:51:59.846: INFO: Got endpoints: latency-svc-x66ms [748.194661ms]
    Mar 16 10:51:59.891: INFO: Created: latency-svc-pbpbv
    Mar 16 10:51:59.897: INFO: Got endpoints: latency-svc-x9gn6 [750.576617ms]
    Mar 16 10:51:59.940: INFO: Created: latency-svc-br68t
    Mar 16 10:51:59.947: INFO: Got endpoints: latency-svc-58xmb [744.986778ms]
    Mar 16 10:51:59.991: INFO: Created: latency-svc-zwhfs
    Mar 16 10:51:59.997: INFO: Got endpoints: latency-svc-g5vbm [750.105026ms]
    Mar 16 10:52:00.041: INFO: Created: latency-svc-jnm7f
    Mar 16 10:52:00.046: INFO: Got endpoints: latency-svc-km9s5 [749.698315ms]
    Mar 16 10:52:00.093: INFO: Created: latency-svc-k2b5l
    Mar 16 10:52:00.097: INFO: Got endpoints: latency-svc-bkf27 [749.414885ms]
    Mar 16 10:52:00.141: INFO: Created: latency-svc-mzrct
    Mar 16 10:52:00.147: INFO: Got endpoints: latency-svc-rsfhr [748.816075ms]
    Mar 16 10:52:00.191: INFO: Created: latency-svc-wfr5f
    Mar 16 10:52:00.197: INFO: Got endpoints: latency-svc-vbcps [746.424755ms]
    Mar 16 10:52:00.241: INFO: Created: latency-svc-44ccr
    Mar 16 10:52:00.247: INFO: Got endpoints: latency-svc-qhdmt [748.774281ms]
    Mar 16 10:52:00.292: INFO: Created: latency-svc-zlwc7
    Mar 16 10:52:00.305: INFO: Got endpoints: latency-svc-mn924 [758.080336ms]
    Mar 16 10:52:00.341: INFO: Created: latency-svc-7jckz
    Mar 16 10:52:00.346: INFO: Got endpoints: latency-svc-rk59h [750.268638ms]
    Mar 16 10:52:00.399: INFO: Got endpoints: latency-svc-z2t5j [751.996637ms]
    Mar 16 10:52:00.402: INFO: Created: latency-svc-qcglb
    Mar 16 10:52:00.444: INFO: Created: latency-svc-nv7kg
    Mar 16 10:52:00.447: INFO: Got endpoints: latency-svc-nlgkp [750.236376ms]
    Mar 16 10:52:00.495: INFO: Created: latency-svc-hvp49
    Mar 16 10:52:00.496: INFO: Got endpoints: latency-svc-qvkmr [746.348447ms]
    Mar 16 10:52:00.545: INFO: Created: latency-svc-xl28m
    Mar 16 10:52:00.546: INFO: Got endpoints: latency-svc-pbpbv [749.487123ms]
    Mar 16 10:52:00.590: INFO: Created: latency-svc-wgzfd
    Mar 16 10:52:00.597: INFO: Got endpoints: latency-svc-br68t [750.792412ms]
    Mar 16 10:52:00.641: INFO: Created: latency-svc-tds4w
    Mar 16 10:52:00.648: INFO: Got endpoints: latency-svc-zwhfs [750.985865ms]
    Mar 16 10:52:00.692: INFO: Created: latency-svc-rt45x
    Mar 16 10:52:00.697: INFO: Got endpoints: latency-svc-jnm7f [750.423141ms]
    Mar 16 10:52:00.743: INFO: Created: latency-svc-wsbsz
    Mar 16 10:52:00.753: INFO: Got endpoints: latency-svc-k2b5l [755.625184ms]
    Mar 16 10:52:00.792: INFO: Created: latency-svc-h9bjp
    Mar 16 10:52:00.797: INFO: Got endpoints: latency-svc-mzrct [750.55403ms]
    Mar 16 10:52:00.849: INFO: Got endpoints: latency-svc-wfr5f [752.081372ms]
    Mar 16 10:52:00.849: INFO: Created: latency-svc-9hdgc
    Mar 16 10:52:00.892: INFO: Created: latency-svc-sgm8q
    Mar 16 10:52:00.897: INFO: Got endpoints: latency-svc-44ccr [749.884604ms]
    Mar 16 10:52:00.943: INFO: Created: latency-svc-8fhks
    Mar 16 10:52:00.948: INFO: Got endpoints: latency-svc-zlwc7 [750.681259ms]
    Mar 16 10:52:00.994: INFO: Created: latency-svc-8clx8
    Mar 16 10:52:00.998: INFO: Got endpoints: latency-svc-7jckz [751.159857ms]
    Mar 16 10:52:01.042: INFO: Created: latency-svc-qslfk
    Mar 16 10:52:01.047: INFO: Got endpoints: latency-svc-qcglb [741.799246ms]
    Mar 16 10:52:01.097: INFO: Got endpoints: latency-svc-nv7kg [750.696596ms]
    Mar 16 10:52:01.097: INFO: Created: latency-svc-prknt
    Mar 16 10:52:01.145: INFO: Created: latency-svc-6hctp
    Mar 16 10:52:01.147: INFO: Got endpoints: latency-svc-hvp49 [748.90081ms]
    Mar 16 10:52:01.192: INFO: Created: latency-svc-qh9s8
    Mar 16 10:52:01.199: INFO: Got endpoints: latency-svc-xl28m [751.928225ms]
    Mar 16 10:52:01.243: INFO: Created: latency-svc-x7xm4
    Mar 16 10:52:01.247: INFO: Got endpoints: latency-svc-wgzfd [751.079098ms]
    Mar 16 10:52:01.293: INFO: Created: latency-svc-mg7t4
    Mar 16 10:52:01.296: INFO: Got endpoints: latency-svc-tds4w [749.748134ms]
    Mar 16 10:52:01.343: INFO: Created: latency-svc-9zlbx
    Mar 16 10:52:01.347: INFO: Got endpoints: latency-svc-rt45x [749.733884ms]
    Mar 16 10:52:01.390: INFO: Created: latency-svc-v92wx
    Mar 16 10:52:01.397: INFO: Got endpoints: latency-svc-wsbsz [748.309136ms]
    Mar 16 10:52:01.445: INFO: Created: latency-svc-zsdx9
    Mar 16 10:52:01.448: INFO: Got endpoints: latency-svc-h9bjp [751.336501ms]
    Mar 16 10:52:01.491: INFO: Created: latency-svc-htbkt
    Mar 16 10:52:01.497: INFO: Got endpoints: latency-svc-9hdgc [744.289389ms]
    Mar 16 10:52:01.544: INFO: Created: latency-svc-bxmc2
    Mar 16 10:52:01.547: INFO: Got endpoints: latency-svc-sgm8q [749.406569ms]
    Mar 16 10:52:01.593: INFO: Created: latency-svc-2zrr9
    Mar 16 10:52:01.598: INFO: Got endpoints: latency-svc-8fhks [749.214831ms]
    Mar 16 10:52:01.642: INFO: Created: latency-svc-lhr9d
    Mar 16 10:52:01.647: INFO: Got endpoints: latency-svc-8clx8 [750.455709ms]
    Mar 16 10:52:01.697: INFO: Got endpoints: latency-svc-qslfk [749.324013ms]
    Mar 16 10:52:01.698: INFO: Created: latency-svc-7d7h9
    Mar 16 10:52:01.743: INFO: Created: latency-svc-chnq6
    Mar 16 10:52:01.746: INFO: Got endpoints: latency-svc-prknt [748.1135ms]
    Mar 16 10:52:01.792: INFO: Created: latency-svc-95wjg
    Mar 16 10:52:01.797: INFO: Got endpoints: latency-svc-6hctp [750.30414ms]
    Mar 16 10:52:01.843: INFO: Created: latency-svc-c5x4x
    Mar 16 10:52:01.846: INFO: Got endpoints: latency-svc-qh9s8 [749.324328ms]
    Mar 16 10:52:01.892: INFO: Created: latency-svc-m2fkn
    Mar 16 10:52:01.896: INFO: Got endpoints: latency-svc-x7xm4 [748.486221ms]
    Mar 16 10:52:01.942: INFO: Created: latency-svc-nsw2f
    Mar 16 10:52:01.947: INFO: Got endpoints: latency-svc-mg7t4 [748.539182ms]
    Mar 16 10:52:01.991: INFO: Created: latency-svc-2mqzf
    Mar 16 10:52:01.997: INFO: Got endpoints: latency-svc-9zlbx [749.913925ms]
    Mar 16 10:52:02.042: INFO: Created: latency-svc-rrp84
    Mar 16 10:52:02.048: INFO: Got endpoints: latency-svc-v92wx [752.274356ms]
    Mar 16 10:52:02.091: INFO: Created: latency-svc-tnwx8
    Mar 16 10:52:02.099: INFO: Got endpoints: latency-svc-zsdx9 [752.185137ms]
    Mar 16 10:52:02.143: INFO: Created: latency-svc-zxdmr
    Mar 16 10:52:02.146: INFO: Got endpoints: latency-svc-htbkt [749.405143ms]
    Mar 16 10:52:02.194: INFO: Created: latency-svc-lhhmp
    Mar 16 10:52:02.196: INFO: Got endpoints: latency-svc-bxmc2 [747.957836ms]
    Mar 16 10:52:02.241: INFO: Created: latency-svc-f7llj
    Mar 16 10:52:02.247: INFO: Got endpoints: latency-svc-2zrr9 [749.454251ms]
    Mar 16 10:52:02.292: INFO: Created: latency-svc-79xtp
    Mar 16 10:52:02.299: INFO: Got endpoints: latency-svc-lhr9d [752.945191ms]
    Mar 16 10:52:02.342: INFO: Created: latency-svc-9ztcl
    Mar 16 10:52:02.346: INFO: Got endpoints: latency-svc-7d7h9 [748.121692ms]
    Mar 16 10:52:02.398: INFO: Got endpoints: latency-svc-chnq6 [750.963984ms]
    Mar 16 10:52:02.399: INFO: Created: latency-svc-vhwdn
    Mar 16 10:52:02.441: INFO: Created: latency-svc-7rbdh
    Mar 16 10:52:02.446: INFO: Got endpoints: latency-svc-95wjg [749.081724ms]
    Mar 16 10:52:02.493: INFO: Created: latency-svc-tnkjp
    Mar 16 10:52:02.497: INFO: Got endpoints: latency-svc-c5x4x [750.27571ms]
    Mar 16 10:52:02.541: INFO: Created: latency-svc-xql9s
    Mar 16 10:52:02.549: INFO: Got endpoints: latency-svc-m2fkn [751.698839ms]
    Mar 16 10:52:02.591: INFO: Created: latency-svc-flkh4
    Mar 16 10:52:02.597: INFO: Got endpoints: latency-svc-nsw2f [750.301529ms]
    Mar 16 10:52:02.644: INFO: Created: latency-svc-46wpz
    Mar 16 10:52:02.647: INFO: Got endpoints: latency-svc-2mqzf [750.723804ms]
    Mar 16 10:52:02.691: INFO: Created: latency-svc-mpg9m
    Mar 16 10:52:02.697: INFO: Got endpoints: latency-svc-rrp84 [749.255295ms]
    Mar 16 10:52:02.741: INFO: Created: latency-svc-xjlhr
    Mar 16 10:52:02.747: INFO: Got endpoints: latency-svc-tnwx8 [749.699031ms]
    Mar 16 10:52:02.792: INFO: Created: latency-svc-qkzdp
    Mar 16 10:52:02.798: INFO: Got endpoints: latency-svc-zxdmr [749.404818ms]
    Mar 16 10:52:02.844: INFO: Created: latency-svc-8z5hn
    Mar 16 10:52:02.846: INFO: Got endpoints: latency-svc-lhhmp [747.30539ms]
    Mar 16 10:52:02.892: INFO: Created: latency-svc-glcsj
    Mar 16 10:52:02.896: INFO: Got endpoints: latency-svc-f7llj [750.220589ms]
    Mar 16 10:52:02.941: INFO: Created: latency-svc-dkrbm
    Mar 16 10:52:02.949: INFO: Got endpoints: latency-svc-79xtp [752.150253ms]
    Mar 16 10:52:02.992: INFO: Created: latency-svc-4v9f5
    Mar 16 10:52:02.997: INFO: Got endpoints: latency-svc-9ztcl [750.247598ms]
    Mar 16 10:52:03.043: INFO: Created: latency-svc-prtsv
    Mar 16 10:52:03.047: INFO: Got endpoints: latency-svc-vhwdn [747.35138ms]
    Mar 16 10:52:03.091: INFO: Created: latency-svc-gbd8b
    Mar 16 10:52:03.098: INFO: Got endpoints: latency-svc-7rbdh [751.534162ms]
    Mar 16 10:52:03.143: INFO: Created: latency-svc-ft79l
    Mar 16 10:52:03.147: INFO: Got endpoints: latency-svc-tnkjp [748.918044ms]
    Mar 16 10:52:03.192: INFO: Created: latency-svc-kdd55
    Mar 16 10:52:03.197: INFO: Got endpoints: latency-svc-xql9s [750.859474ms]
    Mar 16 10:52:03.242: INFO: Created: latency-svc-ln6pl
    Mar 16 10:52:03.247: INFO: Got endpoints: latency-svc-flkh4 [750.080348ms]
    Mar 16 10:52:03.291: INFO: Created: latency-svc-fgr99
    Mar 16 10:52:03.297: INFO: Got endpoints: latency-svc-46wpz [747.634623ms]
    Mar 16 10:52:03.341: INFO: Created: latency-svc-qvfmk
    Mar 16 10:52:03.346: INFO: Got endpoints: latency-svc-mpg9m [749.289772ms]
    Mar 16 10:52:03.391: INFO: Created: latency-svc-g64cb
    Mar 16 10:52:03.396: INFO: Got endpoints: latency-svc-xjlhr [749.304404ms]
    Mar 16 10:52:03.441: INFO: Created: latency-svc-wqgd9
    Mar 16 10:52:03.447: INFO: Got endpoints: latency-svc-qkzdp [749.874109ms]
    Mar 16 10:52:03.491: INFO: Created: latency-svc-pb6bz
    Mar 16 10:52:03.497: INFO: Got endpoints: latency-svc-8z5hn [749.726512ms]
    Mar 16 10:52:03.542: INFO: Created: latency-svc-t2wg6
    Mar 16 10:52:03.547: INFO: Got endpoints: latency-svc-glcsj [748.862188ms]
    Mar 16 10:52:03.591: INFO: Created: latency-svc-772fp
    Mar 16 10:52:03.597: INFO: Got endpoints: latency-svc-dkrbm [750.880457ms]
    Mar 16 10:52:03.642: INFO: Created: latency-svc-twgdp
    Mar 16 10:52:03.646: INFO: Got endpoints: latency-svc-4v9f5 [749.97399ms]
    Mar 16 10:52:03.692: INFO: Created: latency-svc-5tlnx
    Mar 16 10:52:03.697: INFO: Got endpoints: latency-svc-prtsv [747.972606ms]
    Mar 16 10:52:03.746: INFO: Created: latency-svc-5tk5q
    Mar 16 10:52:03.753: INFO: Got endpoints: latency-svc-gbd8b [755.425372ms]
    Mar 16 10:52:03.791: INFO: Created: latency-svc-lkm8c
    Mar 16 10:52:03.796: INFO: Got endpoints: latency-svc-ft79l [749.283629ms]
    Mar 16 10:52:03.847: INFO: Got endpoints: latency-svc-kdd55 [749.440774ms]
    Mar 16 10:52:03.848: INFO: Created: latency-svc-v7pjz
    Mar 16 10:52:03.892: INFO: Created: latency-svc-t62m5
    Mar 16 10:52:03.898: INFO: Got endpoints: latency-svc-ln6pl [750.622062ms]
    Mar 16 10:52:03.943: INFO: Created: latency-svc-qghdr
    Mar 16 10:52:03.947: INFO: Got endpoints: latency-svc-fgr99 [749.665037ms]
    Mar 16 10:52:03.996: INFO: Created: latency-svc-bttct
    Mar 16 10:52:03.998: INFO: Got endpoints: latency-svc-qvfmk [751.201021ms]
    Mar 16 10:52:04.041: INFO: Created: latency-svc-zvmhc
    Mar 16 10:52:04.047: INFO: Got endpoints: latency-svc-g64cb [750.382287ms]
    Mar 16 10:52:04.096: INFO: Created: latency-svc-9wqcd
    Mar 16 10:52:04.102: INFO: Got endpoints: latency-svc-wqgd9 [756.086051ms]
    Mar 16 10:52:04.143: INFO: Created: latency-svc-6p77s
    Mar 16 10:52:04.147: INFO: Got endpoints: latency-svc-pb6bz [750.635418ms]
    Mar 16 10:52:04.201: INFO: Got endpoints: latency-svc-t2wg6 [754.569484ms]
    Mar 16 10:52:04.202: INFO: Created: latency-svc-b4dvn
    Mar 16 10:52:04.241: INFO: Created: latency-svc-klgrr
    Mar 16 10:52:04.246: INFO: Got endpoints: latency-svc-772fp [749.860396ms]
    Mar 16 10:52:04.297: INFO: Created: latency-svc-pbvnr
    Mar 16 10:52:04.298: INFO: Got endpoints: latency-svc-twgdp [751.675025ms]
    Mar 16 10:52:04.342: INFO: Created: latency-svc-n7rjf
    Mar 16 10:52:04.347: INFO: Got endpoints: latency-svc-5tlnx [749.472546ms]
    Mar 16 10:52:04.393: INFO: Created: latency-svc-n5ppt
    Mar 16 10:52:04.396: INFO: Got endpoints: latency-svc-5tk5q [749.708911ms]
    Mar 16 10:52:04.445: INFO: Created: latency-svc-g9mv2
    Mar 16 10:52:04.446: INFO: Got endpoints: latency-svc-lkm8c [749.396379ms]
    Mar 16 10:52:04.491: INFO: Created: latency-svc-q7xlh
    Mar 16 10:52:04.496: INFO: Got endpoints: latency-svc-v7pjz [743.368841ms]
    Mar 16 10:52:04.541: INFO: Created: latency-svc-kn6cl
    Mar 16 10:52:04.546: INFO: Got endpoints: latency-svc-t62m5 [749.834591ms]
    Mar 16 10:52:04.592: INFO: Created: latency-svc-pxjcv
    Mar 16 10:52:04.596: INFO: Got endpoints: latency-svc-qghdr [748.885172ms]
    Mar 16 10:52:04.646: INFO: Created: latency-svc-2jhjb
    Mar 16 10:52:04.647: INFO: Got endpoints: latency-svc-bttct [748.528115ms]
    Mar 16 10:52:04.697: INFO: Created: latency-svc-68vwt
    Mar 16 10:52:04.699: INFO: Got endpoints: latency-svc-zvmhc [752.319182ms]
    Mar 16 10:52:04.742: INFO: Created: latency-svc-6h522
    Mar 16 10:52:04.747: INFO: Got endpoints: latency-svc-9wqcd [748.645009ms]
    Mar 16 10:52:04.794: INFO: Created: latency-svc-6qznb
    Mar 16 10:52:04.796: INFO: Got endpoints: latency-svc-6p77s [748.897544ms]
    Mar 16 10:52:04.842: INFO: Created: latency-svc-2zjtx
    Mar 16 10:52:04.847: INFO: Got endpoints: latency-svc-b4dvn [744.244552ms]
    Mar 16 10:52:04.891: INFO: Created: latency-svc-rq9qc
    Mar 16 10:52:04.899: INFO: Got endpoints: latency-svc-klgrr [752.301138ms]
    Mar 16 10:52:04.942: INFO: Created: latency-svc-2cvdr
    Mar 16 10:52:04.947: INFO: Got endpoints: latency-svc-pbvnr [745.328717ms]
    Mar 16 10:52:04.999: INFO: Created: latency-svc-dz72h
    Mar 16 10:52:04.999: INFO: Got endpoints: latency-svc-n7rjf [752.090967ms]
    Mar 16 10:52:05.045: INFO: Created: latency-svc-mkctf
    Mar 16 10:52:05.046: INFO: Got endpoints: latency-svc-n5ppt [748.031668ms]
    Mar 16 10:52:05.095: INFO: Created: latency-svc-vfhpb
    Mar 16 10:52:05.096: INFO: Got endpoints: latency-svc-g9mv2 [749.606569ms]
    Mar 16 10:52:05.144: INFO: Created: latency-svc-2lvf8
    Mar 16 10:52:05.147: INFO: Got endpoints: latency-svc-q7xlh [750.285045ms]
    Mar 16 10:52:05.191: INFO: Created: latency-svc-gfchv
    Mar 16 10:52:05.197: INFO: Got endpoints: latency-svc-kn6cl [750.719912ms]
    Mar 16 10:52:05.242: INFO: Created: latency-svc-cpths
    Mar 16 10:52:05.248: INFO: Got endpoints: latency-svc-pxjcv [751.641869ms]
    Mar 16 10:52:05.291: INFO: Created: latency-svc-hbvg5
    Mar 16 10:52:05.297: INFO: Got endpoints: latency-svc-2jhjb [750.683968ms]
    Mar 16 10:52:05.343: INFO: Created: latency-svc-rhlft
    Mar 16 10:52:05.352: INFO: Got endpoints: latency-svc-68vwt [755.73852ms]
    Mar 16 10:52:05.397: INFO: Created: latency-svc-q4zwb
    Mar 16 10:52:05.400: INFO: Got endpoints: latency-svc-6h522 [753.387246ms]
    Mar 16 10:52:05.449: INFO: Got endpoints: latency-svc-6qznb [749.426374ms]
    Mar 16 10:52:05.449: INFO: Created: latency-svc-9pvh6
    Mar 16 10:52:05.494: INFO: Created: latency-svc-76vzx
    Mar 16 10:52:05.498: INFO: Got endpoints: latency-svc-2zjtx [751.600465ms]
    Mar 16 10:52:05.543: INFO: Created: latency-svc-7fdqz
    Mar 16 10:52:05.549: INFO: Got endpoints: latency-svc-rq9qc [752.886911ms]
    Mar 16 10:52:05.593: INFO: Created: latency-svc-llrfv
    Mar 16 10:52:05.597: INFO: Got endpoints: latency-svc-2cvdr [750.068491ms]
    Mar 16 10:52:05.644: INFO: Created: latency-svc-q28kz
    Mar 16 10:52:05.648: INFO: Got endpoints: latency-svc-dz72h [748.581035ms]
    Mar 16 10:52:05.691: INFO: Created: latency-svc-txpjj
    Mar 16 10:52:05.697: INFO: Got endpoints: latency-svc-mkctf [750.381046ms]
    Mar 16 10:52:05.742: INFO: Created: latency-svc-bl4jz
    Mar 16 10:52:05.746: INFO: Got endpoints: latency-svc-vfhpb [747.725561ms]
    Mar 16 10:52:05.792: INFO: Created: latency-svc-fb2zq
    Mar 16 10:52:05.796: INFO: Got endpoints: latency-svc-2lvf8 [749.506109ms]
    Mar 16 10:52:05.841: INFO: Created: latency-svc-7pw9v
    Mar 16 10:52:05.847: INFO: Got endpoints: latency-svc-gfchv [750.447628ms]
    Mar 16 10:52:05.890: INFO: Created: latency-svc-qws2d
    Mar 16 10:52:05.897: INFO: Got endpoints: latency-svc-cpths [750.542212ms]
    Mar 16 10:52:05.941: INFO: Created: latency-svc-cdk7l
    Mar 16 10:52:05.947: INFO: Got endpoints: latency-svc-hbvg5 [749.577223ms]
    Mar 16 10:52:05.993: INFO: Created: latency-svc-tgnb5
    Mar 16 10:52:05.996: INFO: Got endpoints: latency-svc-rhlft [748.427638ms]
    Mar 16 10:52:06.040: INFO: Created: latency-svc-6wpxm
    Mar 16 10:52:06.047: INFO: Got endpoints: latency-svc-q4zwb [749.781666ms]
    Mar 16 10:52:06.097: INFO: Got endpoints: latency-svc-9pvh6 [744.862665ms]
    Mar 16 10:52:06.147: INFO: Got endpoints: latency-svc-76vzx [746.684735ms]
    Mar 16 10:52:06.197: INFO: Got endpoints: latency-svc-7fdqz [748.284261ms]
    Mar 16 10:52:06.247: INFO: Got endpoints: latency-svc-llrfv [748.919625ms]
    Mar 16 10:52:06.297: INFO: Got endpoints: latency-svc-q28kz [747.938381ms]
    Mar 16 10:52:06.346: INFO: Got endpoints: latency-svc-txpjj [749.580572ms]
    Mar 16 10:52:06.397: INFO: Got endpoints: latency-svc-bl4jz [749.170479ms]
    Mar 16 10:52:06.447: INFO: Got endpoints: latency-svc-fb2zq [750.030224ms]
    Mar 16 10:52:06.497: INFO: Got endpoints: latency-svc-7pw9v [750.3424ms]
    Mar 16 10:52:06.547: INFO: Got endpoints: latency-svc-qws2d [751.284387ms]
    Mar 16 10:52:06.597: INFO: Got endpoints: latency-svc-cdk7l [749.4971ms]
    Mar 16 10:52:06.649: INFO: Got endpoints: latency-svc-tgnb5 [751.638165ms]
    Mar 16 10:52:06.701: INFO: Got endpoints: latency-svc-6wpxm [754.444697ms]
    Mar 16 10:52:06.701: INFO: Latencies: [95.552768ms 95.712395ms 97.25874ms 97.888672ms 101.00761ms 102.741485ms 106.065159ms 108.346069ms 110.102393ms 110.740518ms 112.379786ms 112.672392ms 113.701123ms 113.756586ms 113.946363ms 117.480707ms 119.538ms 124.051488ms 125.357534ms 127.52321ms 129.038921ms 150.585825ms 165.877605ms 175.565329ms 175.649094ms 176.036423ms 176.221145ms 184.931872ms 188.610111ms 193.477738ms 193.916078ms 194.906339ms 204.045586ms 231.976263ms 237.477285ms 261.51524ms 261.528389ms 261.84715ms 261.90946ms 282.265395ms 339.601935ms 342.654478ms 377.759663ms 420.14615ms 465.401584ms 513.766965ms 545.42069ms 585.13248ms 633.716101ms 687.407206ms 721.807773ms 741.799246ms 743.368841ms 744.244552ms 744.289389ms 744.862665ms 744.986778ms 745.328717ms 746.104827ms 746.348447ms 746.424755ms 746.684735ms 747.30539ms 747.35138ms 747.634623ms 747.725561ms 747.938381ms 747.957836ms 747.972606ms 748.031668ms 748.1135ms 748.121692ms 748.176972ms 748.194661ms 748.284261ms 748.309136ms 748.427638ms 748.486221ms 748.528115ms 748.539182ms 748.581035ms 748.645009ms 748.774281ms 748.816075ms 748.862188ms 748.885172ms 748.897544ms 748.90081ms 748.918044ms 748.919625ms 749.00607ms 749.081724ms 749.170479ms 749.214831ms 749.240537ms 749.255295ms 749.283629ms 749.289772ms 749.304404ms 749.324013ms 749.324328ms 749.396379ms 749.404818ms 749.405143ms 749.406569ms 749.414885ms 749.426374ms 749.440774ms 749.454251ms 749.472546ms 749.487123ms 749.4971ms 749.506109ms 749.577223ms 749.580572ms 749.606569ms 749.665037ms 749.698315ms 749.699031ms 749.708911ms 749.726512ms 749.733884ms 749.748134ms 749.781666ms 749.834591ms 749.860396ms 749.874109ms 749.877898ms 749.884604ms 749.913925ms 749.97399ms 750.030224ms 750.068491ms 750.080348ms 750.105026ms 750.121138ms 750.220589ms 750.236376ms 750.247598ms 750.268638ms 750.27571ms 750.285045ms 750.301529ms 750.30414ms 750.3424ms 750.381046ms 750.382287ms 750.403756ms 750.423141ms 750.447628ms 750.455709ms 750.542212ms 750.55403ms 750.576617ms 750.622062ms 750.635418ms 750.681259ms 750.683968ms 750.696596ms 750.719912ms 750.723804ms 750.792412ms 750.859474ms 750.880457ms 750.963984ms 750.977514ms 750.985865ms 751.079098ms 751.159857ms 751.201021ms 751.284387ms 751.336501ms 751.534162ms 751.600465ms 751.638165ms 751.641869ms 751.675025ms 751.698839ms 751.889707ms 751.928225ms 751.996637ms 752.053654ms 752.081372ms 752.090967ms 752.150253ms 752.185137ms 752.274356ms 752.301138ms 752.319182ms 752.530971ms 752.886911ms 752.945191ms 753.387246ms 754.444697ms 754.569484ms 755.425372ms 755.625184ms 755.73852ms 756.086051ms 758.080336ms]
    Mar 16 10:52:06.701: INFO: 50 %ile: 749.324328ms
    Mar 16 10:52:06.701: INFO: 90 %ile: 751.996637ms
    Mar 16 10:52:06.701: INFO: 99 %ile: 756.086051ms
    Mar 16 10:52:06.701: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:52:06.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7070" for this suite. 03/16/23 10:52:06.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:52:06.886
Mar 16 10:52:06.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 03/16/23 10:52:06.887
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:07.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:07.332
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/16/23 10:52:07.51
STEP: creating 03/16/23 10:52:07.51
STEP: getting 03/16/23 10:52:07.601
STEP: listing 03/16/23 10:52:07.69
STEP: watching 03/16/23 10:52:07.78
Mar 16 10:52:07.780: INFO: starting watch
STEP: cluster-wide listing 03/16/23 10:52:07.868
STEP: cluster-wide watching 03/16/23 10:52:07.957
Mar 16 10:52:07.958: INFO: starting watch
STEP: patching 03/16/23 10:52:08.046
STEP: updating 03/16/23 10:52:08.138
Mar 16 10:52:08.319: INFO: waiting for watch events with expected annotations
Mar 16 10:52:08.319: INFO: saw patched and updated annotations
STEP: patching /status 03/16/23 10:52:08.32
STEP: updating /status 03/16/23 10:52:08.412
STEP: get /status 03/16/23 10:52:08.596
STEP: deleting 03/16/23 10:52:08.686
STEP: deleting a collection 03/16/23 10:52:08.955
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 16 10:52:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1104" for this suite. 03/16/23 10:52:09.226
------------------------------
• [2.430 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:52:06.886
    Mar 16 10:52:06.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 03/16/23 10:52:06.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:07.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:07.332
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/16/23 10:52:07.51
    STEP: creating 03/16/23 10:52:07.51
    STEP: getting 03/16/23 10:52:07.601
    STEP: listing 03/16/23 10:52:07.69
    STEP: watching 03/16/23 10:52:07.78
    Mar 16 10:52:07.780: INFO: starting watch
    STEP: cluster-wide listing 03/16/23 10:52:07.868
    STEP: cluster-wide watching 03/16/23 10:52:07.957
    Mar 16 10:52:07.958: INFO: starting watch
    STEP: patching 03/16/23 10:52:08.046
    STEP: updating 03/16/23 10:52:08.138
    Mar 16 10:52:08.319: INFO: waiting for watch events with expected annotations
    Mar 16 10:52:08.319: INFO: saw patched and updated annotations
    STEP: patching /status 03/16/23 10:52:08.32
    STEP: updating /status 03/16/23 10:52:08.412
    STEP: get /status 03/16/23 10:52:08.596
    STEP: deleting 03/16/23 10:52:08.686
    STEP: deleting a collection 03/16/23 10:52:08.955
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:52:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1104" for this suite. 03/16/23 10:52:09.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:52:09.317
Mar 16 10:52:09.317: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook 03/16/23 10:52:09.319
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:09.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:09.765
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/16/23 10:52:09.942
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/16/23 10:52:10.638
STEP: Deploying the custom resource conversion webhook pod 03/16/23 10:52:10.73
STEP: Wait for the deployment to be ready 03/16/23 10:52:10.91
Mar 16 10:52:11.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 10:52:13.269
STEP: Verifying the service has paired with the endpoint 03/16/23 10:52:13.38
Mar 16 10:52:14.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:15.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:16.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:17.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:18.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:19.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:20.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Mar 16 10:52:21.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 16 10:52:21.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource 03/16/23 10:52:24.172
STEP: v2 custom resource should be converted 03/16/23 10:52:24.262
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:52:24.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6581" for this suite. 03/16/23 10:52:25.166
------------------------------
• [15.939 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:52:09.317
    Mar 16 10:52:09.317: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-webhook 03/16/23 10:52:09.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:09.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:09.765
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/16/23 10:52:09.942
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/16/23 10:52:10.638
    STEP: Deploying the custom resource conversion webhook pod 03/16/23 10:52:10.73
    STEP: Wait for the deployment to be ready 03/16/23 10:52:10.91
    Mar 16 10:52:11.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 52, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 10:52:13.269
    STEP: Verifying the service has paired with the endpoint 03/16/23 10:52:13.38
    Mar 16 10:52:14.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:15.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:16.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:17.380: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:18.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:19.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:20.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    Mar 16 10:52:21.381: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 16 10:52:21.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating a v1 custom resource 03/16/23 10:52:24.172
    STEP: v2 custom resource should be converted 03/16/23 10:52:24.262
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:52:24.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6581" for this suite. 03/16/23 10:52:25.166
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:52:25.257
Mar 16 10:52:25.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:52:25.258
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:25.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:25.715
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1426 03/16/23 10:52:25.897
STEP: creating a selector 03/16/23 10:52:25.897
STEP: Creating the service pods in kubernetes 03/16/23 10:52:25.897
Mar 16 10:52:25.897: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 16 10:52:26.270: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1426" to be "running and ready"
Mar 16 10:52:26.360: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.64403ms
Mar 16 10:52:26.360: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:52:28.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180218521s
Mar 16 10:52:28.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:30.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.179794868s
Mar 16 10:52:30.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:32.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.180301306s
Mar 16 10:52:32.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:34.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.180141503s
Mar 16 10:52:34.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:36.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180226677s
Mar 16 10:52:36.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:38.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.179932207s
Mar 16 10:52:38.450: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 16 10:52:38.450: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 16 10:52:38.540: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1426" to be "running and ready"
Mar 16 10:52:38.629: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.729532ms
Mar 16 10:52:38.629: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 16 10:52:38.629: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/16/23 10:52:38.719
Mar 16 10:52:38.812: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1426" to be "running"
Mar 16 10:52:38.902: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.634519ms
Mar 16 10:52:40.993: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180418533s
Mar 16 10:52:40.993: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 16 10:52:41.082: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 16 10:52:41.083: INFO: Breadth first check of 100.64.1.125 on host 10.250.19.136...
Mar 16 10:52:41.172: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.126:9080/dial?request=hostname&protocol=http&host=100.64.1.125&port=8083&tries=1'] Namespace:pod-network-test-1426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:52:41.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:52:41.173: INFO: ExecWithOptions: Clientset creation
Mar 16 10:52:41.173: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1426/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.125%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 16 10:52:41.951: INFO: Waiting for responses: map[]
Mar 16 10:52:41.951: INFO: reached 100.64.1.125 after 0/1 tries
Mar 16 10:52:41.951: INFO: Breadth first check of 100.64.0.208 on host 10.250.19.246...
Mar 16 10:52:42.048: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.126:9080/dial?request=hostname&protocol=http&host=100.64.0.208&port=8083&tries=1'] Namespace:pod-network-test-1426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:52:42.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:52:42.049: INFO: ExecWithOptions: Clientset creation
Mar 16 10:52:42.049: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1426/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.208%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 16 10:52:42.734: INFO: Waiting for responses: map[]
Mar 16 10:52:42.734: INFO: reached 100.64.0.208 after 0/1 tries
Mar 16 10:52:42.734: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 16 10:52:42.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1426" for this suite. 03/16/23 10:52:42.912
------------------------------
• [17.746 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:52:25.257
    Mar 16 10:52:25.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:52:25.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:25.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:25.715
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1426 03/16/23 10:52:25.897
    STEP: creating a selector 03/16/23 10:52:25.897
    STEP: Creating the service pods in kubernetes 03/16/23 10:52:25.897
    Mar 16 10:52:25.897: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 16 10:52:26.270: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1426" to be "running and ready"
    Mar 16 10:52:26.360: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.64403ms
    Mar 16 10:52:26.360: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:52:28.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.180218521s
    Mar 16 10:52:28.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:30.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.179794868s
    Mar 16 10:52:30.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:32.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.180301306s
    Mar 16 10:52:32.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:34.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.180141503s
    Mar 16 10:52:34.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:36.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180226677s
    Mar 16 10:52:36.450: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:38.450: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.179932207s
    Mar 16 10:52:38.450: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 16 10:52:38.450: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 16 10:52:38.540: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1426" to be "running and ready"
    Mar 16 10:52:38.629: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.729532ms
    Mar 16 10:52:38.629: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 16 10:52:38.629: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/16/23 10:52:38.719
    Mar 16 10:52:38.812: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1426" to be "running"
    Mar 16 10:52:38.902: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.634519ms
    Mar 16 10:52:40.993: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180418533s
    Mar 16 10:52:40.993: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 16 10:52:41.082: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar 16 10:52:41.083: INFO: Breadth first check of 100.64.1.125 on host 10.250.19.136...
    Mar 16 10:52:41.172: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.126:9080/dial?request=hostname&protocol=http&host=100.64.1.125&port=8083&tries=1'] Namespace:pod-network-test-1426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:52:41.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:52:41.173: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:52:41.173: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1426/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.1.125%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 16 10:52:41.951: INFO: Waiting for responses: map[]
    Mar 16 10:52:41.951: INFO: reached 100.64.1.125 after 0/1 tries
    Mar 16 10:52:41.951: INFO: Breadth first check of 100.64.0.208 on host 10.250.19.246...
    Mar 16 10:52:42.048: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.126:9080/dial?request=hostname&protocol=http&host=100.64.0.208&port=8083&tries=1'] Namespace:pod-network-test-1426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:52:42.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:52:42.049: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:52:42.049: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-1426/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.64.1.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.64.0.208%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 16 10:52:42.734: INFO: Waiting for responses: map[]
    Mar 16 10:52:42.734: INFO: reached 100.64.0.208 after 0/1 tries
    Mar 16 10:52:42.734: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:52:42.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1426" for this suite. 03/16/23 10:52:42.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:52:43.004
Mar 16 10:52:43.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:52:43.006
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:43.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:43.452
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 03/16/23 10:52:43.63
Mar 16 10:52:43.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 16 10:52:44.060: INFO: stderr: ""
Mar 16 10:52:44.060: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 03/16/23 10:52:44.06
Mar 16 10:52:44.060: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 16 10:52:44.060: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6904" to be "running and ready, or succeeded"
Mar 16 10:52:44.150: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.525723ms
Mar 16 10:52:44.150: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-250-19-136.ec2.internal' to be 'Running' but was 'Pending'
Mar 16 10:52:46.241: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.18024783s
Mar 16 10:52:46.241: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 16 10:52:46.241: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/16/23 10:52:46.241
Mar 16 10:52:46.241: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator'
Mar 16 10:52:46.720: INFO: stderr: ""
Mar 16 10:52:46.720: INFO: stdout: "I0316 10:52:44.702280       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8wr5 430\nI0316 10:52:44.902373       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xfm 276\nI0316 10:52:45.102328       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/zj4f 349\nI0316 10:52:45.302611       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/l7ww 556\nI0316 10:52:45.502851       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nkt9 248\nI0316 10:52:45.703137       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9tn 491\nI0316 10:52:45.902368       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/8xtt 467\nI0316 10:52:46.102655       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gj6 504\nI0316 10:52:46.302945       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/z56 328\nI0316 10:52:46.503235       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/khrk 333\n"
STEP: limiting log lines 03/16/23 10:52:46.72
Mar 16 10:52:46.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --tail=1'
Mar 16 10:52:47.310: INFO: stderr: ""
Mar 16 10:52:47.310: INFO: stdout: "I0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\n"
Mar 16 10:52:47.310: INFO: got output "I0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\n"
STEP: limiting log bytes 03/16/23 10:52:47.31
Mar 16 10:52:47.310: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --limit-bytes=1'
Mar 16 10:52:47.813: INFO: stderr: ""
Mar 16 10:52:47.813: INFO: stdout: "I"
Mar 16 10:52:47.813: INFO: got output "I"
STEP: exposing timestamps 03/16/23 10:52:47.813
Mar 16 10:52:47.813: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 16 10:52:48.299: INFO: stderr: ""
Mar 16 10:52:48.299: INFO: stdout: "2023-03-16T10:52:48.103215106Z I0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\n"
Mar 16 10:52:48.299: INFO: got output "2023-03-16T10:52:48.103215106Z I0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\n"
STEP: restricting to a time range 03/16/23 10:52:48.299
Mar 16 10:52:50.801: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --since=1s'
Mar 16 10:52:51.259: INFO: stderr: ""
Mar 16 10:52:51.259: INFO: stdout: "I0316 10:52:50.303122       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/zfjr 371\nI0316 10:52:50.502345       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xgqz 360\nI0316 10:52:50.702642       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/45dl 399\nI0316 10:52:50.902853       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/d5n2 334\nI0316 10:52:51.103145       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/hln 587\n"
Mar 16 10:52:51.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --since=24h'
Mar 16 10:52:51.743: INFO: stderr: ""
Mar 16 10:52:51.743: INFO: stdout: "I0316 10:52:44.702280       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8wr5 430\nI0316 10:52:44.902373       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xfm 276\nI0316 10:52:45.102328       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/zj4f 349\nI0316 10:52:45.302611       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/l7ww 556\nI0316 10:52:45.502851       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nkt9 248\nI0316 10:52:45.703137       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9tn 491\nI0316 10:52:45.902368       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/8xtt 467\nI0316 10:52:46.102655       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gj6 504\nI0316 10:52:46.302945       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/z56 328\nI0316 10:52:46.503235       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/khrk 333\nI0316 10:52:46.702458       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/42sk 341\nI0316 10:52:46.902746       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/q8d 220\nI0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\nI0316 10:52:47.303132       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vq6j 371\nI0316 10:52:47.502363       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/xg8 593\nI0316 10:52:47.702663       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/zpvr 393\nI0316 10:52:47.902855       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/8lr 216\nI0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\nI0316 10:52:48.302379       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/7tq 404\nI0316 10:52:48.503800       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2pc 449\nI0316 10:52:48.703138       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/jv6 446\nI0316 10:52:48.902369       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/rmbx 463\nI0316 10:52:49.102658       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/jpjb 277\nI0316 10:52:49.302856       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/k969 399\nI0316 10:52:49.504961       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/4h74 361\nI0316 10:52:49.703327       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/qnl 231\nI0316 10:52:49.902652       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/wmb 471\nI0316 10:52:50.102818       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/2xw 235\nI0316 10:52:50.303122       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/zfjr 371\nI0316 10:52:50.502345       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xgqz 360\nI0316 10:52:50.702642       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/45dl 399\nI0316 10:52:50.902853       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/d5n2 334\nI0316 10:52:51.103145       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/hln 587\nI0316 10:52:51.302406       1 logs_generator.go:76] 33 POST /api/v1/namespaces/ns/pods/qjj4 435\nI0316 10:52:51.502696       1 logs_generator.go:76] 34 GET /api/v1/namespaces/default/pods/p98 524\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Mar 16 10:52:51.744: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 delete pod logs-generator'
Mar 16 10:52:52.583: INFO: stderr: ""
Mar 16 10:52:52.583: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:52:52.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6904" for this suite. 03/16/23 10:52:52.761
------------------------------
• [9.848 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:52:43.004
    Mar 16 10:52:43.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:52:43.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:43.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:43.452
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 03/16/23 10:52:43.63
    Mar 16 10:52:43.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 16 10:52:44.060: INFO: stderr: ""
    Mar 16 10:52:44.060: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 03/16/23 10:52:44.06
    Mar 16 10:52:44.060: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 16 10:52:44.060: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6904" to be "running and ready, or succeeded"
    Mar 16 10:52:44.150: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.525723ms
    Mar 16 10:52:44.150: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-250-19-136.ec2.internal' to be 'Running' but was 'Pending'
    Mar 16 10:52:46.241: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.18024783s
    Mar 16 10:52:46.241: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 16 10:52:46.241: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/16/23 10:52:46.241
    Mar 16 10:52:46.241: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator'
    Mar 16 10:52:46.720: INFO: stderr: ""
    Mar 16 10:52:46.720: INFO: stdout: "I0316 10:52:44.702280       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8wr5 430\nI0316 10:52:44.902373       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xfm 276\nI0316 10:52:45.102328       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/zj4f 349\nI0316 10:52:45.302611       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/l7ww 556\nI0316 10:52:45.502851       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nkt9 248\nI0316 10:52:45.703137       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9tn 491\nI0316 10:52:45.902368       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/8xtt 467\nI0316 10:52:46.102655       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gj6 504\nI0316 10:52:46.302945       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/z56 328\nI0316 10:52:46.503235       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/khrk 333\n"
    STEP: limiting log lines 03/16/23 10:52:46.72
    Mar 16 10:52:46.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --tail=1'
    Mar 16 10:52:47.310: INFO: stderr: ""
    Mar 16 10:52:47.310: INFO: stdout: "I0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\n"
    Mar 16 10:52:47.310: INFO: got output "I0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\n"
    STEP: limiting log bytes 03/16/23 10:52:47.31
    Mar 16 10:52:47.310: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --limit-bytes=1'
    Mar 16 10:52:47.813: INFO: stderr: ""
    Mar 16 10:52:47.813: INFO: stdout: "I"
    Mar 16 10:52:47.813: INFO: got output "I"
    STEP: exposing timestamps 03/16/23 10:52:47.813
    Mar 16 10:52:47.813: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 16 10:52:48.299: INFO: stderr: ""
    Mar 16 10:52:48.299: INFO: stdout: "2023-03-16T10:52:48.103215106Z I0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\n"
    Mar 16 10:52:48.299: INFO: got output "2023-03-16T10:52:48.103215106Z I0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\n"
    STEP: restricting to a time range 03/16/23 10:52:48.299
    Mar 16 10:52:50.801: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --since=1s'
    Mar 16 10:52:51.259: INFO: stderr: ""
    Mar 16 10:52:51.259: INFO: stdout: "I0316 10:52:50.303122       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/zfjr 371\nI0316 10:52:50.502345       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xgqz 360\nI0316 10:52:50.702642       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/45dl 399\nI0316 10:52:50.902853       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/d5n2 334\nI0316 10:52:51.103145       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/hln 587\n"
    Mar 16 10:52:51.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 logs logs-generator logs-generator --since=24h'
    Mar 16 10:52:51.743: INFO: stderr: ""
    Mar 16 10:52:51.743: INFO: stdout: "I0316 10:52:44.702280       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8wr5 430\nI0316 10:52:44.902373       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xfm 276\nI0316 10:52:45.102328       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/zj4f 349\nI0316 10:52:45.302611       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/l7ww 556\nI0316 10:52:45.502851       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nkt9 248\nI0316 10:52:45.703137       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9tn 491\nI0316 10:52:45.902368       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/8xtt 467\nI0316 10:52:46.102655       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gj6 504\nI0316 10:52:46.302945       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/z56 328\nI0316 10:52:46.503235       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/khrk 333\nI0316 10:52:46.702458       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/42sk 341\nI0316 10:52:46.902746       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/q8d 220\nI0316 10:52:47.102845       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ckk 399\nI0316 10:52:47.303132       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vq6j 371\nI0316 10:52:47.502363       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/xg8 593\nI0316 10:52:47.702663       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/zpvr 393\nI0316 10:52:47.902855       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/8lr 216\nI0316 10:52:48.103140       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kmw 397\nI0316 10:52:48.302379       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/7tq 404\nI0316 10:52:48.503800       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2pc 449\nI0316 10:52:48.703138       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/jv6 446\nI0316 10:52:48.902369       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/rmbx 463\nI0316 10:52:49.102658       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/jpjb 277\nI0316 10:52:49.302856       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/k969 399\nI0316 10:52:49.504961       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/4h74 361\nI0316 10:52:49.703327       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/qnl 231\nI0316 10:52:49.902652       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/wmb 471\nI0316 10:52:50.102818       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/2xw 235\nI0316 10:52:50.303122       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/zfjr 371\nI0316 10:52:50.502345       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xgqz 360\nI0316 10:52:50.702642       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/45dl 399\nI0316 10:52:50.902853       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/d5n2 334\nI0316 10:52:51.103145       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/hln 587\nI0316 10:52:51.302406       1 logs_generator.go:76] 33 POST /api/v1/namespaces/ns/pods/qjj4 435\nI0316 10:52:51.502696       1 logs_generator.go:76] 34 GET /api/v1/namespaces/default/pods/p98 524\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Mar 16 10:52:51.744: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6904 delete pod logs-generator'
    Mar 16 10:52:52.583: INFO: stderr: ""
    Mar 16 10:52:52.583: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:52:52.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6904" for this suite. 03/16/23 10:52:52.761
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:52:52.852
Mar 16 10:52:52.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:52:52.853
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:53.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:53.302
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4856 03/16/23 10:52:53.479
STEP: creating a selector 03/16/23 10:52:53.48
STEP: Creating the service pods in kubernetes 03/16/23 10:52:53.48
Mar 16 10:52:53.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 16 10:52:53.850: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4856" to be "running and ready"
Mar 16 10:52:53.939: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.419216ms
Mar 16 10:52:53.939: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:52:56.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.1795808s
Mar 16 10:52:56.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:52:58.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.180288667s
Mar 16 10:52:58.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:53:00.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.180553327s
Mar 16 10:53:00.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:53:02.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.179581388s
Mar 16 10:53:02.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:53:04.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180712286s
Mar 16 10:53:04.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 16 10:53:06.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.18037476s
Mar 16 10:53:06.030: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 16 10:53:06.030: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 16 10:53:06.119: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4856" to be "running and ready"
Mar 16 10:53:06.209: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.461816ms
Mar 16 10:53:06.209: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 16 10:53:06.209: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/16/23 10:53:06.299
Mar 16 10:53:06.486: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4856" to be "running"
Mar 16 10:53:06.575: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.646561ms
Mar 16 10:53:08.666: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180396586s
Mar 16 10:53:08.666: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 16 10:53:08.755: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4856" to be "running"
Mar 16 10:53:08.845: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.568266ms
Mar 16 10:53:08.845: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 16 10:53:08.934: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 16 10:53:08.934: INFO: Going to poll 100.64.1.128 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 16 10:53:09.023: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4856 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:53:09.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:53:09.024: INFO: ExecWithOptions: Clientset creation
Mar 16 10:53:09.024: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4856/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.128+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 16 10:53:10.775: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 16 10:53:10.775: INFO: Going to poll 100.64.0.209 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 16 10:53:10.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4856 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:53:10.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:53:10.865: INFO: ExecWithOptions: Clientset creation
Mar 16 10:53:10.865: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4856/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 16 10:53:12.611: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:12.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4856" for this suite. 03/16/23 10:53:12.789
------------------------------
• [20.027 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:52:52.852
    Mar 16 10:52:52.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pod-network-test 03/16/23 10:52:52.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:52:53.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:52:53.302
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4856 03/16/23 10:52:53.479
    STEP: creating a selector 03/16/23 10:52:53.48
    STEP: Creating the service pods in kubernetes 03/16/23 10:52:53.48
    Mar 16 10:52:53.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 16 10:52:53.850: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4856" to be "running and ready"
    Mar 16 10:52:53.939: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.419216ms
    Mar 16 10:52:53.939: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:52:56.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.1795808s
    Mar 16 10:52:56.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:52:58.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.180288667s
    Mar 16 10:52:58.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:53:00.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.180553327s
    Mar 16 10:53:00.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:53:02.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.179581388s
    Mar 16 10:53:02.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:53:04.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.180712286s
    Mar 16 10:53:04.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 16 10:53:06.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.18037476s
    Mar 16 10:53:06.030: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 16 10:53:06.030: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 16 10:53:06.119: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4856" to be "running and ready"
    Mar 16 10:53:06.209: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 89.461816ms
    Mar 16 10:53:06.209: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 16 10:53:06.209: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/16/23 10:53:06.299
    Mar 16 10:53:06.486: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4856" to be "running"
    Mar 16 10:53:06.575: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.646561ms
    Mar 16 10:53:08.666: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180396586s
    Mar 16 10:53:08.666: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 16 10:53:08.755: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4856" to be "running"
    Mar 16 10:53:08.845: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 89.568266ms
    Mar 16 10:53:08.845: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 16 10:53:08.934: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar 16 10:53:08.934: INFO: Going to poll 100.64.1.128 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Mar 16 10:53:09.023: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4856 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:53:09.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:53:09.024: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:53:09.024: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4856/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.1.128+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 16 10:53:10.775: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 16 10:53:10.775: INFO: Going to poll 100.64.0.209 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Mar 16 10:53:10.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4856 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:53:10.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:53:10.865: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:53:10.865: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4856/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.64.0.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 16 10:53:12.611: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:12.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4856" for this suite. 03/16/23 10:53:12.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:12.88
Mar 16 10:53:12.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 10:53:12.881
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:13.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:13.327
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-b8b16a56-ae80-4459-af6a-6290947da58c 03/16/23 10:53:13.505
STEP: Creating a pod to test consume secrets 03/16/23 10:53:13.595
Mar 16 10:53:13.690: INFO: Waiting up to 5m0s for pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd" in namespace "secrets-3578" to be "Succeeded or Failed"
Mar 16 10:53:13.780: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.251333ms
Mar 16 10:53:15.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179952337s
Mar 16 10:53:17.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179648143s
STEP: Saw pod success 03/16/23 10:53:17.87
Mar 16 10:53:17.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd" satisfied condition "Succeeded or Failed"
Mar 16 10:53:17.959: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 10:53:18.094
Mar 16 10:53:18.187: INFO: Waiting for pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd to disappear
Mar 16 10:53:18.277: INFO: Pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:18.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3578" for this suite. 03/16/23 10:53:18.455
------------------------------
• [5.665 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:12.88
    Mar 16 10:53:12.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 10:53:12.881
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:13.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:13.327
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-b8b16a56-ae80-4459-af6a-6290947da58c 03/16/23 10:53:13.505
    STEP: Creating a pod to test consume secrets 03/16/23 10:53:13.595
    Mar 16 10:53:13.690: INFO: Waiting up to 5m0s for pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd" in namespace "secrets-3578" to be "Succeeded or Failed"
    Mar 16 10:53:13.780: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.251333ms
    Mar 16 10:53:15.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179952337s
    Mar 16 10:53:17.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179648143s
    STEP: Saw pod success 03/16/23 10:53:17.87
    Mar 16 10:53:17.870: INFO: Pod "pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd" satisfied condition "Succeeded or Failed"
    Mar 16 10:53:17.959: INFO: Trying to get logs from node ip-10-250-19-246.ec2.internal pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 10:53:18.094
    Mar 16 10:53:18.187: INFO: Waiting for pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd to disappear
    Mar 16 10:53:18.277: INFO: Pod pod-secrets-b73daa60-2fc8-45c1-87db-6d661a2216bd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:18.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3578" for this suite. 03/16/23 10:53:18.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:18.546
Mar 16 10:53:18.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 03/16/23 10:53:18.547
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:18.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:18.995
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 03/16/23 10:53:19.174
Mar 16 10:53:19.268: INFO: Waiting up to 5m0s for pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2" in namespace "containers-5696" to be "Succeeded or Failed"
Mar 16 10:53:19.362: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 93.64943ms
Mar 16 10:53:21.453: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184449816s
Mar 16 10:53:23.452: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183453411s
STEP: Saw pod success 03/16/23 10:53:23.452
Mar 16 10:53:23.452: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2" satisfied condition "Succeeded or Failed"
Mar 16 10:53:23.541: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:53:23.682
Mar 16 10:53:23.775: INFO: Waiting for pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 to disappear
Mar 16 10:53:23.865: INFO: Pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:23.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5696" for this suite. 03/16/23 10:53:24.042
------------------------------
• [5.587 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:18.546
    Mar 16 10:53:18.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 03/16/23 10:53:18.547
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:18.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:18.995
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 03/16/23 10:53:19.174
    Mar 16 10:53:19.268: INFO: Waiting up to 5m0s for pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2" in namespace "containers-5696" to be "Succeeded or Failed"
    Mar 16 10:53:19.362: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 93.64943ms
    Mar 16 10:53:21.453: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184449816s
    Mar 16 10:53:23.452: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.183453411s
    STEP: Saw pod success 03/16/23 10:53:23.452
    Mar 16 10:53:23.452: INFO: Pod "client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2" satisfied condition "Succeeded or Failed"
    Mar 16 10:53:23.541: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:53:23.682
    Mar 16 10:53:23.775: INFO: Waiting for pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 to disappear
    Mar 16 10:53:23.865: INFO: Pod client-containers-716b1bf6-d6cc-4c10-8c0e-62f9a6a4f1c2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:23.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5696" for this suite. 03/16/23 10:53:24.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:24.133
Mar 16 10:53:24.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 10:53:24.134
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:24.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:24.58
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 03/16/23 10:53:24.757
Mar 16 10:53:24.757: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-886 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/16/23 10:53:24.836
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-886" for this suite. 03/16/23 10:53:25.295
------------------------------
• [1.252 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:24.133
    Mar 16 10:53:24.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 10:53:24.134
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:24.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:24.58
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 03/16/23 10:53:24.757
    Mar 16 10:53:24.757: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-886 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/16/23 10:53:24.836
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-886" for this suite. 03/16/23 10:53:25.295
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:25.386
Mar 16 10:53:25.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:53:25.387
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:25.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:25.832
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-6de02f3a-6f1a-4387-bee4-7630e6bd683b 03/16/23 10:53:26.01
STEP: Creating a pod to test consume configMaps 03/16/23 10:53:26.099
Mar 16 10:53:26.194: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1" in namespace "projected-4718" to be "Succeeded or Failed"
Mar 16 10:53:26.284: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.636608ms
Mar 16 10:53:28.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179811179s
Mar 16 10:53:30.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179998878s
STEP: Saw pod success 03/16/23 10:53:30.374
Mar 16 10:53:30.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1" satisfied condition "Succeeded or Failed"
Mar 16 10:53:30.464: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:53:30.6
Mar 16 10:53:30.693: INFO: Waiting for pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 to disappear
Mar 16 10:53:30.783: INFO: Pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4718" for this suite. 03/16/23 10:53:30.96
------------------------------
• [5.665 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:25.386
    Mar 16 10:53:25.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:53:25.387
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:25.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:25.832
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-6de02f3a-6f1a-4387-bee4-7630e6bd683b 03/16/23 10:53:26.01
    STEP: Creating a pod to test consume configMaps 03/16/23 10:53:26.099
    Mar 16 10:53:26.194: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1" in namespace "projected-4718" to be "Succeeded or Failed"
    Mar 16 10:53:26.284: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.636608ms
    Mar 16 10:53:28.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179811179s
    Mar 16 10:53:30.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179998878s
    STEP: Saw pod success 03/16/23 10:53:30.374
    Mar 16 10:53:30.374: INFO: Pod "pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1" satisfied condition "Succeeded or Failed"
    Mar 16 10:53:30.464: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:53:30.6
    Mar 16 10:53:30.693: INFO: Waiting for pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 to disappear
    Mar 16 10:53:30.783: INFO: Pod pod-projected-configmaps-dad71b86-a6e4-4160-8a07-d2dfe8ac11d1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4718" for this suite. 03/16/23 10:53:30.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:31.052
Mar 16 10:53:31.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 10:53:31.053
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:31.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:31.499
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 03/16/23 10:53:31.765
STEP: waiting for RC to be added 03/16/23 10:53:31.856
STEP: waiting for available Replicas 03/16/23 10:53:31.856
STEP: patching ReplicationController 03/16/23 10:53:32.697
STEP: waiting for RC to be modified 03/16/23 10:53:32.789
STEP: patching ReplicationController status 03/16/23 10:53:32.789
STEP: waiting for RC to be modified 03/16/23 10:53:32.88
STEP: waiting for available Replicas 03/16/23 10:53:32.88
STEP: fetching ReplicationController status 03/16/23 10:53:32.883
STEP: patching ReplicationController scale 03/16/23 10:53:32.973
STEP: waiting for RC to be modified 03/16/23 10:53:33.063
STEP: waiting for ReplicationController's scale to be the max amount 03/16/23 10:53:33.063
STEP: fetching ReplicationController; ensuring that it's patched 03/16/23 10:53:34.451
STEP: updating ReplicationController status 03/16/23 10:53:34.541
STEP: waiting for RC to be modified 03/16/23 10:53:34.632
STEP: listing all ReplicationControllers 03/16/23 10:53:34.632
STEP: checking that ReplicationController has expected values 03/16/23 10:53:34.721
STEP: deleting ReplicationControllers by collection 03/16/23 10:53:34.721
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/16/23 10:53:34.812
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:34.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9910" for this suite. 03/16/23 10:53:35.029
------------------------------
• [4.067 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:31.052
    Mar 16 10:53:31.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 10:53:31.053
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:31.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:31.499
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 03/16/23 10:53:31.765
    STEP: waiting for RC to be added 03/16/23 10:53:31.856
    STEP: waiting for available Replicas 03/16/23 10:53:31.856
    STEP: patching ReplicationController 03/16/23 10:53:32.697
    STEP: waiting for RC to be modified 03/16/23 10:53:32.789
    STEP: patching ReplicationController status 03/16/23 10:53:32.789
    STEP: waiting for RC to be modified 03/16/23 10:53:32.88
    STEP: waiting for available Replicas 03/16/23 10:53:32.88
    STEP: fetching ReplicationController status 03/16/23 10:53:32.883
    STEP: patching ReplicationController scale 03/16/23 10:53:32.973
    STEP: waiting for RC to be modified 03/16/23 10:53:33.063
    STEP: waiting for ReplicationController's scale to be the max amount 03/16/23 10:53:33.063
    STEP: fetching ReplicationController; ensuring that it's patched 03/16/23 10:53:34.451
    STEP: updating ReplicationController status 03/16/23 10:53:34.541
    STEP: waiting for RC to be modified 03/16/23 10:53:34.632
    STEP: listing all ReplicationControllers 03/16/23 10:53:34.632
    STEP: checking that ReplicationController has expected values 03/16/23 10:53:34.721
    STEP: deleting ReplicationControllers by collection 03/16/23 10:53:34.721
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/16/23 10:53:34.812
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:34.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9910" for this suite. 03/16/23 10:53:35.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:35.12
Mar 16 10:53:35.120: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 10:53:35.121
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:35.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:35.566
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9370 03/16/23 10:53:35.744
STEP: creating service affinity-nodeport-transition in namespace services-9370 03/16/23 10:53:35.744
STEP: creating replication controller affinity-nodeport-transition in namespace services-9370 03/16/23 10:53:35.839
I0316 10:53:35.932424    8588 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9370, replica count: 3
I0316 10:53:39.033279    8588 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 10:53:39.388: INFO: Creating new exec pod
Mar 16 10:53:39.481: INFO: Waiting up to 5m0s for pod "execpod-affinity4lhcz" in namespace "services-9370" to be "running"
Mar 16 10:53:39.570: INFO: Pod "execpod-affinity4lhcz": Phase="Pending", Reason="", readiness=false. Elapsed: 89.102044ms
Mar 16 10:53:41.660: INFO: Pod "execpod-affinity4lhcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.179179801s
Mar 16 10:53:41.660: INFO: Pod "execpod-affinity4lhcz" satisfied condition "running"
Mar 16 10:53:42.840: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Mar 16 10:53:43.970: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 16 10:53:43.970: INFO: stdout: ""
Mar 16 10:53:43.970: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 100.109.24.119 80'
Mar 16 10:53:45.095: INFO: stderr: "+ nc -v -z -w 2 100.109.24.119 80\nConnection to 100.109.24.119 80 port [tcp/http] succeeded!\n"
Mar 16 10:53:45.095: INFO: stdout: ""
Mar 16 10:53:45.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 30818'
Mar 16 10:53:46.258: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 30818\nConnection to 10.250.19.136 30818 port [tcp/*] succeeded!\n"
Mar 16 10:53:46.258: INFO: stdout: ""
Mar 16 10:53:46.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 30818'
Mar 16 10:53:47.394: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 30818\nConnection to 10.250.19.246 30818 port [tcp/*] succeeded!\n"
Mar 16 10:53:47.394: INFO: stdout: ""
Mar 16 10:53:47.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:30818/ ; done'
Mar 16 10:53:48.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n"
Mar 16 10:53:48.773: INFO: stdout: "\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw"
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
Mar 16 10:53:48.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:30818/ ; done'
Mar 16 10:53:50.162: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n"
Mar 16 10:53:50.162: INFO: stdout: "\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896"
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
Mar 16 10:53:50.163: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9370, will wait for the garbage collector to delete the pods 03/16/23 10:53:50.255
Mar 16 10:53:50.536: INFO: Deleting ReplicationController affinity-nodeport-transition took: 90.402497ms
Mar 16 10:53:50.636: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.514828ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:52.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9370" for this suite. 03/16/23 10:53:53.026
------------------------------
• [17.997 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:35.12
    Mar 16 10:53:35.120: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 10:53:35.121
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:35.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:35.566
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9370 03/16/23 10:53:35.744
    STEP: creating service affinity-nodeport-transition in namespace services-9370 03/16/23 10:53:35.744
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9370 03/16/23 10:53:35.839
    I0316 10:53:35.932424    8588 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9370, replica count: 3
    I0316 10:53:39.033279    8588 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 10:53:39.388: INFO: Creating new exec pod
    Mar 16 10:53:39.481: INFO: Waiting up to 5m0s for pod "execpod-affinity4lhcz" in namespace "services-9370" to be "running"
    Mar 16 10:53:39.570: INFO: Pod "execpod-affinity4lhcz": Phase="Pending", Reason="", readiness=false. Elapsed: 89.102044ms
    Mar 16 10:53:41.660: INFO: Pod "execpod-affinity4lhcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.179179801s
    Mar 16 10:53:41.660: INFO: Pod "execpod-affinity4lhcz" satisfied condition "running"
    Mar 16 10:53:42.840: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Mar 16 10:53:43.970: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 16 10:53:43.970: INFO: stdout: ""
    Mar 16 10:53:43.970: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 100.109.24.119 80'
    Mar 16 10:53:45.095: INFO: stderr: "+ nc -v -z -w 2 100.109.24.119 80\nConnection to 100.109.24.119 80 port [tcp/http] succeeded!\n"
    Mar 16 10:53:45.095: INFO: stdout: ""
    Mar 16 10:53:45.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 30818'
    Mar 16 10:53:46.258: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 30818\nConnection to 10.250.19.136 30818 port [tcp/*] succeeded!\n"
    Mar 16 10:53:46.258: INFO: stdout: ""
    Mar 16 10:53:46.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 30818'
    Mar 16 10:53:47.394: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 30818\nConnection to 10.250.19.246 30818 port [tcp/*] succeeded!\n"
    Mar 16 10:53:47.394: INFO: stdout: ""
    Mar 16 10:53:47.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:30818/ ; done'
    Mar 16 10:53:48.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n"
    Mar 16 10:53:48.773: INFO: stdout: "\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-crv2q\naffinity-nodeport-transition-x78gw\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-x78gw"
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-crv2q
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:48.773: INFO: Received response from host: affinity-nodeport-transition-x78gw
    Mar 16 10:53:48.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9370 exec execpod-affinity4lhcz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:30818/ ; done'
    Mar 16 10:53:50.162: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:30818/\n"
    Mar 16 10:53:50.162: INFO: stdout: "\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896\naffinity-nodeport-transition-q2896"
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.162: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Received response from host: affinity-nodeport-transition-q2896
    Mar 16 10:53:50.163: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9370, will wait for the garbage collector to delete the pods 03/16/23 10:53:50.255
    Mar 16 10:53:50.536: INFO: Deleting ReplicationController affinity-nodeport-transition took: 90.402497ms
    Mar 16 10:53:50.636: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.514828ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:52.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9370" for this suite. 03/16/23 10:53:53.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:53.117
Mar 16 10:53:53.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 10:53:53.118
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:53.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:53.566
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/16/23 10:53:53.743
Mar 16 10:53:53.837: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6984  567f2b61-1c86-48d5-a3a8-5f0419cb4fff 42557 0 2023-03-16 10:53:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-16 10:53:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcdp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcdp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 16 10:53:53.837: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6984" to be "running and ready"
Mar 16 10:53:53.927: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 89.45578ms
Mar 16 10:53:53.927: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 16 10:53:56.017: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.179395109s
Mar 16 10:53:56.017: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 16 10:53:56.017: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/16/23 10:53:56.017
Mar 16 10:53:56.017: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6984 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:53:56.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:53:56.018: INFO: ExecWithOptions: Clientset creation
Mar 16 10:53:56.018: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-6984/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/16/23 10:53:56.796
Mar 16 10:53:56.796: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6984 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 10:53:56.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 10:53:56.797: INFO: ExecWithOptions: Clientset creation
Mar 16 10:53:56.797: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-6984/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 16 10:53:57.583: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 10:53:57.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6984" for this suite. 03/16/23 10:53:57.852
------------------------------
• [4.825 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:53.117
    Mar 16 10:53:53.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 10:53:53.118
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:53.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:53.566
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/16/23 10:53:53.743
    Mar 16 10:53:53.837: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6984  567f2b61-1c86-48d5-a3a8-5f0419cb4fff 42557 0 2023-03-16 10:53:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-16 10:53:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcdp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmbqx-ip1.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcdp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 16 10:53:53.837: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6984" to be "running and ready"
    Mar 16 10:53:53.927: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 89.45578ms
    Mar 16 10:53:53.927: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 10:53:56.017: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.179395109s
    Mar 16 10:53:56.017: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 16 10:53:56.017: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/16/23 10:53:56.017
    Mar 16 10:53:56.017: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6984 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:53:56.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:53:56.018: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:53:56.018: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-6984/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/16/23 10:53:56.796
    Mar 16 10:53:56.796: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6984 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 10:53:56.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 10:53:56.797: INFO: ExecWithOptions: Clientset creation
    Mar 16 10:53:56.797: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-6984/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 16 10:53:57.583: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:53:57.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6984" for this suite. 03/16/23 10:53:57.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:53:57.943
Mar 16 10:53:57.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 10:53:57.944
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:58.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:58.39
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 03/16/23 10:53:58.927
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:53:59.017
Mar 16 10:53:59.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:53:59.196: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:54:00.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:54:00.464: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:54:01.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:54:01.465: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/16/23 10:54:01.554
Mar 16 10:54:01.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 10:54:01.915: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
Mar 16 10:54:03.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 10:54:03.184: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/16/23 10:54:03.184
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:54:03.363
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9244, will wait for the garbage collector to delete the pods 03/16/23 10:54:03.363
Mar 16 10:54:03.644: INFO: Deleting DaemonSet.extensions daemon-set took: 90.194268ms
Mar 16 10:54:03.745: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.893919ms
Mar 16 10:54:05.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 10:54:05.735: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 10:54:05.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42681"},"items":null}

Mar 16 10:54:05.914: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42682"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:54:06.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9244" for this suite. 03/16/23 10:54:06.361
------------------------------
• [8.508 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:53:57.943
    Mar 16 10:53:57.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 10:53:57.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:53:58.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:53:58.39
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 03/16/23 10:53:58.927
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 10:53:59.017
    Mar 16 10:53:59.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:53:59.196: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:54:00.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:54:00.464: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:54:01.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:54:01.465: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/16/23 10:54:01.554
    Mar 16 10:54:01.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 10:54:01.915: INFO: Node ip-10-250-19-246.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 10:54:03.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 10:54:03.184: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/16/23 10:54:03.184
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 10:54:03.363
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9244, will wait for the garbage collector to delete the pods 03/16/23 10:54:03.363
    Mar 16 10:54:03.644: INFO: Deleting DaemonSet.extensions daemon-set took: 90.194268ms
    Mar 16 10:54:03.745: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.893919ms
    Mar 16 10:54:05.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 10:54:05.735: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 10:54:05.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42681"},"items":null}

    Mar 16 10:54:05.914: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42682"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:54:06.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9244" for this suite. 03/16/23 10:54:06.361
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:54:06.452
Mar 16 10:54:06.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:54:06.453
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:54:06.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:54:06.899
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 16 10:54:07.259: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5606 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 16 10:54:07.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5606" for this suite. 03/16/23 10:54:07.529
------------------------------
• [1.167 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:54:06.452
    Mar 16 10:54:06.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename runtimeclass 03/16/23 10:54:06.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:54:06.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:54:06.899
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 16 10:54:07.259: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5606 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:54:07.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5606" for this suite. 03/16/23 10:54:07.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:54:07.62
Mar 16 10:54:07.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch 03/16/23 10:54:07.621
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:54:07.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:54:08.067
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 16 10:54:08.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR  03/16/23 10:54:10.891
Mar 16 10:54:10.981: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:10Z]] name:name1 resourceVersion:42731 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/16/23 10:54:20.981
Mar 16 10:54:21.073: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:21Z]] name:name2 resourceVersion:42798 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/16/23 10:54:31.076
Mar 16 10:54:31.167: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:31Z]] name:name1 resourceVersion:42853 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/16/23 10:54:41.169
Mar 16 10:54:41.259: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:41Z]] name:name2 resourceVersion:42909 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/16/23 10:54:51.259
Mar 16 10:54:51.350: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:31Z]] name:name1 resourceVersion:42965 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/16/23 10:55:01.354
Mar 16 10:55:01.445: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:41Z]] name:name2 resourceVersion:43051 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:55:11.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1420" for this suite. 03/16/23 10:55:11.803
------------------------------
• [64.273 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:54:07.62
    Mar 16 10:54:07.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-watch 03/16/23 10:54:07.621
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:54:07.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:54:08.067
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 16 10:54:08.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Creating first CR  03/16/23 10:54:10.891
    Mar 16 10:54:10.981: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:10Z]] name:name1 resourceVersion:42731 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/16/23 10:54:20.981
    Mar 16 10:54:21.073: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:21Z]] name:name2 resourceVersion:42798 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/16/23 10:54:31.076
    Mar 16 10:54:31.167: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:31Z]] name:name1 resourceVersion:42853 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/16/23 10:54:41.169
    Mar 16 10:54:41.259: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:41Z]] name:name2 resourceVersion:42909 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/16/23 10:54:51.259
    Mar 16 10:54:51.350: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:31Z]] name:name1 resourceVersion:42965 uid:5a1a91fa-b47b-4b11-ad6f-3cb9cc8eaa72] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/16/23 10:55:01.354
    Mar 16 10:55:01.445: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-16T10:54:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-16T10:54:41Z]] name:name2 resourceVersion:43051 uid:80ef4adb-7013-4b7c-b95c-44e48807217e] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:55:11.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1420" for this suite. 03/16/23 10:55:11.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:55:11.894
Mar 16 10:55:11.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 10:55:11.895
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:12.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:12.34
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba 03/16/23 10:55:12.518
Mar 16 10:55:12.697: INFO: Pod name my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba: Found 1 pods out of 1
Mar 16 10:55:12.697: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba" are running
Mar 16 10:55:12.697: INFO: Waiting up to 5m0s for pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" in namespace "replication-controller-1783" to be "running"
Mar 16 10:55:12.787: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.594725ms
Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v": Phase="Running", Reason="", readiness=true. Elapsed: 2.179904931s
Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" satisfied condition "running"
Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason: Message:}])
Mar 16 10:55:14.877: INFO: Trying to dial the pod
Mar 16 10:55:20.242: INFO: Controller my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba: Got expected result from replica 1 [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v]: "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 10:55:20.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1783" for this suite. 03/16/23 10:55:20.42
------------------------------
• [8.617 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:55:11.894
    Mar 16 10:55:11.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 10:55:11.895
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:12.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:12.34
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba 03/16/23 10:55:12.518
    Mar 16 10:55:12.697: INFO: Pod name my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba: Found 1 pods out of 1
    Mar 16 10:55:12.697: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba" are running
    Mar 16 10:55:12.697: INFO: Waiting up to 5m0s for pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" in namespace "replication-controller-1783" to be "running"
    Mar 16 10:55:12.787: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v": Phase="Pending", Reason="", readiness=false. Elapsed: 89.594725ms
    Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v": Phase="Running", Reason="", readiness=true. Elapsed: 2.179904931s
    Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" satisfied condition "running"
    Mar 16 10:55:14.877: INFO: Pod "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 10:55:12 +0000 UTC Reason: Message:}])
    Mar 16 10:55:14.877: INFO: Trying to dial the pod
    Mar 16 10:55:20.242: INFO: Controller my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba: Got expected result from replica 1 [my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v]: "my-hostname-basic-293e43ce-1a3d-4031-be48-b401e062daba-9h57v", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:55:20.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1783" for this suite. 03/16/23 10:55:20.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:55:20.511
Mar 16 10:55:20.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator 03/16/23 10:55:20.512
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:20.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:20.958
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 16 10:55:21.136: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/16/23 10:55:21.137
Mar 16 10:55:22.643: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:24.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:26.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:28.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:30.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:32.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:34.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:36.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:38.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:40.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:42.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 16 10:55:45.257: INFO: Waited 432.319051ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/16/23 10:55:46.37
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/16/23 10:55:46.46
STEP: List APIServices 03/16/23 10:55:46.551
Mar 16 10:55:46.643: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Mar 16 10:55:48.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-8395" for this suite. 03/16/23 10:55:48.606
------------------------------
• [28.185 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:55:20.511
    Mar 16 10:55:20.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename aggregator 03/16/23 10:55:20.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:20.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:20.958
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 16 10:55:21.136: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/16/23 10:55:21.137
    Mar 16 10:55:22.643: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:24.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:26.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:28.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:30.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:32.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:34.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:36.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:38.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:40.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:42.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 10, 55, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 16 10:55:45.257: INFO: Waited 432.319051ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/16/23 10:55:46.37
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/16/23 10:55:46.46
    STEP: List APIServices 03/16/23 10:55:46.551
    Mar 16 10:55:46.643: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:55:48.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-8395" for this suite. 03/16/23 10:55:48.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:55:48.698
Mar 16 10:55:48.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 10:55:48.699
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:48.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:49.146
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3677 03/16/23 10:55:49.324
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Mar 16 10:55:49.594: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 10:55:59.686: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/16/23 10:55:59.866
W0316 10:55:59.960736    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 16 10:56:00.140: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:56:00.140: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 10:56:10.230: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 10:56:10.230: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/16/23 10:56:10.409
STEP: Delete all of the StatefulSets 03/16/23 10:56:10.499
STEP: Verify that StatefulSets have been deleted 03/16/23 10:56:10.59
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 10:56:10.679: INFO: Deleting all statefulset in ns statefulset-3677
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:10.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3677" for this suite. 03/16/23 10:56:11.124
------------------------------
• [22.517 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:55:48.698
    Mar 16 10:55:48.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 10:55:48.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:55:48.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:55:49.146
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3677 03/16/23 10:55:49.324
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Mar 16 10:55:49.594: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 10:55:59.686: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/16/23 10:55:59.866
    W0316 10:55:59.960736    8588 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 16 10:56:00.140: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:56:00.140: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 10:56:10.230: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 10:56:10.230: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/16/23 10:56:10.409
    STEP: Delete all of the StatefulSets 03/16/23 10:56:10.499
    STEP: Verify that StatefulSets have been deleted 03/16/23 10:56:10.59
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 10:56:10.679: INFO: Deleting all statefulset in ns statefulset-3677
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:10.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3677" for this suite. 03/16/23 10:56:11.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:11.215
Mar 16 10:56:11.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 10:56:11.216
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:11.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:11.663
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 03/16/23 10:56:11.841
Mar 16 10:56:11.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664" in namespace "projected-7382" to be "Succeeded or Failed"
Mar 16 10:56:12.025: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Pending", Reason="", readiness=false. Elapsed: 89.344468ms
Mar 16 10:56:14.115: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179346447s
Mar 16 10:56:16.117: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180921737s
STEP: Saw pod success 03/16/23 10:56:16.117
Mar 16 10:56:16.117: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664" satisfied condition "Succeeded or Failed"
Mar 16 10:56:16.206: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 container client-container: <nil>
STEP: delete the pod 03/16/23 10:56:16.373
Mar 16 10:56:16.466: INFO: Waiting for pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 to disappear
Mar 16 10:56:16.555: INFO: Pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:16.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7382" for this suite. 03/16/23 10:56:16.733
------------------------------
• [5.608 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:11.215
    Mar 16 10:56:11.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 10:56:11.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:11.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:11.663
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 03/16/23 10:56:11.841
    Mar 16 10:56:11.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664" in namespace "projected-7382" to be "Succeeded or Failed"
    Mar 16 10:56:12.025: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Pending", Reason="", readiness=false. Elapsed: 89.344468ms
    Mar 16 10:56:14.115: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179346447s
    Mar 16 10:56:16.117: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180921737s
    STEP: Saw pod success 03/16/23 10:56:16.117
    Mar 16 10:56:16.117: INFO: Pod "downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664" satisfied condition "Succeeded or Failed"
    Mar 16 10:56:16.206: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 container client-container: <nil>
    STEP: delete the pod 03/16/23 10:56:16.373
    Mar 16 10:56:16.466: INFO: Waiting for pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 to disappear
    Mar 16 10:56:16.555: INFO: Pod downwardapi-volume-7245257e-37b9-439f-aa4d-dc001d2f6664 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:16.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7382" for this suite. 03/16/23 10:56:16.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:16.824
Mar 16 10:56:16.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:56:16.825
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:17.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:17.271
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 03/16/23 10:56:17.448
STEP: fetching the ConfigMap 03/16/23 10:56:17.538
STEP: patching the ConfigMap 03/16/23 10:56:17.628
STEP: listing all ConfigMaps in all namespaces with a label selector 03/16/23 10:56:17.718
STEP: deleting the ConfigMap by collection with a label selector 03/16/23 10:56:17.808
STEP: listing all ConfigMaps in test namespace 03/16/23 10:56:17.899
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9542" for this suite. 03/16/23 10:56:18.079
------------------------------
• [1.346 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:16.824
    Mar 16 10:56:16.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:56:16.825
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:17.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:17.271
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 03/16/23 10:56:17.448
    STEP: fetching the ConfigMap 03/16/23 10:56:17.538
    STEP: patching the ConfigMap 03/16/23 10:56:17.628
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/16/23 10:56:17.718
    STEP: deleting the ConfigMap by collection with a label selector 03/16/23 10:56:17.808
    STEP: listing all ConfigMaps in test namespace 03/16/23 10:56:17.899
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9542" for this suite. 03/16/23 10:56:18.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:18.171
Mar 16 10:56:18.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 10:56:18.173
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:18.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:18.618
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-c192c25a-0e04-4ab6-96dc-175d736a97c4 03/16/23 10:56:18.796
STEP: Creating a pod to test consume configMaps 03/16/23 10:56:18.886
Mar 16 10:56:18.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd" in namespace "configmap-5130" to be "Succeeded or Failed"
Mar 16 10:56:19.070: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.267722ms
Mar 16 10:56:21.161: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179962493s
Mar 16 10:56:23.160: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179251195s
STEP: Saw pod success 03/16/23 10:56:23.16
Mar 16 10:56:23.160: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd" satisfied condition "Succeeded or Failed"
Mar 16 10:56:23.249: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd container agnhost-container: <nil>
STEP: delete the pod 03/16/23 10:56:23.386
Mar 16 10:56:23.480: INFO: Waiting for pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd to disappear
Mar 16 10:56:23.569: INFO: Pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5130" for this suite. 03/16/23 10:56:23.747
------------------------------
• [5.666 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:18.171
    Mar 16 10:56:18.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 10:56:18.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:18.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:18.618
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-c192c25a-0e04-4ab6-96dc-175d736a97c4 03/16/23 10:56:18.796
    STEP: Creating a pod to test consume configMaps 03/16/23 10:56:18.886
    Mar 16 10:56:18.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd" in namespace "configmap-5130" to be "Succeeded or Failed"
    Mar 16 10:56:19.070: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.267722ms
    Mar 16 10:56:21.161: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179962493s
    Mar 16 10:56:23.160: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179251195s
    STEP: Saw pod success 03/16/23 10:56:23.16
    Mar 16 10:56:23.160: INFO: Pod "pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd" satisfied condition "Succeeded or Failed"
    Mar 16 10:56:23.249: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 10:56:23.386
    Mar 16 10:56:23.480: INFO: Waiting for pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd to disappear
    Mar 16 10:56:23.569: INFO: Pod pod-configmaps-2710c5f4-59c4-4297-9756-724ed7eeb8bd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5130" for this suite. 03/16/23 10:56:23.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:23.838
Mar 16 10:56:23.838: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass 03/16/23 10:56:23.839
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:24.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:24.286
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/16/23 10:56:24.464
STEP: getting /apis/networking.k8s.io 03/16/23 10:56:24.641
STEP: getting /apis/networking.k8s.iov1 03/16/23 10:56:24.729
STEP: creating 03/16/23 10:56:24.818
STEP: getting 03/16/23 10:56:25.087
STEP: listing 03/16/23 10:56:25.177
STEP: watching 03/16/23 10:56:25.266
Mar 16 10:56:25.267: INFO: starting watch
STEP: patching 03/16/23 10:56:25.355
STEP: updating 03/16/23 10:56:25.445
Mar 16 10:56:25.535: INFO: waiting for watch events with expected annotations
Mar 16 10:56:25.535: INFO: saw patched and updated annotations
STEP: deleting 03/16/23 10:56:25.535
STEP: deleting a collection 03/16/23 10:56:25.803
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:25.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3432" for this suite. 03/16/23 10:56:26.076
------------------------------
• [2.328 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:23.838
    Mar 16 10:56:23.838: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename ingressclass 03/16/23 10:56:23.839
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:24.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:24.286
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/16/23 10:56:24.464
    STEP: getting /apis/networking.k8s.io 03/16/23 10:56:24.641
    STEP: getting /apis/networking.k8s.iov1 03/16/23 10:56:24.729
    STEP: creating 03/16/23 10:56:24.818
    STEP: getting 03/16/23 10:56:25.087
    STEP: listing 03/16/23 10:56:25.177
    STEP: watching 03/16/23 10:56:25.266
    Mar 16 10:56:25.267: INFO: starting watch
    STEP: patching 03/16/23 10:56:25.355
    STEP: updating 03/16/23 10:56:25.445
    Mar 16 10:56:25.535: INFO: waiting for watch events with expected annotations
    Mar 16 10:56:25.535: INFO: saw patched and updated annotations
    STEP: deleting 03/16/23 10:56:25.535
    STEP: deleting a collection 03/16/23 10:56:25.803
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:25.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3432" for this suite. 03/16/23 10:56:26.076
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:26.167
Mar 16 10:56:26.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:56:26.168
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:26.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:26.614
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/16/23 10:56:26.791
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/16/23 10:56:26.88
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/16/23 10:56:26.88
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/16/23 10:56:26.88
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/16/23 10:56:26.969
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/16/23 10:56:26.969
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/16/23 10:56:27.057
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 10:56:27.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-169" for this suite. 03/16/23 10:56:27.148
------------------------------
• [1.071 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:26.167
    Mar 16 10:56:26.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 10:56:26.168
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:26.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:26.614
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/16/23 10:56:26.791
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/16/23 10:56:26.88
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/16/23 10:56:26.88
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/16/23 10:56:26.88
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/16/23 10:56:26.969
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/16/23 10:56:26.969
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/16/23 10:56:27.057
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 10:56:27.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-169" for this suite. 03/16/23 10:56:27.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 10:56:27.241
Mar 16 10:56:27.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 03/16/23 10:56:27.242
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:27.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:27.688
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/16/23 10:56:27.866
STEP: Ensuring no jobs are scheduled 03/16/23 10:56:27.956
------------------------------
Automatically polling progress:
  [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance] (Spec Runtime: 5m0.626s)
    test/e2e/apps/cronjob.go:96
    In [It] (Node Runtime: 5m0.001s)
      test/e2e/apps/cronjob.go:96
      At [By Step] Ensuring no jobs are scheduled (Step Runtime: 4m59.91s)
        test/e2e/apps/cronjob.go:105

      Spec Goroutine
      goroutine 28888 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x7fe5ba8, 0xc000136000}, 0xc005079008, 0x2fdd8ca?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fe5ba8, 0xc000136000}, 0xd8?, 0x2fdc465?, 0x30?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x7fe5ba8, 0xc000136000}, 0x764a183?, 0xc00192de28?, 0x262c967?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:460
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Poll(0x0?, 0xc0fcdb56f90489fc?, 0x3f06f75edad?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:445
      > k8s.io/kubernetes/test/e2e/apps.waitForNoJobs({0x8022ee8?, 0xc0059b64e0}, {0xc0059cadf0, 0xc}, {0xc002ecd220, 0x9}, 0x0)
          test/e2e/apps/cronjob.go:607
      > k8s.io/kubernetes/test/e2e/apps.glob..func2.2()
          test/e2e/apps/cronjob.go:106
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc004be8ab0, 0xc001331440})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
STEP: Ensuring no job exists by listing jobs explicitly 03/16/23 11:01:28.136
STEP: Removing cronjob 03/16/23 11:01:28.225
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 16 11:01:28.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7511" for this suite. 03/16/23 11:01:28.494
• [SLOW TEST] [301.344 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 10:56:27.241
    Mar 16 10:56:27.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 03/16/23 10:56:27.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 10:56:27.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 10:56:27.688
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/16/23 10:56:27.866
    STEP: Ensuring no jobs are scheduled 03/16/23 10:56:27.956
    STEP: Ensuring no job exists by listing jobs explicitly 03/16/23 11:01:28.136
    STEP: Removing cronjob 03/16/23 11:01:28.225
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:01:28.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7511" for this suite. 03/16/23 11:01:28.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:01:28.586
Mar 16 11:01:28.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 11:01:28.588
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:28.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:29.034
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 03/16/23 11:01:29.211
STEP: Creating a ResourceQuota 03/16/23 11:01:34.302
STEP: Ensuring resource quota status is calculated 03/16/23 11:01:34.392
STEP: Creating a ReplicationController 03/16/23 11:01:36.482
STEP: Ensuring resource quota status captures replication controller creation 03/16/23 11:01:36.576
STEP: Deleting a ReplicationController 03/16/23 11:01:38.667
STEP: Ensuring resource quota status released usage 03/16/23 11:01:38.763
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 11:01:40.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5917" for this suite. 03/16/23 11:01:41.03
------------------------------
• [12.534 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:01:28.586
    Mar 16 11:01:28.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 11:01:28.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:28.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:29.034
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 03/16/23 11:01:29.211
    STEP: Creating a ResourceQuota 03/16/23 11:01:34.302
    STEP: Ensuring resource quota status is calculated 03/16/23 11:01:34.392
    STEP: Creating a ReplicationController 03/16/23 11:01:36.482
    STEP: Ensuring resource quota status captures replication controller creation 03/16/23 11:01:36.576
    STEP: Deleting a ReplicationController 03/16/23 11:01:38.667
    STEP: Ensuring resource quota status released usage 03/16/23 11:01:38.763
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:01:40.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5917" for this suite. 03/16/23 11:01:41.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:01:41.121
Mar 16 11:01:41.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 03/16/23 11:01:41.122
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:41.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:41.568
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/16/23 11:01:41.745
STEP: listing all events in all namespaces 03/16/23 11:01:41.835
STEP: patching the test event 03/16/23 11:01:41.925
STEP: fetching the test event 03/16/23 11:01:42.017
STEP: updating the test event 03/16/23 11:01:42.106
STEP: getting the test event 03/16/23 11:01:42.286
STEP: deleting the test event 03/16/23 11:01:42.376
STEP: listing all events in all namespaces 03/16/23 11:01:42.467
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 16 11:01:42.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8681" for this suite. 03/16/23 11:01:42.648
------------------------------
• [1.618 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:01:41.121
    Mar 16 11:01:41.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 03/16/23 11:01:41.122
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:41.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:41.568
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/16/23 11:01:41.745
    STEP: listing all events in all namespaces 03/16/23 11:01:41.835
    STEP: patching the test event 03/16/23 11:01:41.925
    STEP: fetching the test event 03/16/23 11:01:42.017
    STEP: updating the test event 03/16/23 11:01:42.106
    STEP: getting the test event 03/16/23 11:01:42.286
    STEP: deleting the test event 03/16/23 11:01:42.376
    STEP: listing all events in all namespaces 03/16/23 11:01:42.467
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:01:42.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8681" for this suite. 03/16/23 11:01:42.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:01:42.74
Mar 16 11:01:42.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 11:01:42.741
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:43.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:43.186
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 03/16/23 11:01:43.364
Mar 16 11:01:43.364: INFO: namespace kubectl-5677
Mar 16 11:01:43.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 create -f -'
Mar 16 11:01:44.883: INFO: stderr: ""
Mar 16 11:01:44.883: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/16/23 11:01:44.883
Mar 16 11:01:45.973: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 11:01:45.973: INFO: Found 1 / 1
Mar 16 11:01:45.973: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 16 11:01:46.063: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 16 11:01:46.063: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 16 11:01:46.063: INFO: wait on agnhost-primary startup in kubectl-5677 
Mar 16 11:01:46.063: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 logs agnhost-primary-mgtxt agnhost-primary'
Mar 16 11:01:46.523: INFO: stderr: ""
Mar 16 11:01:46.523: INFO: stdout: "Paused\n"
STEP: exposing RC 03/16/23 11:01:46.523
Mar 16 11:01:46.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 16 11:01:46.987: INFO: stderr: ""
Mar 16 11:01:46.987: INFO: stdout: "service/rm2 exposed\n"
Mar 16 11:01:47.076: INFO: Service rm2 in namespace kubectl-5677 found.
STEP: exposing service 03/16/23 11:01:49.256
Mar 16 11:01:49.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 16 11:01:49.704: INFO: stderr: ""
Mar 16 11:01:49.704: INFO: stdout: "service/rm3 exposed\n"
Mar 16 11:01:49.793: INFO: Service rm3 in namespace kubectl-5677 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 11:01:51.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5677" for this suite. 03/16/23 11:01:52.151
------------------------------
• [9.502 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:01:42.74
    Mar 16 11:01:42.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 11:01:42.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:43.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:43.186
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 03/16/23 11:01:43.364
    Mar 16 11:01:43.364: INFO: namespace kubectl-5677
    Mar 16 11:01:43.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 create -f -'
    Mar 16 11:01:44.883: INFO: stderr: ""
    Mar 16 11:01:44.883: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/16/23 11:01:44.883
    Mar 16 11:01:45.973: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 11:01:45.973: INFO: Found 1 / 1
    Mar 16 11:01:45.973: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 16 11:01:46.063: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 16 11:01:46.063: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 16 11:01:46.063: INFO: wait on agnhost-primary startup in kubectl-5677 
    Mar 16 11:01:46.063: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 logs agnhost-primary-mgtxt agnhost-primary'
    Mar 16 11:01:46.523: INFO: stderr: ""
    Mar 16 11:01:46.523: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/16/23 11:01:46.523
    Mar 16 11:01:46.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 16 11:01:46.987: INFO: stderr: ""
    Mar 16 11:01:46.987: INFO: stdout: "service/rm2 exposed\n"
    Mar 16 11:01:47.076: INFO: Service rm2 in namespace kubectl-5677 found.
    STEP: exposing service 03/16/23 11:01:49.256
    Mar 16 11:01:49.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5677 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 16 11:01:49.704: INFO: stderr: ""
    Mar 16 11:01:49.704: INFO: stdout: "service/rm3 exposed\n"
    Mar 16 11:01:49.793: INFO: Service rm3 in namespace kubectl-5677 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:01:51.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5677" for this suite. 03/16/23 11:01:52.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:01:52.242
Mar 16 11:01:52.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 11:01:52.243
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:52.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:52.689
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 03/16/23 11:01:52.866
STEP: Counting existing ResourceQuota 03/16/23 11:01:57.956
STEP: Creating a ResourceQuota 03/16/23 11:02:03.047
STEP: Ensuring resource quota status is calculated 03/16/23 11:02:03.137
STEP: Creating a Secret 03/16/23 11:02:05.229
STEP: Ensuring resource quota status captures secret creation 03/16/23 11:02:05.323
STEP: Deleting a secret 03/16/23 11:02:07.413
STEP: Ensuring resource quota status released usage 03/16/23 11:02:07.504
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1345" for this suite. 03/16/23 11:02:09.771
------------------------------
• [17.620 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:01:52.242
    Mar 16 11:01:52.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 11:01:52.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:01:52.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:01:52.689
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 03/16/23 11:01:52.866
    STEP: Counting existing ResourceQuota 03/16/23 11:01:57.956
    STEP: Creating a ResourceQuota 03/16/23 11:02:03.047
    STEP: Ensuring resource quota status is calculated 03/16/23 11:02:03.137
    STEP: Creating a Secret 03/16/23 11:02:05.229
    STEP: Ensuring resource quota status captures secret creation 03/16/23 11:02:05.323
    STEP: Deleting a secret 03/16/23 11:02:07.413
    STEP: Ensuring resource quota status released usage 03/16/23 11:02:07.504
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1345" for this suite. 03/16/23 11:02:09.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:09.863
Mar 16 11:02:09.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 11:02:09.864
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:10.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:10.31
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 03/16/23 11:02:10.487
Mar 16 11:02:10.583: INFO: Waiting up to 5m0s for pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da" in namespace "var-expansion-888" to be "Succeeded or Failed"
Mar 16 11:02:10.673: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824614ms
Mar 16 11:02:12.763: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180003833s
Mar 16 11:02:14.765: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181645528s
STEP: Saw pod success 03/16/23 11:02:14.765
Mar 16 11:02:14.765: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da" satisfied condition "Succeeded or Failed"
Mar 16 11:02:14.855: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da container dapi-container: <nil>
STEP: delete the pod 03/16/23 11:02:14.991
Mar 16 11:02:15.084: INFO: Waiting for pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da to disappear
Mar 16 11:02:15.174: INFO: Pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:15.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-888" for this suite. 03/16/23 11:02:15.352
------------------------------
• [5.579 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:09.863
    Mar 16 11:02:09.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 11:02:09.864
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:10.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:10.31
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 03/16/23 11:02:10.487
    Mar 16 11:02:10.583: INFO: Waiting up to 5m0s for pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da" in namespace "var-expansion-888" to be "Succeeded or Failed"
    Mar 16 11:02:10.673: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824614ms
    Mar 16 11:02:12.763: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180003833s
    Mar 16 11:02:14.765: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181645528s
    STEP: Saw pod success 03/16/23 11:02:14.765
    Mar 16 11:02:14.765: INFO: Pod "var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da" satisfied condition "Succeeded or Failed"
    Mar 16 11:02:14.855: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da container dapi-container: <nil>
    STEP: delete the pod 03/16/23 11:02:14.991
    Mar 16 11:02:15.084: INFO: Waiting for pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da to disappear
    Mar 16 11:02:15.174: INFO: Pod var-expansion-14602857-afa0-462a-8b8f-c94e0fe1b1da no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:15.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-888" for this suite. 03/16/23 11:02:15.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:15.443
Mar 16 11:02:15.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 03/16/23 11:02:15.444
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:15.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:15.89
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 16 11:02:16.068: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 16 11:02:16.248: INFO: Waiting for terminating namespaces to be deleted...
Mar 16 11:02:16.338: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
Mar 16 11:02:16.519: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.519: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 16 11:02:16.519: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.519: INFO: 	Container proxy ready: true, restart count 0
Mar 16 11:02:16.519: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 11:02:16.519: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.519: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 11:02:16.519: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.519: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 11:02:16.520: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 11:02:16.520: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 11:02:16.520: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 11:02:16.520: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 11:02:16.520: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 11:02:16.520: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 11:02:16.520: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 11:02:16.520: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 11:02:16.520: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 11:02:16.520: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 11:02:16.520: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 11:02:16.520: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 11:02:16.520: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 11:02:16.520: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 11:02:16.520: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container vpn-shoot ready: true, restart count 0
Mar 16 11:02:16.520: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.520: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 16 11:02:16.520: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
Mar 16 11:02:16.616: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Mar 16 11:02:16.616: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container proxy ready: true, restart count 0
Mar 16 11:02:16.616: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 11:02:16.616: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 11:02:16.616: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 11:02:16.616: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 11:02:16.616: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:16.616: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 11:02:16.616: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:16.616: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:16.616: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container coredns ready: true, restart count 0
Mar 16 11:02:16.616: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container coredns ready: true, restart count 0
Mar 16 11:02:16.616: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 11:02:16.616: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 11:02:16.616: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 11:02:16.616: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 11:02:16.616: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 11:02:16.616: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 11:02:16.616: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 11:02:16.616: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 11:02:16.616: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 11:02:16.616: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 11:02:16.616: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 11:02:16.616: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 11:02:16.616: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:16.616: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/16/23 11:02:16.616
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174ce159cee6ad04], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 03/16/23 11:02:16.988
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9502" for this suite. 03/16/23 11:02:18.254
------------------------------
• [2.902 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:15.443
    Mar 16 11:02:15.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 03/16/23 11:02:15.444
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:15.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:15.89
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 16 11:02:16.068: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 16 11:02:16.248: INFO: Waiting for terminating namespaces to be deleted...
    Mar 16 11:02:16.338: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
    Mar 16 11:02:16.519: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.519: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Mar 16 11:02:16.519: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.519: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 11:02:16.519: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 11:02:16.519: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.519: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 11:02:16.519: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.519: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container vpn-shoot ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.520: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 16 11:02:16.520: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
    Mar 16 11:02:16.616: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 11:02:16.616: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:16.616: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/16/23 11:02:16.616
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.174ce159cee6ad04], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 03/16/23 11:02:16.988
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9502" for this suite. 03/16/23 11:02:18.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:18.346
Mar 16 11:02:18.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:02:18.347
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:18.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:18.793
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:02:19.152
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:02:19.851
STEP: Deploying the webhook pod 03/16/23 11:02:19.942
STEP: Wait for the deployment to be ready 03/16/23 11:02:20.123
Mar 16 11:02:20.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:02:22.482
STEP: Verifying the service has paired with the endpoint 03/16/23 11:02:22.576
Mar 16 11:02:23.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 03/16/23 11:02:23.666
STEP: create a pod 03/16/23 11:02:23.951
Mar 16 11:02:24.044: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2617" to be "running"
Mar 16 11:02:24.133: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.572398ms
Mar 16 11:02:26.224: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180447573s
Mar 16 11:02:26.224: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/16/23 11:02:26.224
Mar 16 11:02:26.224: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-2617 attach --namespace=webhook-2617 to-be-attached-pod -i -c=container1'
Mar 16 11:02:26.978: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:27.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2617" for this suite. 03/16/23 11:02:27.614
STEP: Destroying namespace "webhook-2617-markers" for this suite. 03/16/23 11:02:27.704
------------------------------
• [9.449 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:18.346
    Mar 16 11:02:18.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:02:18.347
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:18.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:18.793
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:02:19.152
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:02:19.851
    STEP: Deploying the webhook pod 03/16/23 11:02:19.942
    STEP: Wait for the deployment to be ready 03/16/23 11:02:20.123
    Mar 16 11:02:20.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 2, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:02:22.482
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:02:22.576
    Mar 16 11:02:23.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 03/16/23 11:02:23.666
    STEP: create a pod 03/16/23 11:02:23.951
    Mar 16 11:02:24.044: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2617" to be "running"
    Mar 16 11:02:24.133: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 89.572398ms
    Mar 16 11:02:26.224: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.180447573s
    Mar 16 11:02:26.224: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/16/23 11:02:26.224
    Mar 16 11:02:26.224: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-2617 attach --namespace=webhook-2617 to-be-attached-pod -i -c=container1'
    Mar 16 11:02:26.978: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:27.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2617" for this suite. 03/16/23 11:02:27.614
    STEP: Destroying namespace "webhook-2617-markers" for this suite. 03/16/23 11:02:27.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:27.795
Mar 16 11:02:27.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 03/16/23 11:02:27.796
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:28.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:28.242
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Mar 16 11:02:28.514: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261" in namespace "security-context-test-6064" to be "Succeeded or Failed"
Mar 16 11:02:28.604: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Pending", Reason="", readiness=false. Elapsed: 89.597511ms
Mar 16 11:02:30.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179804086s
Mar 16 11:02:32.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179507708s
Mar 16 11:02:32.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6064" for this suite. 03/16/23 11:02:32.979
------------------------------
• [5.274 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:27.795
    Mar 16 11:02:27.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 03/16/23 11:02:27.796
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:28.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:28.242
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Mar 16 11:02:28.514: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261" in namespace "security-context-test-6064" to be "Succeeded or Failed"
    Mar 16 11:02:28.604: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Pending", Reason="", readiness=false. Elapsed: 89.597511ms
    Mar 16 11:02:30.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179804086s
    Mar 16 11:02:32.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179507708s
    Mar 16 11:02:32.694: INFO: Pod "alpine-nnp-false-cd8f95fb-b8e1-49d0-bfa6-6e4d60c0f261" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6064" for this suite. 03/16/23 11:02:32.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:33.07
Mar 16 11:02:33.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 11:02:33.072
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:33.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:33.518
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 16 11:02:33.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:34.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9546" for this suite. 03/16/23 11:02:34.145
------------------------------
• [1.165 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:33.07
    Mar 16 11:02:33.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename custom-resource-definition 03/16/23 11:02:33.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:33.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:33.518
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 16 11:02:33.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:34.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9546" for this suite. 03/16/23 11:02:34.145
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:34.236
Mar 16 11:02:34.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 11:02:34.237
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:34.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:34.684
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Mar 16 11:02:34.956: INFO: Waiting up to 2m0s for pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" in namespace "var-expansion-4174" to be "container 0 failed with reason CreateContainerConfigError"
Mar 16 11:02:35.046: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473": Phase="Pending", Reason="", readiness=false. Elapsed: 89.540655ms
Mar 16 11:02:37.136: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179677488s
Mar 16 11:02:37.136: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 16 11:02:37.136: INFO: Deleting pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" in namespace "var-expansion-4174"
Mar 16 11:02:37.227: INFO: Wait up to 5m0s for pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:39.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4174" for this suite. 03/16/23 11:02:39.584
------------------------------
• [5.438 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:34.236
    Mar 16 11:02:34.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 11:02:34.237
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:34.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:34.684
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Mar 16 11:02:34.956: INFO: Waiting up to 2m0s for pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" in namespace "var-expansion-4174" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 16 11:02:35.046: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473": Phase="Pending", Reason="", readiness=false. Elapsed: 89.540655ms
    Mar 16 11:02:37.136: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179677488s
    Mar 16 11:02:37.136: INFO: Pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 16 11:02:37.136: INFO: Deleting pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" in namespace "var-expansion-4174"
    Mar 16 11:02:37.227: INFO: Wait up to 5m0s for pod "var-expansion-19a7aeca-7671-49c8-b4ae-a31124159473" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:39.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4174" for this suite. 03/16/23 11:02:39.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:39.674
Mar 16 11:02:39.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 11:02:39.676
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:39.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:40.122
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-232 03/16/23 11:02:40.3
STEP: creating service affinity-nodeport in namespace services-232 03/16/23 11:02:40.3
STEP: creating replication controller affinity-nodeport in namespace services-232 03/16/23 11:02:40.396
I0316 11:02:40.492707    8588 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-232, replica count: 3
I0316 11:02:43.594221    8588 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 11:02:43.950: INFO: Creating new exec pod
Mar 16 11:02:44.043: INFO: Waiting up to 5m0s for pod "execpod-affinityrmh6z" in namespace "services-232" to be "running"
Mar 16 11:02:44.132: INFO: Pod "execpod-affinityrmh6z": Phase="Pending", Reason="", readiness=false. Elapsed: 89.419074ms
Mar 16 11:02:46.223: INFO: Pod "execpod-affinityrmh6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.180565786s
Mar 16 11:02:46.223: INFO: Pod "execpod-affinityrmh6z" satisfied condition "running"
Mar 16 11:02:47.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Mar 16 11:02:48.534: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 16 11:02:48.534: INFO: stdout: ""
Mar 16 11:02:48.534: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 100.109.29.81 80'
Mar 16 11:02:49.655: INFO: stderr: "+ nc -v -z -w 2 100.109.29.81 80\nConnection to 100.109.29.81 80 port [tcp/http] succeeded!\n"
Mar 16 11:02:49.655: INFO: stdout: ""
Mar 16 11:02:49.655: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 31792'
Mar 16 11:02:50.755: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 31792\nConnection to 10.250.19.136 31792 port [tcp/*] succeeded!\n"
Mar 16 11:02:50.755: INFO: stdout: ""
Mar 16 11:02:50.755: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 31792'
Mar 16 11:02:51.857: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 31792\nConnection to 10.250.19.246 31792 port [tcp/*] succeeded!\n"
Mar 16 11:02:51.857: INFO: stdout: ""
Mar 16 11:02:51.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:31792/ ; done'
Mar 16 11:02:53.037: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n"
Mar 16 11:02:53.037: INFO: stdout: "\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw"
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
Mar 16 11:02:53.037: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-232, will wait for the garbage collector to delete the pods 03/16/23 11:02:53.131
Mar 16 11:02:53.411: INFO: Deleting ReplicationController affinity-nodeport took: 90.052896ms
Mar 16 11:02:53.511: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.474181ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 11:02:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-232" for this suite. 03/16/23 11:02:55.4
------------------------------
• [15.815 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:39.674
    Mar 16 11:02:39.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 11:02:39.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:39.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:40.122
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-232 03/16/23 11:02:40.3
    STEP: creating service affinity-nodeport in namespace services-232 03/16/23 11:02:40.3
    STEP: creating replication controller affinity-nodeport in namespace services-232 03/16/23 11:02:40.396
    I0316 11:02:40.492707    8588 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-232, replica count: 3
    I0316 11:02:43.594221    8588 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 11:02:43.950: INFO: Creating new exec pod
    Mar 16 11:02:44.043: INFO: Waiting up to 5m0s for pod "execpod-affinityrmh6z" in namespace "services-232" to be "running"
    Mar 16 11:02:44.132: INFO: Pod "execpod-affinityrmh6z": Phase="Pending", Reason="", readiness=false. Elapsed: 89.419074ms
    Mar 16 11:02:46.223: INFO: Pod "execpod-affinityrmh6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.180565786s
    Mar 16 11:02:46.223: INFO: Pod "execpod-affinityrmh6z" satisfied condition "running"
    Mar 16 11:02:47.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Mar 16 11:02:48.534: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 16 11:02:48.534: INFO: stdout: ""
    Mar 16 11:02:48.534: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 100.109.29.81 80'
    Mar 16 11:02:49.655: INFO: stderr: "+ nc -v -z -w 2 100.109.29.81 80\nConnection to 100.109.29.81 80 port [tcp/http] succeeded!\n"
    Mar 16 11:02:49.655: INFO: stdout: ""
    Mar 16 11:02:49.655: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 31792'
    Mar 16 11:02:50.755: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 31792\nConnection to 10.250.19.136 31792 port [tcp/*] succeeded!\n"
    Mar 16 11:02:50.755: INFO: stdout: ""
    Mar 16 11:02:50.755: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 31792'
    Mar 16 11:02:51.857: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 31792\nConnection to 10.250.19.246 31792 port [tcp/*] succeeded!\n"
    Mar 16 11:02:51.857: INFO: stdout: ""
    Mar 16 11:02:51.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-232 exec execpod-affinityrmh6z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.19.136:31792/ ; done'
    Mar 16 11:02:53.037: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.19.136:31792/\n"
    Mar 16 11:02:53.037: INFO: stdout: "\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw\naffinity-nodeport-6hzgw"
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Received response from host: affinity-nodeport-6hzgw
    Mar 16 11:02:53.037: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-232, will wait for the garbage collector to delete the pods 03/16/23 11:02:53.131
    Mar 16 11:02:53.411: INFO: Deleting ReplicationController affinity-nodeport took: 90.052896ms
    Mar 16 11:02:53.511: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.474181ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:02:55.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-232" for this suite. 03/16/23 11:02:55.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:02:55.49
Mar 16 11:02:55.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred 03/16/23 11:02:55.491
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:55.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:55.937
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 16 11:02:56.115: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 16 11:02:56.295: INFO: Waiting for terminating namespaces to be deleted...
Mar 16 11:02:56.384: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
Mar 16 11:02:56.567: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 16 11:02:56.567: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container proxy ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 11:02:56.567: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 11:02:56.567: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 11:02:56.567: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 11:02:56.567: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 11:02:56.567: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 11:02:56.567: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 11:02:56.567: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 11:02:56.567: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 11:02:56.567: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 11:02:56.567: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 11:02:56.567: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 11:02:56.567: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 11:02:56.567: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container vpn-shoot ready: true, restart count 0
Mar 16 11:02:56.567: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.567: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 16 11:02:56.567: INFO: 
Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
Mar 16 11:02:56.662: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Mar 16 11:02:56.662: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container proxy ready: true, restart count 0
Mar 16 11:02:56.662: INFO: 	Container sidecar ready: true, restart count 0
Mar 16 11:02:56.662: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container blackbox-exporter ready: true, restart count 0
Mar 16 11:02:56.662: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
Mar 16 11:02:56.662: INFO: 	Container calico-node ready: true, restart count 0
Mar 16 11:02:56.662: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:56.662: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container calico-typha ready: true, restart count 0
Mar 16 11:02:56.662: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:56.662: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
Mar 16 11:02:56.662: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container coredns ready: true, restart count 0
Mar 16 11:02:56.662: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container coredns ready: true, restart count 0
Mar 16 11:02:56.662: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container csi-driver ready: true, restart count 0
Mar 16 11:02:56.662: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Mar 16 11:02:56.662: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 16 11:02:56.662: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container egress-filter-applier ready: true, restart count 0
Mar 16 11:02:56.662: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container conntrack-fix ready: true, restart count 0
Mar 16 11:02:56.662: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 16 11:02:56.662: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container metrics-server ready: true, restart count 0
Mar 16 11:02:56.662: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container network-problem-detector-host ready: true, restart count 0
Mar 16 11:02:56.662: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
Mar 16 11:02:56.662: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container node-exporter ready: true, restart count 0
Mar 16 11:02:56.662: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container node-cache ready: true, restart count 0
Mar 16 11:02:56.662: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container node-problem-detector ready: true, restart count 0
Mar 16 11:02:56.662: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
Mar 16 11:02:56.662: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 11:02:56.662
Mar 16 11:02:56.757: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7651" to be "running"
Mar 16 11:02:56.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 89.395017ms
Mar 16 11:02:58.937: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.18053119s
Mar 16 11:02:58.937: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 11:02:59.027
STEP: Trying to apply a random label on the found node. 03/16/23 11:02:59.296
STEP: verifying the node has the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 95 03/16/23 11:02:59.396
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/16/23 11:02:59.485
Mar 16 11:02:59.578: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7651" to be "not pending"
Mar 16 11:02:59.668: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.485072ms
Mar 16 11:03:01.758: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.179512758s
Mar 16 11:03:01.758: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.19.136 on the node which pod4 resides and expect not scheduled 03/16/23 11:03:01.758
Mar 16 11:03:01.851: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7651" to be "not pending"
Mar 16 11:03:01.941: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.492525ms
Mar 16 11:03:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179694263s
Mar 16 11:03:06.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181240539s
Mar 16 11:03:08.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180106733s
Mar 16 11:03:10.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.180852907s
Mar 16 11:03:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.17946101s
Mar 16 11:03:14.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.181075635s
Mar 16 11:03:16.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.179995019s
Mar 16 11:03:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.179898755s
Mar 16 11:03:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.18036598s
Mar 16 11:03:22.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.180937308s
Mar 16 11:03:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179990206s
Mar 16 11:03:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.179925361s
Mar 16 11:03:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.17993584s
Mar 16 11:03:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.18040941s
Mar 16 11:03:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.179448694s
Mar 16 11:03:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.179340908s
Mar 16 11:03:36.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.180773007s
Mar 16 11:03:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.179996439s
Mar 16 11:03:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.18023118s
Mar 16 11:03:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.18078065s
Mar 16 11:03:44.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.18106259s
Mar 16 11:03:46.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.179956283s
Mar 16 11:03:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.179622499s
Mar 16 11:03:50.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.180993594s
Mar 16 11:03:52.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.179827561s
Mar 16 11:03:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.179802911s
Mar 16 11:03:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.17966378s
Mar 16 11:03:58.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.179197143s
Mar 16 11:04:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.180937313s
Mar 16 11:04:02.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.180181276s
Mar 16 11:04:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.179894698s
Mar 16 11:04:06.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.18005713s
Mar 16 11:04:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.179967699s
Mar 16 11:04:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.179264821s
Mar 16 11:04:12.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.180845273s
Mar 16 11:04:14.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.180134657s
Mar 16 11:04:16.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.180775186s
Mar 16 11:04:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.179669896s
Mar 16 11:04:20.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.179660411s
Mar 16 11:04:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.17952268s
Mar 16 11:04:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.179434985s
Mar 16 11:04:26.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.180816141s
Mar 16 11:04:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.179309573s
Mar 16 11:04:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.180626985s
Mar 16 11:04:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.179868067s
Mar 16 11:04:34.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.180561076s
Mar 16 11:04:36.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.181331387s
Mar 16 11:04:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.180054702s
Mar 16 11:04:40.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.179849308s
Mar 16 11:04:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.180106467s
Mar 16 11:04:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.179314669s
Mar 16 11:04:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.180881569s
Mar 16 11:04:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.179778218s
Mar 16 11:04:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.179832185s
Mar 16 11:04:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.180074541s
Mar 16 11:04:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.179779531s
Mar 16 11:04:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.179994085s
Mar 16 11:04:58.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.180050137s
Mar 16 11:05:00.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.180025509s
Mar 16 11:05:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.179842321s
Mar 16 11:05:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.179740191s
Mar 16 11:05:06.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.179876243s
Mar 16 11:05:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.179778455s
Mar 16 11:05:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.179749689s
Mar 16 11:05:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.179646701s
Mar 16 11:05:14.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.179788041s
Mar 16 11:05:16.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.179927237s
Mar 16 11:05:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.179957148s
Mar 16 11:05:20.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.182148239s
Mar 16 11:05:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.179767897s
Mar 16 11:05:24.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.180253547s
Mar 16 11:05:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.179936297s
Mar 16 11:05:28.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.180112158s
Mar 16 11:05:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.180609801s
Mar 16 11:05:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.179735297s
Mar 16 11:05:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.179910736s
Mar 16 11:05:36.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.179883502s
Mar 16 11:05:38.036: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.184072887s
Mar 16 11:05:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.180066635s
Mar 16 11:05:42.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.179757641s
Mar 16 11:05:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.179657554s
Mar 16 11:05:46.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.179813804s
Mar 16 11:05:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.179903068s
Mar 16 11:05:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.17980282s
Mar 16 11:05:52.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.179696344s
Mar 16 11:05:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.179798497s
Mar 16 11:05:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.180003791s
Mar 16 11:05:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.180075008s
Mar 16 11:06:00.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.179833521s
Mar 16 11:06:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.179846362s
Mar 16 11:06:04.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.180699027s
Mar 16 11:06:06.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.181176743s
Mar 16 11:06:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.17999589s
Mar 16 11:06:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.17910718s
Mar 16 11:06:12.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.180096865s
Mar 16 11:06:14.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.179632997s
Mar 16 11:06:16.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.181267646s
Mar 16 11:06:18.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.180251982s
Mar 16 11:06:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.180884599s
Mar 16 11:06:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.179798842s
Mar 16 11:06:24.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.180355497s
Mar 16 11:06:26.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.181112231s
Mar 16 11:06:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.18000821s
Mar 16 11:06:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.180544845s
Mar 16 11:06:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.179898582s
Mar 16 11:06:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.179977151s
Mar 16 11:06:36.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.179737405s
Mar 16 11:06:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.179751243s
Mar 16 11:06:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.18007904s
Mar 16 11:06:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.180431494s
Mar 16 11:06:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.179607694s
Mar 16 11:06:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.180855785s
Mar 16 11:06:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.180057303s
Mar 16 11:06:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.179182425s
Mar 16 11:06:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.180800258s
Mar 16 11:06:54.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.180442263s
Mar 16 11:06:56.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.18141922s
Mar 16 11:06:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.180184842s
Mar 16 11:07:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.180068258s
Mar 16 11:07:02.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.18020464s
Mar 16 11:07:04.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.180336146s
Mar 16 11:07:06.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.180957119s
Mar 16 11:07:08.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.180306393s
Mar 16 11:07:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.179857898s
Mar 16 11:07:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.179928594s
Mar 16 11:07:14.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.180062236s
Mar 16 11:07:16.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.181688878s
Mar 16 11:07:18.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.180153555s
Mar 16 11:07:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.180062112s
Mar 16 11:07:22.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.180145008s
Mar 16 11:07:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.180010314s
Mar 16 11:07:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.179993724s
Mar 16 11:07:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.180039782s
Mar 16 11:07:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.180694313s
Mar 16 11:07:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.17983068s
Mar 16 11:07:34.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.180373136s
Mar 16 11:07:36.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.180813806s
Mar 16 11:07:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.18000867s
Mar 16 11:07:40.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.179653179s
Mar 16 11:07:42.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.181114823s
Mar 16 11:07:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.179666927s
Mar 16 11:07:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.180177105s
Mar 16 11:07:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.180043176s
Mar 16 11:07:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.179624993s
Mar 16 11:07:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.180786423s
Mar 16 11:07:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.179918717s
Mar 16 11:07:56.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.180697385s
------------------------------
Automatically polling progress:
  [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance] (Spec Runtime: 5m1.173s)
    test/e2e/scheduling/predicates.go:704
    In [It] (Node Runtime: 5m0.001s)
      test/e2e/scheduling/predicates.go:704
      At [By Step] Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.19.136 on the node which pod4 resides and expect not scheduled (Step Runtime: 4m54.905s)
        test/e2e/scheduling/predicates.go:723

      Spec Goroutine
      goroutine 29932 [select]
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.WaitForWithContext({0x7fe5ba8, 0xc000136000}, 0xc002483518, 0x2fdd8ca?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:660
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fe5ba8, 0xc000136000}, 0x90?, 0x2fdc465?, 0x70?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:596
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediateWithContext({0x7fe5ba8, 0xc000136000}, 0x75b9afa?, 0xc003fc5be0?, 0x262c967?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:528
        k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollImmediate(0x75bb862?, 0x4?, 0x76f8717?)
          vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:514
        k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodCondition({0x8022ee8?, 0xc002cc7040}, {0xc000803600, 0xf}, {0x75bc1ea, 0x4}, {0x75d35f4, 0xb}, 0xc0071d12a0?, 0x789a630)
          test/e2e/framework/pod/wait.go:290
        k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodNotPending({0x8022ee8?, 0xc002cc7040?}, {0xc000803600?, 0x0?}, {0x75bc1ea?, 0x0?})
          test/e2e/framework/pod/wait.go:585
      > k8s.io/kubernetes/test/e2e/scheduling.createHostPortPodOnNode(0xc0004193b0, {0x75bc1ea, 0x4}, {0xc000803600, 0xf}, {0xc004c2c9e0, 0xd}, 0xd432, {0x75bac1d, 0x3}, ...)
          test/e2e/scheduling/predicates.go:1153
      > k8s.io/kubernetes/test/e2e/scheduling.glob..func4.13()
          test/e2e/scheduling/predicates.go:724
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func3({0xc00756c0c0, 0x0})
          vendor/github.com/onsi/ginkgo/v2/internal/node.go:449
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func2()
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:750
        k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
          vendor/github.com/onsi/ginkgo/v2/internal/suite.go:738
------------------------------
Mar 16 11:07:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.180101782s
Mar 16 11:08:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.180077413s
Mar 16 11:08:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.180056208s
Mar 16 11:08:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.269624511s
STEP: removing the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 off the node ip-10-250-19-136.ec2.internal 03/16/23 11:08:02.121
STEP: verifying the node doesn't have the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 03/16/23 11:08:02.481
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:08:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7651" for this suite. 03/16/23 11:08:02.66
• [SLOW TEST] [307.261 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:02:55.49
    Mar 16 11:02:55.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-pred 03/16/23 11:02:55.491
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:02:55.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:02:55.937
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 16 11:02:56.115: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 16 11:02:56.295: INFO: Waiting for terminating namespaces to be deleted...
    Mar 16 11:02:56.384: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-136.ec2.internal before test
    Mar 16 11:02:56.567: INFO: addons-nginx-ingress-controller-dfbd87bb4-tpjwc from kube-system started at 2023-03-16 09:40:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: apiserver-proxy-lt25p from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: blackbox-exporter-5778995784-z5cvh from kube-system started at 2023-03-16 10:01:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: calico-node-z796q from kube-system started at 2023-03-16 09:33:54 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: calico-typha-deploy-7c5596cb97-brtbb from kube-system started at 2023-03-16 09:35:04 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: csi-driver-node-mgpvm from kube-system started at 2023-03-16 09:37:01 +0000 UTC (3 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: egress-filter-applier-cj2st from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: kube-proxy-worker-1-v1.26.1-rwknf from kube-system started at 2023-03-16 10:07:51 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: metrics-server-96cc55556-ltn74 from kube-system started at 2023-03-16 09:42:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: network-problem-detector-host-9gcfg from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: network-problem-detector-pod-tqj4w from kube-system started at 2023-03-16 09:33:54 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: node-exporter-pvzcs from kube-system started at 2023-03-16 09:43:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: node-local-dns-cks6x from kube-system started at 2023-03-16 10:08:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: node-problem-detector-s6t66 from kube-system started at 2023-03-16 10:04:59 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: vpn-shoot-55c8b48df6-6fhfb from kube-system started at 2023-03-16 09:37:01 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container vpn-shoot ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: kubernetes-dashboard-5dd889f4f-zm2nt from kubernetes-dashboard started at 2023-03-16 09:41:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.567: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 16 11:02:56.567: INFO: 
    Logging pods the apiserver thinks is on node ip-10-250-19-246.ec2.internal before test
    Mar 16 11:02:56.662: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-9d4d8946b-9pd2c from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: apiserver-proxy-6595h from kube-system started at 2023-03-16 09:33:50 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container proxy ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: 	Container sidecar ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: blackbox-exporter-5778995784-cxqf6 from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: calico-node-9fxnl from kube-system started at 2023-03-16 09:33:51 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container add-snat-rule-to-upstream-dns ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: 	Container calico-node ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: calico-node-vertical-autoscaler-6fb7c9c6c9-wj8xh from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: calico-typha-deploy-7c5596cb97-wzz4f from kube-system started at 2023-03-16 09:35:11 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: calico-typha-horizontal-autoscaler-86db97cb8-fm2gn from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: calico-typha-vertical-autoscaler-8d8b46f5-pbqln from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: coredns-678f577694-8bdpf from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: coredns-678f577694-np4gr from kube-system started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container coredns ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: csi-driver-node-fcrrs from kube-system started at 2023-03-16 09:40:51 +0000 UTC (3 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container csi-driver ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: egress-filter-applier-nwmq2 from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container egress-filter-applier ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: kube-proxy-worker-1-v1.26.1-k2gbl from kube-system started at 2023-03-16 10:06:52 +0000 UTC (2 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container conntrack-fix ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: metrics-server-96cc55556-kzjrb from kube-system started at 2023-03-16 09:37:00 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: network-problem-detector-host-gj6px from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container network-problem-detector-host ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: network-problem-detector-pod-fbzrg from kube-system started at 2023-03-16 09:33:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container network-problem-detector-pod ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: node-exporter-nkxbw from kube-system started at 2023-03-16 09:42:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container node-exporter ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: node-local-dns-zssn4 from kube-system started at 2023-03-16 10:09:51 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container node-cache ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: node-problem-detector-jv6z7 from kube-system started at 2023-03-16 10:03:52 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container node-problem-detector ready: true, restart count 0
    Mar 16 11:02:56.662: INFO: dashboard-metrics-scraper-5cc9c9d874-588cq from kubernetes-dashboard started at 2023-03-16 09:34:39 +0000 UTC (1 container statuses recorded)
    Mar 16 11:02:56.662: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/16/23 11:02:56.662
    Mar 16 11:02:56.757: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7651" to be "running"
    Mar 16 11:02:56.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 89.395017ms
    Mar 16 11:02:58.937: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.18053119s
    Mar 16 11:02:58.937: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/16/23 11:02:59.027
    STEP: Trying to apply a random label on the found node. 03/16/23 11:02:59.296
    STEP: verifying the node has the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 95 03/16/23 11:02:59.396
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/16/23 11:02:59.485
    Mar 16 11:02:59.578: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7651" to be "not pending"
    Mar 16 11:02:59.668: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.485072ms
    Mar 16 11:03:01.758: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.179512758s
    Mar 16 11:03:01.758: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.19.136 on the node which pod4 resides and expect not scheduled 03/16/23 11:03:01.758
    Mar 16 11:03:01.851: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7651" to be "not pending"
    Mar 16 11:03:01.941: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.492525ms
    Mar 16 11:03:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179694263s
    Mar 16 11:03:06.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181240539s
    Mar 16 11:03:08.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180106733s
    Mar 16 11:03:10.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.180852907s
    Mar 16 11:03:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.17946101s
    Mar 16 11:03:14.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.181075635s
    Mar 16 11:03:16.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.179995019s
    Mar 16 11:03:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.179898755s
    Mar 16 11:03:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.18036598s
    Mar 16 11:03:22.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.180937308s
    Mar 16 11:03:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179990206s
    Mar 16 11:03:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.179925361s
    Mar 16 11:03:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.17993584s
    Mar 16 11:03:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.18040941s
    Mar 16 11:03:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.179448694s
    Mar 16 11:03:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.179340908s
    Mar 16 11:03:36.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.180773007s
    Mar 16 11:03:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.179996439s
    Mar 16 11:03:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.18023118s
    Mar 16 11:03:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.18078065s
    Mar 16 11:03:44.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.18106259s
    Mar 16 11:03:46.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.179956283s
    Mar 16 11:03:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.179622499s
    Mar 16 11:03:50.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.180993594s
    Mar 16 11:03:52.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.179827561s
    Mar 16 11:03:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.179802911s
    Mar 16 11:03:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.17966378s
    Mar 16 11:03:58.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.179197143s
    Mar 16 11:04:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.180937313s
    Mar 16 11:04:02.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.180181276s
    Mar 16 11:04:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.179894698s
    Mar 16 11:04:06.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.18005713s
    Mar 16 11:04:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.179967699s
    Mar 16 11:04:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.179264821s
    Mar 16 11:04:12.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.180845273s
    Mar 16 11:04:14.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.180134657s
    Mar 16 11:04:16.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.180775186s
    Mar 16 11:04:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.179669896s
    Mar 16 11:04:20.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.179660411s
    Mar 16 11:04:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.17952268s
    Mar 16 11:04:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.179434985s
    Mar 16 11:04:26.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.180816141s
    Mar 16 11:04:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.179309573s
    Mar 16 11:04:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.180626985s
    Mar 16 11:04:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.179868067s
    Mar 16 11:04:34.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.180561076s
    Mar 16 11:04:36.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.181331387s
    Mar 16 11:04:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.180054702s
    Mar 16 11:04:40.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.179849308s
    Mar 16 11:04:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.180106467s
    Mar 16 11:04:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.179314669s
    Mar 16 11:04:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.180881569s
    Mar 16 11:04:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.179778218s
    Mar 16 11:04:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.179832185s
    Mar 16 11:04:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.180074541s
    Mar 16 11:04:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.179779531s
    Mar 16 11:04:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.179994085s
    Mar 16 11:04:58.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.180050137s
    Mar 16 11:05:00.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.180025509s
    Mar 16 11:05:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.179842321s
    Mar 16 11:05:04.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.179740191s
    Mar 16 11:05:06.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.179876243s
    Mar 16 11:05:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.179778455s
    Mar 16 11:05:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.179749689s
    Mar 16 11:05:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.179646701s
    Mar 16 11:05:14.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.179788041s
    Mar 16 11:05:16.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.179927237s
    Mar 16 11:05:18.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.179957148s
    Mar 16 11:05:20.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.182148239s
    Mar 16 11:05:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.179767897s
    Mar 16 11:05:24.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.180253547s
    Mar 16 11:05:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.179936297s
    Mar 16 11:05:28.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.180112158s
    Mar 16 11:05:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.180609801s
    Mar 16 11:05:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.179735297s
    Mar 16 11:05:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.179910736s
    Mar 16 11:05:36.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.179883502s
    Mar 16 11:05:38.036: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.184072887s
    Mar 16 11:05:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.180066635s
    Mar 16 11:05:42.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.179757641s
    Mar 16 11:05:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.179657554s
    Mar 16 11:05:46.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.179813804s
    Mar 16 11:05:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.179903068s
    Mar 16 11:05:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.17980282s
    Mar 16 11:05:52.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.179696344s
    Mar 16 11:05:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.179798497s
    Mar 16 11:05:56.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.180003791s
    Mar 16 11:05:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.180075008s
    Mar 16 11:06:00.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.179833521s
    Mar 16 11:06:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.179846362s
    Mar 16 11:06:04.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.180699027s
    Mar 16 11:06:06.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.181176743s
    Mar 16 11:06:08.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.17999589s
    Mar 16 11:06:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.17910718s
    Mar 16 11:06:12.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.180096865s
    Mar 16 11:06:14.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.179632997s
    Mar 16 11:06:16.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.181267646s
    Mar 16 11:06:18.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.180251982s
    Mar 16 11:06:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.180884599s
    Mar 16 11:06:22.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.179798842s
    Mar 16 11:06:24.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.180355497s
    Mar 16 11:06:26.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.181112231s
    Mar 16 11:06:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.18000821s
    Mar 16 11:06:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.180544845s
    Mar 16 11:06:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.179898582s
    Mar 16 11:06:34.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.179977151s
    Mar 16 11:06:36.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.179737405s
    Mar 16 11:06:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.179751243s
    Mar 16 11:06:40.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.18007904s
    Mar 16 11:06:42.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.180431494s
    Mar 16 11:06:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.179607694s
    Mar 16 11:06:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.180855785s
    Mar 16 11:06:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.180057303s
    Mar 16 11:06:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.179182425s
    Mar 16 11:06:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.180800258s
    Mar 16 11:06:54.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.180442263s
    Mar 16 11:06:56.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.18141922s
    Mar 16 11:06:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.180184842s
    Mar 16 11:07:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.180068258s
    Mar 16 11:07:02.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.18020464s
    Mar 16 11:07:04.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.180336146s
    Mar 16 11:07:06.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.180957119s
    Mar 16 11:07:08.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.180306393s
    Mar 16 11:07:10.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.179857898s
    Mar 16 11:07:12.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.179928594s
    Mar 16 11:07:14.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.180062236s
    Mar 16 11:07:16.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.181688878s
    Mar 16 11:07:18.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.180153555s
    Mar 16 11:07:20.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.180062112s
    Mar 16 11:07:22.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.180145008s
    Mar 16 11:07:24.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.180010314s
    Mar 16 11:07:26.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.179993724s
    Mar 16 11:07:28.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.180039782s
    Mar 16 11:07:30.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.180694313s
    Mar 16 11:07:32.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.17983068s
    Mar 16 11:07:34.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.180373136s
    Mar 16 11:07:36.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.180813806s
    Mar 16 11:07:38.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.18000867s
    Mar 16 11:07:40.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.179653179s
    Mar 16 11:07:42.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.181114823s
    Mar 16 11:07:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.179666927s
    Mar 16 11:07:46.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.180177105s
    Mar 16 11:07:48.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.180043176s
    Mar 16 11:07:50.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.179624993s
    Mar 16 11:07:52.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.180786423s
    Mar 16 11:07:54.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.179918717s
    Mar 16 11:07:56.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.180697385s
    Mar 16 11:07:58.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.180101782s
    Mar 16 11:08:00.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.180077413s
    Mar 16 11:08:02.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.180056208s
    Mar 16 11:08:02.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.269624511s
    STEP: removing the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 off the node ip-10-250-19-136.ec2.internal 03/16/23 11:08:02.121
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-12569762-7938-4b5a-9716-eaa07c2355f4 03/16/23 11:08:02.481
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:08:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7651" for this suite. 03/16/23 11:08:02.66
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:08:02.76
Mar 16 11:08:02.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 11:08:02.761
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:03.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:03.207
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 03/16/23 11:08:03.385
Mar 16 11:08:03.385: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-192 proxy --unix-socket=/tmp/kubectl-proxy-unix2700251677/test'
STEP: retrieving proxy /api/ output 03/16/23 11:08:03.448
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 11:08:03.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-192" for this suite. 03/16/23 11:08:03.539
------------------------------
• [0.870 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:08:02.76
    Mar 16 11:08:02.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 11:08:02.761
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:03.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:03.207
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 03/16/23 11:08:03.385
    Mar 16 11:08:03.385: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-192 proxy --unix-socket=/tmp/kubectl-proxy-unix2700251677/test'
    STEP: retrieving proxy /api/ output 03/16/23 11:08:03.448
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:08:03.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-192" for this suite. 03/16/23 11:08:03.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:08:03.63
Mar 16 11:08:03.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context 03/16/23 11:08:03.632
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:03.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:04.078
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/16/23 11:08:04.256
Mar 16 11:08:04.359: INFO: Waiting up to 5m0s for pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241" in namespace "security-context-9126" to be "Succeeded or Failed"
Mar 16 11:08:04.449: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Pending", Reason="", readiness=false. Elapsed: 89.545442ms
Mar 16 11:08:06.540: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180043722s
Mar 16 11:08:08.539: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179624475s
STEP: Saw pod success 03/16/23 11:08:08.539
Mar 16 11:08:08.539: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241" satisfied condition "Succeeded or Failed"
Mar 16 11:08:08.629: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 container test-container: <nil>
STEP: delete the pod 03/16/23 11:08:08.767
Mar 16 11:08:08.860: INFO: Waiting for pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 to disappear
Mar 16 11:08:08.949: INFO: Pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 11:08:08.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9126" for this suite. 03/16/23 11:08:09.127
------------------------------
• [5.587 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:08:03.63
    Mar 16 11:08:03.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context 03/16/23 11:08:03.632
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:03.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:04.078
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/16/23 11:08:04.256
    Mar 16 11:08:04.359: INFO: Waiting up to 5m0s for pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241" in namespace "security-context-9126" to be "Succeeded or Failed"
    Mar 16 11:08:04.449: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Pending", Reason="", readiness=false. Elapsed: 89.545442ms
    Mar 16 11:08:06.540: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180043722s
    Mar 16 11:08:08.539: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179624475s
    STEP: Saw pod success 03/16/23 11:08:08.539
    Mar 16 11:08:08.539: INFO: Pod "security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241" satisfied condition "Succeeded or Failed"
    Mar 16 11:08:08.629: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 container test-container: <nil>
    STEP: delete the pod 03/16/23 11:08:08.767
    Mar 16 11:08:08.860: INFO: Waiting for pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 to disappear
    Mar 16 11:08:08.949: INFO: Pod security-context-c413fe6c-9a36-4aab-9a2c-45a038ff5241 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:08:08.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9126" for this suite. 03/16/23 11:08:09.127
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:08:09.218
Mar 16 11:08:09.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csiinlinevolumes 03/16/23 11:08:09.219
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:09.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:09.666
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 03/16/23 11:08:09.843
STEP: getting 03/16/23 11:08:10.114
STEP: listing 03/16/23 11:08:10.293
STEP: deleting 03/16/23 11:08:10.383
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:08:10.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6348" for this suite. 03/16/23 11:08:10.92
------------------------------
• [1.798 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:08:09.218
    Mar 16 11:08:09.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename csiinlinevolumes 03/16/23 11:08:09.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:09.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:09.666
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 03/16/23 11:08:09.843
    STEP: getting 03/16/23 11:08:10.114
    STEP: listing 03/16/23 11:08:10.293
    STEP: deleting 03/16/23 11:08:10.383
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:08:10.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6348" for this suite. 03/16/23 11:08:10.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:08:11.017
Mar 16 11:08:11.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod 03/16/23 11:08:11.018
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:11.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:11.464
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Mar 16 11:08:11.642: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 11:09:12.366: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Mar 16 11:09:12.456: INFO: Starting informer...
STEP: Starting pod... 03/16/23 11:09:12.456
Mar 16 11:09:12.639: INFO: Pod is running on ip-10-250-19-136.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 03/16/23 11:09:12.639
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:09:12.825
STEP: Waiting short time to make sure Pod is queued for deletion 03/16/23 11:09:12.917
Mar 16 11:09:12.917: INFO: Pod wasn't evicted. Proceeding
Mar 16 11:09:12.917: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:09:13.103
STEP: Waiting some time to make sure that toleration time passed. 03/16/23 11:09:13.193
Mar 16 11:10:28.194: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:10:28.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8493" for this suite. 03/16/23 11:10:28.372
------------------------------
• [137.445 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:08:11.017
    Mar 16 11:08:11.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-single-pod 03/16/23 11:08:11.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:08:11.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:08:11.464
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Mar 16 11:08:11.642: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 11:09:12.366: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Mar 16 11:09:12.456: INFO: Starting informer...
    STEP: Starting pod... 03/16/23 11:09:12.456
    Mar 16 11:09:12.639: INFO: Pod is running on ip-10-250-19-136.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 03/16/23 11:09:12.639
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:09:12.825
    STEP: Waiting short time to make sure Pod is queued for deletion 03/16/23 11:09:12.917
    Mar 16 11:09:12.917: INFO: Pod wasn't evicted. Proceeding
    Mar 16 11:09:12.917: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:09:13.103
    STEP: Waiting some time to make sure that toleration time passed. 03/16/23 11:09:13.193
    Mar 16 11:10:28.194: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:10:28.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8493" for this suite. 03/16/23 11:10:28.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:10:28.462
Mar 16 11:10:28.463: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob 03/16/23 11:10:28.464
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:10:28.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:10:28.91
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/16/23 11:10:29.088
STEP: Ensuring more than one job is running at a time 03/16/23 11:10:29.178
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/16/23 11:12:01.269
STEP: Removing cronjob 03/16/23 11:12:01.359
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 16 11:12:01.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7362" for this suite. 03/16/23 11:12:01.628
------------------------------
• [93.256 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:10:28.462
    Mar 16 11:10:28.463: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename cronjob 03/16/23 11:10:28.464
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:10:28.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:10:28.91
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/16/23 11:10:29.088
    STEP: Ensuring more than one job is running at a time 03/16/23 11:10:29.178
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/16/23 11:12:01.269
    STEP: Removing cronjob 03/16/23 11:12:01.359
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:12:01.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7362" for this suite. 03/16/23 11:12:01.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:12:01.719
Mar 16 11:12:01.719: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 11:12:01.72
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:01.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:02.168
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Mar 16 11:12:02.707: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 11:12:02.797
Mar 16 11:12:02.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 11:12:02.977: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 11:12:04.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 11:12:04.245: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 03/16/23 11:12:04.604
STEP: Check that daemon pods images are updated. 03/16/23 11:12:04.786
Mar 16 11:12:04.875: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 16 11:12:06.056: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 16 11:12:07.056: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 16 11:12:08.057: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 16 11:12:08.057: INFO: Pod daemon-set-qgp4t is not available
Mar 16 11:12:09.057: INFO: Pod daemon-set-pc2xq is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/16/23 11:12:09.234
Mar 16 11:12:09.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 16 11:12:09.414: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 11:12:10.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 11:12:10.683: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 11:12:11.131
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4993, will wait for the garbage collector to delete the pods 03/16/23 11:12:11.131
Mar 16 11:12:11.412: INFO: Deleting DaemonSet.extensions daemon-set took: 90.925039ms
Mar 16 11:12:11.512: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164028ms
Mar 16 11:12:12.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 11:12:12.703: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 11:12:12.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50034"},"items":null}

Mar 16 11:12:12.881: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50035"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:12:13.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4993" for this suite. 03/16/23 11:12:13.328
------------------------------
• [11.700 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:12:01.719
    Mar 16 11:12:01.719: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 11:12:01.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:01.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:02.168
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Mar 16 11:12:02.707: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/16/23 11:12:02.797
    Mar 16 11:12:02.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 11:12:02.977: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 11:12:04.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 11:12:04.245: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 03/16/23 11:12:04.604
    STEP: Check that daemon pods images are updated. 03/16/23 11:12:04.786
    Mar 16 11:12:04.875: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 16 11:12:06.056: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 16 11:12:07.056: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 16 11:12:08.057: INFO: Wrong image for pod: daemon-set-p8f8j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 16 11:12:08.057: INFO: Pod daemon-set-qgp4t is not available
    Mar 16 11:12:09.057: INFO: Pod daemon-set-pc2xq is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/16/23 11:12:09.234
    Mar 16 11:12:09.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 16 11:12:09.414: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 11:12:10.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 11:12:10.683: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 11:12:11.131
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4993, will wait for the garbage collector to delete the pods 03/16/23 11:12:11.131
    Mar 16 11:12:11.412: INFO: Deleting DaemonSet.extensions daemon-set took: 90.925039ms
    Mar 16 11:12:11.512: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.164028ms
    Mar 16 11:12:12.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 11:12:12.703: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 11:12:12.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50034"},"items":null}

    Mar 16 11:12:12.881: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50035"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:12:13.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4993" for this suite. 03/16/23 11:12:13.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:12:13.419
Mar 16 11:12:13.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:12:13.42
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:13.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:13.867
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:12:14.225
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:12:14.505
STEP: Deploying the webhook pod 03/16/23 11:12:14.596
STEP: Wait for the deployment to be ready 03/16/23 11:12:14.777
Mar 16 11:12:15.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:12:17.136
STEP: Verifying the service has paired with the endpoint 03/16/23 11:12:17.23
Mar 16 11:12:18.230: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/16/23 11:12:18.32
STEP: create a namespace for the webhook 03/16/23 11:12:18.603
STEP: create a configmap should be unconditionally rejected by the webhook 03/16/23 11:12:18.694
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:12:19.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8506" for this suite. 03/16/23 11:12:19.746
STEP: Destroying namespace "webhook-8506-markers" for this suite. 03/16/23 11:12:19.836
------------------------------
• [6.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:12:13.419
    Mar 16 11:12:13.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:12:13.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:13.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:13.867
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:12:14.225
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:12:14.505
    STEP: Deploying the webhook pod 03/16/23 11:12:14.596
    STEP: Wait for the deployment to be ready 03/16/23 11:12:14.777
    Mar 16 11:12:15.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 12, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:12:17.136
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:12:17.23
    Mar 16 11:12:18.230: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/16/23 11:12:18.32
    STEP: create a namespace for the webhook 03/16/23 11:12:18.603
    STEP: create a configmap should be unconditionally rejected by the webhook 03/16/23 11:12:18.694
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:12:19.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8506" for this suite. 03/16/23 11:12:19.746
    STEP: Destroying namespace "webhook-8506-markers" for this suite. 03/16/23 11:12:19.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:12:19.929
Mar 16 11:12:19.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 11:12:19.931
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:20.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:20.377
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 03/16/23 11:12:20.555
STEP: waiting for pod running 03/16/23 11:12:20.65
Mar 16 11:12:20.650: INFO: Waiting up to 2m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040" to be "running"
Mar 16 11:12:20.740: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Pending", Reason="", readiness=false. Elapsed: 89.465323ms
Mar 16 11:12:22.830: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Running", Reason="", readiness=true. Elapsed: 2.179697878s
Mar 16 11:12:22.830: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" satisfied condition "running"
STEP: creating a file in subpath 03/16/23 11:12:22.83
Mar 16 11:12:22.920: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2040 PodName:var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 11:12:22.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 11:12:22.921: INFO: ExecWithOptions: Clientset creation
Mar 16 11:12:22.921: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-2040/pods/var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/16/23 11:12:23.704
Mar 16 11:12:23.793: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2040 PodName:var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 16 11:12:23.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 11:12:23.794: INFO: ExecWithOptions: Clientset creation
Mar 16 11:12:23.794: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-2040/pods/var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/16/23 11:12:24.536
Mar 16 11:12:25.218: INFO: Successfully updated pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68"
STEP: waiting for annotated pod running 03/16/23 11:12:25.218
Mar 16 11:12:25.218: INFO: Waiting up to 2m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040" to be "running"
Mar 16 11:12:25.308: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Running", Reason="", readiness=true. Elapsed: 89.526266ms
Mar 16 11:12:25.308: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" satisfied condition "running"
STEP: deleting the pod gracefully 03/16/23 11:12:25.308
Mar 16 11:12:25.308: INFO: Deleting pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040"
Mar 16 11:12:25.399: INFO: Wait up to 5m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 11:12:57.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2040" for this suite. 03/16/23 11:12:57.844
------------------------------
• [38.004 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:12:19.929
    Mar 16 11:12:19.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 11:12:19.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:20.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:20.377
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 03/16/23 11:12:20.555
    STEP: waiting for pod running 03/16/23 11:12:20.65
    Mar 16 11:12:20.650: INFO: Waiting up to 2m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040" to be "running"
    Mar 16 11:12:20.740: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Pending", Reason="", readiness=false. Elapsed: 89.465323ms
    Mar 16 11:12:22.830: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Running", Reason="", readiness=true. Elapsed: 2.179697878s
    Mar 16 11:12:22.830: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" satisfied condition "running"
    STEP: creating a file in subpath 03/16/23 11:12:22.83
    Mar 16 11:12:22.920: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2040 PodName:var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 11:12:22.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 11:12:22.921: INFO: ExecWithOptions: Clientset creation
    Mar 16 11:12:22.921: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-2040/pods/var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/16/23 11:12:23.704
    Mar 16 11:12:23.793: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2040 PodName:var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 16 11:12:23.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 11:12:23.794: INFO: ExecWithOptions: Clientset creation
    Mar 16 11:12:23.794: INFO: ExecWithOptions: execute(POST https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-2040/pods/var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/16/23 11:12:24.536
    Mar 16 11:12:25.218: INFO: Successfully updated pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68"
    STEP: waiting for annotated pod running 03/16/23 11:12:25.218
    Mar 16 11:12:25.218: INFO: Waiting up to 2m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040" to be "running"
    Mar 16 11:12:25.308: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68": Phase="Running", Reason="", readiness=true. Elapsed: 89.526266ms
    Mar 16 11:12:25.308: INFO: Pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" satisfied condition "running"
    STEP: deleting the pod gracefully 03/16/23 11:12:25.308
    Mar 16 11:12:25.308: INFO: Deleting pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" in namespace "var-expansion-2040"
    Mar 16 11:12:25.399: INFO: Wait up to 5m0s for pod "var-expansion-36fdc23f-91ad-4f4a-8a15-2283819b5c68" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:12:57.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2040" for this suite. 03/16/23 11:12:57.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:12:57.935
Mar 16 11:12:57.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 11:12:57.936
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:58.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:58.384
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/16/23 11:12:58.739
Mar 16 11:12:58.834: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1917" to be "running and ready"
Mar 16 11:12:58.924: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.5439ms
Mar 16 11:12:58.924: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:13:01.014: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.180191203s
Mar 16 11:13:01.014: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 16 11:13:01.014: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 03/16/23 11:13:01.104
Mar 16 11:13:01.197: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1917" to be "running and ready"
Mar 16 11:13:01.286: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.354546ms
Mar 16 11:13:01.286: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:13:03.377: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180234957s
Mar 16 11:13:03.377: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 16 11:13:03.377: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/16/23 11:13:03.467
STEP: delete the pod with lifecycle hook 03/16/23 11:13:03.561
Mar 16 11:13:03.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 16 11:13:03.742: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 16 11:13:05.743: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 16 11:13:05.833: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 16 11:13:05.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1917" for this suite. 03/16/23 11:13:06.011
------------------------------
• [8.167 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:12:57.935
    Mar 16 11:12:57.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 11:12:57.936
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:12:58.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:12:58.384
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/16/23 11:12:58.739
    Mar 16 11:12:58.834: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1917" to be "running and ready"
    Mar 16 11:12:58.924: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.5439ms
    Mar 16 11:12:58.924: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:13:01.014: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.180191203s
    Mar 16 11:13:01.014: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 16 11:13:01.014: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 03/16/23 11:13:01.104
    Mar 16 11:13:01.197: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1917" to be "running and ready"
    Mar 16 11:13:01.286: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.354546ms
    Mar 16 11:13:01.286: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:13:03.377: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180234957s
    Mar 16 11:13:03.377: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 16 11:13:03.377: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/16/23 11:13:03.467
    STEP: delete the pod with lifecycle hook 03/16/23 11:13:03.561
    Mar 16 11:13:03.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 16 11:13:03.742: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 16 11:13:05.743: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 16 11:13:05.833: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:13:05.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1917" for this suite. 03/16/23 11:13:06.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:13:06.104
Mar 16 11:13:06.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:13:06.106
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:06.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:06.552
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 03/16/23 11:13:06.73
Mar 16 11:13:06.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc" in namespace "downward-api-2078" to be "Succeeded or Failed"
Mar 16 11:13:06.914: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.780581ms
Mar 16 11:13:09.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18115139s
Mar 16 11:13:11.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180828767s
STEP: Saw pod success 03/16/23 11:13:11.006
Mar 16 11:13:11.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc" satisfied condition "Succeeded or Failed"
Mar 16 11:13:11.095: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc container client-container: <nil>
STEP: delete the pod 03/16/23 11:13:11.192
Mar 16 11:13:11.284: INFO: Waiting for pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc to disappear
Mar 16 11:13:11.374: INFO: Pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 11:13:11.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2078" for this suite. 03/16/23 11:13:11.552
------------------------------
• [5.538 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:13:06.104
    Mar 16 11:13:06.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:13:06.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:06.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:06.552
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 03/16/23 11:13:06.73
    Mar 16 11:13:06.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc" in namespace "downward-api-2078" to be "Succeeded or Failed"
    Mar 16 11:13:06.914: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.780581ms
    Mar 16 11:13:09.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18115139s
    Mar 16 11:13:11.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180828767s
    STEP: Saw pod success 03/16/23 11:13:11.006
    Mar 16 11:13:11.006: INFO: Pod "downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc" satisfied condition "Succeeded or Failed"
    Mar 16 11:13:11.095: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc container client-container: <nil>
    STEP: delete the pod 03/16/23 11:13:11.192
    Mar 16 11:13:11.284: INFO: Waiting for pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc to disappear
    Mar 16 11:13:11.374: INFO: Pod downwardapi-volume-7042d121-d440-404b-8aa9-12219e667afc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:13:11.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2078" for this suite. 03/16/23 11:13:11.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:13:11.644
Mar 16 11:13:11.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 11:13:11.646
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:11.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:12.093
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 03/16/23 11:13:12.271
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:12.539
STEP: Creating a service in the namespace 03/16/23 11:13:12.716
STEP: Deleting the namespace 03/16/23 11:13:12.811
STEP: Waiting for the namespace to be removed. 03/16/23 11:13:12.901
STEP: Recreating the namespace 03/16/23 11:13:18.991
STEP: Verifying there is no service in the namespace 03/16/23 11:13:19.26
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:13:19.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4577" for this suite. 03/16/23 11:13:19.528
STEP: Destroying namespace "nsdeletetest-6447" for this suite. 03/16/23 11:13:19.619
Mar 16 11:13:19.708: INFO: Namespace nsdeletetest-6447 was already deleted
STEP: Destroying namespace "nsdeletetest-4702" for this suite. 03/16/23 11:13:19.709
------------------------------
• [8.155 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:13:11.644
    Mar 16 11:13:11.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 11:13:11.646
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:11.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:12.093
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 03/16/23 11:13:12.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:12.539
    STEP: Creating a service in the namespace 03/16/23 11:13:12.716
    STEP: Deleting the namespace 03/16/23 11:13:12.811
    STEP: Waiting for the namespace to be removed. 03/16/23 11:13:12.901
    STEP: Recreating the namespace 03/16/23 11:13:18.991
    STEP: Verifying there is no service in the namespace 03/16/23 11:13:19.26
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:13:19.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4577" for this suite. 03/16/23 11:13:19.528
    STEP: Destroying namespace "nsdeletetest-6447" for this suite. 03/16/23 11:13:19.619
    Mar 16 11:13:19.708: INFO: Namespace nsdeletetest-6447 was already deleted
    STEP: Destroying namespace "nsdeletetest-4702" for this suite. 03/16/23 11:13:19.709
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:13:19.8
Mar 16 11:13:19.800: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 11:13:19.801
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:20.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:20.247
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-30a67ff9-46a4-4cc6-808d-a6a076885a53 03/16/23 11:13:20.425
STEP: Creating a pod to test consume secrets 03/16/23 11:13:20.515
Mar 16 11:13:20.610: INFO: Waiting up to 5m0s for pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23" in namespace "secrets-8618" to be "Succeeded or Failed"
Mar 16 11:13:20.699: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Pending", Reason="", readiness=false. Elapsed: 89.350839ms
Mar 16 11:13:22.789: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179535439s
Mar 16 11:13:24.790: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180701431s
STEP: Saw pod success 03/16/23 11:13:24.79
Mar 16 11:13:24.791: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23" satisfied condition "Succeeded or Failed"
Mar 16 11:13:24.880: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 11:13:24.975
Mar 16 11:13:25.073: INFO: Waiting for pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 to disappear
Mar 16 11:13:25.162: INFO: Pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 11:13:25.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8618" for this suite. 03/16/23 11:13:25.34
------------------------------
• [5.631 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:13:19.8
    Mar 16 11:13:19.800: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 11:13:19.801
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:20.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:20.247
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-30a67ff9-46a4-4cc6-808d-a6a076885a53 03/16/23 11:13:20.425
    STEP: Creating a pod to test consume secrets 03/16/23 11:13:20.515
    Mar 16 11:13:20.610: INFO: Waiting up to 5m0s for pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23" in namespace "secrets-8618" to be "Succeeded or Failed"
    Mar 16 11:13:20.699: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Pending", Reason="", readiness=false. Elapsed: 89.350839ms
    Mar 16 11:13:22.789: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179535439s
    Mar 16 11:13:24.790: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180701431s
    STEP: Saw pod success 03/16/23 11:13:24.79
    Mar 16 11:13:24.791: INFO: Pod "pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23" satisfied condition "Succeeded or Failed"
    Mar 16 11:13:24.880: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 11:13:24.975
    Mar 16 11:13:25.073: INFO: Waiting for pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 to disappear
    Mar 16 11:13:25.162: INFO: Pod pod-secrets-67ea8ba9-e566-48fb-be55-b8c876647a23 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:13:25.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8618" for this suite. 03/16/23 11:13:25.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:13:25.431
Mar 16 11:13:25.431: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 11:13:25.432
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:25.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:25.879
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 in namespace container-probe-8577 03/16/23 11:13:26.057
Mar 16 11:13:26.151: INFO: Waiting up to 5m0s for pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7" in namespace "container-probe-8577" to be "not pending"
Mar 16 11:13:26.241: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.425549ms
Mar 16 11:13:28.331: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179307736s
Mar 16 11:13:28.331: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7" satisfied condition "not pending"
Mar 16 11:13:28.331: INFO: Started pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 in namespace container-probe-8577
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 11:13:28.331
Mar 16 11:13:28.421: INFO: Initial restart count of pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 is 0
STEP: deleting the pod 03/16/23 11:17:28.844
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 11:17:28.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8577" for this suite. 03/16/23 11:17:29.117
------------------------------
• [243.776 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:13:25.431
    Mar 16 11:13:25.431: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 11:13:25.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:13:25.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:13:25.879
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 in namespace container-probe-8577 03/16/23 11:13:26.057
    Mar 16 11:13:26.151: INFO: Waiting up to 5m0s for pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7" in namespace "container-probe-8577" to be "not pending"
    Mar 16 11:13:26.241: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.425549ms
    Mar 16 11:13:28.331: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.179307736s
    Mar 16 11:13:28.331: INFO: Pod "busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7" satisfied condition "not pending"
    Mar 16 11:13:28.331: INFO: Started pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 in namespace container-probe-8577
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 11:13:28.331
    Mar 16 11:13:28.421: INFO: Initial restart count of pod busybox-457f819a-058b-47a3-bf31-d7c70e48c4a7 is 0
    STEP: deleting the pod 03/16/23 11:17:28.844
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:17:28.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8577" for this suite. 03/16/23 11:17:29.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:17:29.208
Mar 16 11:17:29.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 11:17:29.209
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:29.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:29.656
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 03/16/23 11:17:29.833
Mar 16 11:17:29.937: INFO: Waiting up to 5m0s for pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38" in namespace "var-expansion-5458" to be "Succeeded or Failed"
Mar 16 11:17:30.027: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Pending", Reason="", readiness=false. Elapsed: 89.657896ms
Mar 16 11:17:32.118: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180773291s
Mar 16 11:17:34.117: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179628135s
STEP: Saw pod success 03/16/23 11:17:34.117
Mar 16 11:17:34.117: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38" satisfied condition "Succeeded or Failed"
Mar 16 11:17:34.207: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 container dapi-container: <nil>
STEP: delete the pod 03/16/23 11:17:34.303
Mar 16 11:17:34.396: INFO: Waiting for pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 to disappear
Mar 16 11:17:34.485: INFO: Pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 11:17:34.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5458" for this suite. 03/16/23 11:17:34.663
------------------------------
• [5.546 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:17:29.208
    Mar 16 11:17:29.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 11:17:29.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:29.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:29.656
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 03/16/23 11:17:29.833
    Mar 16 11:17:29.937: INFO: Waiting up to 5m0s for pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38" in namespace "var-expansion-5458" to be "Succeeded or Failed"
    Mar 16 11:17:30.027: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Pending", Reason="", readiness=false. Elapsed: 89.657896ms
    Mar 16 11:17:32.118: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180773291s
    Mar 16 11:17:34.117: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179628135s
    STEP: Saw pod success 03/16/23 11:17:34.117
    Mar 16 11:17:34.117: INFO: Pod "var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38" satisfied condition "Succeeded or Failed"
    Mar 16 11:17:34.207: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 container dapi-container: <nil>
    STEP: delete the pod 03/16/23 11:17:34.303
    Mar 16 11:17:34.396: INFO: Waiting for pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 to disappear
    Mar 16 11:17:34.485: INFO: Pod var-expansion-d3092941-5bc0-4498-bd7c-048ad35a7e38 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:17:34.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5458" for this suite. 03/16/23 11:17:34.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:17:34.755
Mar 16 11:17:34.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:17:34.756
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:35.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:35.202
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-7efe4dd7-d588-4285-a376-abc1f8781ba4 03/16/23 11:17:35.379
STEP: Creating a pod to test consume secrets 03/16/23 11:17:35.469
Mar 16 11:17:35.563: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6" in namespace "projected-7058" to be "Succeeded or Failed"
Mar 16 11:17:35.652: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.294672ms
Mar 16 11:17:37.742: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179239523s
Mar 16 11:17:39.743: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179948892s
STEP: Saw pod success 03/16/23 11:17:39.743
Mar 16 11:17:39.743: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6" satisfied condition "Succeeded or Failed"
Mar 16 11:17:39.833: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/16/23 11:17:39.927
Mar 16 11:17:40.021: INFO: Waiting for pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 to disappear
Mar 16 11:17:40.110: INFO: Pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 11:17:40.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7058" for this suite. 03/16/23 11:17:40.288
------------------------------
• [5.626 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:17:34.755
    Mar 16 11:17:34.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:17:34.756
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:35.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:35.202
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-7efe4dd7-d588-4285-a376-abc1f8781ba4 03/16/23 11:17:35.379
    STEP: Creating a pod to test consume secrets 03/16/23 11:17:35.469
    Mar 16 11:17:35.563: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6" in namespace "projected-7058" to be "Succeeded or Failed"
    Mar 16 11:17:35.652: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.294672ms
    Mar 16 11:17:37.742: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179239523s
    Mar 16 11:17:39.743: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179948892s
    STEP: Saw pod success 03/16/23 11:17:39.743
    Mar 16 11:17:39.743: INFO: Pod "pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6" satisfied condition "Succeeded or Failed"
    Mar 16 11:17:39.833: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 11:17:39.927
    Mar 16 11:17:40.021: INFO: Waiting for pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 to disappear
    Mar 16 11:17:40.110: INFO: Pod pod-projected-secrets-079865ab-a5f5-46d6-9d14-497b83a235f6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:17:40.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7058" for this suite. 03/16/23 11:17:40.288
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:17:40.382
Mar 16 11:17:40.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods 03/16/23 11:17:40.383
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:40.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:40.829
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Mar 16 11:17:41.006: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 11:18:41.991: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Mar 16 11:18:42.083: INFO: Starting informer...
STEP: Starting pods... 03/16/23 11:18:42.083
Mar 16 11:18:42.357: INFO: Pod1 is running on ip-10-250-19-136.ec2.internal. Tainting Node
Mar 16 11:18:42.541: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8956" to be "running"
Mar 16 11:18:42.630: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.313346ms
Mar 16 11:18:44.721: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.179587204s
Mar 16 11:18:44.721: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 16 11:18:44.721: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8956" to be "running"
Mar 16 11:18:44.810: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 89.570271ms
Mar 16 11:18:44.810: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 16 11:18:44.810: INFO: Pod2 is running on ip-10-250-19-136.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 03/16/23 11:18:44.81
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:18:45.083
STEP: Waiting for Pod1 and Pod2 to be deleted 03/16/23 11:18:45.175
Mar 16 11:18:50.555: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 16 11:19:10.591: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:19:10.776
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:19:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8956" for this suite. 03/16/23 11:19:10.956
------------------------------
• [90.665 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:17:40.382
    Mar 16 11:17:40.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename taint-multiple-pods 03/16/23 11:17:40.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:17:40.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:17:40.829
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Mar 16 11:17:41.006: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 11:18:41.991: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Mar 16 11:18:42.083: INFO: Starting informer...
    STEP: Starting pods... 03/16/23 11:18:42.083
    Mar 16 11:18:42.357: INFO: Pod1 is running on ip-10-250-19-136.ec2.internal. Tainting Node
    Mar 16 11:18:42.541: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8956" to be "running"
    Mar 16 11:18:42.630: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.313346ms
    Mar 16 11:18:44.721: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.179587204s
    Mar 16 11:18:44.721: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 16 11:18:44.721: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8956" to be "running"
    Mar 16 11:18:44.810: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 89.570271ms
    Mar 16 11:18:44.810: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 16 11:18:44.810: INFO: Pod2 is running on ip-10-250-19-136.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 03/16/23 11:18:44.81
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:18:45.083
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/16/23 11:18:45.175
    Mar 16 11:18:50.555: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 16 11:19:10.591: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/16/23 11:19:10.776
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:19:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8956" for this suite. 03/16/23 11:19:10.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:19:11.048
Mar 16 11:19:11.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch 03/16/23 11:19:11.049
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:11.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:11.497
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/16/23 11:19:11.675
STEP: creating a new configmap 03/16/23 11:19:11.764
STEP: modifying the configmap once 03/16/23 11:19:11.854
STEP: closing the watch once it receives two notifications 03/16/23 11:19:12.033
Mar 16 11:19:12.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52906 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 11:19:12.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52907 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/16/23 11:19:12.033
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/16/23 11:19:12.213
STEP: deleting the configmap 03/16/23 11:19:12.301
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/16/23 11:19:12.393
Mar 16 11:19:12.393: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52908 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 16 11:19:12.393: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52910 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 16 11:19:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9790" for this suite. 03/16/23 11:19:12.484
------------------------------
• [1.527 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:19:11.048
    Mar 16 11:19:11.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename watch 03/16/23 11:19:11.049
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:11.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:11.497
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/16/23 11:19:11.675
    STEP: creating a new configmap 03/16/23 11:19:11.764
    STEP: modifying the configmap once 03/16/23 11:19:11.854
    STEP: closing the watch once it receives two notifications 03/16/23 11:19:12.033
    Mar 16 11:19:12.033: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52906 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 11:19:12.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52907 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/16/23 11:19:12.033
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/16/23 11:19:12.213
    STEP: deleting the configmap 03/16/23 11:19:12.301
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/16/23 11:19:12.393
    Mar 16 11:19:12.393: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52908 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 16 11:19:12.393: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9790  ff98e956-b5d6-42c4-88ef-670c236eb454 52910 0 2023-03-16 11:19:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-16 11:19:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:19:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9790" for this suite. 03/16/23 11:19:12.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:19:12.575
Mar 16 11:19:12.575: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:19:12.577
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:12.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:13.024
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/16/23 11:19:13.201
Mar 16 11:19:13.296: INFO: Waiting up to 5m0s for pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483" in namespace "emptydir-9558" to be "Succeeded or Failed"
Mar 16 11:19:13.386: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Pending", Reason="", readiness=false. Elapsed: 89.705009ms
Mar 16 11:19:15.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179728409s
Mar 16 11:19:17.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179581238s
STEP: Saw pod success 03/16/23 11:19:17.476
Mar 16 11:19:17.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483" satisfied condition "Succeeded or Failed"
Mar 16 11:19:17.565: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 container test-container: <nil>
STEP: delete the pod 03/16/23 11:19:17.807
Mar 16 11:19:17.900: INFO: Waiting for pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 to disappear
Mar 16 11:19:17.989: INFO: Pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:19:17.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9558" for this suite. 03/16/23 11:19:18.167
------------------------------
• [5.683 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:19:12.575
    Mar 16 11:19:12.575: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:19:12.577
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:12.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:13.024
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/16/23 11:19:13.201
    Mar 16 11:19:13.296: INFO: Waiting up to 5m0s for pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483" in namespace "emptydir-9558" to be "Succeeded or Failed"
    Mar 16 11:19:13.386: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Pending", Reason="", readiness=false. Elapsed: 89.705009ms
    Mar 16 11:19:15.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179728409s
    Mar 16 11:19:17.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179581238s
    STEP: Saw pod success 03/16/23 11:19:17.476
    Mar 16 11:19:17.476: INFO: Pod "pod-4cc99314-1522-4cea-ae16-eccce29cb483" satisfied condition "Succeeded or Failed"
    Mar 16 11:19:17.565: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 container test-container: <nil>
    STEP: delete the pod 03/16/23 11:19:17.807
    Mar 16 11:19:17.900: INFO: Waiting for pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 to disappear
    Mar 16 11:19:17.989: INFO: Pod pod-4cc99314-1522-4cea-ae16-eccce29cb483 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:19:17.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9558" for this suite. 03/16/23 11:19:18.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:19:18.259
Mar 16 11:19:18.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 11:19:18.26
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:18.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:18.707
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1300 03/16/23 11:19:18.884
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[] 03/16/23 11:19:18.979
Mar 16 11:19:19.247: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1300 03/16/23 11:19:19.247
Mar 16 11:19:19.342: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1300" to be "running and ready"
Mar 16 11:19:19.431: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.78156ms
Mar 16 11:19:19.432: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:19:21.521: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.179696919s
Mar 16 11:19:21.521: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 16 11:19:21.521: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod1:[80]] 03/16/23 11:19:21.611
Mar 16 11:19:21.969: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/16/23 11:19:21.969
Mar 16 11:19:21.969: INFO: Creating new exec pod
Mar 16 11:19:22.061: INFO: Waiting up to 5m0s for pod "execpodgmmlr" in namespace "services-1300" to be "running"
Mar 16 11:19:22.155: INFO: Pod "execpodgmmlr": Phase="Pending", Reason="", readiness=false. Elapsed: 93.962157ms
Mar 16 11:19:24.246: INFO: Pod "execpodgmmlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.184566764s
Mar 16 11:19:24.246: INFO: Pod "execpodgmmlr" satisfied condition "running"
Mar 16 11:19:25.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 16 11:19:26.399: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:26.399: INFO: stdout: ""
Mar 16 11:19:26.399: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
Mar 16 11:19:27.560: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:27.561: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1300 03/16/23 11:19:27.561
Mar 16 11:19:27.655: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1300" to be "running and ready"
Mar 16 11:19:27.744: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.198144ms
Mar 16 11:19:27.744: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:19:29.834: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.178971147s
Mar 16 11:19:29.834: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 16 11:19:29.834: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod1:[80] pod2:[80]] 03/16/23 11:19:29.924
Mar 16 11:19:30.370: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/16/23 11:19:30.37
Mar 16 11:19:31.370: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 16 11:19:32.559: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:32.559: INFO: stdout: ""
Mar 16 11:19:32.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
Mar 16 11:19:33.652: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:33.652: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1300 03/16/23 11:19:33.652
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod2:[80]] 03/16/23 11:19:33.75
Mar 16 11:19:34.109: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/16/23 11:19:34.109
Mar 16 11:19:35.110: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 16 11:19:36.212: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:36.212: INFO: stdout: ""
Mar 16 11:19:36.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
Mar 16 11:19:37.349: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
Mar 16 11:19:37.349: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1300 03/16/23 11:19:37.349
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[] 03/16/23 11:19:37.442
Mar 16 11:19:37.709: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 11:19:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1300" for this suite. 03/16/23 11:19:37.982
------------------------------
• [19.812 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:19:18.259
    Mar 16 11:19:18.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 11:19:18.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:18.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:18.707
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1300 03/16/23 11:19:18.884
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[] 03/16/23 11:19:18.979
    Mar 16 11:19:19.247: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1300 03/16/23 11:19:19.247
    Mar 16 11:19:19.342: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1300" to be "running and ready"
    Mar 16 11:19:19.431: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.78156ms
    Mar 16 11:19:19.432: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:19:21.521: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.179696919s
    Mar 16 11:19:21.521: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 16 11:19:21.521: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod1:[80]] 03/16/23 11:19:21.611
    Mar 16 11:19:21.969: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/16/23 11:19:21.969
    Mar 16 11:19:21.969: INFO: Creating new exec pod
    Mar 16 11:19:22.061: INFO: Waiting up to 5m0s for pod "execpodgmmlr" in namespace "services-1300" to be "running"
    Mar 16 11:19:22.155: INFO: Pod "execpodgmmlr": Phase="Pending", Reason="", readiness=false. Elapsed: 93.962157ms
    Mar 16 11:19:24.246: INFO: Pod "execpodgmmlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.184566764s
    Mar 16 11:19:24.246: INFO: Pod "execpodgmmlr" satisfied condition "running"
    Mar 16 11:19:25.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 16 11:19:26.399: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:26.399: INFO: stdout: ""
    Mar 16 11:19:26.399: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
    Mar 16 11:19:27.560: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:27.561: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1300 03/16/23 11:19:27.561
    Mar 16 11:19:27.655: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1300" to be "running and ready"
    Mar 16 11:19:27.744: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.198144ms
    Mar 16 11:19:27.744: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:19:29.834: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.178971147s
    Mar 16 11:19:29.834: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 16 11:19:29.834: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod1:[80] pod2:[80]] 03/16/23 11:19:29.924
    Mar 16 11:19:30.370: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/16/23 11:19:30.37
    Mar 16 11:19:31.370: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 16 11:19:32.559: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:32.559: INFO: stdout: ""
    Mar 16 11:19:32.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
    Mar 16 11:19:33.652: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:33.652: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1300 03/16/23 11:19:33.652
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[pod2:[80]] 03/16/23 11:19:33.75
    Mar 16 11:19:34.109: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/16/23 11:19:34.109
    Mar 16 11:19:35.110: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 16 11:19:36.212: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:36.212: INFO: stdout: ""
    Mar 16 11:19:36.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1300 exec execpodgmmlr -- /bin/sh -x -c nc -v -z -w 2 100.107.180.142 80'
    Mar 16 11:19:37.349: INFO: stderr: "+ nc -v -z -w 2 100.107.180.142 80\nConnection to 100.107.180.142 80 port [tcp/http] succeeded!\n"
    Mar 16 11:19:37.349: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1300 03/16/23 11:19:37.349
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1300 to expose endpoints map[] 03/16/23 11:19:37.442
    Mar 16 11:19:37.709: INFO: successfully validated that service endpoint-test2 in namespace services-1300 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:19:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1300" for this suite. 03/16/23 11:19:37.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:19:38.072
Mar 16 11:19:38.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion 03/16/23 11:19:38.073
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:38.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:38.519
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 03/16/23 11:19:38.697
Mar 16 11:19:38.791: INFO: Waiting up to 2m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449" to be "running"
Mar 16 11:19:38.881: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.594087ms
Mar 16 11:19:40.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180372412s
Mar 16 11:19:42.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.180173666s
Mar 16 11:19:44.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.179921034s
Mar 16 11:19:46.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179813873s
Mar 16 11:19:48.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.181592715s
Mar 16 11:19:50.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.179982588s
Mar 16 11:19:52.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180002093s
Mar 16 11:19:54.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.180022809s
Mar 16 11:19:56.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.181032773s
Mar 16 11:19:58.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.179690825s
Mar 16 11:20:00.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179994206s
Mar 16 11:20:02.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.179813843s
Mar 16 11:20:04.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.179774131s
Mar 16 11:20:06.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.181138566s
Mar 16 11:20:08.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.18079048s
Mar 16 11:20:10.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.180520673s
Mar 16 11:20:12.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.179982768s
Mar 16 11:20:14.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.180857843s
Mar 16 11:20:16.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.180061462s
Mar 16 11:20:18.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.1817419s
Mar 16 11:20:20.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.181159369s
Mar 16 11:20:22.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.179673415s
Mar 16 11:20:24.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.179371399s
Mar 16 11:20:26.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.179838541s
Mar 16 11:20:28.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.179806892s
Mar 16 11:20:30.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.180927649s
Mar 16 11:20:32.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.17981768s
Mar 16 11:20:34.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.180469857s
Mar 16 11:20:36.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.180201456s
Mar 16 11:20:38.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.179810124s
Mar 16 11:20:40.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.179986493s
Mar 16 11:20:42.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.180115155s
Mar 16 11:20:44.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.18067004s
Mar 16 11:20:46.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.181303484s
Mar 16 11:20:48.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.181063341s
Mar 16 11:20:50.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.179932178s
Mar 16 11:20:52.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.180238997s
Mar 16 11:20:54.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.180955885s
Mar 16 11:20:56.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.180529667s
Mar 16 11:20:58.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.181307367s
Mar 16 11:21:00.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.180663616s
Mar 16 11:21:02.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.179942438s
Mar 16 11:21:04.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.181717902s
Mar 16 11:21:06.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.181316639s
Mar 16 11:21:08.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.18036405s
Mar 16 11:21:10.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.180925518s
Mar 16 11:21:12.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.179929221s
Mar 16 11:21:14.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.18014572s
Mar 16 11:21:16.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.179972417s
Mar 16 11:21:18.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.181202782s
Mar 16 11:21:20.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.180057911s
Mar 16 11:21:22.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.180325711s
Mar 16 11:21:24.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.180252892s
Mar 16 11:21:26.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.182023824s
Mar 16 11:21:28.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.180521318s
Mar 16 11:21:30.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.18070033s
Mar 16 11:21:32.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.1800177s
Mar 16 11:21:34.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.180815987s
Mar 16 11:21:36.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.181210144s
Mar 16 11:21:38.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.180825549s
Mar 16 11:21:39.062: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.270580288s
STEP: updating the pod 03/16/23 11:21:39.062
Mar 16 11:21:39.744: INFO: Successfully updated pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e"
STEP: waiting for pod running 03/16/23 11:21:39.744
Mar 16 11:21:39.744: INFO: Waiting up to 2m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449" to be "running"
Mar 16 11:21:39.833: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.381454ms
Mar 16 11:21:41.924: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Running", Reason="", readiness=true. Elapsed: 2.179755955s
Mar 16 11:21:41.924: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" satisfied condition "running"
STEP: deleting the pod gracefully 03/16/23 11:21:41.924
Mar 16 11:21:41.924: INFO: Deleting pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449"
Mar 16 11:21:42.015: INFO: Wait up to 5m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:14.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5449" for this suite. 03/16/23 11:22:14.372
------------------------------
• [156.390 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:19:38.072
    Mar 16 11:19:38.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename var-expansion 03/16/23 11:19:38.073
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:19:38.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:19:38.519
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 03/16/23 11:19:38.697
    Mar 16 11:19:38.791: INFO: Waiting up to 2m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449" to be "running"
    Mar 16 11:19:38.881: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.594087ms
    Mar 16 11:19:40.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180372412s
    Mar 16 11:19:42.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.180173666s
    Mar 16 11:19:44.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.179921034s
    Mar 16 11:19:46.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179813873s
    Mar 16 11:19:48.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.181592715s
    Mar 16 11:19:50.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.179982588s
    Mar 16 11:19:52.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180002093s
    Mar 16 11:19:54.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.180022809s
    Mar 16 11:19:56.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.181032773s
    Mar 16 11:19:58.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.179690825s
    Mar 16 11:20:00.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179994206s
    Mar 16 11:20:02.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.179813843s
    Mar 16 11:20:04.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.179774131s
    Mar 16 11:20:06.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.181138566s
    Mar 16 11:20:08.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.18079048s
    Mar 16 11:20:10.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.180520673s
    Mar 16 11:20:12.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.179982768s
    Mar 16 11:20:14.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.180857843s
    Mar 16 11:20:16.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.180061462s
    Mar 16 11:20:18.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.1817419s
    Mar 16 11:20:20.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.181159369s
    Mar 16 11:20:22.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.179673415s
    Mar 16 11:20:24.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.179371399s
    Mar 16 11:20:26.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.179838541s
    Mar 16 11:20:28.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.179806892s
    Mar 16 11:20:30.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.180927649s
    Mar 16 11:20:32.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.17981768s
    Mar 16 11:20:34.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.180469857s
    Mar 16 11:20:36.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.180201456s
    Mar 16 11:20:38.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.179810124s
    Mar 16 11:20:40.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.179986493s
    Mar 16 11:20:42.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.180115155s
    Mar 16 11:20:44.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.18067004s
    Mar 16 11:20:46.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.181303484s
    Mar 16 11:20:48.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.181063341s
    Mar 16 11:20:50.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.179932178s
    Mar 16 11:20:52.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.180238997s
    Mar 16 11:20:54.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.180955885s
    Mar 16 11:20:56.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.180529667s
    Mar 16 11:20:58.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.181307367s
    Mar 16 11:21:00.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.180663616s
    Mar 16 11:21:02.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.179942438s
    Mar 16 11:21:04.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.181717902s
    Mar 16 11:21:06.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.181316639s
    Mar 16 11:21:08.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.18036405s
    Mar 16 11:21:10.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.180925518s
    Mar 16 11:21:12.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.179929221s
    Mar 16 11:21:14.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.18014572s
    Mar 16 11:21:16.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.179972417s
    Mar 16 11:21:18.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.181202782s
    Mar 16 11:21:20.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.180057911s
    Mar 16 11:21:22.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.180325711s
    Mar 16 11:21:24.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.180252892s
    Mar 16 11:21:26.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.182023824s
    Mar 16 11:21:28.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.180521318s
    Mar 16 11:21:30.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.18070033s
    Mar 16 11:21:32.971: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.1800177s
    Mar 16 11:21:34.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.180815987s
    Mar 16 11:21:36.973: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.181210144s
    Mar 16 11:21:38.972: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.180825549s
    Mar 16 11:21:39.062: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.270580288s
    STEP: updating the pod 03/16/23 11:21:39.062
    Mar 16 11:21:39.744: INFO: Successfully updated pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e"
    STEP: waiting for pod running 03/16/23 11:21:39.744
    Mar 16 11:21:39.744: INFO: Waiting up to 2m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449" to be "running"
    Mar 16 11:21:39.833: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.381454ms
    Mar 16 11:21:41.924: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e": Phase="Running", Reason="", readiness=true. Elapsed: 2.179755955s
    Mar 16 11:21:41.924: INFO: Pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" satisfied condition "running"
    STEP: deleting the pod gracefully 03/16/23 11:21:41.924
    Mar 16 11:21:41.924: INFO: Deleting pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" in namespace "var-expansion-5449"
    Mar 16 11:21:42.015: INFO: Wait up to 5m0s for pod "var-expansion-04bd8ae4-a941-4f79-af8c-23bc4c89964e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:14.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5449" for this suite. 03/16/23 11:22:14.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:14.464
Mar 16 11:22:14.464: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:22:14.465
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:14.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:14.912
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/16/23 11:22:15.089
Mar 16 11:22:15.195: INFO: Waiting up to 5m0s for pod "pod-87290c06-d800-4f0e-a161-cdb23013126b" in namespace "emptydir-7857" to be "Succeeded or Failed"
Mar 16 11:22:15.285: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.607037ms
Mar 16 11:22:17.375: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179485437s
Mar 16 11:22:19.376: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180726606s
STEP: Saw pod success 03/16/23 11:22:19.376
Mar 16 11:22:19.376: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b" satisfied condition "Succeeded or Failed"
Mar 16 11:22:19.466: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-87290c06-d800-4f0e-a161-cdb23013126b container test-container: <nil>
STEP: delete the pod 03/16/23 11:22:19.56
Mar 16 11:22:19.653: INFO: Waiting for pod pod-87290c06-d800-4f0e-a161-cdb23013126b to disappear
Mar 16 11:22:19.743: INFO: Pod pod-87290c06-d800-4f0e-a161-cdb23013126b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:19.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7857" for this suite. 03/16/23 11:22:19.921
------------------------------
• [5.547 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:14.464
    Mar 16 11:22:14.464: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:22:14.465
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:14.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:14.912
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/16/23 11:22:15.089
    Mar 16 11:22:15.195: INFO: Waiting up to 5m0s for pod "pod-87290c06-d800-4f0e-a161-cdb23013126b" in namespace "emptydir-7857" to be "Succeeded or Failed"
    Mar 16 11:22:15.285: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.607037ms
    Mar 16 11:22:17.375: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179485437s
    Mar 16 11:22:19.376: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180726606s
    STEP: Saw pod success 03/16/23 11:22:19.376
    Mar 16 11:22:19.376: INFO: Pod "pod-87290c06-d800-4f0e-a161-cdb23013126b" satisfied condition "Succeeded or Failed"
    Mar 16 11:22:19.466: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-87290c06-d800-4f0e-a161-cdb23013126b container test-container: <nil>
    STEP: delete the pod 03/16/23 11:22:19.56
    Mar 16 11:22:19.653: INFO: Waiting for pod pod-87290c06-d800-4f0e-a161-cdb23013126b to disappear
    Mar 16 11:22:19.743: INFO: Pod pod-87290c06-d800-4f0e-a161-cdb23013126b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:19.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7857" for this suite. 03/16/23 11:22:19.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:20.013
Mar 16 11:22:20.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:20.014
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:20.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:20.46
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 03/16/23 11:22:20.638
Mar 16 11:22:20.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0" in namespace "downward-api-5007" to be "Succeeded or Failed"
Mar 16 11:22:20.822: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.67165ms
Mar 16 11:22:22.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180443878s
Mar 16 11:22:24.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180296105s
STEP: Saw pod success 03/16/23 11:22:24.913
Mar 16 11:22:24.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0" satisfied condition "Succeeded or Failed"
Mar 16 11:22:25.003: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 container client-container: <nil>
STEP: delete the pod 03/16/23 11:22:25.096
Mar 16 11:22:25.191: INFO: Waiting for pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 to disappear
Mar 16 11:22:25.280: INFO: Pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:25.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5007" for this suite. 03/16/23 11:22:25.458
------------------------------
• [5.535 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:20.013
    Mar 16 11:22:20.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:20.014
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:20.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:20.46
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 03/16/23 11:22:20.638
    Mar 16 11:22:20.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0" in namespace "downward-api-5007" to be "Succeeded or Failed"
    Mar 16 11:22:20.822: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.67165ms
    Mar 16 11:22:22.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180443878s
    Mar 16 11:22:24.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180296105s
    STEP: Saw pod success 03/16/23 11:22:24.913
    Mar 16 11:22:24.913: INFO: Pod "downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0" satisfied condition "Succeeded or Failed"
    Mar 16 11:22:25.003: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 container client-container: <nil>
    STEP: delete the pod 03/16/23 11:22:25.096
    Mar 16 11:22:25.191: INFO: Waiting for pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 to disappear
    Mar 16 11:22:25.280: INFO: Pod downwardapi-volume-c154bec7-b88d-42d1-b10e-ebe01b5b88f0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:25.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5007" for this suite. 03/16/23 11:22:25.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:25.549
Mar 16 11:22:25.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:25.55
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:25.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:25.996
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 03/16/23 11:22:26.174
Mar 16 11:22:26.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee" in namespace "downward-api-6987" to be "Succeeded or Failed"
Mar 16 11:22:26.358: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Pending", Reason="", readiness=false. Elapsed: 89.583572ms
Mar 16 11:22:28.449: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180605999s
Mar 16 11:22:30.448: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179642859s
STEP: Saw pod success 03/16/23 11:22:30.448
Mar 16 11:22:30.448: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee" satisfied condition "Succeeded or Failed"
Mar 16 11:22:30.538: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee container client-container: <nil>
STEP: delete the pod 03/16/23 11:22:30.632
Mar 16 11:22:30.725: INFO: Waiting for pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee to disappear
Mar 16 11:22:30.814: INFO: Pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:30.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6987" for this suite. 03/16/23 11:22:30.993
------------------------------
• [5.534 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:25.549
    Mar 16 11:22:25.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:25.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:25.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:25.996
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 03/16/23 11:22:26.174
    Mar 16 11:22:26.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee" in namespace "downward-api-6987" to be "Succeeded or Failed"
    Mar 16 11:22:26.358: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Pending", Reason="", readiness=false. Elapsed: 89.583572ms
    Mar 16 11:22:28.449: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180605999s
    Mar 16 11:22:30.448: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179642859s
    STEP: Saw pod success 03/16/23 11:22:30.448
    Mar 16 11:22:30.448: INFO: Pod "downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee" satisfied condition "Succeeded or Failed"
    Mar 16 11:22:30.538: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee container client-container: <nil>
    STEP: delete the pod 03/16/23 11:22:30.632
    Mar 16 11:22:30.725: INFO: Waiting for pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee to disappear
    Mar 16 11:22:30.814: INFO: Pod downwardapi-volume-0a68a20b-6f5c-41fb-9305-daff417b7eee no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:30.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6987" for this suite. 03/16/23 11:22:30.993
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:31.083
Mar 16 11:22:31.083: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:22:31.084
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:31.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:31.531
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-540d7d97-c914-47b9-8946-dd89d799d3b6 03/16/23 11:22:31.708
STEP: Creating a pod to test consume configMaps 03/16/23 11:22:31.798
Mar 16 11:22:31.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272" in namespace "projected-495" to be "Succeeded or Failed"
Mar 16 11:22:31.982: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Pending", Reason="", readiness=false. Elapsed: 89.573378ms
Mar 16 11:22:34.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179909942s
Mar 16 11:22:36.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180444778s
STEP: Saw pod success 03/16/23 11:22:36.073
Mar 16 11:22:36.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272" satisfied condition "Succeeded or Failed"
Mar 16 11:22:36.162: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 11:22:36.26
Mar 16 11:22:36.354: INFO: Waiting for pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 to disappear
Mar 16 11:22:36.443: INFO: Pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-495" for this suite. 03/16/23 11:22:36.621
------------------------------
• [5.628 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:31.083
    Mar 16 11:22:31.083: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:22:31.084
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:31.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:31.531
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-540d7d97-c914-47b9-8946-dd89d799d3b6 03/16/23 11:22:31.708
    STEP: Creating a pod to test consume configMaps 03/16/23 11:22:31.798
    Mar 16 11:22:31.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272" in namespace "projected-495" to be "Succeeded or Failed"
    Mar 16 11:22:31.982: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Pending", Reason="", readiness=false. Elapsed: 89.573378ms
    Mar 16 11:22:34.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179909942s
    Mar 16 11:22:36.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180444778s
    STEP: Saw pod success 03/16/23 11:22:36.073
    Mar 16 11:22:36.073: INFO: Pod "pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272" satisfied condition "Succeeded or Failed"
    Mar 16 11:22:36.162: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 11:22:36.26
    Mar 16 11:22:36.354: INFO: Waiting for pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 to disappear
    Mar 16 11:22:36.443: INFO: Pod pod-projected-configmaps-5be7c848-5b02-43bb-afe4-1e5124a27272 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-495" for this suite. 03/16/23 11:22:36.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:36.712
Mar 16 11:22:36.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:36.714
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:36.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:37.16
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 03/16/23 11:22:37.338
Mar 16 11:22:37.433: INFO: Waiting up to 5m0s for pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc" in namespace "downward-api-2467" to be "Succeeded or Failed"
Mar 16 11:22:37.523: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789714ms
Mar 16 11:22:39.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180625838s
Mar 16 11:22:41.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180795969s
STEP: Saw pod success 03/16/23 11:22:41.614
Mar 16 11:22:41.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc" satisfied condition "Succeeded or Failed"
Mar 16 11:22:41.704: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc container dapi-container: <nil>
STEP: delete the pod 03/16/23 11:22:41.798
Mar 16 11:22:41.891: INFO: Waiting for pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc to disappear
Mar 16 11:22:41.980: INFO: Pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 16 11:22:41.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2467" for this suite. 03/16/23 11:22:42.158
------------------------------
• [5.540 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:36.712
    Mar 16 11:22:36.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:22:36.714
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:36.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:37.16
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 03/16/23 11:22:37.338
    Mar 16 11:22:37.433: INFO: Waiting up to 5m0s for pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc" in namespace "downward-api-2467" to be "Succeeded or Failed"
    Mar 16 11:22:37.523: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789714ms
    Mar 16 11:22:39.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180625838s
    Mar 16 11:22:41.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180795969s
    STEP: Saw pod success 03/16/23 11:22:41.614
    Mar 16 11:22:41.614: INFO: Pod "downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc" satisfied condition "Succeeded or Failed"
    Mar 16 11:22:41.704: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc container dapi-container: <nil>
    STEP: delete the pod 03/16/23 11:22:41.798
    Mar 16 11:22:41.891: INFO: Waiting for pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc to disappear
    Mar 16 11:22:41.980: INFO: Pod downward-api-c73b4c4a-1d33-4327-8d1b-ba6c319d6dcc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:22:41.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2467" for this suite. 03/16/23 11:22:42.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:22:42.254
Mar 16 11:22:42.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 11:22:42.256
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:42.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:42.702
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/16/23 11:22:42.88
Mar 16 11:22:42.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Mar 16 11:22:47.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:23:08.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6222" for this suite. 03/16/23 11:23:08.661
------------------------------
• [26.497 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:22:42.254
    Mar 16 11:22:42.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename crd-publish-openapi 03/16/23 11:22:42.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:22:42.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:22:42.702
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/16/23 11:22:42.88
    Mar 16 11:22:42.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    Mar 16 11:22:47.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:23:08.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6222" for this suite. 03/16/23 11:23:08.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:23:08.752
Mar 16 11:23:08.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 11:23:08.753
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:09.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:09.199
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7602 03/16/23 11:23:09.377
STEP: changing the ExternalName service to type=NodePort 03/16/23 11:23:09.467
STEP: creating replication controller externalname-service in namespace services-7602 03/16/23 11:23:09.652
I0316 11:23:09.743495    8588 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7602, replica count: 2
I0316 11:23:12.844176    8588 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 16 11:23:12.844: INFO: Creating new exec pod
Mar 16 11:23:12.941: INFO: Waiting up to 5m0s for pod "execpodd2pvg" in namespace "services-7602" to be "running"
Mar 16 11:23:13.031: INFO: Pod "execpodd2pvg": Phase="Pending", Reason="", readiness=false. Elapsed: 89.652867ms
Mar 16 11:23:15.122: INFO: Pod "execpodd2pvg": Phase="Running", Reason="", readiness=true. Elapsed: 2.180825657s
Mar 16 11:23:15.122: INFO: Pod "execpodd2pvg" satisfied condition "running"
Mar 16 11:23:16.301: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 16 11:23:17.446: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 16 11:23:17.446: INFO: stdout: ""
Mar 16 11:23:17.447: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 100.106.90.148 80'
Mar 16 11:23:18.526: INFO: stderr: "+ nc -v -z -w 2 100.106.90.148 80\nConnection to 100.106.90.148 80 port [tcp/http] succeeded!\n"
Mar 16 11:23:18.526: INFO: stdout: ""
Mar 16 11:23:18.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 31128'
Mar 16 11:23:19.638: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 31128\nConnection to 10.250.19.136 31128 port [tcp/*] succeeded!\n"
Mar 16 11:23:19.638: INFO: stdout: ""
Mar 16 11:23:19.638: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 31128'
Mar 16 11:23:20.741: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 31128\nConnection to 10.250.19.246 31128 port [tcp/*] succeeded!\n"
Mar 16 11:23:20.741: INFO: stdout: ""
Mar 16 11:23:20.741: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 11:23:20.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7602" for this suite. 03/16/23 11:23:21.017
------------------------------
• [12.355 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:23:08.752
    Mar 16 11:23:08.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 11:23:08.753
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:09.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:09.199
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7602 03/16/23 11:23:09.377
    STEP: changing the ExternalName service to type=NodePort 03/16/23 11:23:09.467
    STEP: creating replication controller externalname-service in namespace services-7602 03/16/23 11:23:09.652
    I0316 11:23:09.743495    8588 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7602, replica count: 2
    I0316 11:23:12.844176    8588 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 16 11:23:12.844: INFO: Creating new exec pod
    Mar 16 11:23:12.941: INFO: Waiting up to 5m0s for pod "execpodd2pvg" in namespace "services-7602" to be "running"
    Mar 16 11:23:13.031: INFO: Pod "execpodd2pvg": Phase="Pending", Reason="", readiness=false. Elapsed: 89.652867ms
    Mar 16 11:23:15.122: INFO: Pod "execpodd2pvg": Phase="Running", Reason="", readiness=true. Elapsed: 2.180825657s
    Mar 16 11:23:15.122: INFO: Pod "execpodd2pvg" satisfied condition "running"
    Mar 16 11:23:16.301: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 16 11:23:17.446: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 16 11:23:17.446: INFO: stdout: ""
    Mar 16 11:23:17.447: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 100.106.90.148 80'
    Mar 16 11:23:18.526: INFO: stderr: "+ nc -v -z -w 2 100.106.90.148 80\nConnection to 100.106.90.148 80 port [tcp/http] succeeded!\n"
    Mar 16 11:23:18.526: INFO: stdout: ""
    Mar 16 11:23:18.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 10.250.19.136 31128'
    Mar 16 11:23:19.638: INFO: stderr: "+ nc -v -z -w 2 10.250.19.136 31128\nConnection to 10.250.19.136 31128 port [tcp/*] succeeded!\n"
    Mar 16 11:23:19.638: INFO: stdout: ""
    Mar 16 11:23:19.638: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7602 exec execpodd2pvg -- /bin/sh -x -c nc -v -z -w 2 10.250.19.246 31128'
    Mar 16 11:23:20.741: INFO: stderr: "+ nc -v -z -w 2 10.250.19.246 31128\nConnection to 10.250.19.246 31128 port [tcp/*] succeeded!\n"
    Mar 16 11:23:20.741: INFO: stdout: ""
    Mar 16 11:23:20.741: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:23:20.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7602" for this suite. 03/16/23 11:23:21.017
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:23:21.108
Mar 16 11:23:21.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:23:21.109
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:21.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:21.555
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:23:21.915
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:23:22.642
STEP: Deploying the webhook pod 03/16/23 11:23:22.734
STEP: Wait for the deployment to be ready 03/16/23 11:23:22.914
Mar 16 11:23:23.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:23:25.274
STEP: Verifying the service has paired with the endpoint 03/16/23 11:23:25.369
Mar 16 11:23:26.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/16/23 11:23:26.459
STEP: create a pod that should be updated by the webhook 03/16/23 11:23:26.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:23:26.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4139" for this suite. 03/16/23 11:23:27.511
STEP: Destroying namespace "webhook-4139-markers" for this suite. 03/16/23 11:23:27.602
------------------------------
• [6.584 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:23:21.108
    Mar 16 11:23:21.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:23:21.109
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:21.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:21.555
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:23:21.915
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:23:22.642
    STEP: Deploying the webhook pod 03/16/23 11:23:22.734
    STEP: Wait for the deployment to be ready 03/16/23 11:23:22.914
    Mar 16 11:23:23.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 23, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:23:25.274
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:23:25.369
    Mar 16 11:23:26.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/16/23 11:23:26.459
    STEP: create a pod that should be updated by the webhook 03/16/23 11:23:26.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:23:26.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4139" for this suite. 03/16/23 11:23:27.511
    STEP: Destroying namespace "webhook-4139-markers" for this suite. 03/16/23 11:23:27.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:23:27.693
Mar 16 11:23:27.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 03/16/23 11:23:27.694
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:27.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:28.14
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/16/23 11:23:28.318
STEP: listing events in all namespaces 03/16/23 11:23:28.408
STEP: listing events in test namespace 03/16/23 11:23:28.499
STEP: listing events with field selection filtering on source 03/16/23 11:23:28.588
STEP: listing events with field selection filtering on reportingController 03/16/23 11:23:28.678
STEP: getting the test event 03/16/23 11:23:28.768
STEP: patching the test event 03/16/23 11:23:28.857
STEP: getting the test event 03/16/23 11:23:28.949
STEP: updating the test event 03/16/23 11:23:29.038
STEP: getting the test event 03/16/23 11:23:29.129
STEP: deleting the test event 03/16/23 11:23:29.219
STEP: listing events in all namespaces 03/16/23 11:23:29.31
STEP: listing events in test namespace 03/16/23 11:23:29.4
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 16 11:23:29.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4872" for this suite. 03/16/23 11:23:29.581
------------------------------
• [1.978 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:23:27.693
    Mar 16 11:23:27.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 03/16/23 11:23:27.694
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:27.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:28.14
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/16/23 11:23:28.318
    STEP: listing events in all namespaces 03/16/23 11:23:28.408
    STEP: listing events in test namespace 03/16/23 11:23:28.499
    STEP: listing events with field selection filtering on source 03/16/23 11:23:28.588
    STEP: listing events with field selection filtering on reportingController 03/16/23 11:23:28.678
    STEP: getting the test event 03/16/23 11:23:28.768
    STEP: patching the test event 03/16/23 11:23:28.857
    STEP: getting the test event 03/16/23 11:23:28.949
    STEP: updating the test event 03/16/23 11:23:29.038
    STEP: getting the test event 03/16/23 11:23:29.129
    STEP: deleting the test event 03/16/23 11:23:29.219
    STEP: listing events in all namespaces 03/16/23 11:23:29.31
    STEP: listing events in test namespace 03/16/23 11:23:29.4
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:23:29.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4872" for this suite. 03/16/23 11:23:29.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:23:29.671
Mar 16 11:23:29.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice 03/16/23 11:23:29.672
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:29.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:30.119
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 16 11:23:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4799" for this suite. 03/16/23 11:23:30.936
------------------------------
• [1.355 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:23:29.671
    Mar 16 11:23:29.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename endpointslice 03/16/23 11:23:29.672
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:29.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:30.119
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:23:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4799" for this suite. 03/16/23 11:23:30.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:23:31.027
Mar 16 11:23:31.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 11:23:31.028
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:31.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:31.476
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4467 03/16/23 11:23:31.653
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 03/16/23 11:23:31.744
Mar 16 11:23:31.924: INFO: Found 1 stateful pods, waiting for 3
Mar 16 11:23:42.015: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 11:23:42.015: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 11:23:42.015: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 16 11:23:42.285: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 11:23:43.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 11:23:43.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 11:23:43.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/16/23 11:23:43.583
Mar 16 11:23:43.774: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/16/23 11:23:43.774
STEP: Updating Pods in reverse ordinal order 03/16/23 11:23:43.953
Mar 16 11:23:44.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 11:23:45.227: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 11:23:45.227: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 11:23:45.227: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/16/23 11:23:55.768
Mar 16 11:23:55.768: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 16 11:23:56.873: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 16 11:23:56.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 16 11:23:56.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 16 11:24:07.424: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/16/23 11:24:07.603
Mar 16 11:24:07.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 16 11:24:08.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 16 11:24:08.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 16 11:24:08.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 11:24:19.445: INFO: Deleting all statefulset in ns statefulset-4467
Mar 16 11:24:19.534: INFO: Scaling statefulset ss2 to 0
Mar 16 11:24:29.897: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 11:24:29.986: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:30.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4467" for this suite. 03/16/23 11:24:30.434
------------------------------
• [59.498 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:23:31.027
    Mar 16 11:23:31.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 11:23:31.028
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:23:31.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:23:31.476
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4467 03/16/23 11:23:31.653
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 03/16/23 11:23:31.744
    Mar 16 11:23:31.924: INFO: Found 1 stateful pods, waiting for 3
    Mar 16 11:23:42.015: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 11:23:42.015: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 11:23:42.015: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 16 11:23:42.285: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 11:23:43.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 11:23:43.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 11:23:43.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/16/23 11:23:43.583
    Mar 16 11:23:43.774: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/16/23 11:23:43.774
    STEP: Updating Pods in reverse ordinal order 03/16/23 11:23:43.953
    Mar 16 11:23:44.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 11:23:45.227: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 11:23:45.227: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 11:23:45.227: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/16/23 11:23:55.768
    Mar 16 11:23:55.768: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 16 11:23:56.873: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 16 11:23:56.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 16 11:23:56.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 16 11:24:07.424: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/16/23 11:24:07.603
    Mar 16 11:24:07.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-4467 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 16 11:24:08.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 16 11:24:08.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 16 11:24:08.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 11:24:19.445: INFO: Deleting all statefulset in ns statefulset-4467
    Mar 16 11:24:19.534: INFO: Scaling statefulset ss2 to 0
    Mar 16 11:24:29.897: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 11:24:29.986: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:30.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4467" for this suite. 03/16/23 11:24:30.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:30.528
Mar 16 11:24:30.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 03/16/23 11:24:30.53
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:30.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:30.976
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 03/16/23 11:24:31.158
STEP: wait for the container to reach Failed 03/16/23 11:24:31.252
STEP: get the container status 03/16/23 11:24:34.613
STEP: the container should be terminated 03/16/23 11:24:34.703
STEP: the termination message should be set 03/16/23 11:24:34.703
Mar 16 11:24:34.703: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/16/23 11:24:34.703
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:34.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-84" for this suite. 03/16/23 11:24:35.064
------------------------------
• [4.627 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:30.528
    Mar 16 11:24:30.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 03/16/23 11:24:30.53
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:30.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:30.976
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 03/16/23 11:24:31.158
    STEP: wait for the container to reach Failed 03/16/23 11:24:31.252
    STEP: get the container status 03/16/23 11:24:34.613
    STEP: the container should be terminated 03/16/23 11:24:34.703
    STEP: the termination message should be set 03/16/23 11:24:34.703
    Mar 16 11:24:34.703: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/16/23 11:24:34.703
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:34.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-84" for this suite. 03/16/23 11:24:35.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:35.156
Mar 16 11:24:35.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 11:24:35.157
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:35.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:35.604
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/16/23 11:24:35.782
Mar 16 11:24:35.962: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/16/23 11:24:35.962
Mar 16 11:24:35.962: INFO: Waiting up to 5m0s for pod "test-rs-rpsbq" in namespace "replicaset-379" to be "running"
Mar 16 11:24:36.052: INFO: Pod "test-rs-rpsbq": Phase="Pending", Reason="", readiness=false. Elapsed: 89.747122ms
Mar 16 11:24:38.143: INFO: Pod "test-rs-rpsbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.180810843s
Mar 16 11:24:38.143: INFO: Pod "test-rs-rpsbq" satisfied condition "running"
STEP: getting scale subresource 03/16/23 11:24:38.143
STEP: updating a scale subresource 03/16/23 11:24:38.233
STEP: verifying the replicaset Spec.Replicas was modified 03/16/23 11:24:38.323
STEP: Patch a scale subresource 03/16/23 11:24:38.413
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:38.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-379" for this suite. 03/16/23 11:24:38.77
------------------------------
• [3.705 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:35.156
    Mar 16 11:24:35.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 11:24:35.157
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:35.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:35.604
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/16/23 11:24:35.782
    Mar 16 11:24:35.962: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/16/23 11:24:35.962
    Mar 16 11:24:35.962: INFO: Waiting up to 5m0s for pod "test-rs-rpsbq" in namespace "replicaset-379" to be "running"
    Mar 16 11:24:36.052: INFO: Pod "test-rs-rpsbq": Phase="Pending", Reason="", readiness=false. Elapsed: 89.747122ms
    Mar 16 11:24:38.143: INFO: Pod "test-rs-rpsbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.180810843s
    Mar 16 11:24:38.143: INFO: Pod "test-rs-rpsbq" satisfied condition "running"
    STEP: getting scale subresource 03/16/23 11:24:38.143
    STEP: updating a scale subresource 03/16/23 11:24:38.233
    STEP: verifying the replicaset Spec.Replicas was modified 03/16/23 11:24:38.323
    STEP: Patch a scale subresource 03/16/23 11:24:38.413
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:38.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-379" for this suite. 03/16/23 11:24:38.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:38.862
Mar 16 11:24:38.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:24:38.863
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:39.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:39.309
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:24:39.668
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:24:40.284
STEP: Deploying the webhook pod 03/16/23 11:24:40.376
STEP: Wait for the deployment to be ready 03/16/23 11:24:40.557
Mar 16 11:24:40.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:24:42.917
STEP: Verifying the service has paired with the endpoint 03/16/23 11:24:43.01
Mar 16 11:24:44.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/16/23 11:24:44.1
STEP: create a configmap that should be updated by the webhook 03/16/23 11:24:44.387
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:44.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9177" for this suite. 03/16/23 11:24:45.231
STEP: Destroying namespace "webhook-9177-markers" for this suite. 03/16/23 11:24:45.322
------------------------------
• [6.551 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:38.862
    Mar 16 11:24:38.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:24:38.863
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:39.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:39.309
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:24:39.668
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:24:40.284
    STEP: Deploying the webhook pod 03/16/23 11:24:40.376
    STEP: Wait for the deployment to be ready 03/16/23 11:24:40.557
    Mar 16 11:24:40.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 24, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:24:42.917
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:24:43.01
    Mar 16 11:24:44.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/16/23 11:24:44.1
    STEP: create a configmap that should be updated by the webhook 03/16/23 11:24:44.387
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:44.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9177" for this suite. 03/16/23 11:24:45.231
    STEP: Destroying namespace "webhook-9177-markers" for this suite. 03/16/23 11:24:45.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:45.415
Mar 16 11:24:45.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller 03/16/23 11:24:45.417
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:45.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:45.863
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 03/16/23 11:24:46.041
STEP: When the matched label of one of its pods change 03/16/23 11:24:46.132
Mar 16 11:24:46.222: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/16/23 11:24:46.402
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:46.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-447" for this suite. 03/16/23 11:24:46.584
------------------------------
• [1.259 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:45.415
    Mar 16 11:24:45.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replication-controller 03/16/23 11:24:45.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:45.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:45.863
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 03/16/23 11:24:46.041
    STEP: When the matched label of one of its pods change 03/16/23 11:24:46.132
    Mar 16 11:24:46.222: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/16/23 11:24:46.402
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:46.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-447" for this suite. 03/16/23 11:24:46.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:46.676
Mar 16 11:24:46.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:24:46.677
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:46.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:47.124
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 03/16/23 11:24:47.302
Mar 16 11:24:47.397: INFO: Waiting up to 5m0s for pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c" in namespace "downward-api-5683" to be "Succeeded or Failed"
Mar 16 11:24:47.487: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.128084ms
Mar 16 11:24:49.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18077427s
Mar 16 11:24:51.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181129248s
STEP: Saw pod success 03/16/23 11:24:51.578
Mar 16 11:24:51.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c" satisfied condition "Succeeded or Failed"
Mar 16 11:24:51.668: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c container dapi-container: <nil>
STEP: delete the pod 03/16/23 11:24:51.763
Mar 16 11:24:51.857: INFO: Waiting for pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c to disappear
Mar 16 11:24:51.946: INFO: Pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 16 11:24:51.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5683" for this suite. 03/16/23 11:24:52.124
------------------------------
• [5.539 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:46.676
    Mar 16 11:24:46.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:24:46.677
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:46.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:47.124
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 03/16/23 11:24:47.302
    Mar 16 11:24:47.397: INFO: Waiting up to 5m0s for pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c" in namespace "downward-api-5683" to be "Succeeded or Failed"
    Mar 16 11:24:47.487: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Pending", Reason="", readiness=false. Elapsed: 90.128084ms
    Mar 16 11:24:49.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18077427s
    Mar 16 11:24:51.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181129248s
    STEP: Saw pod success 03/16/23 11:24:51.578
    Mar 16 11:24:51.578: INFO: Pod "downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c" satisfied condition "Succeeded or Failed"
    Mar 16 11:24:51.668: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c container dapi-container: <nil>
    STEP: delete the pod 03/16/23 11:24:51.763
    Mar 16 11:24:51.857: INFO: Waiting for pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c to disappear
    Mar 16 11:24:51.946: INFO: Pod downward-api-49615bf4-f8a0-4fdf-aba4-778b519fc07c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:24:51.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5683" for this suite. 03/16/23 11:24:52.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:24:52.215
Mar 16 11:24:52.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe 03/16/23 11:24:52.216
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:52.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:52.662
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b in namespace container-probe-9204 03/16/23 11:24:52.84
Mar 16 11:24:52.935: INFO: Waiting up to 5m0s for pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b" in namespace "container-probe-9204" to be "not pending"
Mar 16 11:24:53.025: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.887823ms
Mar 16 11:24:55.115: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.180473985s
Mar 16 11:24:55.115: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b" satisfied condition "not pending"
Mar 16 11:24:55.115: INFO: Started pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b in namespace container-probe-9204
STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 11:24:55.115
Mar 16 11:24:55.205: INFO: Initial restart count of pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b is 0
STEP: deleting the pod 03/16/23 11:28:55.65
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 16 11:28:55.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9204" for this suite. 03/16/23 11:28:55.923
------------------------------
• [243.799 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:24:52.215
    Mar 16 11:24:52.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-probe 03/16/23 11:24:52.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:24:52.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:24:52.662
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b in namespace container-probe-9204 03/16/23 11:24:52.84
    Mar 16 11:24:52.935: INFO: Waiting up to 5m0s for pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b" in namespace "container-probe-9204" to be "not pending"
    Mar 16 11:24:53.025: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.887823ms
    Mar 16 11:24:55.115: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.180473985s
    Mar 16 11:24:55.115: INFO: Pod "test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b" satisfied condition "not pending"
    Mar 16 11:24:55.115: INFO: Started pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b in namespace container-probe-9204
    STEP: checking the pod's current state and verifying that restartCount is present 03/16/23 11:24:55.115
    Mar 16 11:24:55.205: INFO: Initial restart count of pod test-webserver-3d8bad69-229a-4fd5-9260-87c97d5aca3b is 0
    STEP: deleting the pod 03/16/23 11:28:55.65
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:28:55.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9204" for this suite. 03/16/23 11:28:55.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:28:56.014
Mar 16 11:28:56.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:28:56.016
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:28:56.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:28:56.466
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:28:56.826
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:28:57.991
STEP: Deploying the webhook pod 03/16/23 11:28:58.081
STEP: Wait for the deployment to be ready 03/16/23 11:28:58.263
Mar 16 11:28:58.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:29:00.623
STEP: Verifying the service has paired with the endpoint 03/16/23 11:29:00.717
Mar 16 11:29:01.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 03/16/23 11:29:01.808
STEP: Creating a custom resource definition that should be denied by the webhook 03/16/23 11:29:02.095
Mar 16 11:29:02.095: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:02.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3705" for this suite. 03/16/23 11:29:02.89
STEP: Destroying namespace "webhook-3705-markers" for this suite. 03/16/23 11:29:02.98
------------------------------
• [7.057 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:28:56.014
    Mar 16 11:28:56.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:28:56.016
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:28:56.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:28:56.466
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:28:56.826
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:28:57.991
    STEP: Deploying the webhook pod 03/16/23 11:28:58.081
    STEP: Wait for the deployment to be ready 03/16/23 11:28:58.263
    Mar 16 11:28:58.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 28, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:29:00.623
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:29:00.717
    Mar 16 11:29:01.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/16/23 11:29:01.808
    STEP: Creating a custom resource definition that should be denied by the webhook 03/16/23 11:29:02.095
    Mar 16 11:29:02.095: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:02.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3705" for this suite. 03/16/23 11:29:02.89
    STEP: Destroying namespace "webhook-3705-markers" for this suite. 03/16/23 11:29:02.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:03.072
Mar 16 11:29:03.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset 03/16/23 11:29:03.073
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:03.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:03.52
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3987 03/16/23 11:29:03.699
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-3987 03/16/23 11:29:03.878
Mar 16 11:29:04.059: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 16 11:29:14.151: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/16/23 11:29:14.33
STEP: Getting /status 03/16/23 11:29:14.424
Mar 16 11:29:14.515: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/16/23 11:29:14.515
Mar 16 11:29:14.701: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/16/23 11:29:14.701
Mar 16 11:29:14.790: INFO: Observed &StatefulSet event: ADDED
Mar 16 11:29:14.790: INFO: Found Statefulset ss in namespace statefulset-3987 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 16 11:29:14.790: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/16/23 11:29:14.79
Mar 16 11:29:14.790: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 16 11:29:14.882: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/16/23 11:29:14.882
Mar 16 11:29:14.971: INFO: Observed &StatefulSet event: ADDED
Mar 16 11:29:14.971: INFO: Observed Statefulset ss in namespace statefulset-3987 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 16 11:29:14.972: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 16 11:29:14.972: INFO: Deleting all statefulset in ns statefulset-3987
Mar 16 11:29:15.061: INFO: Scaling statefulset ss to 0
Mar 16 11:29:25.426: INFO: Waiting for statefulset status.replicas updated to 0
Mar 16 11:29:25.516: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:25.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3987" for this suite. 03/16/23 11:29:25.964
------------------------------
• [22.983 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:03.072
    Mar 16 11:29:03.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename statefulset 03/16/23 11:29:03.073
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:03.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:03.52
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3987 03/16/23 11:29:03.699
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-3987 03/16/23 11:29:03.878
    Mar 16 11:29:04.059: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 16 11:29:14.151: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/16/23 11:29:14.33
    STEP: Getting /status 03/16/23 11:29:14.424
    Mar 16 11:29:14.515: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/16/23 11:29:14.515
    Mar 16 11:29:14.701: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/16/23 11:29:14.701
    Mar 16 11:29:14.790: INFO: Observed &StatefulSet event: ADDED
    Mar 16 11:29:14.790: INFO: Found Statefulset ss in namespace statefulset-3987 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 16 11:29:14.790: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/16/23 11:29:14.79
    Mar 16 11:29:14.790: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 16 11:29:14.882: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/16/23 11:29:14.882
    Mar 16 11:29:14.971: INFO: Observed &StatefulSet event: ADDED
    Mar 16 11:29:14.971: INFO: Observed Statefulset ss in namespace statefulset-3987 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 16 11:29:14.972: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 16 11:29:14.972: INFO: Deleting all statefulset in ns statefulset-3987
    Mar 16 11:29:15.061: INFO: Scaling statefulset ss to 0
    Mar 16 11:29:25.426: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 16 11:29:25.516: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:25.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3987" for this suite. 03/16/23 11:29:25.964
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:26.055
Mar 16 11:29:26.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 11:29:26.056
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:26.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:26.504
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/16/23 11:29:26.682
Mar 16 11:29:26.779: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9190" to be "running and ready"
Mar 16 11:29:26.869: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 89.689168ms
Mar 16 11:29:26.869: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:29:28.960: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.180491295s
Mar 16 11:29:28.960: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 16 11:29:28.960: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/16/23 11:29:29.049
STEP: Then the orphan pod is adopted 03/16/23 11:29:29.14
STEP: When the matched label of one of its pods change 03/16/23 11:29:29.23
Mar 16 11:29:29.320: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/16/23 11:29:29.502
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:29.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9190" for this suite. 03/16/23 11:29:29.77
------------------------------
• [3.806 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:26.055
    Mar 16 11:29:26.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 11:29:26.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:26.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:26.504
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/16/23 11:29:26.682
    Mar 16 11:29:26.779: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9190" to be "running and ready"
    Mar 16 11:29:26.869: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 89.689168ms
    Mar 16 11:29:26.869: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:29:28.960: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.180491295s
    Mar 16 11:29:28.960: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 16 11:29:28.960: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/16/23 11:29:29.049
    STEP: Then the orphan pod is adopted 03/16/23 11:29:29.14
    STEP: When the matched label of one of its pods change 03/16/23 11:29:29.23
    Mar 16 11:29:29.320: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/16/23 11:29:29.502
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:29.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9190" for this suite. 03/16/23 11:29:29.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:29.903
Mar 16 11:29:29.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers 03/16/23 11:29:29.905
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:30.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:30.353
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 03/16/23 11:29:30.531
Mar 16 11:29:30.625: INFO: Waiting up to 5m0s for pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae" in namespace "containers-6463" to be "Succeeded or Failed"
Mar 16 11:29:30.715: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 89.582296ms
Mar 16 11:29:32.805: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180200317s
Mar 16 11:29:34.804: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179459043s
STEP: Saw pod success 03/16/23 11:29:34.804
Mar 16 11:29:34.805: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae" satisfied condition "Succeeded or Failed"
Mar 16 11:29:34.894: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae container agnhost-container: <nil>
STEP: delete the pod 03/16/23 11:29:35.034
Mar 16 11:29:35.128: INFO: Waiting for pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae to disappear
Mar 16 11:29:35.218: INFO: Pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:35.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6463" for this suite. 03/16/23 11:29:35.396
------------------------------
• [5.584 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:29.903
    Mar 16 11:29:29.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename containers 03/16/23 11:29:29.905
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:30.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:30.353
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 03/16/23 11:29:30.531
    Mar 16 11:29:30.625: INFO: Waiting up to 5m0s for pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae" in namespace "containers-6463" to be "Succeeded or Failed"
    Mar 16 11:29:30.715: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 89.582296ms
    Mar 16 11:29:32.805: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180200317s
    Mar 16 11:29:34.804: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179459043s
    STEP: Saw pod success 03/16/23 11:29:34.804
    Mar 16 11:29:34.805: INFO: Pod "client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae" satisfied condition "Succeeded or Failed"
    Mar 16 11:29:34.894: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 11:29:35.034
    Mar 16 11:29:35.128: INFO: Waiting for pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae to disappear
    Mar 16 11:29:35.218: INFO: Pod client-containers-b56d46fa-7b88-4a00-b3dd-55b321a6b5ae no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:35.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6463" for this suite. 03/16/23 11:29:35.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:35.487
Mar 16 11:29:35.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 11:29:35.489
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:35.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:35.936
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/16/23 11:29:36.292
Mar 16 11:29:36.389: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4988" to be "running and ready"
Mar 16 11:29:36.479: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.950517ms
Mar 16 11:29:36.479: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:29:38.570: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.181407422s
Mar 16 11:29:38.570: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 16 11:29:38.570: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 03/16/23 11:29:38.66
Mar 16 11:29:38.753: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4988" to be "running and ready"
Mar 16 11:29:38.843: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.797334ms
Mar 16 11:29:38.843: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:29:40.934: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180309292s
Mar 16 11:29:40.934: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 16 11:29:40.934: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/16/23 11:29:41.023
Mar 16 11:29:41.114: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 16 11:29:41.205: INFO: Pod pod-with-prestop-http-hook still exists
Mar 16 11:29:43.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 16 11:29:43.296: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/16/23 11:29:43.296
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:43.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4988" for this suite. 03/16/23 11:29:43.568
------------------------------
• [8.172 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:35.487
    Mar 16 11:29:35.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/16/23 11:29:35.489
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:35.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:35.936
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/16/23 11:29:36.292
    Mar 16 11:29:36.389: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4988" to be "running and ready"
    Mar 16 11:29:36.479: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 89.950517ms
    Mar 16 11:29:36.479: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:29:38.570: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.181407422s
    Mar 16 11:29:38.570: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 16 11:29:38.570: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 03/16/23 11:29:38.66
    Mar 16 11:29:38.753: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4988" to be "running and ready"
    Mar 16 11:29:38.843: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 89.797334ms
    Mar 16 11:29:38.843: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:29:40.934: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.180309292s
    Mar 16 11:29:40.934: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 16 11:29:40.934: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/16/23 11:29:41.023
    Mar 16 11:29:41.114: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 16 11:29:41.205: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 16 11:29:43.206: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 16 11:29:43.296: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/16/23 11:29:43.296
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:43.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4988" for this suite. 03/16/23 11:29:43.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:43.66
Mar 16 11:29:43.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset 03/16/23 11:29:43.661
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:43.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:44.108
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 16 11:29:44.286: INFO: Creating ReplicaSet my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413
Mar 16 11:29:44.466: INFO: Pod name my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413: Found 1 pods out of 1
Mar 16 11:29:44.466: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413" is running
Mar 16 11:29:44.466: INFO: Waiting up to 5m0s for pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" in namespace "replicaset-4530" to be "running"
Mar 16 11:29:44.556: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm": Phase="Pending", Reason="", readiness=false. Elapsed: 89.64853ms
Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm": Phase="Running", Reason="", readiness=true. Elapsed: 2.180248212s
Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" satisfied condition "running"
Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason: Message:}])
Mar 16 11:29:46.647: INFO: Trying to dial the pod
Mar 16 11:29:52.018: INFO: Controller my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413: Got expected result from replica 1 [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm]: "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4530" for this suite. 03/16/23 11:29:52.197
------------------------------
• [8.627 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:43.66
    Mar 16 11:29:43.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename replicaset 03/16/23 11:29:43.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:43.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:44.108
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 16 11:29:44.286: INFO: Creating ReplicaSet my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413
    Mar 16 11:29:44.466: INFO: Pod name my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413: Found 1 pods out of 1
    Mar 16 11:29:44.466: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413" is running
    Mar 16 11:29:44.466: INFO: Waiting up to 5m0s for pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" in namespace "replicaset-4530" to be "running"
    Mar 16 11:29:44.556: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm": Phase="Pending", Reason="", readiness=false. Elapsed: 89.64853ms
    Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm": Phase="Running", Reason="", readiness=true. Elapsed: 2.180248212s
    Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" satisfied condition "running"
    Mar 16 11:29:46.647: INFO: Pod "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-16 11:29:44 +0000 UTC Reason: Message:}])
    Mar 16 11:29:46.647: INFO: Trying to dial the pod
    Mar 16 11:29:52.018: INFO: Controller my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413: Got expected result from replica 1 [my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm]: "my-hostname-basic-70a10ac6-5460-4d94-9657-a7f78b2a3413-45cbm", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4530" for this suite. 03/16/23 11:29:52.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:52.288
Mar 16 11:29:52.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:29:52.289
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:52.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:52.736
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 03/16/23 11:29:52.914
Mar 16 11:29:53.009: INFO: Waiting up to 5m0s for pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a" in namespace "emptydir-6611" to be "Succeeded or Failed"
Mar 16 11:29:53.098: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.48982ms
Mar 16 11:29:55.188: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179336501s
Mar 16 11:29:57.189: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180511303s
STEP: Saw pod success 03/16/23 11:29:57.189
Mar 16 11:29:57.189: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a" satisfied condition "Succeeded or Failed"
Mar 16 11:29:57.279: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a container test-container: <nil>
STEP: delete the pod 03/16/23 11:29:57.414
Mar 16 11:29:57.530: INFO: Waiting for pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a to disappear
Mar 16 11:29:57.620: INFO: Pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:57.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6611" for this suite. 03/16/23 11:29:57.798
------------------------------
• [5.600 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:52.288
    Mar 16 11:29:52.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:29:52.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:52.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:52.736
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/16/23 11:29:52.914
    Mar 16 11:29:53.009: INFO: Waiting up to 5m0s for pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a" in namespace "emptydir-6611" to be "Succeeded or Failed"
    Mar 16 11:29:53.098: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.48982ms
    Mar 16 11:29:55.188: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179336501s
    Mar 16 11:29:57.189: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180511303s
    STEP: Saw pod success 03/16/23 11:29:57.189
    Mar 16 11:29:57.189: INFO: Pod "pod-a0590e8c-737a-41a0-9eb1-b1286afde36a" satisfied condition "Succeeded or Failed"
    Mar 16 11:29:57.279: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a container test-container: <nil>
    STEP: delete the pod 03/16/23 11:29:57.414
    Mar 16 11:29:57.530: INFO: Waiting for pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a to disappear
    Mar 16 11:29:57.620: INFO: Pod pod-a0590e8c-737a-41a0-9eb1-b1286afde36a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:57.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6611" for this suite. 03/16/23 11:29:57.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:57.889
Mar 16 11:29:57.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 11:29:57.891
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:58.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:58.339
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-248a725f-394d-45f4-ba38-845e78bf2b2f 03/16/23 11:29:58.517
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:58.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6684" for this suite. 03/16/23 11:29:58.697
------------------------------
• [0.898 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:57.889
    Mar 16 11:29:57.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 11:29:57.891
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:58.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:58.339
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-248a725f-394d-45f4-ba38-845e78bf2b2f 03/16/23 11:29:58.517
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:58.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6684" for this suite. 03/16/23 11:29:58.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:58.79
Mar 16 11:29:58.790: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces 03/16/23 11:29:58.792
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:59.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:59.239
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6568" 03/16/23 11:29:59.417
Mar 16 11:29:59.597: INFO: Namespace "namespaces-6568" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e8c1f544-07ec-46eb-9446-bdafdb3fef6d", "kubernetes.io/metadata.name":"namespaces-6568", "namespaces-6568":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:29:59.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6568" for this suite. 03/16/23 11:29:59.688
------------------------------
• [0.989 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:58.79
    Mar 16 11:29:58.790: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename namespaces 03/16/23 11:29:58.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:29:59.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:29:59.239
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6568" 03/16/23 11:29:59.417
    Mar 16 11:29:59.597: INFO: Namespace "namespaces-6568" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e8c1f544-07ec-46eb-9446-bdafdb3fef6d", "kubernetes.io/metadata.name":"namespaces-6568", "namespaces-6568":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:29:59.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6568" for this suite. 03/16/23 11:29:59.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:29:59.779
Mar 16 11:29:59.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption 03/16/23 11:29:59.78
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:30:00.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:30:00.228
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 16 11:30:00.679: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 16 11:31:01.504: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 03/16/23 11:31:01.594
Mar 16 11:31:01.789: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 16 11:31:01.883: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 16 11:31:02.077: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 16 11:31:02.171: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/16/23 11:31:02.171
Mar 16 11:31:02.171: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3193" to be "running"
Mar 16 11:31:02.261: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765404ms
Mar 16 11:31:04.351: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.179838444s
Mar 16 11:31:04.351: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 16 11:31:04.351: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
Mar 16 11:31:04.441: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.916087ms
Mar 16 11:31:04.441: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 16 11:31:04.441: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
Mar 16 11:31:04.531: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.03417ms
Mar 16 11:31:04.531: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 16 11:31:04.531: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
Mar 16 11:31:04.621: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.844471ms
Mar 16 11:31:04.621: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/16/23 11:31:04.621
Mar 16 11:31:04.716: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3193" to be "running"
Mar 16 11:31:04.807: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 90.418238ms
Mar 16 11:31:06.897: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180916181s
Mar 16 11:31:08.898: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181418871s
Mar 16 11:31:10.898: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.182198996s
Mar 16 11:31:10.898: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:31:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3193" for this suite. 03/16/23 11:31:11.993
------------------------------
• [72.304 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:29:59.779
    Mar 16 11:29:59.779: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename sched-preemption 03/16/23 11:29:59.78
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:30:00.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:30:00.228
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 16 11:30:00.679: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 16 11:31:01.504: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 03/16/23 11:31:01.594
    Mar 16 11:31:01.789: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 16 11:31:01.883: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 16 11:31:02.077: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 16 11:31:02.171: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/16/23 11:31:02.171
    Mar 16 11:31:02.171: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3193" to be "running"
    Mar 16 11:31:02.261: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 89.765404ms
    Mar 16 11:31:04.351: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.179838444s
    Mar 16 11:31:04.351: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 16 11:31:04.351: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
    Mar 16 11:31:04.441: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.916087ms
    Mar 16 11:31:04.441: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 16 11:31:04.441: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
    Mar 16 11:31:04.531: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 90.03417ms
    Mar 16 11:31:04.531: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 16 11:31:04.531: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3193" to be "running"
    Mar 16 11:31:04.621: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.844471ms
    Mar 16 11:31:04.621: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/16/23 11:31:04.621
    Mar 16 11:31:04.716: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3193" to be "running"
    Mar 16 11:31:04.807: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 90.418238ms
    Mar 16 11:31:06.897: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180916181s
    Mar 16 11:31:08.898: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181418871s
    Mar 16 11:31:10.898: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.182198996s
    Mar 16 11:31:10.898: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:31:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3193" for this suite. 03/16/23 11:31:11.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:31:12.084
Mar 16 11:31:12.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:31:12.085
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:31:12.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:31:12.533
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-5c5288d2-8bf2-47af-81b4-4cccb7c30835 03/16/23 11:31:12.803
STEP: Creating configMap with name cm-test-opt-upd-e7c7e70c-626e-42a5-9320-bec835384881 03/16/23 11:31:12.893
STEP: Creating the pod 03/16/23 11:31:12.983
Mar 16 11:31:13.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b" in namespace "projected-2371" to be "running and ready"
Mar 16 11:31:13.168: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824953ms
Mar 16 11:31:13.169: INFO: The phase of Pod pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:31:15.259: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b": Phase="Running", Reason="", readiness=true. Elapsed: 2.179982057s
Mar 16 11:31:15.259: INFO: The phase of Pod pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b is Running (Ready = true)
Mar 16 11:31:15.259: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-5c5288d2-8bf2-47af-81b4-4cccb7c30835 03/16/23 11:31:15.634
STEP: Updating configmap cm-test-opt-upd-e7c7e70c-626e-42a5-9320-bec835384881 03/16/23 11:31:15.724
STEP: Creating configMap with name cm-test-opt-create-5d1776ea-1bd0-472f-936e-c3485acc51a3 03/16/23 11:31:15.814
STEP: waiting to observe update in volume 03/16/23 11:31:15.904
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:32:42.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2371" for this suite. 03/16/23 11:32:42.373
------------------------------
• [90.379 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:31:12.084
    Mar 16 11:31:12.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:31:12.085
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:31:12.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:31:12.533
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-5c5288d2-8bf2-47af-81b4-4cccb7c30835 03/16/23 11:31:12.803
    STEP: Creating configMap with name cm-test-opt-upd-e7c7e70c-626e-42a5-9320-bec835384881 03/16/23 11:31:12.893
    STEP: Creating the pod 03/16/23 11:31:12.983
    Mar 16 11:31:13.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b" in namespace "projected-2371" to be "running and ready"
    Mar 16 11:31:13.168: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.824953ms
    Mar 16 11:31:13.169: INFO: The phase of Pod pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:31:15.259: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b": Phase="Running", Reason="", readiness=true. Elapsed: 2.179982057s
    Mar 16 11:31:15.259: INFO: The phase of Pod pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b is Running (Ready = true)
    Mar 16 11:31:15.259: INFO: Pod "pod-projected-configmaps-c0c92b84-2cd8-4a66-bb19-0e00ea51323b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-5c5288d2-8bf2-47af-81b4-4cccb7c30835 03/16/23 11:31:15.634
    STEP: Updating configmap cm-test-opt-upd-e7c7e70c-626e-42a5-9320-bec835384881 03/16/23 11:31:15.724
    STEP: Creating configMap with name cm-test-opt-create-5d1776ea-1bd0-472f-936e-c3485acc51a3 03/16/23 11:31:15.814
    STEP: waiting to observe update in volume 03/16/23 11:31:15.904
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:32:42.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2371" for this suite. 03/16/23 11:32:42.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:32:42.465
Mar 16 11:32:42.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events 03/16/23 11:32:42.466
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:42.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:42.915
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/16/23 11:32:43.093
Mar 16 11:32:43.185: INFO: created test-event-1
Mar 16 11:32:43.275: INFO: created test-event-2
Mar 16 11:32:43.365: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/16/23 11:32:43.365
STEP: delete collection of events 03/16/23 11:32:43.455
Mar 16 11:32:43.455: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/16/23 11:32:43.55
Mar 16 11:32:43.550: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 16 11:32:43.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6519" for this suite. 03/16/23 11:32:43.731
------------------------------
• [1.356 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:32:42.465
    Mar 16 11:32:42.465: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename events 03/16/23 11:32:42.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:42.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:42.915
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/16/23 11:32:43.093
    Mar 16 11:32:43.185: INFO: created test-event-1
    Mar 16 11:32:43.275: INFO: created test-event-2
    Mar 16 11:32:43.365: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/16/23 11:32:43.365
    STEP: delete collection of events 03/16/23 11:32:43.455
    Mar 16 11:32:43.455: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/16/23 11:32:43.55
    Mar 16 11:32:43.550: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:32:43.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6519" for this suite. 03/16/23 11:32:43.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:32:43.822
Mar 16 11:32:43.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services 03/16/23 11:32:43.823
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:44.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:44.271
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1229 03/16/23 11:32:44.449
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[] 03/16/23 11:32:44.543
Mar 16 11:32:44.812: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1229 03/16/23 11:32:44.812
Mar 16 11:32:44.909: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1229" to be "running and ready"
Mar 16 11:32:44.999: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.734127ms
Mar 16 11:32:44.999: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:32:47.090: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.18058712s
Mar 16 11:32:47.090: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 16 11:32:47.090: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100]] 03/16/23 11:32:47.18
Mar 16 11:32:47.538: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1229 03/16/23 11:32:47.538
Mar 16 11:32:47.631: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1229" to be "running and ready"
Mar 16 11:32:47.723: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 92.0253ms
Mar 16 11:32:47.723: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:32:49.814: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.182635373s
Mar 16 11:32:49.814: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 16 11:32:49.814: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100] pod2:[101]] 03/16/23 11:32:49.904
Mar 16 11:32:50.351: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/16/23 11:32:50.352
Mar 16 11:32:50.352: INFO: Creating new exec pod
Mar 16 11:32:50.445: INFO: Waiting up to 5m0s for pod "execpodrv9zj" in namespace "services-1229" to be "running"
Mar 16 11:32:50.535: INFO: Pod "execpodrv9zj": Phase="Pending", Reason="", readiness=false. Elapsed: 89.756373ms
Mar 16 11:32:52.625: INFO: Pod "execpodrv9zj": Phase="Running", Reason="", readiness=true. Elapsed: 2.180182524s
Mar 16 11:32:52.625: INFO: Pod "execpodrv9zj" satisfied condition "running"
Mar 16 11:32:53.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Mar 16 11:32:54.723: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 16 11:32:54.723: INFO: stdout: ""
Mar 16 11:32:54.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 100.108.98.55 80'
Mar 16 11:32:55.917: INFO: stderr: "+ nc -v -z -w 2 100.108.98.55 80\nConnection to 100.108.98.55 80 port [tcp/http] succeeded!\n"
Mar 16 11:32:55.917: INFO: stdout: ""
Mar 16 11:32:55.918: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Mar 16 11:32:57.010: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 16 11:32:57.010: INFO: stdout: ""
Mar 16 11:32:57.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 100.108.98.55 81'
Mar 16 11:32:58.139: INFO: stderr: "+ nc -v -z -w 2 100.108.98.55 81\nConnection to 100.108.98.55 81 port [tcp/*] succeeded!\n"
Mar 16 11:32:58.139: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1229 03/16/23 11:32:58.139
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod2:[101]] 03/16/23 11:32:58.232
Mar 16 11:32:58.590: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1229 03/16/23 11:32:58.59
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[] 03/16/23 11:32:58.683
Mar 16 11:32:58.951: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 16 11:32:59.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1229" for this suite. 03/16/23 11:32:59.224
------------------------------
• [15.492 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:32:43.822
    Mar 16 11:32:43.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename services 03/16/23 11:32:43.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:44.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:44.271
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1229 03/16/23 11:32:44.449
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[] 03/16/23 11:32:44.543
    Mar 16 11:32:44.812: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1229 03/16/23 11:32:44.812
    Mar 16 11:32:44.909: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1229" to be "running and ready"
    Mar 16 11:32:44.999: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.734127ms
    Mar 16 11:32:44.999: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:32:47.090: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.18058712s
    Mar 16 11:32:47.090: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 16 11:32:47.090: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100]] 03/16/23 11:32:47.18
    Mar 16 11:32:47.538: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1229 03/16/23 11:32:47.538
    Mar 16 11:32:47.631: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1229" to be "running and ready"
    Mar 16 11:32:47.723: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 92.0253ms
    Mar 16 11:32:47.723: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:32:49.814: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.182635373s
    Mar 16 11:32:49.814: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 16 11:32:49.814: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod1:[100] pod2:[101]] 03/16/23 11:32:49.904
    Mar 16 11:32:50.351: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/16/23 11:32:50.352
    Mar 16 11:32:50.352: INFO: Creating new exec pod
    Mar 16 11:32:50.445: INFO: Waiting up to 5m0s for pod "execpodrv9zj" in namespace "services-1229" to be "running"
    Mar 16 11:32:50.535: INFO: Pod "execpodrv9zj": Phase="Pending", Reason="", readiness=false. Elapsed: 89.756373ms
    Mar 16 11:32:52.625: INFO: Pod "execpodrv9zj": Phase="Running", Reason="", readiness=true. Elapsed: 2.180182524s
    Mar 16 11:32:52.625: INFO: Pod "execpodrv9zj" satisfied condition "running"
    Mar 16 11:32:53.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Mar 16 11:32:54.723: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 16 11:32:54.723: INFO: stdout: ""
    Mar 16 11:32:54.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 100.108.98.55 80'
    Mar 16 11:32:55.917: INFO: stderr: "+ nc -v -z -w 2 100.108.98.55 80\nConnection to 100.108.98.55 80 port [tcp/http] succeeded!\n"
    Mar 16 11:32:55.917: INFO: stdout: ""
    Mar 16 11:32:55.918: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Mar 16 11:32:57.010: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 16 11:32:57.010: INFO: stdout: ""
    Mar 16 11:32:57.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1229 exec execpodrv9zj -- /bin/sh -x -c nc -v -z -w 2 100.108.98.55 81'
    Mar 16 11:32:58.139: INFO: stderr: "+ nc -v -z -w 2 100.108.98.55 81\nConnection to 100.108.98.55 81 port [tcp/*] succeeded!\n"
    Mar 16 11:32:58.139: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1229 03/16/23 11:32:58.139
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[pod2:[101]] 03/16/23 11:32:58.232
    Mar 16 11:32:58.590: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1229 03/16/23 11:32:58.59
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1229 to expose endpoints map[] 03/16/23 11:32:58.683
    Mar 16 11:32:58.951: INFO: successfully validated that service multi-endpoint-test in namespace services-1229 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:32:59.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1229" for this suite. 03/16/23 11:32:59.224
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:32:59.314
Mar 16 11:32:59.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets 03/16/23 11:32:59.316
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:59.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:59.765
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Mar 16 11:33:00.394: INFO: Create a RollingUpdate DaemonSet
Mar 16 11:33:00.484: INFO: Check that daemon pods launch on every node of the cluster
Mar 16 11:33:00.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 11:33:00.665: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
Mar 16 11:33:01.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 16 11:33:01.934: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Mar 16 11:33:01.934: INFO: Update the DaemonSet to trigger a rollout
Mar 16 11:33:02.114: INFO: Updating DaemonSet daemon-set
Mar 16 11:33:05.567: INFO: Roll back the DaemonSet before rollout is complete
Mar 16 11:33:05.748: INFO: Updating DaemonSet daemon-set
Mar 16 11:33:05.748: INFO: Make sure DaemonSet rollback is complete
Mar 16 11:33:09.019: INFO: Pod daemon-set-r6xwh is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/16/23 11:33:09.377
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7673, will wait for the garbage collector to delete the pods 03/16/23 11:33:09.377
Mar 16 11:33:09.658: INFO: Deleting DaemonSet.extensions daemon-set took: 90.527716ms
Mar 16 11:33:09.758: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.222111ms
Mar 16 11:33:12.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 16 11:33:12.749: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 16 11:33:12.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59147"},"items":null}

Mar 16 11:33:12.928: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59147"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:13.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7673" for this suite. 03/16/23 11:33:13.377
------------------------------
• [14.153 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:32:59.314
    Mar 16 11:32:59.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename daemonsets 03/16/23 11:32:59.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:32:59.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:32:59.765
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Mar 16 11:33:00.394: INFO: Create a RollingUpdate DaemonSet
    Mar 16 11:33:00.484: INFO: Check that daemon pods launch on every node of the cluster
    Mar 16 11:33:00.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 11:33:00.665: INFO: Node ip-10-250-19-136.ec2.internal is running 0 daemon pod, expected 1
    Mar 16 11:33:01.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 16 11:33:01.934: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Mar 16 11:33:01.934: INFO: Update the DaemonSet to trigger a rollout
    Mar 16 11:33:02.114: INFO: Updating DaemonSet daemon-set
    Mar 16 11:33:05.567: INFO: Roll back the DaemonSet before rollout is complete
    Mar 16 11:33:05.748: INFO: Updating DaemonSet daemon-set
    Mar 16 11:33:05.748: INFO: Make sure DaemonSet rollback is complete
    Mar 16 11:33:09.019: INFO: Pod daemon-set-r6xwh is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/16/23 11:33:09.377
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7673, will wait for the garbage collector to delete the pods 03/16/23 11:33:09.377
    Mar 16 11:33:09.658: INFO: Deleting DaemonSet.extensions daemon-set took: 90.527716ms
    Mar 16 11:33:09.758: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.222111ms
    Mar 16 11:33:12.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 16 11:33:12.749: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 16 11:33:12.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59147"},"items":null}

    Mar 16 11:33:12.928: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59147"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:13.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7673" for this suite. 03/16/23 11:33:13.377
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:13.468
Mar 16 11:33:13.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:33:13.469
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:13.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:13.916
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:33:14.275
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:33:14.721
STEP: Deploying the webhook pod 03/16/23 11:33:14.812
STEP: Wait for the deployment to be ready 03/16/23 11:33:14.993
Mar 16 11:33:15.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:33:17.354
STEP: Verifying the service has paired with the endpoint 03/16/23 11:33:17.449
Mar 16 11:33:18.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Mar 16 11:33:18.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5963-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 11:33:18.809
STEP: Creating a custom resource while v1 is storage version 03/16/23 11:33:19.103
STEP: Patching Custom Resource Definition to set v2 as storage 03/16/23 11:33:21.397
STEP: Patching the custom resource while v2 is storage version 03/16/23 11:33:21.498
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:22.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6532" for this suite. 03/16/23 11:33:22.576
STEP: Destroying namespace "webhook-6532-markers" for this suite. 03/16/23 11:33:22.666
------------------------------
• [9.289 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:13.468
    Mar 16 11:33:13.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:33:13.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:13.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:13.916
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:33:14.275
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:33:14.721
    STEP: Deploying the webhook pod 03/16/23 11:33:14.812
    STEP: Wait for the deployment to be ready 03/16/23 11:33:14.993
    Mar 16 11:33:15.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 33, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:33:17.354
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:33:17.449
    Mar 16 11:33:18.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Mar 16 11:33:18.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5963-crds.webhook.example.com via the AdmissionRegistration API 03/16/23 11:33:18.809
    STEP: Creating a custom resource while v1 is storage version 03/16/23 11:33:19.103
    STEP: Patching Custom Resource Definition to set v2 as storage 03/16/23 11:33:21.397
    STEP: Patching the custom resource while v2 is storage version 03/16/23 11:33:21.498
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:22.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6532" for this suite. 03/16/23 11:33:22.576
    STEP: Destroying namespace "webhook-6532-markers" for this suite. 03/16/23 11:33:22.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:22.758
Mar 16 11:33:22.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 11:33:22.759
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:23.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:23.207
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-fdf4a268-8125-4c07-866e-76c395a2b15b 03/16/23 11:33:23.384
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5406" for this suite. 03/16/23 11:33:23.564
------------------------------
• [0.897 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:22.758
    Mar 16 11:33:22.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 11:33:22.759
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:23.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:23.207
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-fdf4a268-8125-4c07-866e-76c395a2b15b 03/16/23 11:33:23.384
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5406" for this suite. 03/16/23 11:33:23.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:23.656
Mar 16 11:33:23.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc 03/16/23 11:33:23.658
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:23.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:24.105
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/16/23 11:33:24.373
STEP: delete the rc 03/16/23 11:33:29.554
STEP: wait for the rc to be deleted 03/16/23 11:33:29.645
Mar 16 11:33:30.920: INFO: 84 pods remaining
Mar 16 11:33:30.921: INFO: 84 pods has nil DeletionTimestamp
Mar 16 11:33:30.921: INFO: 
Mar 16 11:33:31.919: INFO: 70 pods remaining
Mar 16 11:33:31.919: INFO: 70 pods has nil DeletionTimestamp
Mar 16 11:33:31.919: INFO: 
Mar 16 11:33:32.917: INFO: 64 pods remaining
Mar 16 11:33:32.917: INFO: 64 pods has nil DeletionTimestamp
Mar 16 11:33:32.917: INFO: 
Mar 16 11:33:34.091: INFO: 40 pods remaining
Mar 16 11:33:34.091: INFO: 40 pods has nil DeletionTimestamp
Mar 16 11:33:34.091: INFO: 
Mar 16 11:33:35.003: INFO: 40 pods remaining
Mar 16 11:33:35.003: INFO: 40 pods has nil DeletionTimestamp
Mar 16 11:33:35.003: INFO: 
Mar 16 11:33:35.825: INFO: 10 pods remaining
Mar 16 11:33:35.825: INFO: 10 pods has nil DeletionTimestamp
Mar 16 11:33:35.825: INFO: 
STEP: Gathering metrics 03/16/23 11:33:36.825
W0316 11:33:37.010625    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 16 11:33:37.010: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:37.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1747" for this suite. 03/16/23 11:33:37.101
------------------------------
• [13.535 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:23.656
    Mar 16 11:33:23.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename gc 03/16/23 11:33:23.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:23.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:24.105
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/16/23 11:33:24.373
    STEP: delete the rc 03/16/23 11:33:29.554
    STEP: wait for the rc to be deleted 03/16/23 11:33:29.645
    Mar 16 11:33:30.920: INFO: 84 pods remaining
    Mar 16 11:33:30.921: INFO: 84 pods has nil DeletionTimestamp
    Mar 16 11:33:30.921: INFO: 
    Mar 16 11:33:31.919: INFO: 70 pods remaining
    Mar 16 11:33:31.919: INFO: 70 pods has nil DeletionTimestamp
    Mar 16 11:33:31.919: INFO: 
    Mar 16 11:33:32.917: INFO: 64 pods remaining
    Mar 16 11:33:32.917: INFO: 64 pods has nil DeletionTimestamp
    Mar 16 11:33:32.917: INFO: 
    Mar 16 11:33:34.091: INFO: 40 pods remaining
    Mar 16 11:33:34.091: INFO: 40 pods has nil DeletionTimestamp
    Mar 16 11:33:34.091: INFO: 
    Mar 16 11:33:35.003: INFO: 40 pods remaining
    Mar 16 11:33:35.003: INFO: 40 pods has nil DeletionTimestamp
    Mar 16 11:33:35.003: INFO: 
    Mar 16 11:33:35.825: INFO: 10 pods remaining
    Mar 16 11:33:35.825: INFO: 10 pods has nil DeletionTimestamp
    Mar 16 11:33:35.825: INFO: 
    STEP: Gathering metrics 03/16/23 11:33:36.825
    W0316 11:33:37.010625    8588 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 16 11:33:37.010: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:37.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1747" for this suite. 03/16/23 11:33:37.101
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:37.192
Mar 16 11:33:37.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:37.193
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:37.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:37.64
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/16/23 11:33:37.818
Mar 16 11:33:37.913: INFO: Waiting up to 5m0s for pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6" in namespace "emptydir-9966" to be "Succeeded or Failed"
Mar 16 11:33:38.003: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.488085ms
Mar 16 11:33:40.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17996912s
Mar 16 11:33:42.095: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181516569s
Mar 16 11:33:44.094: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180529809s
Mar 16 11:33:46.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.180200785s
STEP: Saw pod success 03/16/23 11:33:46.093
Mar 16 11:33:46.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6" satisfied condition "Succeeded or Failed"
Mar 16 11:33:46.183: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 container test-container: <nil>
STEP: delete the pod 03/16/23 11:33:46.322
Mar 16 11:33:46.415: INFO: Waiting for pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 to disappear
Mar 16 11:33:46.504: INFO: Pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:46.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9966" for this suite. 03/16/23 11:33:46.683
------------------------------
• [9.582 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:37.192
    Mar 16 11:33:37.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:37.193
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:37.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:37.64
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/16/23 11:33:37.818
    Mar 16 11:33:37.913: INFO: Waiting up to 5m0s for pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6" in namespace "emptydir-9966" to be "Succeeded or Failed"
    Mar 16 11:33:38.003: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.488085ms
    Mar 16 11:33:40.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17996912s
    Mar 16 11:33:42.095: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181516569s
    Mar 16 11:33:44.094: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180529809s
    Mar 16 11:33:46.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.180200785s
    STEP: Saw pod success 03/16/23 11:33:46.093
    Mar 16 11:33:46.093: INFO: Pod "pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6" satisfied condition "Succeeded or Failed"
    Mar 16 11:33:46.183: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 container test-container: <nil>
    STEP: delete the pod 03/16/23 11:33:46.322
    Mar 16 11:33:46.415: INFO: Waiting for pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 to disappear
    Mar 16 11:33:46.504: INFO: Pod pod-f4f58978-ff60-41b2-8e59-f70b7433f2f6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:46.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9966" for this suite. 03/16/23 11:33:46.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:46.775
Mar 16 11:33:46.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:46.777
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:47.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:47.224
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/16/23 11:33:47.402
Mar 16 11:33:47.496: INFO: Waiting up to 5m0s for pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac" in namespace "emptydir-8767" to be "Succeeded or Failed"
Mar 16 11:33:47.586: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 89.50481ms
Mar 16 11:33:49.677: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180463598s
Mar 16 11:33:51.678: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181230905s
STEP: Saw pod success 03/16/23 11:33:51.678
Mar 16 11:33:51.678: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac" satisfied condition "Succeeded or Failed"
Mar 16 11:33:51.767: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-b369c684-6e57-4c06-a39d-b185565f4dac container test-container: <nil>
STEP: delete the pod 03/16/23 11:33:51.938
Mar 16 11:33:52.031: INFO: Waiting for pod pod-b369c684-6e57-4c06-a39d-b185565f4dac to disappear
Mar 16 11:33:52.121: INFO: Pod pod-b369c684-6e57-4c06-a39d-b185565f4dac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:52.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8767" for this suite. 03/16/23 11:33:52.299
------------------------------
• [5.614 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:46.775
    Mar 16 11:33:46.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:46.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:47.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:47.224
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/16/23 11:33:47.402
    Mar 16 11:33:47.496: INFO: Waiting up to 5m0s for pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac" in namespace "emptydir-8767" to be "Succeeded or Failed"
    Mar 16 11:33:47.586: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 89.50481ms
    Mar 16 11:33:49.677: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180463598s
    Mar 16 11:33:51.678: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181230905s
    STEP: Saw pod success 03/16/23 11:33:51.678
    Mar 16 11:33:51.678: INFO: Pod "pod-b369c684-6e57-4c06-a39d-b185565f4dac" satisfied condition "Succeeded or Failed"
    Mar 16 11:33:51.767: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-b369c684-6e57-4c06-a39d-b185565f4dac container test-container: <nil>
    STEP: delete the pod 03/16/23 11:33:51.938
    Mar 16 11:33:52.031: INFO: Waiting for pod pod-b369c684-6e57-4c06-a39d-b185565f4dac to disappear
    Mar 16 11:33:52.121: INFO: Pod pod-b369c684-6e57-4c06-a39d-b185565f4dac no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:52.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8767" for this suite. 03/16/23 11:33:52.299
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:52.39
Mar 16 11:33:52.390: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:52.392
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:52.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:52.844
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 03/16/23 11:33:53.024
Mar 16 11:33:53.119: INFO: Waiting up to 5m0s for pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a" in namespace "emptydir-9269" to be "Succeeded or Failed"
Mar 16 11:33:53.209: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.491601ms
Mar 16 11:33:55.299: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Running", Reason="", readiness=false. Elapsed: 2.180207698s
Mar 16 11:33:57.300: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180811499s
STEP: Saw pod success 03/16/23 11:33:57.3
Mar 16 11:33:57.300: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a" satisfied condition "Succeeded or Failed"
Mar 16 11:33:57.390: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a container test-container: <nil>
STEP: delete the pod 03/16/23 11:33:57.53
Mar 16 11:33:57.624: INFO: Waiting for pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a to disappear
Mar 16 11:33:57.713: INFO: Pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 16 11:33:57.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9269" for this suite. 03/16/23 11:33:57.892
------------------------------
• [5.592 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:52.39
    Mar 16 11:33:52.390: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename emptydir 03/16/23 11:33:52.392
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:52.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:52.844
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/16/23 11:33:53.024
    Mar 16 11:33:53.119: INFO: Waiting up to 5m0s for pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a" in namespace "emptydir-9269" to be "Succeeded or Failed"
    Mar 16 11:33:53.209: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.491601ms
    Mar 16 11:33:55.299: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Running", Reason="", readiness=false. Elapsed: 2.180207698s
    Mar 16 11:33:57.300: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180811499s
    STEP: Saw pod success 03/16/23 11:33:57.3
    Mar 16 11:33:57.300: INFO: Pod "pod-5faf719f-78a2-4fa9-9533-5627285bf89a" satisfied condition "Succeeded or Failed"
    Mar 16 11:33:57.390: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a container test-container: <nil>
    STEP: delete the pod 03/16/23 11:33:57.53
    Mar 16 11:33:57.624: INFO: Waiting for pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a to disappear
    Mar 16 11:33:57.713: INFO: Pod pod-5faf719f-78a2-4fa9-9533-5627285bf89a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:33:57.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9269" for this suite. 03/16/23 11:33:57.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:33:57.983
Mar 16 11:33:57.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods 03/16/23 11:33:57.984
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:58.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:58.431
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 03/16/23 11:33:58.609
STEP: submitting the pod to kubernetes 03/16/23 11:33:58.61
Mar 16 11:33:58.704: INFO: Waiting up to 5m0s for pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" in namespace "pods-2857" to be "running and ready"
Mar 16 11:33:58.794: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Pending", Reason="", readiness=false. Elapsed: 89.939335ms
Mar 16 11:33:58.794: INFO: The phase of Pod pod-update-fe62c9a5-f919-415e-ad4e-f992582469df is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:34:00.885: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Running", Reason="", readiness=true. Elapsed: 2.180965002s
Mar 16 11:34:00.885: INFO: The phase of Pod pod-update-fe62c9a5-f919-415e-ad4e-f992582469df is Running (Ready = true)
Mar 16 11:34:00.885: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/16/23 11:34:00.975
STEP: updating the pod 03/16/23 11:34:01.065
Mar 16 11:34:01.748: INFO: Successfully updated pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df"
Mar 16 11:34:01.748: INFO: Waiting up to 5m0s for pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" in namespace "pods-2857" to be "running"
Mar 16 11:34:01.838: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Running", Reason="", readiness=true. Elapsed: 89.856968ms
Mar 16 11:34:01.838: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/16/23 11:34:01.838
Mar 16 11:34:01.928: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 16 11:34:01.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2857" for this suite. 03/16/23 11:34:02.107
------------------------------
• [4.215 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:33:57.983
    Mar 16 11:33:57.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename pods 03/16/23 11:33:57.984
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:33:58.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:33:58.431
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 03/16/23 11:33:58.609
    STEP: submitting the pod to kubernetes 03/16/23 11:33:58.61
    Mar 16 11:33:58.704: INFO: Waiting up to 5m0s for pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" in namespace "pods-2857" to be "running and ready"
    Mar 16 11:33:58.794: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Pending", Reason="", readiness=false. Elapsed: 89.939335ms
    Mar 16 11:33:58.794: INFO: The phase of Pod pod-update-fe62c9a5-f919-415e-ad4e-f992582469df is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:34:00.885: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Running", Reason="", readiness=true. Elapsed: 2.180965002s
    Mar 16 11:34:00.885: INFO: The phase of Pod pod-update-fe62c9a5-f919-415e-ad4e-f992582469df is Running (Ready = true)
    Mar 16 11:34:00.885: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/16/23 11:34:00.975
    STEP: updating the pod 03/16/23 11:34:01.065
    Mar 16 11:34:01.748: INFO: Successfully updated pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df"
    Mar 16 11:34:01.748: INFO: Waiting up to 5m0s for pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" in namespace "pods-2857" to be "running"
    Mar 16 11:34:01.838: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df": Phase="Running", Reason="", readiness=true. Elapsed: 89.856968ms
    Mar 16 11:34:01.838: INFO: Pod "pod-update-fe62c9a5-f919-415e-ad4e-f992582469df" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/16/23 11:34:01.838
    Mar 16 11:34:01.928: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:34:01.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2857" for this suite. 03/16/23 11:34:02.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:34:02.198
Mar 16 11:34:02.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container 03/16/23 11:34:02.199
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:02.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:02.647
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 03/16/23 11:34:02.825
Mar 16 11:34:02.825: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:34:05.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8811" for this suite. 03/16/23 11:34:06.162
------------------------------
• [4.055 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:34:02.198
    Mar 16 11:34:02.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename init-container 03/16/23 11:34:02.199
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:02.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:02.647
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 03/16/23 11:34:02.825
    Mar 16 11:34:02.825: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:34:05.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8811" for this suite. 03/16/23 11:34:06.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:34:06.254
Mar 16 11:34:06.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 11:34:06.255
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:06.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:06.703
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 03/16/23 11:34:06.882
Mar 16 11:34:06.882: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 create -f -'
Mar 16 11:34:08.029: INFO: stderr: ""
Mar 16 11:34:08.029: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 11:34:08.029
Mar 16 11:34:08.029: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 11:34:08.479: INFO: stderr: ""
Mar 16 11:34:08.479: INFO: stdout: "update-demo-nautilus-czp7l update-demo-nautilus-ngxn5 "
Mar 16 11:34:08.479: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 11:34:08.822: INFO: stderr: ""
Mar 16 11:34:08.822: INFO: stdout: ""
Mar 16 11:34:08.822: INFO: update-demo-nautilus-czp7l is created but not running
Mar 16 11:34:13.822: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 16 11:34:14.253: INFO: stderr: ""
Mar 16 11:34:14.253: INFO: stdout: "update-demo-nautilus-czp7l update-demo-nautilus-ngxn5 "
Mar 16 11:34:14.253: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 11:34:14.601: INFO: stderr: ""
Mar 16 11:34:14.601: INFO: stdout: "true"
Mar 16 11:34:14.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 11:34:14.940: INFO: stderr: ""
Mar 16 11:34:14.940: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 11:34:14.940: INFO: validating pod update-demo-nautilus-czp7l
Mar 16 11:34:15.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 11:34:15.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 11:34:15.131: INFO: update-demo-nautilus-czp7l is verified up and running
Mar 16 11:34:15.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-ngxn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 16 11:34:15.476: INFO: stderr: ""
Mar 16 11:34:15.476: INFO: stdout: "true"
Mar 16 11:34:15.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-ngxn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 16 11:34:15.820: INFO: stderr: ""
Mar 16 11:34:15.820: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 16 11:34:15.820: INFO: validating pod update-demo-nautilus-ngxn5
Mar 16 11:34:16.002: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 16 11:34:16.003: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 16 11:34:16.003: INFO: update-demo-nautilus-ngxn5 is verified up and running
STEP: using delete to clean up resources 03/16/23 11:34:16.003
Mar 16 11:34:16.003: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 delete --grace-period=0 --force -f -'
Mar 16 11:34:16.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 16 11:34:16.439: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 16 11:34:16.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get rc,svc -l name=update-demo --no-headers'
Mar 16 11:34:16.879: INFO: stderr: "No resources found in kubectl-3696 namespace.\n"
Mar 16 11:34:16.879: INFO: stdout: ""
Mar 16 11:34:16.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 16 11:34:17.221: INFO: stderr: ""
Mar 16 11:34:17.221: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 11:34:17.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3696" for this suite. 03/16/23 11:34:17.399
------------------------------
• [11.236 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:34:06.254
    Mar 16 11:34:06.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 11:34:06.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:06.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:06.703
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 03/16/23 11:34:06.882
    Mar 16 11:34:06.882: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 create -f -'
    Mar 16 11:34:08.029: INFO: stderr: ""
    Mar 16 11:34:08.029: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/16/23 11:34:08.029
    Mar 16 11:34:08.029: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 11:34:08.479: INFO: stderr: ""
    Mar 16 11:34:08.479: INFO: stdout: "update-demo-nautilus-czp7l update-demo-nautilus-ngxn5 "
    Mar 16 11:34:08.479: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 11:34:08.822: INFO: stderr: ""
    Mar 16 11:34:08.822: INFO: stdout: ""
    Mar 16 11:34:08.822: INFO: update-demo-nautilus-czp7l is created but not running
    Mar 16 11:34:13.822: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 16 11:34:14.253: INFO: stderr: ""
    Mar 16 11:34:14.253: INFO: stdout: "update-demo-nautilus-czp7l update-demo-nautilus-ngxn5 "
    Mar 16 11:34:14.253: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 11:34:14.601: INFO: stderr: ""
    Mar 16 11:34:14.601: INFO: stdout: "true"
    Mar 16 11:34:14.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-czp7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 11:34:14.940: INFO: stderr: ""
    Mar 16 11:34:14.940: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 11:34:14.940: INFO: validating pod update-demo-nautilus-czp7l
    Mar 16 11:34:15.130: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 11:34:15.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 11:34:15.131: INFO: update-demo-nautilus-czp7l is verified up and running
    Mar 16 11:34:15.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-ngxn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 16 11:34:15.476: INFO: stderr: ""
    Mar 16 11:34:15.476: INFO: stdout: "true"
    Mar 16 11:34:15.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods update-demo-nautilus-ngxn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 16 11:34:15.820: INFO: stderr: ""
    Mar 16 11:34:15.820: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 16 11:34:15.820: INFO: validating pod update-demo-nautilus-ngxn5
    Mar 16 11:34:16.002: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 16 11:34:16.003: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 16 11:34:16.003: INFO: update-demo-nautilus-ngxn5 is verified up and running
    STEP: using delete to clean up resources 03/16/23 11:34:16.003
    Mar 16 11:34:16.003: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 delete --grace-period=0 --force -f -'
    Mar 16 11:34:16.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 16 11:34:16.439: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 16 11:34:16.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get rc,svc -l name=update-demo --no-headers'
    Mar 16 11:34:16.879: INFO: stderr: "No resources found in kubectl-3696 namespace.\n"
    Mar 16 11:34:16.879: INFO: stdout: ""
    Mar 16 11:34:16.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 16 11:34:17.221: INFO: stderr: ""
    Mar 16 11:34:17.221: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:34:17.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3696" for this suite. 03/16/23 11:34:17.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:34:17.491
Mar 16 11:34:17.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:34:17.492
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:17.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:17.94
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7cfafaa5-d911-4fda-a41d-811f96f45119 03/16/23 11:34:18.209
STEP: Creating the pod 03/16/23 11:34:18.299
Mar 16 11:34:18.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166" in namespace "projected-1029" to be "running and ready"
Mar 16 11:34:18.485: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166": Phase="Pending", Reason="", readiness=false. Elapsed: 89.939326ms
Mar 16 11:34:18.485: INFO: The phase of Pod pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:34:20.576: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166": Phase="Running", Reason="", readiness=true. Elapsed: 2.181019578s
Mar 16 11:34:20.576: INFO: The phase of Pod pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166 is Running (Ready = true)
Mar 16 11:34:20.576: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-7cfafaa5-d911-4fda-a41d-811f96f45119 03/16/23 11:34:20.76
STEP: waiting to observe update in volume 03/16/23 11:34:20.851
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:35:48.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1029" for this suite. 03/16/23 11:35:49.164
------------------------------
• [91.763 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:34:17.491
    Mar 16 11:34:17.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:34:17.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:34:17.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:34:17.94
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-7cfafaa5-d911-4fda-a41d-811f96f45119 03/16/23 11:34:18.209
    STEP: Creating the pod 03/16/23 11:34:18.299
    Mar 16 11:34:18.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166" in namespace "projected-1029" to be "running and ready"
    Mar 16 11:34:18.485: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166": Phase="Pending", Reason="", readiness=false. Elapsed: 89.939326ms
    Mar 16 11:34:18.485: INFO: The phase of Pod pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:34:20.576: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166": Phase="Running", Reason="", readiness=true. Elapsed: 2.181019578s
    Mar 16 11:34:20.576: INFO: The phase of Pod pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166 is Running (Ready = true)
    Mar 16 11:34:20.576: INFO: Pod "pod-projected-configmaps-9c58392e-b3c7-4667-82cc-044fcb678166" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-7cfafaa5-d911-4fda-a41d-811f96f45119 03/16/23 11:34:20.76
    STEP: waiting to observe update in volume 03/16/23 11:34:20.851
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:35:48.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1029" for this suite. 03/16/23 11:35:49.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:35:49.255
Mar 16 11:35:49.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl 03/16/23 11:35:49.256
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:35:49.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:35:49.703
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 11:35:49.881
Mar 16 11:35:49.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 16 11:35:50.242: INFO: stderr: ""
Mar 16 11:35:50.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/16/23 11:35:50.242
Mar 16 11:35:50.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Mar 16 11:35:50.915: INFO: stderr: ""
Mar 16 11:35:50.915: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 11:35:50.915
Mar 16 11:35:51.005: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 delete pods e2e-test-httpd-pod'
Mar 16 11:35:54.317: INFO: stderr: ""
Mar 16 11:35:54.317: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 16 11:35:54.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2071" for this suite. 03/16/23 11:35:54.494
------------------------------
• [5.330 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:35:49.255
    Mar 16 11:35:49.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename kubectl 03/16/23 11:35:49.256
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:35:49.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:35:49.703
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 11:35:49.881
    Mar 16 11:35:49.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 16 11:35:50.242: INFO: stderr: ""
    Mar 16 11:35:50.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/16/23 11:35:50.242
    Mar 16 11:35:50.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Mar 16 11:35:50.915: INFO: stderr: ""
    Mar 16 11:35:50.915: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/16/23 11:35:50.915
    Mar 16 11:35:51.005: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbqx-ip1.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2071 delete pods e2e-test-httpd-pod'
    Mar 16 11:35:54.317: INFO: stderr: ""
    Mar 16 11:35:54.317: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:35:54.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2071" for this suite. 03/16/23 11:35:54.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:35:54.587
Mar 16 11:35:54.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api 03/16/23 11:35:54.588
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:35:54.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:35:55.035
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 03/16/23 11:35:55.213
Mar 16 11:35:55.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe" in namespace "downward-api-594" to be "Succeeded or Failed"
Mar 16 11:35:55.398: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 89.996918ms
Mar 16 11:35:57.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180409584s
Mar 16 11:35:59.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181021808s
STEP: Saw pod success 03/16/23 11:35:59.489
Mar 16 11:35:59.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe" satisfied condition "Succeeded or Failed"
Mar 16 11:35:59.579: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe container client-container: <nil>
STEP: delete the pod 03/16/23 11:35:59.673
Mar 16 11:35:59.767: INFO: Waiting for pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe to disappear
Mar 16 11:35:59.856: INFO: Pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 16 11:35:59.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-594" for this suite. 03/16/23 11:36:00.035
------------------------------
• [5.539 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:35:54.587
    Mar 16 11:35:54.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename downward-api 03/16/23 11:35:54.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:35:54.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:35:55.035
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 03/16/23 11:35:55.213
    Mar 16 11:35:55.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe" in namespace "downward-api-594" to be "Succeeded or Failed"
    Mar 16 11:35:55.398: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 89.996918ms
    Mar 16 11:35:57.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180409584s
    Mar 16 11:35:59.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.181021808s
    STEP: Saw pod success 03/16/23 11:35:59.489
    Mar 16 11:35:59.489: INFO: Pod "downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe" satisfied condition "Succeeded or Failed"
    Mar 16 11:35:59.579: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe container client-container: <nil>
    STEP: delete the pod 03/16/23 11:35:59.673
    Mar 16 11:35:59.767: INFO: Waiting for pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe to disappear
    Mar 16 11:35:59.856: INFO: Pod downwardapi-volume-28bfcfc8-1390-42e6-a044-1b32b7342cfe no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:35:59.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-594" for this suite. 03/16/23 11:36:00.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:36:00.126
Mar 16 11:36:00.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets 03/16/23 11:36:00.127
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:00.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:00.574
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-d7984ee3-08f2-4739-8c5b-2191efc2fe12 03/16/23 11:36:00.759
STEP: Creating a pod to test consume secrets 03/16/23 11:36:00.85
Mar 16 11:36:00.944: INFO: Waiting up to 5m0s for pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820" in namespace "secrets-5436" to be "Succeeded or Failed"
Mar 16 11:36:01.034: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Pending", Reason="", readiness=false. Elapsed: 90.091111ms
Mar 16 11:36:03.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180674271s
Mar 16 11:36:05.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180774024s
STEP: Saw pod success 03/16/23 11:36:05.125
Mar 16 11:36:05.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820" satisfied condition "Succeeded or Failed"
Mar 16 11:36:05.215: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 container secret-volume-test: <nil>
STEP: delete the pod 03/16/23 11:36:05.309
Mar 16 11:36:05.404: INFO: Waiting for pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 to disappear
Mar 16 11:36:05.493: INFO: Pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 16 11:36:05.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5436" for this suite. 03/16/23 11:36:05.672
------------------------------
• [5.636 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:36:00.126
    Mar 16 11:36:00.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename secrets 03/16/23 11:36:00.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:00.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:00.574
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-d7984ee3-08f2-4739-8c5b-2191efc2fe12 03/16/23 11:36:00.759
    STEP: Creating a pod to test consume secrets 03/16/23 11:36:00.85
    Mar 16 11:36:00.944: INFO: Waiting up to 5m0s for pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820" in namespace "secrets-5436" to be "Succeeded or Failed"
    Mar 16 11:36:01.034: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Pending", Reason="", readiness=false. Elapsed: 90.091111ms
    Mar 16 11:36:03.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180674271s
    Mar 16 11:36:05.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180774024s
    STEP: Saw pod success 03/16/23 11:36:05.125
    Mar 16 11:36:05.125: INFO: Pod "pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820" satisfied condition "Succeeded or Failed"
    Mar 16 11:36:05.215: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 container secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 11:36:05.309
    Mar 16 11:36:05.404: INFO: Waiting for pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 to disappear
    Mar 16 11:36:05.493: INFO: Pod pod-secrets-188736af-f799-4eab-95d9-eaa0006ca820 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:36:05.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5436" for this suite. 03/16/23 11:36:05.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:36:05.763
Mar 16 11:36:05.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns 03/16/23 11:36:05.764
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:06.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:06.211
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/16/23 11:36:06.389
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/16/23 11:36:06.389
STEP: creating a pod to probe DNS 03/16/23 11:36:06.389
STEP: submitting the pod to kubernetes 03/16/23 11:36:06.389
Mar 16 11:36:06.485: INFO: Waiting up to 15m0s for pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661" in namespace "dns-2652" to be "running"
Mar 16 11:36:06.575: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789403ms
Mar 16 11:36:08.666: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661": Phase="Running", Reason="", readiness=true. Elapsed: 2.180807618s
Mar 16 11:36:08.666: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661" satisfied condition "running"
STEP: retrieving the pod 03/16/23 11:36:08.666
STEP: looking for the results for each expected name from probers 03/16/23 11:36:08.755
Mar 16 11:36:09.314: INFO: DNS probes using dns-2652/dns-test-546084a5-14c3-4050-be94-76c0364f8661 succeeded

STEP: deleting the pod 03/16/23 11:36:09.314
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 16 11:36:09.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2652" for this suite. 03/16/23 11:36:09.587
------------------------------
• [3.914 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:36:05.763
    Mar 16 11:36:05.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename dns 03/16/23 11:36:05.764
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:06.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:06.211
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/16/23 11:36:06.389
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/16/23 11:36:06.389
    STEP: creating a pod to probe DNS 03/16/23 11:36:06.389
    STEP: submitting the pod to kubernetes 03/16/23 11:36:06.389
    Mar 16 11:36:06.485: INFO: Waiting up to 15m0s for pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661" in namespace "dns-2652" to be "running"
    Mar 16 11:36:06.575: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789403ms
    Mar 16 11:36:08.666: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661": Phase="Running", Reason="", readiness=true. Elapsed: 2.180807618s
    Mar 16 11:36:08.666: INFO: Pod "dns-test-546084a5-14c3-4050-be94-76c0364f8661" satisfied condition "running"
    STEP: retrieving the pod 03/16/23 11:36:08.666
    STEP: looking for the results for each expected name from probers 03/16/23 11:36:08.755
    Mar 16 11:36:09.314: INFO: DNS probes using dns-2652/dns-test-546084a5-14c3-4050-be94-76c0364f8661 succeeded

    STEP: deleting the pod 03/16/23 11:36:09.314
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:36:09.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2652" for this suite. 03/16/23 11:36:09.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:36:09.678
Mar 16 11:36:09.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:36:09.679
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:09.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:10.127
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:36:10.486
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:36:11.011
STEP: Deploying the webhook pod 03/16/23 11:36:11.102
STEP: Wait for the deployment to be ready 03/16/23 11:36:11.283
Mar 16 11:36:11.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:36:13.643
STEP: Verifying the service has paired with the endpoint 03/16/23 11:36:13.738
Mar 16 11:36:14.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 03/16/23 11:36:14.827
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/16/23 11:36:14.916
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/16/23 11:36:14.916
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/16/23 11:36:14.916
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/16/23 11:36:15.005
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/16/23 11:36:15.005
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/16/23 11:36:15.094
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:36:15.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9917" for this suite. 03/16/23 11:36:15.641
STEP: Destroying namespace "webhook-9917-markers" for this suite. 03/16/23 11:36:15.732
------------------------------
• [6.144 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:36:09.678
    Mar 16 11:36:09.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:36:09.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:09.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:10.127
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:36:10.486
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:36:11.011
    STEP: Deploying the webhook pod 03/16/23 11:36:11.102
    STEP: Wait for the deployment to be ready 03/16/23 11:36:11.283
    Mar 16 11:36:11.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:36:13.643
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:36:13.738
    Mar 16 11:36:14.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 03/16/23 11:36:14.827
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/16/23 11:36:14.916
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/16/23 11:36:14.916
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/16/23 11:36:14.916
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/16/23 11:36:15.005
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/16/23 11:36:15.005
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/16/23 11:36:15.094
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:36:15.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9917" for this suite. 03/16/23 11:36:15.641
    STEP: Destroying namespace "webhook-9917-markers" for this suite. 03/16/23 11:36:15.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:36:15.823
Mar 16 11:36:15.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota 03/16/23 11:36:15.825
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:16.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:16.271
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 03/16/23 11:36:33.538
STEP: Creating a ResourceQuota 03/16/23 11:36:38.629
STEP: Ensuring resource quota status is calculated 03/16/23 11:36:38.719
STEP: Creating a ConfigMap 03/16/23 11:36:40.81
STEP: Ensuring resource quota status captures configMap creation 03/16/23 11:36:40.904
STEP: Deleting a ConfigMap 03/16/23 11:36:42.995
STEP: Ensuring resource quota status released usage 03/16/23 11:36:43.086
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 16 11:36:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-100" for this suite. 03/16/23 11:36:45.355
------------------------------
• [29.622 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:36:15.823
    Mar 16 11:36:15.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename resourcequota 03/16/23 11:36:15.825
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:16.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:16.271
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 03/16/23 11:36:33.538
    STEP: Creating a ResourceQuota 03/16/23 11:36:38.629
    STEP: Ensuring resource quota status is calculated 03/16/23 11:36:38.719
    STEP: Creating a ConfigMap 03/16/23 11:36:40.81
    STEP: Ensuring resource quota status captures configMap creation 03/16/23 11:36:40.904
    STEP: Deleting a ConfigMap 03/16/23 11:36:42.995
    STEP: Ensuring resource quota status released usage 03/16/23 11:36:43.086
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:36:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-100" for this suite. 03/16/23 11:36:45.355
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:36:45.445
Mar 16 11:36:45.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook 03/16/23 11:36:45.446
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:45.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:45.895
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/16/23 11:36:46.254
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:36:46.684
STEP: Deploying the webhook pod 03/16/23 11:36:46.776
STEP: Wait for the deployment to be ready 03/16/23 11:36:46.956
Mar 16 11:36:47.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/16/23 11:36:49.316
STEP: Verifying the service has paired with the endpoint 03/16/23 11:36:49.411
Mar 16 11:36:50.412: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/16/23 11:36:50.501
STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:50.501
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/16/23 11:36:50.79
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/16/23 11:36:51.971
STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:51.971
STEP: Having no error when timeout is longer than webhook latency 03/16/23 11:36:53.469
STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:53.469
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/16/23 11:36:59.028
STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:59.028
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 16 11:37:04.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-891" for this suite. 03/16/23 11:37:05.111
STEP: Destroying namespace "webhook-891-markers" for this suite. 03/16/23 11:37:05.201
------------------------------
• [19.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:36:45.445
    Mar 16 11:36:45.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename webhook 03/16/23 11:36:45.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:36:45.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:36:45.895
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/16/23 11:36:46.254
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/16/23 11:36:46.684
    STEP: Deploying the webhook pod 03/16/23 11:36:46.776
    STEP: Wait for the deployment to be ready 03/16/23 11:36:46.956
    Mar 16 11:36:47.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 16, 11, 36, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/16/23 11:36:49.316
    STEP: Verifying the service has paired with the endpoint 03/16/23 11:36:49.411
    Mar 16 11:36:50.412: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/16/23 11:36:50.501
    STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:50.501
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/16/23 11:36:50.79
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/16/23 11:36:51.971
    STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:51.971
    STEP: Having no error when timeout is longer than webhook latency 03/16/23 11:36:53.469
    STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:53.469
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/16/23 11:36:59.028
    STEP: Registering slow webhook via the AdmissionRegistration API 03/16/23 11:36:59.028
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:37:04.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-891" for this suite. 03/16/23 11:37:05.111
    STEP: Destroying namespace "webhook-891-markers" for this suite. 03/16/23 11:37:05.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:37:05.292
Mar 16 11:37:05.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts 03/16/23 11:37:05.293
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:05.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:05.743
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Mar 16 11:37:06.194: INFO: created pod pod-service-account-defaultsa
Mar 16 11:37:06.194: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 16 11:37:06.287: INFO: created pod pod-service-account-mountsa
Mar 16 11:37:06.287: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 16 11:37:06.379: INFO: created pod pod-service-account-nomountsa
Mar 16 11:37:06.379: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 16 11:37:06.471: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 16 11:37:06.471: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 16 11:37:06.564: INFO: created pod pod-service-account-mountsa-mountspec
Mar 16 11:37:06.564: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 16 11:37:06.656: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 16 11:37:06.657: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 16 11:37:06.749: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 16 11:37:06.749: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 16 11:37:06.841: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 16 11:37:06.841: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 16 11:37:06.934: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 16 11:37:06.935: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 16 11:37:06.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2005" for this suite. 03/16/23 11:37:07.025
------------------------------
• [1.823 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:37:05.292
    Mar 16 11:37:05.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename svcaccounts 03/16/23 11:37:05.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:05.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:05.743
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Mar 16 11:37:06.194: INFO: created pod pod-service-account-defaultsa
    Mar 16 11:37:06.194: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 16 11:37:06.287: INFO: created pod pod-service-account-mountsa
    Mar 16 11:37:06.287: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 16 11:37:06.379: INFO: created pod pod-service-account-nomountsa
    Mar 16 11:37:06.379: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 16 11:37:06.471: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 16 11:37:06.471: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 16 11:37:06.564: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 16 11:37:06.564: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 16 11:37:06.656: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 16 11:37:06.657: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 16 11:37:06.749: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 16 11:37:06.749: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 16 11:37:06.841: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 16 11:37:06.841: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 16 11:37:06.934: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 16 11:37:06.935: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:37:06.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2005" for this suite. 03/16/23 11:37:07.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:37:07.116
Mar 16 11:37:07.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap 03/16/23 11:37:07.117
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:07.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:07.563
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-853f4596-222d-4f38-98a0-fb437e0cde95 03/16/23 11:37:07.831
STEP: Creating configMap with name cm-test-opt-upd-6185a9af-0f72-4a69-b36d-1c7e82de62e9 03/16/23 11:37:07.921
STEP: Creating the pod 03/16/23 11:37:08.01
Mar 16 11:37:08.107: INFO: Waiting up to 5m0s for pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981" in namespace "configmap-6868" to be "running and ready"
Mar 16 11:37:08.196: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981": Phase="Pending", Reason="", readiness=false. Elapsed: 89.801464ms
Mar 16 11:37:08.197: INFO: The phase of Pod pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981 is Pending, waiting for it to be Running (with Ready = true)
Mar 16 11:37:10.287: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981": Phase="Running", Reason="", readiness=true. Elapsed: 2.180438152s
Mar 16 11:37:10.287: INFO: The phase of Pod pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981 is Running (Ready = true)
Mar 16 11:37:10.287: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-853f4596-222d-4f38-98a0-fb437e0cde95 03/16/23 11:37:10.698
STEP: Updating configmap cm-test-opt-upd-6185a9af-0f72-4a69-b36d-1c7e82de62e9 03/16/23 11:37:10.788
STEP: Creating configMap with name cm-test-opt-create-aaa09cbe-458d-4c47-99a8-679625a651d7 03/16/23 11:37:10.878
STEP: waiting to observe update in volume 03/16/23 11:37:10.968
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:37:13.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6868" for this suite. 03/16/23 11:37:13.523
------------------------------
• [6.497 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:37:07.116
    Mar 16 11:37:07.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename configmap 03/16/23 11:37:07.117
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:07.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:07.563
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-853f4596-222d-4f38-98a0-fb437e0cde95 03/16/23 11:37:07.831
    STEP: Creating configMap with name cm-test-opt-upd-6185a9af-0f72-4a69-b36d-1c7e82de62e9 03/16/23 11:37:07.921
    STEP: Creating the pod 03/16/23 11:37:08.01
    Mar 16 11:37:08.107: INFO: Waiting up to 5m0s for pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981" in namespace "configmap-6868" to be "running and ready"
    Mar 16 11:37:08.196: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981": Phase="Pending", Reason="", readiness=false. Elapsed: 89.801464ms
    Mar 16 11:37:08.197: INFO: The phase of Pod pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981 is Pending, waiting for it to be Running (with Ready = true)
    Mar 16 11:37:10.287: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981": Phase="Running", Reason="", readiness=true. Elapsed: 2.180438152s
    Mar 16 11:37:10.287: INFO: The phase of Pod pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981 is Running (Ready = true)
    Mar 16 11:37:10.287: INFO: Pod "pod-configmaps-947b8284-20d6-4629-a8ed-d5a89eb7f981" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-853f4596-222d-4f38-98a0-fb437e0cde95 03/16/23 11:37:10.698
    STEP: Updating configmap cm-test-opt-upd-6185a9af-0f72-4a69-b36d-1c7e82de62e9 03/16/23 11:37:10.788
    STEP: Creating configMap with name cm-test-opt-create-aaa09cbe-458d-4c47-99a8-679625a651d7 03/16/23 11:37:10.878
    STEP: waiting to observe update in volume 03/16/23 11:37:10.968
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:37:13.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6868" for this suite. 03/16/23 11:37:13.523
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:37:13.614
Mar 16 11:37:13.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job 03/16/23 11:37:13.615
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:13.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:14.06
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 03/16/23 11:37:14.238
STEP: Ensuring active pods == parallelism 03/16/23 11:37:14.328
STEP: delete a job 03/16/23 11:37:16.418
STEP: deleting Job.batch foo in namespace job-4259, will wait for the garbage collector to delete the pods 03/16/23 11:37:16.419
Mar 16 11:37:16.700: INFO: Deleting Job.batch foo took: 90.464522ms
Mar 16 11:37:16.801: INFO: Terminating Job.batch foo pods took: 100.907246ms
STEP: Ensuring job was deleted 03/16/23 11:37:49.702
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 16 11:37:49.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4259" for this suite. 03/16/23 11:37:49.968
------------------------------
• [36.445 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:37:13.614
    Mar 16 11:37:13.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename job 03/16/23 11:37:13.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:13.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:14.06
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 03/16/23 11:37:14.238
    STEP: Ensuring active pods == parallelism 03/16/23 11:37:14.328
    STEP: delete a job 03/16/23 11:37:16.418
    STEP: deleting Job.batch foo in namespace job-4259, will wait for the garbage collector to delete the pods 03/16/23 11:37:16.419
    Mar 16 11:37:16.700: INFO: Deleting Job.batch foo took: 90.464522ms
    Mar 16 11:37:16.801: INFO: Terminating Job.batch foo pods took: 100.907246ms
    STEP: Ensuring job was deleted 03/16/23 11:37:49.702
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:37:49.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4259" for this suite. 03/16/23 11:37:49.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:37:50.061
Mar 16 11:37:50.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:37:50.062
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:50.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:50.507
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-4e2ca2f5-00f5-4f98-bfbf-69c6db0d613a 03/16/23 11:37:50.685
STEP: Creating a pod to test consume secrets 03/16/23 11:37:50.775
Mar 16 11:37:50.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc" in namespace "projected-3715" to be "Succeeded or Failed"
Mar 16 11:37:50.959: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.460037ms
Mar 16 11:37:53.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180161697s
Mar 16 11:37:55.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179931678s
STEP: Saw pod success 03/16/23 11:37:55.05
Mar 16 11:37:55.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc" satisfied condition "Succeeded or Failed"
Mar 16 11:37:55.139: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc container projected-secret-volume-test: <nil>
STEP: delete the pod 03/16/23 11:37:55.274
Mar 16 11:37:55.368: INFO: Waiting for pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc to disappear
Mar 16 11:37:55.457: INFO: Pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 16 11:37:55.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3715" for this suite. 03/16/23 11:37:55.635
------------------------------
• [5.665 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:37:50.061
    Mar 16 11:37:50.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:37:50.062
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:50.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:50.507
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-4e2ca2f5-00f5-4f98-bfbf-69c6db0d613a 03/16/23 11:37:50.685
    STEP: Creating a pod to test consume secrets 03/16/23 11:37:50.775
    Mar 16 11:37:50.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc" in namespace "projected-3715" to be "Succeeded or Failed"
    Mar 16 11:37:50.959: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.460037ms
    Mar 16 11:37:53.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180161697s
    Mar 16 11:37:55.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179931678s
    STEP: Saw pod success 03/16/23 11:37:55.05
    Mar 16 11:37:55.050: INFO: Pod "pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc" satisfied condition "Succeeded or Failed"
    Mar 16 11:37:55.139: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/16/23 11:37:55.274
    Mar 16 11:37:55.368: INFO: Waiting for pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc to disappear
    Mar 16 11:37:55.457: INFO: Pod pod-projected-secrets-ce2ec0df-d67f-4bd7-a488-e3e9a2285afc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:37:55.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3715" for this suite. 03/16/23 11:37:55.635
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:37:55.726
Mar 16 11:37:55.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime 03/16/23 11:37:55.727
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:55.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:56.174
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 03/16/23 11:37:56.351
STEP: wait for the container to reach Succeeded 03/16/23 11:37:56.445
STEP: get the container status 03/16/23 11:37:59.806
STEP: the container should be terminated 03/16/23 11:37:59.895
STEP: the termination message should be set 03/16/23 11:37:59.896
Mar 16 11:37:59.896: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/16/23 11:37:59.896
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 16 11:38:00.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9457" for this suite. 03/16/23 11:38:00.257
------------------------------
• [4.621 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:37:55.726
    Mar 16 11:37:55.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename container-runtime 03/16/23 11:37:55.727
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:37:55.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:37:56.174
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 03/16/23 11:37:56.351
    STEP: wait for the container to reach Succeeded 03/16/23 11:37:56.445
    STEP: get the container status 03/16/23 11:37:59.806
    STEP: the container should be terminated 03/16/23 11:37:59.895
    STEP: the termination message should be set 03/16/23 11:37:59.896
    Mar 16 11:37:59.896: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/16/23 11:37:59.896
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:38:00.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9457" for this suite. 03/16/23 11:38:00.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:38:00.348
Mar 16 11:38:00.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test 03/16/23 11:38:00.349
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:38:00.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:38:00.795
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Mar 16 11:38:01.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e" in namespace "security-context-test-5006" to be "Succeeded or Failed"
Mar 16 11:38:01.157: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.582837ms
Mar 16 11:38:03.248: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180597848s
Mar 16 11:38:05.250: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182556093s
Mar 16 11:38:05.250: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 16 11:38:05.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5006" for this suite. 03/16/23 11:38:05.429
------------------------------
• [5.171 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:38:00.348
    Mar 16 11:38:00.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename security-context-test 03/16/23 11:38:00.349
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:38:00.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:38:00.795
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Mar 16 11:38:01.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e" in namespace "security-context-test-5006" to be "Succeeded or Failed"
    Mar 16 11:38:01.157: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.582837ms
    Mar 16 11:38:03.248: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180597848s
    Mar 16 11:38:05.250: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.182556093s
    Mar 16 11:38:05.250: INFO: Pod "busybox-user-65534-3f8188ab-7128-416b-81e7-a0d135ac913e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:38:05.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5006" for this suite. 03/16/23 11:38:05.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/16/23 11:38:05.533
Mar 16 11:38:05.534: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected 03/16/23 11:38:05.535
STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:38:05.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:38:05.993
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-f32c4ec0-0c4b-4428-86e3-24c4335a66da 03/16/23 11:38:06.171
STEP: Creating a pod to test consume configMaps 03/16/23 11:38:06.26
Mar 16 11:38:06.355: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2" in namespace "projected-1305" to be "Succeeded or Failed"
Mar 16 11:38:06.449: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 94.7786ms
Mar 16 11:38:08.539: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18464067s
Mar 16 11:38:10.540: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.185031638s
STEP: Saw pod success 03/16/23 11:38:10.54
Mar 16 11:38:10.540: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2" satisfied condition "Succeeded or Failed"
Mar 16 11:38:10.629: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 container agnhost-container: <nil>
STEP: delete the pod 03/16/23 11:38:10.766
Mar 16 11:38:10.859: INFO: Waiting for pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 to disappear
Mar 16 11:38:10.949: INFO: Pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 16 11:38:10.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1305" for this suite. 03/16/23 11:38:11.127
------------------------------
• [5.684 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/16/23 11:38:05.533
    Mar 16 11:38:05.534: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
    STEP: Building a namespace api object, basename projected 03/16/23 11:38:05.535
    STEP: Waiting for a default service account to be provisioned in namespace 03/16/23 11:38:05.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/16/23 11:38:05.993
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-f32c4ec0-0c4b-4428-86e3-24c4335a66da 03/16/23 11:38:06.171
    STEP: Creating a pod to test consume configMaps 03/16/23 11:38:06.26
    Mar 16 11:38:06.355: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2" in namespace "projected-1305" to be "Succeeded or Failed"
    Mar 16 11:38:06.449: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 94.7786ms
    Mar 16 11:38:08.539: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18464067s
    Mar 16 11:38:10.540: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.185031638s
    STEP: Saw pod success 03/16/23 11:38:10.54
    Mar 16 11:38:10.540: INFO: Pod "pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2" satisfied condition "Succeeded or Failed"
    Mar 16 11:38:10.629: INFO: Trying to get logs from node ip-10-250-19-136.ec2.internal pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 container agnhost-container: <nil>
    STEP: delete the pod 03/16/23 11:38:10.766
    Mar 16 11:38:10.859: INFO: Waiting for pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 to disappear
    Mar 16 11:38:10.949: INFO: Pod pod-projected-configmaps-e1ba3ea1-fc62-45ef-8b93-e85f54ed1fa2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 16 11:38:10.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1305" for this suite. 03/16/23 11:38:11.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Mar 16 11:38:11.219: INFO: Running AfterSuite actions on node 1
Mar 16 11:38:11.219: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Mar 16 11:38:11.219: INFO: Running AfterSuite actions on node 1
    Mar 16 11:38:11.219: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.116 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6834.038 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--ginkgo.dryRun is deprecated, use --ginkgo.dry-run instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m
  [38;5;11m--ginkgo.flakeAttempts is deprecated, use --ginkgo.flake-attempts instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


Ginkgo ran 1 suite in 1h53m54.608099862s
Test Suite Passed
