I0322 20:04:27.118744      18 e2e.go:126] Starting e2e run "e69c15cc-2126-4a9e-901a-fc3a13da8e2f" on Ginkgo node 1
Mar 22 20:04:27.172: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1679515466 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 22 20:04:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:04:27.597: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0322 20:04:27.599095      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 22 20:04:27.638: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 22 20:04:27.678: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 22 20:04:27.678: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar 22 20:04:27.678: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cpc-bridge-proxy' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-do-node' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'do-node-agent' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 22 20:04:27.690: INFO: e2e test version: v1.26.3
Mar 22 20:04:27.693: INFO: kube-apiserver version: v1.26.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 22 20:04:27.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:04:27.699: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.104 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 22 20:04:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:04:27.597: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0322 20:04:27.599095      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 22 20:04:27.638: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 22 20:04:27.678: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 22 20:04:27.678: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Mar 22 20:04:27.678: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cpc-bridge-proxy' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-do-node' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'do-node-agent' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar 22 20:04:27.690: INFO: e2e test version: v1.26.3
    Mar 22 20:04:27.693: INFO: kube-apiserver version: v1.26.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 22 20:04:27.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:04:27.699: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:04:27.749
Mar 22 20:04:27.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption 03/22/23 20:04:27.75
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:04:27.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:04:27.772
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 22 20:04:27.796: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 20:05:27.881: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 03/22/23 20:05:27.888
Mar 22 20:05:27.922: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 22 20:05:27.930: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 22 20:05:27.952: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 22 20:05:27.961: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 22 20:05:27.988: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 22 20:05:28.007: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/22/23 20:05:28.007
Mar 22 20:05:28.007: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:28.017: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.136587ms
Mar 22 20:05:30.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015137286s
Mar 22 20:05:32.025: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017197882s
Mar 22 20:05:32.025: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 22 20:05:32.025: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.028: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.719037ms
Mar 22 20:05:32.028: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 20:05:32.028: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.033: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.211594ms
Mar 22 20:05:32.033: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 20:05:32.033: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.038: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.674188ms
Mar 22 20:05:32.039: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 20:05:32.039: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.043: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.115778ms
Mar 22 20:05:32.043: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 20:05:32.043: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.053: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.650257ms
Mar 22 20:05:32.053: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/22/23 20:05:32.053
Mar 22 20:05:32.064: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3574" to be "running"
Mar 22 20:05:32.069: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27203ms
Mar 22 20:05:34.074: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009028599s
Mar 22 20:05:36.074: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00927619s
Mar 22 20:05:38.075: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010325557s
Mar 22 20:05:38.075: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:05:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3574" for this suite. 03/22/23 20:05:38.191
------------------------------
• [SLOW TEST] [70.452 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:04:27.749
    Mar 22 20:04:27.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption 03/22/23 20:04:27.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:04:27.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:04:27.772
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 22 20:04:27.796: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 20:05:27.881: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 03/22/23 20:05:27.888
    Mar 22 20:05:27.922: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 22 20:05:27.930: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 22 20:05:27.952: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 22 20:05:27.961: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 22 20:05:27.988: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 22 20:05:28.007: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/22/23 20:05:28.007
    Mar 22 20:05:28.007: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:28.017: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.136587ms
    Mar 22 20:05:30.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015137286s
    Mar 22 20:05:32.025: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017197882s
    Mar 22 20:05:32.025: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 22 20:05:32.025: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.028: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.719037ms
    Mar 22 20:05:32.028: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 20:05:32.028: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.033: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.211594ms
    Mar 22 20:05:32.033: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 20:05:32.033: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.038: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.674188ms
    Mar 22 20:05:32.039: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 20:05:32.039: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.043: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.115778ms
    Mar 22 20:05:32.043: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 20:05:32.043: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.053: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.650257ms
    Mar 22 20:05:32.053: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/22/23 20:05:32.053
    Mar 22 20:05:32.064: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3574" to be "running"
    Mar 22 20:05:32.069: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27203ms
    Mar 22 20:05:34.074: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009028599s
    Mar 22 20:05:36.074: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00927619s
    Mar 22 20:05:38.075: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010325557s
    Mar 22 20:05:38.075: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:05:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3574" for this suite. 03/22/23 20:05:38.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:05:38.201
Mar 22 20:05:38.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 20:05:38.203
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:05:38.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:05:38.231
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3912 03/22/23 20:05:38.257
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3912 03/22/23 20:05:38.274
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3912 03/22/23 20:05:38.285
Mar 22 20:05:38.291: INFO: Found 0 stateful pods, waiting for 1
Mar 22 20:05:48.300: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/22/23 20:05:48.3
Mar 22 20:05:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:05:48.759: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:05:48.759: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:05:48.759: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 20:05:48.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 22 20:05:58.774: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 20:05:58.774: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:05:58.806: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 22 20:05:58.806: INFO: ss-0  pool-v7t41yxh0-q56kk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  }]
Mar 22 20:05:58.806: INFO: ss-1                        Pending         []
Mar 22 20:05:58.807: INFO: 
Mar 22 20:05:58.807: INFO: StatefulSet ss has not reached scale 3, at 2
Mar 22 20:05:59.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992476718s
Mar 22 20:06:00.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985354009s
Mar 22 20:06:01.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978601276s
Mar 22 20:06:02.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963246168s
Mar 22 20:06:03.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954611824s
Mar 22 20:06:04.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.94598014s
Mar 22 20:06:05.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93961383s
Mar 22 20:06:06.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933991373s
Mar 22 20:06:07.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.57298ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3912 03/22/23 20:06:08.88
Mar 22 20:06:08.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 20:06:09.190: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 20:06:09.190: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 20:06:09.190: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 20:06:09.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 20:06:09.479: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 22 20:06:09.479: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 20:06:09.479: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 20:06:09.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 20:06:09.801: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 22 20:06:09.801: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 20:06:09.801: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 20:06:09.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 20:06:09.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 20:06:09.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/22/23 20:06:09.807
Mar 22 20:06:09.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:06:10.087: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:06:10.087: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:06:10.087: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 20:06:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:06:10.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:06:10.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:06:10.359: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 20:06:10.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:06:10.672: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:06:10.672: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:06:10.672: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 20:06:10.672: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:06:10.677: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 22 20:06:20.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 20:06:20.693: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 20:06:20.693: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 20:06:20.709: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 22 20:06:20.709: INFO: ss-0  pool-v7t41yxh0-q56kk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  }]
Mar 22 20:06:20.709: INFO: ss-1  pool-v7t41yxh0-q56k5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
Mar 22 20:06:20.709: INFO: ss-2  pool-v7t41yxh0-q56kh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
Mar 22 20:06:20.710: INFO: 
Mar 22 20:06:20.710: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 22 20:06:21.715: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Mar 22 20:06:21.716: INFO: ss-2  pool-v7t41yxh0-q56kh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
Mar 22 20:06:21.716: INFO: 
Mar 22 20:06:21.716: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 22 20:06:22.721: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988578896s
Mar 22 20:06:23.726: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98327642s
Mar 22 20:06:24.732: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977866752s
Mar 22 20:06:25.737: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972319387s
Mar 22 20:06:26.743: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.967065743s
Mar 22 20:06:27.750: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960856766s
Mar 22 20:06:28.756: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.953927361s
Mar 22 20:06:29.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.450695ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3912 03/22/23 20:06:30.762
Mar 22 20:06:30.772: INFO: Scaling statefulset ss to 0
Mar 22 20:06:30.792: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 20:06:30.796: INFO: Deleting all statefulset in ns statefulset-3912
Mar 22 20:06:30.801: INFO: Scaling statefulset ss to 0
Mar 22 20:06:30.816: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:06:30.820: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:30.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3912" for this suite. 03/22/23 20:06:30.844
------------------------------
• [SLOW TEST] [52.649 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:05:38.201
    Mar 22 20:05:38.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 20:05:38.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:05:38.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:05:38.231
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3912 03/22/23 20:05:38.257
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3912 03/22/23 20:05:38.274
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3912 03/22/23 20:05:38.285
    Mar 22 20:05:38.291: INFO: Found 0 stateful pods, waiting for 1
    Mar 22 20:05:48.300: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/22/23 20:05:48.3
    Mar 22 20:05:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:05:48.759: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:05:48.759: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:05:48.759: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 20:05:48.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 22 20:05:58.774: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 20:05:58.774: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:05:58.806: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Mar 22 20:05:58.806: INFO: ss-0  pool-v7t41yxh0-q56kk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  }]
    Mar 22 20:05:58.806: INFO: ss-1                        Pending         []
    Mar 22 20:05:58.807: INFO: 
    Mar 22 20:05:58.807: INFO: StatefulSet ss has not reached scale 3, at 2
    Mar 22 20:05:59.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992476718s
    Mar 22 20:06:00.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985354009s
    Mar 22 20:06:01.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978601276s
    Mar 22 20:06:02.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963246168s
    Mar 22 20:06:03.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954611824s
    Mar 22 20:06:04.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.94598014s
    Mar 22 20:06:05.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93961383s
    Mar 22 20:06:06.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933991373s
    Mar 22 20:06:07.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.57298ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3912 03/22/23 20:06:08.88
    Mar 22 20:06:08.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 20:06:09.190: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 20:06:09.190: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 20:06:09.190: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 20:06:09.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 20:06:09.479: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 22 20:06:09.479: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 20:06:09.479: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 20:06:09.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 20:06:09.801: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 22 20:06:09.801: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 20:06:09.801: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 20:06:09.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 20:06:09.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 20:06:09.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/22/23 20:06:09.807
    Mar 22 20:06:09.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:06:10.087: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:06:10.087: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:06:10.087: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 20:06:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:06:10.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:06:10.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:06:10.359: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 20:06:10.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-3912 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:06:10.672: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:06:10.672: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:06:10.672: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 20:06:10.672: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:06:10.677: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar 22 20:06:20.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 20:06:20.693: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 20:06:20.693: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 20:06:20.709: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Mar 22 20:06:20.709: INFO: ss-0  pool-v7t41yxh0-q56kk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:38 +0000 UTC  }]
    Mar 22 20:06:20.709: INFO: ss-1  pool-v7t41yxh0-q56k5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
    Mar 22 20:06:20.709: INFO: ss-2  pool-v7t41yxh0-q56kh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
    Mar 22 20:06:20.710: INFO: 
    Mar 22 20:06:20.710: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 22 20:06:21.715: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Mar 22 20:06:21.716: INFO: ss-2  pool-v7t41yxh0-q56kh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:06:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:05:58 +0000 UTC  }]
    Mar 22 20:06:21.716: INFO: 
    Mar 22 20:06:21.716: INFO: StatefulSet ss has not reached scale 0, at 1
    Mar 22 20:06:22.721: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988578896s
    Mar 22 20:06:23.726: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98327642s
    Mar 22 20:06:24.732: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977866752s
    Mar 22 20:06:25.737: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972319387s
    Mar 22 20:06:26.743: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.967065743s
    Mar 22 20:06:27.750: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960856766s
    Mar 22 20:06:28.756: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.953927361s
    Mar 22 20:06:29.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.450695ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3912 03/22/23 20:06:30.762
    Mar 22 20:06:30.772: INFO: Scaling statefulset ss to 0
    Mar 22 20:06:30.792: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 20:06:30.796: INFO: Deleting all statefulset in ns statefulset-3912
    Mar 22 20:06:30.801: INFO: Scaling statefulset ss to 0
    Mar 22 20:06:30.816: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:06:30.820: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:30.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3912" for this suite. 03/22/23 20:06:30.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:30.857
Mar 22 20:06:30.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:06:30.859
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:30.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:30.883
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/22/23 20:06:30.891
Mar 22 20:06:30.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:06:32.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4461" for this suite. 03/22/23 20:06:40.037
------------------------------
• [SLOW TEST] [9.203 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:30.857
    Mar 22 20:06:30.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:06:30.859
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:30.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:30.883
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/22/23 20:06:30.891
    Mar 22 20:06:30.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:06:32.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4461" for this suite. 03/22/23 20:06:40.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:40.065
Mar 22 20:06:40.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:06:40.066
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:40.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:40.091
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 03/22/23 20:06:40.098
Mar 22 20:06:40.113: INFO: Waiting up to 5m0s for pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857" in namespace "downward-api-7078" to be "Succeeded or Failed"
Mar 22 20:06:40.121: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521491ms
Mar 22 20:06:42.135: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022669697s
Mar 22 20:06:44.129: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015730431s
Mar 22 20:06:46.127: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014333925s
STEP: Saw pod success 03/22/23 20:06:46.127
Mar 22 20:06:46.128: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857" satisfied condition "Succeeded or Failed"
Mar 22 20:06:46.133: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 container dapi-container: <nil>
STEP: delete the pod 03/22/23 20:06:46.184
Mar 22 20:06:46.211: INFO: Waiting for pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 to disappear
Mar 22 20:06:46.215: INFO: Pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:46.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7078" for this suite. 03/22/23 20:06:46.223
------------------------------
• [SLOW TEST] [6.165 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:40.065
    Mar 22 20:06:40.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:06:40.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:40.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:40.091
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 03/22/23 20:06:40.098
    Mar 22 20:06:40.113: INFO: Waiting up to 5m0s for pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857" in namespace "downward-api-7078" to be "Succeeded or Failed"
    Mar 22 20:06:40.121: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521491ms
    Mar 22 20:06:42.135: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022669697s
    Mar 22 20:06:44.129: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015730431s
    Mar 22 20:06:46.127: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014333925s
    STEP: Saw pod success 03/22/23 20:06:46.127
    Mar 22 20:06:46.128: INFO: Pod "downward-api-8e2aa4de-e814-4b79-acde-5b5968521857" satisfied condition "Succeeded or Failed"
    Mar 22 20:06:46.133: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 20:06:46.184
    Mar 22 20:06:46.211: INFO: Waiting for pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 to disappear
    Mar 22 20:06:46.215: INFO: Pod downward-api-8e2aa4de-e814-4b79-acde-5b5968521857 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:46.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7078" for this suite. 03/22/23 20:06:46.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:46.237
Mar 22 20:06:46.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename podtemplate 03/22/23 20:06:46.238
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.262
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/22/23 20:06:46.269
STEP: Replace a pod template 03/22/23 20:06:46.279
Mar 22 20:06:46.290: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6769" for this suite. 03/22/23 20:06:46.296
------------------------------
• [0.068 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:46.237
    Mar 22 20:06:46.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename podtemplate 03/22/23 20:06:46.238
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.262
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/22/23 20:06:46.269
    STEP: Replace a pod template 03/22/23 20:06:46.279
    Mar 22 20:06:46.290: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6769" for this suite. 03/22/23 20:06:46.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:46.309
Mar 22 20:06:46.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename server-version 03/22/23 20:06:46.311
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.342
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/22/23 20:06:46.351
STEP: Confirm major version 03/22/23 20:06:46.354
Mar 22 20:06:46.354: INFO: Major version: 1
STEP: Confirm minor version 03/22/23 20:06:46.354
Mar 22 20:06:46.354: INFO: cleanMinorVersion: 26
Mar 22 20:06:46.354: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:46.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5853" for this suite. 03/22/23 20:06:46.361
------------------------------
• [0.062 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:46.309
    Mar 22 20:06:46.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename server-version 03/22/23 20:06:46.311
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.342
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/22/23 20:06:46.351
    STEP: Confirm major version 03/22/23 20:06:46.354
    Mar 22 20:06:46.354: INFO: Major version: 1
    STEP: Confirm minor version 03/22/23 20:06:46.354
    Mar 22 20:06:46.354: INFO: cleanMinorVersion: 26
    Mar 22 20:06:46.354: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:46.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5853" for this suite. 03/22/23 20:06:46.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:46.373
Mar 22 20:06:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 20:06:46.375
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.396
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 03/22/23 20:06:46.407
STEP: Ensure pods equal to parallelism count is attached to the job 03/22/23 20:06:46.415
STEP: patching /status 03/22/23 20:06:50.422
STEP: updating /status 03/22/23 20:06:50.433
STEP: get /status 03/22/23 20:06:50.445
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:50.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-306" for this suite. 03/22/23 20:06:50.458
------------------------------
• [4.094 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:46.373
    Mar 22 20:06:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 20:06:46.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:46.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:46.396
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 03/22/23 20:06:46.407
    STEP: Ensure pods equal to parallelism count is attached to the job 03/22/23 20:06:46.415
    STEP: patching /status 03/22/23 20:06:50.422
    STEP: updating /status 03/22/23 20:06:50.433
    STEP: get /status 03/22/23 20:06:50.445
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:50.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-306" for this suite. 03/22/23 20:06:50.458
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:50.472
Mar 22 20:06:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:06:50.474
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:50.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:50.497
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 03/22/23 20:06:50.505
Mar 22 20:06:50.516: INFO: Waiting up to 5m0s for pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4" in namespace "projected-3402" to be "running and ready"
Mar 22 20:06:50.521: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.597226ms
Mar 22 20:06:50.521: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:06:52.528: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011910106s
Mar 22 20:06:52.529: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:06:54.529: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012688872s
Mar 22 20:06:54.529: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Running (Ready = true)
Mar 22 20:06:54.529: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4" satisfied condition "running and ready"
Mar 22 20:06:55.087: INFO: Successfully updated pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:06:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3402" for this suite. 03/22/23 20:06:57.115
------------------------------
• [SLOW TEST] [6.652 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:50.472
    Mar 22 20:06:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:06:50.474
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:50.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:50.497
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 03/22/23 20:06:50.505
    Mar 22 20:06:50.516: INFO: Waiting up to 5m0s for pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4" in namespace "projected-3402" to be "running and ready"
    Mar 22 20:06:50.521: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.597226ms
    Mar 22 20:06:50.521: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:06:52.528: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011910106s
    Mar 22 20:06:52.529: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:06:54.529: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012688872s
    Mar 22 20:06:54.529: INFO: The phase of Pod annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4 is Running (Ready = true)
    Mar 22 20:06:54.529: INFO: Pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4" satisfied condition "running and ready"
    Mar 22 20:06:55.087: INFO: Successfully updated pod "annotationupdate833288e6-c778-4d4b-9160-5e897ae1b6e4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:06:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3402" for this suite. 03/22/23 20:06:57.115
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:06:57.125
Mar 22 20:06:57.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:06:57.126
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:57.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:57.161
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6862 03/22/23 20:06:57.17
STEP: changing the ExternalName service to type=ClusterIP 03/22/23 20:06:57.179
STEP: creating replication controller externalname-service in namespace services-6862 03/22/23 20:06:57.206
I0322 20:06:57.218600      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6862, replica count: 2
I0322 20:07:00.272245      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 20:07:03.273115      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:07:03.273: INFO: Creating new exec pod
Mar 22 20:07:03.281: INFO: Waiting up to 5m0s for pod "execpodlqlzt" in namespace "services-6862" to be "running"
Mar 22 20:07:03.288: INFO: Pod "execpodlqlzt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735313ms
Mar 22 20:07:05.297: INFO: Pod "execpodlqlzt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015764596s
Mar 22 20:07:05.297: INFO: Pod "execpodlqlzt" satisfied condition "running"
Mar 22 20:07:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6862 exec execpodlqlzt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 22 20:07:06.670: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 22 20:07:06.670: INFO: stdout: ""
Mar 22 20:07:06.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6862 exec execpodlqlzt -- /bin/sh -x -c nc -v -z -w 2 10.245.252.126 80'
Mar 22 20:07:06.978: INFO: stderr: "+ nc -v -z -w 2 10.245.252.126 80\nConnection to 10.245.252.126 80 port [tcp/http] succeeded!\n"
Mar 22 20:07:06.978: INFO: stdout: ""
Mar 22 20:07:06.978: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:07:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6862" for this suite. 03/22/23 20:07:07.012
------------------------------
• [SLOW TEST] [9.914 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:06:57.125
    Mar 22 20:06:57.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:06:57.126
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:06:57.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:06:57.161
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6862 03/22/23 20:06:57.17
    STEP: changing the ExternalName service to type=ClusterIP 03/22/23 20:06:57.179
    STEP: creating replication controller externalname-service in namespace services-6862 03/22/23 20:06:57.206
    I0322 20:06:57.218600      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6862, replica count: 2
    I0322 20:07:00.272245      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0322 20:07:03.273115      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:07:03.273: INFO: Creating new exec pod
    Mar 22 20:07:03.281: INFO: Waiting up to 5m0s for pod "execpodlqlzt" in namespace "services-6862" to be "running"
    Mar 22 20:07:03.288: INFO: Pod "execpodlqlzt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735313ms
    Mar 22 20:07:05.297: INFO: Pod "execpodlqlzt": Phase="Running", Reason="", readiness=true. Elapsed: 2.015764596s
    Mar 22 20:07:05.297: INFO: Pod "execpodlqlzt" satisfied condition "running"
    Mar 22 20:07:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6862 exec execpodlqlzt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 22 20:07:06.670: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 22 20:07:06.670: INFO: stdout: ""
    Mar 22 20:07:06.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6862 exec execpodlqlzt -- /bin/sh -x -c nc -v -z -w 2 10.245.252.126 80'
    Mar 22 20:07:06.978: INFO: stderr: "+ nc -v -z -w 2 10.245.252.126 80\nConnection to 10.245.252.126 80 port [tcp/http] succeeded!\n"
    Mar 22 20:07:06.978: INFO: stdout: ""
    Mar 22 20:07:06.978: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:07:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6862" for this suite. 03/22/23 20:07:07.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:07:07.042
Mar 22 20:07:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:07:07.043
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:07.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:07.092
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/22/23 20:07:07.103
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:07.118
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:07.118
STEP: creating a pod to probe DNS 03/22/23 20:07:07.119
STEP: submitting the pod to kubernetes 03/22/23 20:07:07.12
Mar 22 20:07:07.134: INFO: Waiting up to 15m0s for pod "dns-test-60447896-012d-4447-b44d-00534827656a" in namespace "dns-3540" to be "running"
Mar 22 20:07:07.149: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.240061ms
Mar 22 20:07:09.155: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020701927s
Mar 22 20:07:11.155: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020912022s
Mar 22 20:07:13.164: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029920873s
Mar 22 20:07:15.156: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02194719s
Mar 22 20:07:17.179: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Running", Reason="", readiness=true. Elapsed: 10.045467379s
Mar 22 20:07:17.182: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:07:17.182
STEP: looking for the results for each expected name from probers 03/22/23 20:07:17.193
Mar 22 20:07:17.245: INFO: DNS probes using dns-test-60447896-012d-4447-b44d-00534827656a succeeded

STEP: deleting the pod 03/22/23 20:07:17.246
STEP: changing the externalName to bar.example.com 03/22/23 20:07:17.263
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:17.275
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:17.275
STEP: creating a second pod to probe DNS 03/22/23 20:07:17.275
STEP: submitting the pod to kubernetes 03/22/23 20:07:17.275
Mar 22 20:07:17.289: INFO: Waiting up to 15m0s for pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf" in namespace "dns-3540" to be "running"
Mar 22 20:07:17.300: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.093995ms
Mar 22 20:07:19.306: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016940387s
Mar 22 20:07:21.305: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Running", Reason="", readiness=true. Elapsed: 4.016350938s
Mar 22 20:07:21.305: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:07:21.305
STEP: looking for the results for each expected name from probers 03/22/23 20:07:21.309
Mar 22 20:07:21.341: INFO: File wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local from pod  dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 22 20:07:21.348: INFO: File jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local from pod  dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 22 20:07:21.348: INFO: Lookups using dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf failed for: [wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local]

Mar 22 20:07:26.367: INFO: DNS probes using dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf succeeded

STEP: deleting the pod 03/22/23 20:07:26.367
STEP: changing the service to type=ClusterIP 03/22/23 20:07:26.383
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:26.402
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
 03/22/23 20:07:26.402
STEP: creating a third pod to probe DNS 03/22/23 20:07:26.403
STEP: submitting the pod to kubernetes 03/22/23 20:07:26.408
Mar 22 20:07:26.418: INFO: Waiting up to 15m0s for pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108" in namespace "dns-3540" to be "running"
Mar 22 20:07:26.458: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108": Phase="Pending", Reason="", readiness=false. Elapsed: 40.20365ms
Mar 22 20:07:28.464: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108": Phase="Running", Reason="", readiness=true. Elapsed: 2.045998102s
Mar 22 20:07:28.464: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:07:28.464
STEP: looking for the results for each expected name from probers 03/22/23 20:07:28.469
Mar 22 20:07:28.504: INFO: DNS probes using dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108 succeeded

STEP: deleting the pod 03/22/23 20:07:28.504
STEP: deleting the test externalName service 03/22/23 20:07:28.519
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:07:28.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3540" for this suite. 03/22/23 20:07:28.558
------------------------------
• [SLOW TEST] [21.525 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:07:07.042
    Mar 22 20:07:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:07:07.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:07.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:07.092
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/22/23 20:07:07.103
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:07.118
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:07.118
    STEP: creating a pod to probe DNS 03/22/23 20:07:07.119
    STEP: submitting the pod to kubernetes 03/22/23 20:07:07.12
    Mar 22 20:07:07.134: INFO: Waiting up to 15m0s for pod "dns-test-60447896-012d-4447-b44d-00534827656a" in namespace "dns-3540" to be "running"
    Mar 22 20:07:07.149: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.240061ms
    Mar 22 20:07:09.155: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020701927s
    Mar 22 20:07:11.155: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020912022s
    Mar 22 20:07:13.164: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029920873s
    Mar 22 20:07:15.156: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02194719s
    Mar 22 20:07:17.179: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a": Phase="Running", Reason="", readiness=true. Elapsed: 10.045467379s
    Mar 22 20:07:17.182: INFO: Pod "dns-test-60447896-012d-4447-b44d-00534827656a" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:07:17.182
    STEP: looking for the results for each expected name from probers 03/22/23 20:07:17.193
    Mar 22 20:07:17.245: INFO: DNS probes using dns-test-60447896-012d-4447-b44d-00534827656a succeeded

    STEP: deleting the pod 03/22/23 20:07:17.246
    STEP: changing the externalName to bar.example.com 03/22/23 20:07:17.263
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:17.275
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:17.275
    STEP: creating a second pod to probe DNS 03/22/23 20:07:17.275
    STEP: submitting the pod to kubernetes 03/22/23 20:07:17.275
    Mar 22 20:07:17.289: INFO: Waiting up to 15m0s for pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf" in namespace "dns-3540" to be "running"
    Mar 22 20:07:17.300: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.093995ms
    Mar 22 20:07:19.306: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016940387s
    Mar 22 20:07:21.305: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf": Phase="Running", Reason="", readiness=true. Elapsed: 4.016350938s
    Mar 22 20:07:21.305: INFO: Pod "dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:07:21.305
    STEP: looking for the results for each expected name from probers 03/22/23 20:07:21.309
    Mar 22 20:07:21.341: INFO: File wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local from pod  dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 22 20:07:21.348: INFO: File jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local from pod  dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 22 20:07:21.348: INFO: Lookups using dns-3540/dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf failed for: [wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local]

    Mar 22 20:07:26.367: INFO: DNS probes using dns-test-107511df-d28d-4517-a43c-fda1a3bc5cdf succeeded

    STEP: deleting the pod 03/22/23 20:07:26.367
    STEP: changing the service to type=ClusterIP 03/22/23 20:07:26.383
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:26.402
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3540.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3540.svc.cluster.local; sleep 1; done
     03/22/23 20:07:26.402
    STEP: creating a third pod to probe DNS 03/22/23 20:07:26.403
    STEP: submitting the pod to kubernetes 03/22/23 20:07:26.408
    Mar 22 20:07:26.418: INFO: Waiting up to 15m0s for pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108" in namespace "dns-3540" to be "running"
    Mar 22 20:07:26.458: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108": Phase="Pending", Reason="", readiness=false. Elapsed: 40.20365ms
    Mar 22 20:07:28.464: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108": Phase="Running", Reason="", readiness=true. Elapsed: 2.045998102s
    Mar 22 20:07:28.464: INFO: Pod "dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:07:28.464
    STEP: looking for the results for each expected name from probers 03/22/23 20:07:28.469
    Mar 22 20:07:28.504: INFO: DNS probes using dns-test-8d4431fb-72fa-4e09-994c-13ba9fe90108 succeeded

    STEP: deleting the pod 03/22/23 20:07:28.504
    STEP: deleting the test externalName service 03/22/23 20:07:28.519
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:07:28.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3540" for this suite. 03/22/23 20:07:28.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:07:28.57
Mar 22 20:07:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:07:28.573
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:28.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:28.598
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 03/22/23 20:07:45.612
STEP: Creating a ResourceQuota 03/22/23 20:07:50.617
STEP: Ensuring resource quota status is calculated 03/22/23 20:07:50.627
STEP: Creating a ConfigMap 03/22/23 20:07:52.633
STEP: Ensuring resource quota status captures configMap creation 03/22/23 20:07:52.647
STEP: Deleting a ConfigMap 03/22/23 20:07:54.662
STEP: Ensuring resource quota status released usage 03/22/23 20:07:54.67
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:07:56.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3190" for this suite. 03/22/23 20:07:56.687
------------------------------
• [SLOW TEST] [28.126 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:07:28.57
    Mar 22 20:07:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:07:28.573
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:28.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:28.598
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 03/22/23 20:07:45.612
    STEP: Creating a ResourceQuota 03/22/23 20:07:50.617
    STEP: Ensuring resource quota status is calculated 03/22/23 20:07:50.627
    STEP: Creating a ConfigMap 03/22/23 20:07:52.633
    STEP: Ensuring resource quota status captures configMap creation 03/22/23 20:07:52.647
    STEP: Deleting a ConfigMap 03/22/23 20:07:54.662
    STEP: Ensuring resource quota status released usage 03/22/23 20:07:54.67
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:07:56.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3190" for this suite. 03/22/23 20:07:56.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:07:56.699
Mar 22 20:07:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:07:56.701
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:56.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:56.726
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Mar 22 20:07:56.763: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/22/23 20:07:56.771
Mar 22 20:07:56.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:56.777: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/22/23 20:07:56.777
Mar 22 20:07:56.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:56.809: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:07:57.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:57.815: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:07:58.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 22 20:07:58.815: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/22/23 20:07:58.82
Mar 22 20:07:58.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:58.869: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/22/23 20:07:58.869
Mar 22 20:07:58.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:58.911: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:07:59.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:07:59.918: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:08:00.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:08:00.925: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:08:01.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:08:01.997: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:08:02.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:08:02.917: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:08:03.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 22 20:08:03.927: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:08:03.949
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8583, will wait for the garbage collector to delete the pods 03/22/23 20:08:03.949
Mar 22 20:08:04.035: INFO: Deleting DaemonSet.extensions daemon-set took: 8.989634ms
Mar 22 20:08:04.145: INFO: Terminating DaemonSet.extensions daemon-set pods took: 109.712743ms
Mar 22 20:08:06.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:08:06.553: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 20:08:06.562: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8634"},"items":null}

Mar 22 20:08:06.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8634"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8583" for this suite. 03/22/23 20:08:06.638
------------------------------
• [SLOW TEST] [9.955 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:07:56.699
    Mar 22 20:07:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:07:56.701
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:07:56.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:07:56.726
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Mar 22 20:07:56.763: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/22/23 20:07:56.771
    Mar 22 20:07:56.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:56.777: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/22/23 20:07:56.777
    Mar 22 20:07:56.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:56.809: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:07:57.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:57.815: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:07:58.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 22 20:07:58.815: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/22/23 20:07:58.82
    Mar 22 20:07:58.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:58.869: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/22/23 20:07:58.869
    Mar 22 20:07:58.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:58.911: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:07:59.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:07:59.918: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:08:00.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:08:00.925: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:08:01.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:08:01.997: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:08:02.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:08:02.917: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:08:03.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 22 20:08:03.927: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:08:03.949
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8583, will wait for the garbage collector to delete the pods 03/22/23 20:08:03.949
    Mar 22 20:08:04.035: INFO: Deleting DaemonSet.extensions daemon-set took: 8.989634ms
    Mar 22 20:08:04.145: INFO: Terminating DaemonSet.extensions daemon-set pods took: 109.712743ms
    Mar 22 20:08:06.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:08:06.553: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 20:08:06.562: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8634"},"items":null}

    Mar 22 20:08:06.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8634"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8583" for this suite. 03/22/23 20:08:06.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:06.659
Mar 22 20:08:06.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:08:06.661
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:06.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:06.7
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 03/22/23 20:08:06.765
STEP: watching for Pod to be ready 03/22/23 20:08:06.794
Mar 22 20:08:06.810: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 22 20:08:06.814: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
Mar 22 20:08:06.815: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
Mar 22 20:08:07.913: INFO: Found Pod pod-test in namespace pods-6391 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/22/23 20:08:07.92
STEP: getting the Pod and ensuring that it's patched 03/22/23 20:08:07.939
STEP: replacing the Pod's status Ready condition to False 03/22/23 20:08:07.945
STEP: check the Pod again to ensure its Ready conditions are False 03/22/23 20:08:07.972
STEP: deleting the Pod via a Collection with a LabelSelector 03/22/23 20:08:07.973
STEP: watching for the Pod to be deleted 03/22/23 20:08:07.984
Mar 22 20:08:07.988: INFO: observed event type MODIFIED
Mar 22 20:08:09.921: INFO: observed event type MODIFIED
Mar 22 20:08:10.934: INFO: observed event type MODIFIED
Mar 22 20:08:10.946: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6391" for this suite. 03/22/23 20:08:10.972
------------------------------
• [4.321 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:06.659
    Mar 22 20:08:06.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:08:06.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:06.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:06.7
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 03/22/23 20:08:06.765
    STEP: watching for Pod to be ready 03/22/23 20:08:06.794
    Mar 22 20:08:06.810: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 22 20:08:06.814: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
    Mar 22 20:08:06.815: INFO: observed Pod pod-test in namespace pods-6391 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
    Mar 22 20:08:07.913: INFO: Found Pod pod-test in namespace pods-6391 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 20:08:06 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/22/23 20:08:07.92
    STEP: getting the Pod and ensuring that it's patched 03/22/23 20:08:07.939
    STEP: replacing the Pod's status Ready condition to False 03/22/23 20:08:07.945
    STEP: check the Pod again to ensure its Ready conditions are False 03/22/23 20:08:07.972
    STEP: deleting the Pod via a Collection with a LabelSelector 03/22/23 20:08:07.973
    STEP: watching for the Pod to be deleted 03/22/23 20:08:07.984
    Mar 22 20:08:07.988: INFO: observed event type MODIFIED
    Mar 22 20:08:09.921: INFO: observed event type MODIFIED
    Mar 22 20:08:10.934: INFO: observed event type MODIFIED
    Mar 22 20:08:10.946: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6391" for this suite. 03/22/23 20:08:10.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:10.98
Mar 22 20:08:10.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename endpointslice 03/22/23 20:08:10.99
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:11.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:11.033
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 03/22/23 20:08:16.173
STEP: referencing matching pods with named port 03/22/23 20:08:21.202
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/22/23 20:08:26.222
STEP: recreating EndpointSlices after they've been deleted 03/22/23 20:08:31.25
Mar 22 20:08:31.306: INFO: EndpointSlice for Service endpointslice-2092/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:41.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2092" for this suite. 03/22/23 20:08:41.329
------------------------------
• [SLOW TEST] [30.400 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:10.98
    Mar 22 20:08:10.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename endpointslice 03/22/23 20:08:10.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:11.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:11.033
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 03/22/23 20:08:16.173
    STEP: referencing matching pods with named port 03/22/23 20:08:21.202
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/22/23 20:08:26.222
    STEP: recreating EndpointSlices after they've been deleted 03/22/23 20:08:31.25
    Mar 22 20:08:31.306: INFO: EndpointSlice for Service endpointslice-2092/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:41.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2092" for this suite. 03/22/23 20:08:41.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:41.41
Mar 22 20:08:41.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:08:41.414
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:41.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:41.452
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:08:41.494
Mar 22 20:08:41.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90" in namespace "projected-1955" to be "Succeeded or Failed"
Mar 22 20:08:41.515: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.326113ms
Mar 22 20:08:43.522: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014011284s
Mar 22 20:08:45.524: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015985879s
STEP: Saw pod success 03/22/23 20:08:45.524
Mar 22 20:08:45.524: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90" satisfied condition "Succeeded or Failed"
Mar 22 20:08:45.528: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 container client-container: <nil>
STEP: delete the pod 03/22/23 20:08:45.57
Mar 22 20:08:45.585: INFO: Waiting for pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 to disappear
Mar 22 20:08:45.588: INFO: Pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:45.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1955" for this suite. 03/22/23 20:08:45.594
------------------------------
• [4.192 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:41.41
    Mar 22 20:08:41.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:08:41.414
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:41.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:41.452
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:08:41.494
    Mar 22 20:08:41.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90" in namespace "projected-1955" to be "Succeeded or Failed"
    Mar 22 20:08:41.515: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.326113ms
    Mar 22 20:08:43.522: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014011284s
    Mar 22 20:08:45.524: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015985879s
    STEP: Saw pod success 03/22/23 20:08:45.524
    Mar 22 20:08:45.524: INFO: Pod "downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90" satisfied condition "Succeeded or Failed"
    Mar 22 20:08:45.528: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:08:45.57
    Mar 22 20:08:45.585: INFO: Waiting for pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 to disappear
    Mar 22 20:08:45.588: INFO: Pod downwardapi-volume-81ce7e22-04cc-42d6-b35d-26a6a1939e90 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:45.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1955" for this suite. 03/22/23 20:08:45.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:45.602
Mar 22 20:08:45.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:08:45.603
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:45.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:45.638
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:08:45.684
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:08:47.148
STEP: Deploying the webhook pod 03/22/23 20:08:47.186
STEP: Wait for the deployment to be ready 03/22/23 20:08:47.228
Mar 22 20:08:47.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/22/23 20:08:49.267
STEP: Verifying the service has paired with the endpoint 03/22/23 20:08:49.281
Mar 22 20:08:50.287: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 03/22/23 20:08:50.304
STEP: Creating a custom resource definition that should be denied by the webhook 03/22/23 20:08:50.384
Mar 22 20:08:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:50.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5972" for this suite. 03/22/23 20:08:50.531
STEP: Destroying namespace "webhook-5972-markers" for this suite. 03/22/23 20:08:50.543
------------------------------
• [4.953 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:45.602
    Mar 22 20:08:45.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:08:45.603
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:45.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:45.638
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:08:45.684
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:08:47.148
    STEP: Deploying the webhook pod 03/22/23 20:08:47.186
    STEP: Wait for the deployment to be ready 03/22/23 20:08:47.228
    Mar 22 20:08:47.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 8, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/22/23 20:08:49.267
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:08:49.281
    Mar 22 20:08:50.287: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/22/23 20:08:50.304
    STEP: Creating a custom resource definition that should be denied by the webhook 03/22/23 20:08:50.384
    Mar 22 20:08:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:50.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5972" for this suite. 03/22/23 20:08:50.531
    STEP: Destroying namespace "webhook-5972-markers" for this suite. 03/22/23 20:08:50.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:50.556
Mar 22 20:08:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption 03/22/23 20:08:50.557
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:50.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:50.587
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 03/22/23 20:08:50.605
STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.622
STEP: updating the pdb 03/22/23 20:08:50.635
STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.652
STEP: patching the pdb 03/22/23 20:08:50.662
STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.677
STEP: Waiting for the pdb to be deleted 03/22/23 20:08:50.691
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:50.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9095" for this suite. 03/22/23 20:08:50.704
------------------------------
• [0.171 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:50.556
    Mar 22 20:08:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption 03/22/23 20:08:50.557
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:50.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:50.587
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 03/22/23 20:08:50.605
    STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.622
    STEP: updating the pdb 03/22/23 20:08:50.635
    STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.652
    STEP: patching the pdb 03/22/23 20:08:50.662
    STEP: Waiting for the pdb to be processed 03/22/23 20:08:50.677
    STEP: Waiting for the pdb to be deleted 03/22/23 20:08:50.691
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:50.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9095" for this suite. 03/22/23 20:08:50.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:50.728
Mar 22 20:08:50.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:08:50.729
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:50.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:50.759
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bc12dd0b-6fb4-499d-9d6f-8c3f1d121eb7 03/22/23 20:08:50.773
STEP: Creating the pod 03/22/23 20:08:50.78
Mar 22 20:08:50.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e" in namespace "projected-3227" to be "running and ready"
Mar 22 20:08:50.801: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.959404ms
Mar 22 20:08:50.801: INFO: The phase of Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:08:52.807: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013079093s
Mar 22 20:08:52.807: INFO: The phase of Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e is Running (Ready = true)
Mar 22 20:08:52.807: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-bc12dd0b-6fb4-499d-9d6f-8c3f1d121eb7 03/22/23 20:08:52.823
STEP: waiting to observe update in volume 03/22/23 20:08:52.832
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:54.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3227" for this suite. 03/22/23 20:08:54.876
------------------------------
• [4.157 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:50.728
    Mar 22 20:08:50.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:08:50.729
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:50.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:50.759
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-bc12dd0b-6fb4-499d-9d6f-8c3f1d121eb7 03/22/23 20:08:50.773
    STEP: Creating the pod 03/22/23 20:08:50.78
    Mar 22 20:08:50.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e" in namespace "projected-3227" to be "running and ready"
    Mar 22 20:08:50.801: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.959404ms
    Mar 22 20:08:50.801: INFO: The phase of Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:08:52.807: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013079093s
    Mar 22 20:08:52.807: INFO: The phase of Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e is Running (Ready = true)
    Mar 22 20:08:52.807: INFO: Pod "pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-bc12dd0b-6fb4-499d-9d6f-8c3f1d121eb7 03/22/23 20:08:52.823
    STEP: waiting to observe update in volume 03/22/23 20:08:52.832
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:54.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3227" for this suite. 03/22/23 20:08:54.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:54.892
Mar 22 20:08:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:08:54.894
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:54.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:54.924
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 03/22/23 20:08:54.931
Mar 22 20:08:54.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-464 create -f -'
Mar 22 20:08:55.824: INFO: stderr: ""
Mar 22 20:08:55.824: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/22/23 20:08:55.824
Mar 22 20:08:56.831: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:08:56.831: INFO: Found 0 / 1
Mar 22 20:08:57.833: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:08:57.833: INFO: Found 1 / 1
Mar 22 20:08:57.833: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/22/23 20:08:57.833
Mar 22 20:08:57.840: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:08:57.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 20:08:57.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-464 patch pod agnhost-primary-kkrpx -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 22 20:08:58.124: INFO: stderr: ""
Mar 22 20:08:58.124: INFO: stdout: "pod/agnhost-primary-kkrpx patched\n"
STEP: checking annotations 03/22/23 20:08:58.124
Mar 22 20:08:58.130: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:08:58.130: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:08:58.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-464" for this suite. 03/22/23 20:08:58.141
------------------------------
• [3.258 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:54.892
    Mar 22 20:08:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:08:54.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:54.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:54.924
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 03/22/23 20:08:54.931
    Mar 22 20:08:54.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-464 create -f -'
    Mar 22 20:08:55.824: INFO: stderr: ""
    Mar 22 20:08:55.824: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/22/23 20:08:55.824
    Mar 22 20:08:56.831: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:08:56.831: INFO: Found 0 / 1
    Mar 22 20:08:57.833: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:08:57.833: INFO: Found 1 / 1
    Mar 22 20:08:57.833: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/22/23 20:08:57.833
    Mar 22 20:08:57.840: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:08:57.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 22 20:08:57.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-464 patch pod agnhost-primary-kkrpx -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 22 20:08:58.124: INFO: stderr: ""
    Mar 22 20:08:58.124: INFO: stdout: "pod/agnhost-primary-kkrpx patched\n"
    STEP: checking annotations 03/22/23 20:08:58.124
    Mar 22 20:08:58.130: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:08:58.130: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:08:58.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-464" for this suite. 03/22/23 20:08:58.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:08:58.15
Mar 22 20:08:58.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-pred 03/22/23 20:08:58.156
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:58.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:58.178
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 22 20:08:58.184: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 20:08:58.197: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 20:08:58.215: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
Mar 22 20:08:58.225: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:08:58.226: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container coredns ready: true, restart count 0
Mar 22 20:08:58.226: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:08:58.226: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:08:58.226: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:08:58.226: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:08:58.226: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:08:58.226: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:08:58.226: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 20:08:58.226: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.226: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:08:58.226: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 20:08:58.226: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
Mar 22 20:08:58.235: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.235: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:08:58.236: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.236: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 22 20:08:58.236: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.236: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:08:58.236: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.237: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:08:58.237: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:08:58.244: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.244: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:08:58.244: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.244: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:08:58.244: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.244: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:08:58.244: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.244: INFO: 	Container e2e ready: true, restart count 0
Mar 22 20:08:58.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:08:58.244: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:08:58.244: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 20:08:58.244: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
Mar 22 20:08:58.255: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:08:58.255: INFO: coredns-9765d8f5f-mzj8v from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container coredns ready: true, restart count 0
Mar 22 20:08:58.255: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:08:58.255: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:08:58.255: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:08:58.255: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:08:58.255: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:08:58.255: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:08:58.255: INFO: agnhost-primary-kkrpx from kubectl-464 started at 2023-03-22 20:08:55 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container agnhost-primary ready: true, restart count 0
Mar 22 20:08:58.255: INFO: pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e from projected-3227 started at 2023-03-22 20:08:50 +0000 UTC (1 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container agnhost-container ready: true, restart count 0
Mar 22 20:08:58.255: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:08:58.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:08:58.255: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node pool-v7t41yxh0-q56k5 03/22/23 20:08:58.283
STEP: verifying the node has the label node pool-v7t41yxh0-q56kh 03/22/23 20:08:58.304
STEP: verifying the node has the label node pool-v7t41yxh0-q56kk 03/22/23 20:08:58.346
Mar 22 20:08:58.393: INFO: Pod cilium-8ks5z requesting resource cpu=300m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod cilium-bzq4m requesting resource cpu=300m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod cilium-nkg6k requesting resource cpu=300m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod cilium-operator-55c59769f8-kp64k requesting resource cpu=100m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod coredns-9765d8f5f-k57jv requesting resource cpu=100m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod coredns-9765d8f5f-mzj8v requesting resource cpu=100m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-4xt8w requesting resource cpu=100m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-bkpg5 requesting resource cpu=100m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-mvx4m requesting resource cpu=100m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod csi-do-node-9svlx requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod csi-do-node-mlbms requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod csi-do-node-q2wlc requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod do-node-agent-4mgkz requesting resource cpu=102m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod do-node-agent-g7j2s requesting resource cpu=102m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod do-node-agent-wt4p9 requesting resource cpu=102m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-7nwnk requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-nrnds requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-s74g2 requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod kube-proxy-cgktr requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod kube-proxy-p8crx requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod kube-proxy-tr8ql requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod agnhost-primary-kkrpx requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod sonobuoy requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod sonobuoy-e2e-job-0f9deb38bbcd4941 requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
STEP: Starting Pods to consume most of the cluster CPU. 03/22/23 20:08:58.393
Mar 22 20:08:58.393: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56k5
Mar 22 20:08:58.408: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56kh
Mar 22 20:08:58.416: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56kk
Mar 22 20:08:58.434: INFO: Waiting up to 5m0s for pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349" in namespace "sched-pred-6617" to be "running"
Mar 22 20:08:58.453: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349": Phase="Pending", Reason="", readiness=false. Elapsed: 19.563768ms
Mar 22 20:09:00.460: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349": Phase="Running", Reason="", readiness=true. Elapsed: 2.026473917s
Mar 22 20:09:00.461: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349" satisfied condition "running"
Mar 22 20:09:00.461: INFO: Waiting up to 5m0s for pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136" in namespace "sched-pred-6617" to be "running"
Mar 22 20:09:00.466: INFO: Pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136": Phase="Running", Reason="", readiness=true. Elapsed: 4.543665ms
Mar 22 20:09:00.466: INFO: Pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136" satisfied condition "running"
Mar 22 20:09:00.466: INFO: Waiting up to 5m0s for pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e" in namespace "sched-pred-6617" to be "running"
Mar 22 20:09:00.472: INFO: Pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e": Phase="Running", Reason="", readiness=true. Elapsed: 5.213357ms
Mar 22 20:09:00.472: INFO: Pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/22/23 20:09:00.472
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa67c73edd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136 to pool-v7t41yxh0-q56kh] 03/22/23 20:09:00.484
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa9c4e9fd0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.485
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa9eb9e6d8], Reason = [Created], Message = [Created container filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136] 03/22/23 20:09:00.485
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aaab773574], Reason = [Started], Message = [Started container filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136] 03/22/23 20:09:00.486
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa68456adb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e to pool-v7t41yxh0-q56kk] 03/22/23 20:09:00.486
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa96446500], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.486
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa988f9a6f], Reason = [Created], Message = [Created container filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e] 03/22/23 20:09:00.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aaa17f8df9], Reason = [Started], Message = [Started container filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e] 03/22/23 20:09:00.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa66c2ead2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349 to pool-v7t41yxh0-q56k5] 03/22/23 20:09:00.488
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa94049b4f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.488
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa96375332], Reason = [Created], Message = [Created container filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349] 03/22/23 20:09:00.488
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa9e927239], Reason = [Started], Message = [Started container filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349] 03/22/23 20:09:00.488
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174ed6aae3636a1c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 03/22/23 20:09:00.514
STEP: removing the label node off the node pool-v7t41yxh0-q56k5 03/22/23 20:09:01.523
STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.542
STEP: removing the label node off the node pool-v7t41yxh0-q56kh 03/22/23 20:09:01.547
STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.574
STEP: removing the label node off the node pool-v7t41yxh0-q56kk 03/22/23 20:09:01.627
STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.688
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:01.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6617" for this suite. 03/22/23 20:09:01.75
------------------------------
• [3.614 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:08:58.15
    Mar 22 20:08:58.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-pred 03/22/23 20:08:58.156
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:08:58.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:08:58.178
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 22 20:08:58.184: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 22 20:08:58.197: INFO: Waiting for terminating namespaces to be deleted...
    Mar 22 20:08:58.215: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
    Mar 22 20:08:58.225: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.226: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 20:08:58.226: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
    Mar 22 20:08:58.235: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.235: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:08:58.236: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.236: INFO: 	Container cilium-operator ready: true, restart count 0
    Mar 22 20:08:58.236: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.236: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:08:58.236: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.237: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:08:58.237: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.244: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.244: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.244: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.244: INFO: 	Container e2e ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 20:08:58.244: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
    Mar 22 20:08:58.255: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: coredns-9765d8f5f-mzj8v from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: agnhost-primary-kkrpx from kubectl-464 started at 2023-03-22 20:08:55 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container agnhost-primary ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e from projected-3227 started at 2023-03-22 20:08:50 +0000 UTC (1 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container agnhost-container ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:08:58.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:08:58.255: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node pool-v7t41yxh0-q56k5 03/22/23 20:08:58.283
    STEP: verifying the node has the label node pool-v7t41yxh0-q56kh 03/22/23 20:08:58.304
    STEP: verifying the node has the label node pool-v7t41yxh0-q56kk 03/22/23 20:08:58.346
    Mar 22 20:08:58.393: INFO: Pod cilium-8ks5z requesting resource cpu=300m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod cilium-bzq4m requesting resource cpu=300m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod cilium-nkg6k requesting resource cpu=300m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod cilium-operator-55c59769f8-kp64k requesting resource cpu=100m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod coredns-9765d8f5f-k57jv requesting resource cpu=100m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod coredns-9765d8f5f-mzj8v requesting resource cpu=100m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-4xt8w requesting resource cpu=100m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-bkpg5 requesting resource cpu=100m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod cpc-bridge-proxy-mvx4m requesting resource cpu=100m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod csi-do-node-9svlx requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod csi-do-node-mlbms requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod csi-do-node-q2wlc requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod do-node-agent-4mgkz requesting resource cpu=102m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod do-node-agent-g7j2s requesting resource cpu=102m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod do-node-agent-wt4p9 requesting resource cpu=102m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-7nwnk requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-nrnds requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod konnectivity-agent-s74g2 requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod kube-proxy-cgktr requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod kube-proxy-p8crx requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod kube-proxy-tr8ql requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod agnhost-primary-kkrpx requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod pod-projected-configmaps-c4661a78-4c60-4177-ab70-7a4527526d5e requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod sonobuoy requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod sonobuoy-e2e-job-0f9deb38bbcd4941 requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq requesting resource cpu=0m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv requesting resource cpu=0m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.393: INFO: Pod sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 requesting resource cpu=0m on Node pool-v7t41yxh0-q56kh
    STEP: Starting Pods to consume most of the cluster CPU. 03/22/23 20:08:58.393
    Mar 22 20:08:58.393: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56k5
    Mar 22 20:08:58.408: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56kh
    Mar 22 20:08:58.416: INFO: Creating a pod which consumes cpu=908m on Node pool-v7t41yxh0-q56kk
    Mar 22 20:08:58.434: INFO: Waiting up to 5m0s for pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349" in namespace "sched-pred-6617" to be "running"
    Mar 22 20:08:58.453: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349": Phase="Pending", Reason="", readiness=false. Elapsed: 19.563768ms
    Mar 22 20:09:00.460: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349": Phase="Running", Reason="", readiness=true. Elapsed: 2.026473917s
    Mar 22 20:09:00.461: INFO: Pod "filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349" satisfied condition "running"
    Mar 22 20:09:00.461: INFO: Waiting up to 5m0s for pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136" in namespace "sched-pred-6617" to be "running"
    Mar 22 20:09:00.466: INFO: Pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136": Phase="Running", Reason="", readiness=true. Elapsed: 4.543665ms
    Mar 22 20:09:00.466: INFO: Pod "filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136" satisfied condition "running"
    Mar 22 20:09:00.466: INFO: Waiting up to 5m0s for pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e" in namespace "sched-pred-6617" to be "running"
    Mar 22 20:09:00.472: INFO: Pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e": Phase="Running", Reason="", readiness=true. Elapsed: 5.213357ms
    Mar 22 20:09:00.472: INFO: Pod "filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/22/23 20:09:00.472
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa67c73edd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136 to pool-v7t41yxh0-q56kh] 03/22/23 20:09:00.484
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa9c4e9fd0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.485
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aa9eb9e6d8], Reason = [Created], Message = [Created container filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136] 03/22/23 20:09:00.485
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136.174ed6aaab773574], Reason = [Started], Message = [Started container filler-pod-4e989422-28aa-4f0a-8c39-0cd423984136] 03/22/23 20:09:00.486
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa68456adb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e to pool-v7t41yxh0-q56kk] 03/22/23 20:09:00.486
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa96446500], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.486
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aa988f9a6f], Reason = [Created], Message = [Created container filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e] 03/22/23 20:09:00.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e.174ed6aaa17f8df9], Reason = [Started], Message = [Started container filler-pod-77ea9bfa-855e-4e20-b877-e92441ac414e] 03/22/23 20:09:00.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa66c2ead2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6617/filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349 to pool-v7t41yxh0-q56k5] 03/22/23 20:09:00.488
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa94049b4f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/22/23 20:09:00.488
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa96375332], Reason = [Created], Message = [Created container filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349] 03/22/23 20:09:00.488
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349.174ed6aa9e927239], Reason = [Started], Message = [Started container filler-pod-79dbf558-311c-46ef-b528-7f4a0481a349] 03/22/23 20:09:00.488
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174ed6aae3636a1c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 03/22/23 20:09:00.514
    STEP: removing the label node off the node pool-v7t41yxh0-q56k5 03/22/23 20:09:01.523
    STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.542
    STEP: removing the label node off the node pool-v7t41yxh0-q56kh 03/22/23 20:09:01.547
    STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.574
    STEP: removing the label node off the node pool-v7t41yxh0-q56kk 03/22/23 20:09:01.627
    STEP: verifying the node doesn't have the label node 03/22/23 20:09:01.688
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:01.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6617" for this suite. 03/22/23 20:09:01.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:01.785
Mar 22 20:09:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:09:01.787
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:01.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:01.815
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 03/22/23 20:09:01.834
STEP: waiting for available Endpoint 03/22/23 20:09:01.841
STEP: listing all Endpoints 03/22/23 20:09:01.848
STEP: updating the Endpoint 03/22/23 20:09:01.862
STEP: fetching the Endpoint 03/22/23 20:09:01.884
STEP: patching the Endpoint 03/22/23 20:09:01.902
STEP: fetching the Endpoint 03/22/23 20:09:01.916
STEP: deleting the Endpoint by Collection 03/22/23 20:09:01.921
STEP: waiting for Endpoint deletion 03/22/23 20:09:01.93
STEP: fetching the Endpoint 03/22/23 20:09:01.934
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-139" for this suite. 03/22/23 20:09:01.983
------------------------------
• [0.205 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:01.785
    Mar 22 20:09:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:09:01.787
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:01.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:01.815
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 03/22/23 20:09:01.834
    STEP: waiting for available Endpoint 03/22/23 20:09:01.841
    STEP: listing all Endpoints 03/22/23 20:09:01.848
    STEP: updating the Endpoint 03/22/23 20:09:01.862
    STEP: fetching the Endpoint 03/22/23 20:09:01.884
    STEP: patching the Endpoint 03/22/23 20:09:01.902
    STEP: fetching the Endpoint 03/22/23 20:09:01.916
    STEP: deleting the Endpoint by Collection 03/22/23 20:09:01.921
    STEP: waiting for Endpoint deletion 03/22/23 20:09:01.93
    STEP: fetching the Endpoint 03/22/23 20:09:01.934
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-139" for this suite. 03/22/23 20:09:01.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:01.998
Mar 22 20:09:01.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:09:02
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:02.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:02.085
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 03/22/23 20:09:02.1
Mar 22 20:09:02.121: INFO: Waiting up to 5m0s for pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8" in namespace "emptydir-1612" to be "Succeeded or Failed"
Mar 22 20:09:02.128: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.143308ms
Mar 22 20:09:04.141: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019485548s
Mar 22 20:09:06.136: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015101711s
Mar 22 20:09:08.134: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012779853s
STEP: Saw pod success 03/22/23 20:09:08.135
Mar 22 20:09:08.135: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8" satisfied condition "Succeeded or Failed"
Mar 22 20:09:08.144: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 container test-container: <nil>
STEP: delete the pod 03/22/23 20:09:08.156
Mar 22 20:09:08.172: INFO: Waiting for pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 to disappear
Mar 22 20:09:08.177: INFO: Pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1612" for this suite. 03/22/23 20:09:08.19
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:01.998
    Mar 22 20:09:01.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:09:02
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:02.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:02.085
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/22/23 20:09:02.1
    Mar 22 20:09:02.121: INFO: Waiting up to 5m0s for pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8" in namespace "emptydir-1612" to be "Succeeded or Failed"
    Mar 22 20:09:02.128: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.143308ms
    Mar 22 20:09:04.141: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019485548s
    Mar 22 20:09:06.136: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015101711s
    Mar 22 20:09:08.134: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012779853s
    STEP: Saw pod success 03/22/23 20:09:08.135
    Mar 22 20:09:08.135: INFO: Pod "pod-410afb4f-effd-4bef-997c-f1d5117ab9c8" satisfied condition "Succeeded or Failed"
    Mar 22 20:09:08.144: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:09:08.156
    Mar 22 20:09:08.172: INFO: Waiting for pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 to disappear
    Mar 22 20:09:08.177: INFO: Pod pod-410afb4f-effd-4bef-997c-f1d5117ab9c8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1612" for this suite. 03/22/23 20:09:08.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:08.206
Mar 22 20:09:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:09:08.207
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:08.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:08.242
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:09:08.25
Mar 22 20:09:08.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112" in namespace "projected-4128" to be "Succeeded or Failed"
Mar 22 20:09:08.266: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387762ms
Mar 22 20:09:10.286: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02613259s
Mar 22 20:09:12.274: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014239526s
STEP: Saw pod success 03/22/23 20:09:12.274
Mar 22 20:09:12.275: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112" satisfied condition "Succeeded or Failed"
Mar 22 20:09:12.281: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 container client-container: <nil>
STEP: delete the pod 03/22/23 20:09:12.293
Mar 22 20:09:12.309: INFO: Waiting for pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 to disappear
Mar 22 20:09:12.321: INFO: Pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:12.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4128" for this suite. 03/22/23 20:09:12.329
------------------------------
• [4.130 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:08.206
    Mar 22 20:09:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:09:08.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:08.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:08.242
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:09:08.25
    Mar 22 20:09:08.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112" in namespace "projected-4128" to be "Succeeded or Failed"
    Mar 22 20:09:08.266: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387762ms
    Mar 22 20:09:10.286: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02613259s
    Mar 22 20:09:12.274: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014239526s
    STEP: Saw pod success 03/22/23 20:09:12.274
    Mar 22 20:09:12.275: INFO: Pod "downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112" satisfied condition "Succeeded or Failed"
    Mar 22 20:09:12.281: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:09:12.293
    Mar 22 20:09:12.309: INFO: Waiting for pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 to disappear
    Mar 22 20:09:12.321: INFO: Pod downwardapi-volume-a4676d58-f983-41e7-b6e8-722a044e3112 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:12.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4128" for this suite. 03/22/23 20:09:12.329
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:12.342
Mar 22 20:09:12.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:09:12.343
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:12.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:12.367
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:09:12.39
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:09:13.332
STEP: Deploying the webhook pod 03/22/23 20:09:13.343
STEP: Wait for the deployment to be ready 03/22/23 20:09:13.36
Mar 22 20:09:13.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:09:15.407
STEP: Verifying the service has paired with the endpoint 03/22/23 20:09:15.424
Mar 22 20:09:16.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 03/22/23 20:09:16.527
STEP: Creating a configMap that should be mutated 03/22/23 20:09:16.59
STEP: Deleting the collection of validation webhooks 03/22/23 20:09:16.687
STEP: Creating a configMap that should not be mutated 03/22/23 20:09:16.735
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:16.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9411" for this suite. 03/22/23 20:09:16.821
STEP: Destroying namespace "webhook-9411-markers" for this suite. 03/22/23 20:09:16.83
------------------------------
• [4.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:12.342
    Mar 22 20:09:12.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:09:12.343
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:12.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:12.367
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:09:12.39
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:09:13.332
    STEP: Deploying the webhook pod 03/22/23 20:09:13.343
    STEP: Wait for the deployment to be ready 03/22/23 20:09:13.36
    Mar 22 20:09:13.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:09:15.407
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:09:15.424
    Mar 22 20:09:16.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 03/22/23 20:09:16.527
    STEP: Creating a configMap that should be mutated 03/22/23 20:09:16.59
    STEP: Deleting the collection of validation webhooks 03/22/23 20:09:16.687
    STEP: Creating a configMap that should not be mutated 03/22/23 20:09:16.735
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:16.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9411" for this suite. 03/22/23 20:09:16.821
    STEP: Destroying namespace "webhook-9411-markers" for this suite. 03/22/23 20:09:16.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:16.84
Mar 22 20:09:16.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:09:16.843
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:16.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:16.89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 03/22/23 20:09:16.897
Mar 22 20:09:16.907: INFO: Waiting up to 5m0s for pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8" in namespace "downward-api-5604" to be "Succeeded or Failed"
Mar 22 20:09:16.919: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.515018ms
Mar 22 20:09:18.928: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021003318s
Mar 22 20:09:20.925: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018227872s
STEP: Saw pod success 03/22/23 20:09:20.926
Mar 22 20:09:20.926: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8" satisfied condition "Succeeded or Failed"
Mar 22 20:09:20.930: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 container dapi-container: <nil>
STEP: delete the pod 03/22/23 20:09:20.941
Mar 22 20:09:20.954: INFO: Waiting for pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 to disappear
Mar 22 20:09:20.958: INFO: Pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:20.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5604" for this suite. 03/22/23 20:09:20.966
------------------------------
• [4.140 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:16.84
    Mar 22 20:09:16.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:09:16.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:16.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:16.89
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 03/22/23 20:09:16.897
    Mar 22 20:09:16.907: INFO: Waiting up to 5m0s for pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8" in namespace "downward-api-5604" to be "Succeeded or Failed"
    Mar 22 20:09:16.919: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.515018ms
    Mar 22 20:09:18.928: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021003318s
    Mar 22 20:09:20.925: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018227872s
    STEP: Saw pod success 03/22/23 20:09:20.926
    Mar 22 20:09:20.926: INFO: Pod "downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8" satisfied condition "Succeeded or Failed"
    Mar 22 20:09:20.930: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 20:09:20.941
    Mar 22 20:09:20.954: INFO: Waiting for pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 to disappear
    Mar 22 20:09:20.958: INFO: Pod downward-api-559a4289-4c35-4fc2-b5e0-2c0ac30dc8b8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:20.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5604" for this suite. 03/22/23 20:09:20.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:20.984
Mar 22 20:09:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename hostport 03/22/23 20:09:20.986
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:21.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:21.016
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/22/23 20:09:21.03
Mar 22 20:09:21.045: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8658" to be "running and ready"
Mar 22 20:09:21.049: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856356ms
Mar 22 20:09:21.049: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:23.056: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010299508s
Mar 22 20:09:23.056: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:25.055: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009773092s
Mar 22 20:09:25.056: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 22 20:09:25.056: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.124.0.3 on the node which pod1 resides and expect scheduled 03/22/23 20:09:25.056
Mar 22 20:09:25.064: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8658" to be "running and ready"
Mar 22 20:09:25.071: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277324ms
Mar 22 20:09:25.072: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:27.080: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013324987s
Mar 22 20:09:27.088: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:29.079: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011475866s
Mar 22 20:09:29.079: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 22 20:09:29.079: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.124.0.3 but use UDP protocol on the node which pod2 resides 03/22/23 20:09:29.079
Mar 22 20:09:29.094: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8658" to be "running and ready"
Mar 22 20:09:29.099: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77889ms
Mar 22 20:09:29.099: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:31.106: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011805857s
Mar 22 20:09:31.106: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:33.105: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.011062877s
Mar 22 20:09:33.105: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 22 20:09:33.105: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 22 20:09:33.115: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8658" to be "running and ready"
Mar 22 20:09:33.123: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.648012ms
Mar 22 20:09:33.123: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:35.130: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015200995s
Mar 22 20:09:35.130: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 22 20:09:35.130: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/22/23 20:09:35.134
Mar 22 20:09:35.135: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.124.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:09:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:09:35.136: INFO: ExecWithOptions: Clientset creation
Mar 22 20:09:35.136: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.124.0.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.124.0.3, port: 54323 03/22/23 20:09:35.423
Mar 22 20:09:35.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.124.0.3:54323/hostname] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:09:35.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:09:35.424: INFO: ExecWithOptions: Clientset creation
Mar 22 20:09:35.424: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.124.0.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.124.0.3, port: 54323 UDP 03/22/23 20:09:35.68
Mar 22 20:09:35.680: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.124.0.3 54323] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:09:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:09:35.681: INFO: ExecWithOptions: Clientset creation
Mar 22 20:09:35.682: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.124.0.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8658" for this suite. 03/22/23 20:09:40.886
------------------------------
• [SLOW TEST] [19.910 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:20.984
    Mar 22 20:09:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename hostport 03/22/23 20:09:20.986
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:21.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:21.016
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/22/23 20:09:21.03
    Mar 22 20:09:21.045: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8658" to be "running and ready"
    Mar 22 20:09:21.049: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856356ms
    Mar 22 20:09:21.049: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:23.056: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010299508s
    Mar 22 20:09:23.056: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:25.055: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009773092s
    Mar 22 20:09:25.056: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 22 20:09:25.056: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.124.0.3 on the node which pod1 resides and expect scheduled 03/22/23 20:09:25.056
    Mar 22 20:09:25.064: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8658" to be "running and ready"
    Mar 22 20:09:25.071: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277324ms
    Mar 22 20:09:25.072: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:27.080: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013324987s
    Mar 22 20:09:27.088: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:29.079: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011475866s
    Mar 22 20:09:29.079: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 22 20:09:29.079: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.124.0.3 but use UDP protocol on the node which pod2 resides 03/22/23 20:09:29.079
    Mar 22 20:09:29.094: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8658" to be "running and ready"
    Mar 22 20:09:29.099: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77889ms
    Mar 22 20:09:29.099: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:31.106: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011805857s
    Mar 22 20:09:31.106: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:33.105: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.011062877s
    Mar 22 20:09:33.105: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 22 20:09:33.105: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 22 20:09:33.115: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8658" to be "running and ready"
    Mar 22 20:09:33.123: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.648012ms
    Mar 22 20:09:33.123: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:35.130: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015200995s
    Mar 22 20:09:35.130: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 22 20:09:35.130: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/22/23 20:09:35.134
    Mar 22 20:09:35.135: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.124.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:09:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:09:35.136: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:09:35.136: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.124.0.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.124.0.3, port: 54323 03/22/23 20:09:35.423
    Mar 22 20:09:35.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.124.0.3:54323/hostname] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:09:35.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:09:35.424: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:09:35.424: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.124.0.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.124.0.3, port: 54323 UDP 03/22/23 20:09:35.68
    Mar 22 20:09:35.680: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.124.0.3 54323] Namespace:hostport-8658 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:09:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:09:35.681: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:09:35.682: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/hostport-8658/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.124.0.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8658" for this suite. 03/22/23 20:09:40.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:40.897
Mar 22 20:09:40.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 20:09:40.898
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:40.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:40.924
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/22/23 20:09:40.937
Mar 22 20:09:40.947: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4525" to be "running and ready"
Mar 22 20:09:40.952: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64878ms
Mar 22 20:09:40.952: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:42.962: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015033111s
Mar 22 20:09:42.962: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 22 20:09:42.963: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 03/22/23 20:09:42.968
Mar 22 20:09:42.977: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4525" to be "running and ready"
Mar 22 20:09:42.982: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079175ms
Mar 22 20:09:42.982: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:09:44.989: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012397636s
Mar 22 20:09:44.990: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 22 20:09:44.990: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/22/23 20:09:44.995
Mar 22 20:09:45.006: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 20:09:45.010: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 20:09:47.011: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 20:09:47.017: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 22 20:09:49.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 22 20:09:49.017: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/22/23 20:09:49.018
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4525" for this suite. 03/22/23 20:09:49.096
------------------------------
• [SLOW TEST] [8.210 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:40.897
    Mar 22 20:09:40.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 20:09:40.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:40.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:40.924
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/22/23 20:09:40.937
    Mar 22 20:09:40.947: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4525" to be "running and ready"
    Mar 22 20:09:40.952: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64878ms
    Mar 22 20:09:40.952: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:42.962: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015033111s
    Mar 22 20:09:42.962: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 22 20:09:42.963: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 03/22/23 20:09:42.968
    Mar 22 20:09:42.977: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4525" to be "running and ready"
    Mar 22 20:09:42.982: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079175ms
    Mar 22 20:09:42.982: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:09:44.989: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012397636s
    Mar 22 20:09:44.990: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 22 20:09:44.990: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/22/23 20:09:44.995
    Mar 22 20:09:45.006: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 22 20:09:45.010: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 22 20:09:47.011: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 22 20:09:47.017: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 22 20:09:49.012: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 22 20:09:49.017: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/22/23 20:09:49.018
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4525" for this suite. 03/22/23 20:09:49.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:49.109
Mar 22 20:09:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:09:49.11
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:49.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:49.149
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 22 20:09:49.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6368" for this suite. 03/22/23 20:09:49.732
------------------------------
• [0.632 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:49.109
    Mar 22 20:09:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:09:49.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:49.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:49.149
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 22 20:09:49.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:49.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6368" for this suite. 03/22/23 20:09:49.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:49.741
Mar 22 20:09:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 20:09:49.745
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:49.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:49.772
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a 03/22/23 20:09:49.78
Mar 22 20:09:49.792: INFO: Pod name my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Found 0 pods out of 1
Mar 22 20:09:54.800: INFO: Pod name my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Found 1 pods out of 1
Mar 22 20:09:54.800: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a" are running
Mar 22 20:09:54.800: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" in namespace "replication-controller-5765" to be "running"
Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b": Phase="Running", Reason="", readiness=true. Elapsed: 4.866612ms
Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" satisfied condition "running"
Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:49 +0000 UTC Reason: Message:}])
Mar 22 20:09:54.805: INFO: Trying to dial the pod
Mar 22 20:09:59.852: INFO: Controller my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Got expected result from replica 1 [my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b]: "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:09:59.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5765" for this suite. 03/22/23 20:09:59.86
------------------------------
• [SLOW TEST] [10.130 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:49.741
    Mar 22 20:09:49.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 20:09:49.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:49.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:49.772
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a 03/22/23 20:09:49.78
    Mar 22 20:09:49.792: INFO: Pod name my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Found 0 pods out of 1
    Mar 22 20:09:54.800: INFO: Pod name my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Found 1 pods out of 1
    Mar 22 20:09:54.800: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a" are running
    Mar 22 20:09:54.800: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" in namespace "replication-controller-5765" to be "running"
    Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b": Phase="Running", Reason="", readiness=true. Elapsed: 4.866612ms
    Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" satisfied condition "running"
    Mar 22 20:09:54.805: INFO: Pod "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 20:09:49 +0000 UTC Reason: Message:}])
    Mar 22 20:09:54.805: INFO: Trying to dial the pod
    Mar 22 20:09:59.852: INFO: Controller my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a: Got expected result from replica 1 [my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b]: "my-hostname-basic-f5706eca-a56d-488a-9b85-06f981a76e9a-dv88b", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:09:59.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5765" for this suite. 03/22/23 20:09:59.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:09:59.874
Mar 22 20:09:59.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pod-network-test 03/22/23 20:09:59.876
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:59.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:59.923
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6024 03/22/23 20:09:59.931
STEP: creating a selector 03/22/23 20:09:59.931
STEP: Creating the service pods in kubernetes 03/22/23 20:09:59.931
Mar 22 20:09:59.932: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 22 20:09:59.994: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6024" to be "running and ready"
Mar 22 20:10:00.007: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837227ms
Mar 22 20:10:00.007: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:10:02.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020625576s
Mar 22 20:10:02.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:10:04.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020467779s
Mar 22 20:10:04.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:10:06.014: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020225016s
Mar 22 20:10:06.014: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:10:08.023: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028867825s
Mar 22 20:10:08.023: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:10:10.022: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027452559s
Mar 22 20:10:10.022: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:10:12.013: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019136924s
Mar 22 20:10:12.014: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 22 20:10:12.014: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 22 20:10:12.019: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6024" to be "running and ready"
Mar 22 20:10:12.024: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.743611ms
Mar 22 20:10:12.024: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 20:10:14.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010549518s
Mar 22 20:10:14.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 20:10:16.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010408558s
Mar 22 20:10:16.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 20:10:18.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011028365s
Mar 22 20:10:18.031: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 20:10:20.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.010583629s
Mar 22 20:10:20.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 20:10:22.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.010556003s
Mar 22 20:10:22.030: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 22 20:10:22.030: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 22 20:10:22.035: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6024" to be "running and ready"
Mar 22 20:10:22.040: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.441682ms
Mar 22 20:10:22.040: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 22 20:10:22.040: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/22/23 20:10:22.044
Mar 22 20:10:22.052: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6024" to be "running"
Mar 22 20:10:22.056: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.661266ms
Mar 22 20:10:24.068: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015661889s
Mar 22 20:10:24.069: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 22 20:10:24.075: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 22 20:10:24.076: INFO: Breadth first check of 10.244.1.19 on host 10.124.0.2...
Mar 22 20:10:24.080: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.1.19&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:10:24.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:10:24.082: INFO: ExecWithOptions: Clientset creation
Mar 22 20:10:24.082: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:10:24.254: INFO: Waiting for responses: map[]
Mar 22 20:10:24.254: INFO: reached 10.244.1.19 after 0/1 tries
Mar 22 20:10:24.254: INFO: Breadth first check of 10.244.0.61 on host 10.124.0.3...
Mar 22 20:10:24.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.0.61&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:10:24.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:10:24.260: INFO: ExecWithOptions: Clientset creation
Mar 22 20:10:24.260: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:10:24.418: INFO: Waiting for responses: map[]
Mar 22 20:10:24.418: INFO: reached 10.244.0.61 after 0/1 tries
Mar 22 20:10:24.418: INFO: Breadth first check of 10.244.0.151 on host 10.124.0.4...
Mar 22 20:10:24.424: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.0.151&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:10:24.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:10:24.425: INFO: ExecWithOptions: Clientset creation
Mar 22 20:10:24.425: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.151%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:10:24.588: INFO: Waiting for responses: map[]
Mar 22 20:10:24.588: INFO: reached 10.244.0.151 after 0/1 tries
Mar 22 20:10:24.588: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6024" for this suite. 03/22/23 20:10:24.595
------------------------------
• [SLOW TEST] [24.734 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:09:59.874
    Mar 22 20:09:59.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pod-network-test 03/22/23 20:09:59.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:09:59.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:09:59.923
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6024 03/22/23 20:09:59.931
    STEP: creating a selector 03/22/23 20:09:59.931
    STEP: Creating the service pods in kubernetes 03/22/23 20:09:59.931
    Mar 22 20:09:59.932: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 22 20:09:59.994: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6024" to be "running and ready"
    Mar 22 20:10:00.007: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.837227ms
    Mar 22 20:10:00.007: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:10:02.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020625576s
    Mar 22 20:10:02.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:10:04.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020467779s
    Mar 22 20:10:04.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:10:06.014: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020225016s
    Mar 22 20:10:06.014: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:10:08.023: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028867825s
    Mar 22 20:10:08.023: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:10:10.022: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027452559s
    Mar 22 20:10:10.022: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:10:12.013: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019136924s
    Mar 22 20:10:12.014: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 22 20:10:12.014: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 22 20:10:12.019: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6024" to be "running and ready"
    Mar 22 20:10:12.024: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.743611ms
    Mar 22 20:10:12.024: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 20:10:14.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010549518s
    Mar 22 20:10:14.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 20:10:16.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010408558s
    Mar 22 20:10:16.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 20:10:18.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011028365s
    Mar 22 20:10:18.031: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 20:10:20.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.010583629s
    Mar 22 20:10:20.030: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 20:10:22.030: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.010556003s
    Mar 22 20:10:22.030: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 22 20:10:22.030: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 22 20:10:22.035: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6024" to be "running and ready"
    Mar 22 20:10:22.040: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.441682ms
    Mar 22 20:10:22.040: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 22 20:10:22.040: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/22/23 20:10:22.044
    Mar 22 20:10:22.052: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6024" to be "running"
    Mar 22 20:10:22.056: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.661266ms
    Mar 22 20:10:24.068: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015661889s
    Mar 22 20:10:24.069: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 22 20:10:24.075: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 22 20:10:24.076: INFO: Breadth first check of 10.244.1.19 on host 10.124.0.2...
    Mar 22 20:10:24.080: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.1.19&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:10:24.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:10:24.082: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:10:24.082: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:10:24.254: INFO: Waiting for responses: map[]
    Mar 22 20:10:24.254: INFO: reached 10.244.1.19 after 0/1 tries
    Mar 22 20:10:24.254: INFO: Breadth first check of 10.244.0.61 on host 10.124.0.3...
    Mar 22 20:10:24.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.0.61&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:10:24.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:10:24.260: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:10:24.260: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:10:24.418: INFO: Waiting for responses: map[]
    Mar 22 20:10:24.418: INFO: reached 10.244.0.61 after 0/1 tries
    Mar 22 20:10:24.418: INFO: Breadth first check of 10.244.0.151 on host 10.124.0.4...
    Mar 22 20:10:24.424: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.179:9080/dial?request=hostname&protocol=udp&host=10.244.0.151&port=8081&tries=1'] Namespace:pod-network-test-6024 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:10:24.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:10:24.425: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:10:24.425: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6024/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.151%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:10:24.588: INFO: Waiting for responses: map[]
    Mar 22 20:10:24.588: INFO: reached 10.244.0.151 after 0/1 tries
    Mar 22 20:10:24.588: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6024" for this suite. 03/22/23 20:10:24.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:24.612
Mar 22 20:10:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 20:10:24.614
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:24.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:24.646
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-f44r8" 03/22/23 20:10:24.653
Mar 22 20:10:24.662: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
Mar 22 20:10:25.670: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
Mar 22 20:10:25.676: INFO: Found 1 replicas for "e2e-rc-f44r8" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-f44r8" 03/22/23 20:10:25.676
STEP: Updating a scale subresource 03/22/23 20:10:25.692
STEP: Verifying replicas where modified for replication controller "e2e-rc-f44r8" 03/22/23 20:10:25.7
Mar 22 20:10:25.712: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
Mar 22 20:10:25.721: INFO: Found 2 replicas for "e2e-rc-f44r8" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8421" for this suite. 03/22/23 20:10:25.73
------------------------------
• [1.126 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:24.612
    Mar 22 20:10:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 20:10:24.614
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:24.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:24.646
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-f44r8" 03/22/23 20:10:24.653
    Mar 22 20:10:24.662: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
    Mar 22 20:10:25.670: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
    Mar 22 20:10:25.676: INFO: Found 1 replicas for "e2e-rc-f44r8" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-f44r8" 03/22/23 20:10:25.676
    STEP: Updating a scale subresource 03/22/23 20:10:25.692
    STEP: Verifying replicas where modified for replication controller "e2e-rc-f44r8" 03/22/23 20:10:25.7
    Mar 22 20:10:25.712: INFO: Get Replication Controller "e2e-rc-f44r8" to confirm replicas
    Mar 22 20:10:25.721: INFO: Found 2 replicas for "e2e-rc-f44r8" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8421" for this suite. 03/22/23 20:10:25.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:25.741
Mar 22 20:10:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:10:25.742
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:25.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:25.77
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/22/23 20:10:25.783
STEP: getting /apis/node.k8s.io 03/22/23 20:10:25.79
STEP: getting /apis/node.k8s.io/v1 03/22/23 20:10:25.793
STEP: creating 03/22/23 20:10:25.797
STEP: watching 03/22/23 20:10:25.818
Mar 22 20:10:25.818: INFO: starting watch
STEP: getting 03/22/23 20:10:25.829
STEP: listing 03/22/23 20:10:25.838
STEP: patching 03/22/23 20:10:25.842
STEP: updating 03/22/23 20:10:25.852
Mar 22 20:10:25.860: INFO: waiting for watch events with expected annotations
STEP: deleting 03/22/23 20:10:25.86
STEP: deleting a collection 03/22/23 20:10:25.876
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:25.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6896" for this suite. 03/22/23 20:10:25.904
------------------------------
• [0.169 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:25.741
    Mar 22 20:10:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:10:25.742
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:25.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:25.77
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/22/23 20:10:25.783
    STEP: getting /apis/node.k8s.io 03/22/23 20:10:25.79
    STEP: getting /apis/node.k8s.io/v1 03/22/23 20:10:25.793
    STEP: creating 03/22/23 20:10:25.797
    STEP: watching 03/22/23 20:10:25.818
    Mar 22 20:10:25.818: INFO: starting watch
    STEP: getting 03/22/23 20:10:25.829
    STEP: listing 03/22/23 20:10:25.838
    STEP: patching 03/22/23 20:10:25.842
    STEP: updating 03/22/23 20:10:25.852
    Mar 22 20:10:25.860: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/22/23 20:10:25.86
    STEP: deleting a collection 03/22/23 20:10:25.876
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:25.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6896" for this suite. 03/22/23 20:10:25.904
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:25.911
Mar 22 20:10:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-webhook 03/22/23 20:10:25.912
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:25.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:25.938
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/22/23 20:10:25.946
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/22/23 20:10:26.946
STEP: Deploying the custom resource conversion webhook pod 03/22/23 20:10:26.957
STEP: Wait for the deployment to be ready 03/22/23 20:10:26.975
Mar 22 20:10:26.989: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:10:29.005
STEP: Verifying the service has paired with the endpoint 03/22/23 20:10:29.018
Mar 22 20:10:30.019: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 22 20:10:30.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Creating a v1 custom resource 03/22/23 20:10:32.192
STEP: Create a v2 custom resource 03/22/23 20:10:32.22
STEP: List CRs in v1 03/22/23 20:10:32.325
STEP: List CRs in v2 03/22/23 20:10:32.335
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:32.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8814" for this suite. 03/22/23 20:10:32.923
------------------------------
• [SLOW TEST] [7.025 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:25.911
    Mar 22 20:10:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-webhook 03/22/23 20:10:25.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:25.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:25.938
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/22/23 20:10:25.946
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/22/23 20:10:26.946
    STEP: Deploying the custom resource conversion webhook pod 03/22/23 20:10:26.957
    STEP: Wait for the deployment to be ready 03/22/23 20:10:26.975
    Mar 22 20:10:26.989: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:10:29.005
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:10:29.018
    Mar 22 20:10:30.019: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 22 20:10:30.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Creating a v1 custom resource 03/22/23 20:10:32.192
    STEP: Create a v2 custom resource 03/22/23 20:10:32.22
    STEP: List CRs in v1 03/22/23 20:10:32.325
    STEP: List CRs in v2 03/22/23 20:10:32.335
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:32.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8814" for this suite. 03/22/23 20:10:32.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:32.937
Mar 22 20:10:32.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:10:32.938
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:32.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:32.974
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-c93fc343-00eb-4b1c-983a-ced726b519fc 03/22/23 20:10:32.996
STEP: Creating a pod to test consume secrets 03/22/23 20:10:33.022
Mar 22 20:10:33.042: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad" in namespace "projected-4025" to be "Succeeded or Failed"
Mar 22 20:10:33.050: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.712963ms
Mar 22 20:10:35.056: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013345784s
Mar 22 20:10:37.056: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013842938s
STEP: Saw pod success 03/22/23 20:10:37.056
Mar 22 20:10:37.057: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad" satisfied condition "Succeeded or Failed"
Mar 22 20:10:37.062: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad container projected-secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:10:37.088
Mar 22 20:10:37.100: INFO: Waiting for pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad to disappear
Mar 22 20:10:37.105: INFO: Pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:37.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4025" for this suite. 03/22/23 20:10:37.114
------------------------------
• [4.184 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:32.937
    Mar 22 20:10:32.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:10:32.938
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:32.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:32.974
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-c93fc343-00eb-4b1c-983a-ced726b519fc 03/22/23 20:10:32.996
    STEP: Creating a pod to test consume secrets 03/22/23 20:10:33.022
    Mar 22 20:10:33.042: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad" in namespace "projected-4025" to be "Succeeded or Failed"
    Mar 22 20:10:33.050: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.712963ms
    Mar 22 20:10:35.056: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013345784s
    Mar 22 20:10:37.056: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013842938s
    STEP: Saw pod success 03/22/23 20:10:37.056
    Mar 22 20:10:37.057: INFO: Pod "pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad" satisfied condition "Succeeded or Failed"
    Mar 22 20:10:37.062: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:10:37.088
    Mar 22 20:10:37.100: INFO: Waiting for pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad to disappear
    Mar 22 20:10:37.105: INFO: Pod pod-projected-secrets-0a39da0e-0c64-4844-8e83-59306e2212ad no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:37.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4025" for this suite. 03/22/23 20:10:37.114
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:37.124
Mar 22 20:10:37.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename proxy 03/22/23 20:10:37.125
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:37.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:37.152
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 22 20:10:37.160: INFO: Creating pod...
Mar 22 20:10:37.170: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2760" to be "running"
Mar 22 20:10:37.174: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431953ms
Mar 22 20:10:39.183: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012969063s
Mar 22 20:10:39.183: INFO: Pod "agnhost" satisfied condition "running"
Mar 22 20:10:39.183: INFO: Creating service...
Mar 22 20:10:39.196: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=DELETE
Mar 22 20:10:39.237: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 22 20:10:39.237: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=OPTIONS
Mar 22 20:10:39.248: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 22 20:10:39.249: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=PATCH
Mar 22 20:10:39.260: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 22 20:10:39.260: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=POST
Mar 22 20:10:39.276: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 22 20:10:39.277: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=PUT
Mar 22 20:10:39.285: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 22 20:10:39.285: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 22 20:10:39.297: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 22 20:10:39.298: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 22 20:10:39.316: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 22 20:10:39.316: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 22 20:10:39.330: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 22 20:10:39.330: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=POST
Mar 22 20:10:39.356: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 22 20:10:39.356: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=PUT
Mar 22 20:10:39.368: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 22 20:10:39.368: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=GET
Mar 22 20:10:39.376: INFO: http.Client request:GET StatusCode:301
Mar 22 20:10:39.376: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=GET
Mar 22 20:10:39.383: INFO: http.Client request:GET StatusCode:301
Mar 22 20:10:39.384: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=HEAD
Mar 22 20:10:39.399: INFO: http.Client request:HEAD StatusCode:301
Mar 22 20:10:39.399: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 22 20:10:39.404: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:39.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2760" for this suite. 03/22/23 20:10:39.412
------------------------------
• [2.296 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:37.124
    Mar 22 20:10:37.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename proxy 03/22/23 20:10:37.125
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:37.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:37.152
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 22 20:10:37.160: INFO: Creating pod...
    Mar 22 20:10:37.170: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2760" to be "running"
    Mar 22 20:10:37.174: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431953ms
    Mar 22 20:10:39.183: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012969063s
    Mar 22 20:10:39.183: INFO: Pod "agnhost" satisfied condition "running"
    Mar 22 20:10:39.183: INFO: Creating service...
    Mar 22 20:10:39.196: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=DELETE
    Mar 22 20:10:39.237: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 22 20:10:39.237: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=OPTIONS
    Mar 22 20:10:39.248: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 22 20:10:39.249: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=PATCH
    Mar 22 20:10:39.260: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 22 20:10:39.260: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=POST
    Mar 22 20:10:39.276: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 22 20:10:39.277: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=PUT
    Mar 22 20:10:39.285: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 22 20:10:39.285: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 22 20:10:39.297: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 22 20:10:39.298: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 22 20:10:39.316: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 22 20:10:39.316: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 22 20:10:39.330: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 22 20:10:39.330: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=POST
    Mar 22 20:10:39.356: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 22 20:10:39.356: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 22 20:10:39.368: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 22 20:10:39.368: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=GET
    Mar 22 20:10:39.376: INFO: http.Client request:GET StatusCode:301
    Mar 22 20:10:39.376: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=GET
    Mar 22 20:10:39.383: INFO: http.Client request:GET StatusCode:301
    Mar 22 20:10:39.384: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/pods/agnhost/proxy?method=HEAD
    Mar 22 20:10:39.399: INFO: http.Client request:HEAD StatusCode:301
    Mar 22 20:10:39.399: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-2760/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 22 20:10:39.404: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:39.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2760" for this suite. 03/22/23 20:10:39.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:39.422
Mar 22 20:10:39.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:10:39.424
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:39.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:39.446
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 03/22/23 20:10:39.454
Mar 22 20:10:39.463: INFO: Waiting up to 5m0s for pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485" in namespace "emptydir-6932" to be "Succeeded or Failed"
Mar 22 20:10:39.469: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Pending", Reason="", readiness=false. Elapsed: 5.415073ms
Mar 22 20:10:41.475: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011924293s
Mar 22 20:10:43.475: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205975s
STEP: Saw pod success 03/22/23 20:10:43.475
Mar 22 20:10:43.476: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485" satisfied condition "Succeeded or Failed"
Mar 22 20:10:43.482: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 container test-container: <nil>
STEP: delete the pod 03/22/23 20:10:43.493
Mar 22 20:10:43.507: INFO: Waiting for pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 to disappear
Mar 22 20:10:43.511: INFO: Pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:43.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6932" for this suite. 03/22/23 20:10:43.522
------------------------------
• [4.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:39.422
    Mar 22 20:10:39.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:10:39.424
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:39.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:39.446
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 03/22/23 20:10:39.454
    Mar 22 20:10:39.463: INFO: Waiting up to 5m0s for pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485" in namespace "emptydir-6932" to be "Succeeded or Failed"
    Mar 22 20:10:39.469: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Pending", Reason="", readiness=false. Elapsed: 5.415073ms
    Mar 22 20:10:41.475: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011924293s
    Mar 22 20:10:43.475: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205975s
    STEP: Saw pod success 03/22/23 20:10:43.475
    Mar 22 20:10:43.476: INFO: Pod "pod-bf4eb46b-233f-4945-83f5-42736fe39485" satisfied condition "Succeeded or Failed"
    Mar 22 20:10:43.482: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:10:43.493
    Mar 22 20:10:43.507: INFO: Waiting for pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 to disappear
    Mar 22 20:10:43.511: INFO: Pod pod-bf4eb46b-233f-4945-83f5-42736fe39485 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:43.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6932" for this suite. 03/22/23 20:10:43.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:43.532
Mar 22 20:10:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:10:43.534
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:43.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:43.572
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:10:43.583
Mar 22 20:10:43.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 22 20:10:43.714: INFO: stderr: ""
Mar 22 20:10:43.714: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/22/23 20:10:43.714
STEP: verifying the pod e2e-test-httpd-pod was created 03/22/23 20:10:48.766
Mar 22 20:10:48.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 get pod e2e-test-httpd-pod -o json'
Mar 22 20:10:48.980: INFO: stderr: ""
Mar 22 20:10:48.980: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-22T20:10:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7992\",\n        \"resourceVersion\": \"10346\",\n        \"uid\": \"e160da06-2b33-4e48-b3d7-d9d84809999a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-sc48j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pool-v7t41yxh0-q56kk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-sc48j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c310fbb0be69d6176564198812ab21677427b3dc69de0eb99eeff604c68df223\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-22T20:10:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.124.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.179\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.0.179\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-22T20:10:43Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/22/23 20:10:48.981
Mar 22 20:10:48.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 replace -f -'
Mar 22 20:10:50.446: INFO: stderr: ""
Mar 22 20:10:50.446: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/22/23 20:10:50.446
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Mar 22 20:10:50.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 delete pods e2e-test-httpd-pod'
Mar 22 20:10:52.760: INFO: stderr: ""
Mar 22 20:10:52.760: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7992" for this suite. 03/22/23 20:10:52.767
------------------------------
• [SLOW TEST] [9.244 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:43.532
    Mar 22 20:10:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:10:43.534
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:43.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:43.572
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:10:43.583
    Mar 22 20:10:43.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 22 20:10:43.714: INFO: stderr: ""
    Mar 22 20:10:43.714: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/22/23 20:10:43.714
    STEP: verifying the pod e2e-test-httpd-pod was created 03/22/23 20:10:48.766
    Mar 22 20:10:48.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 get pod e2e-test-httpd-pod -o json'
    Mar 22 20:10:48.980: INFO: stderr: ""
    Mar 22 20:10:48.980: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-22T20:10:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7992\",\n        \"resourceVersion\": \"10346\",\n        \"uid\": \"e160da06-2b33-4e48-b3d7-d9d84809999a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-sc48j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pool-v7t41yxh0-q56kk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-sc48j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-22T20:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c310fbb0be69d6176564198812ab21677427b3dc69de0eb99eeff604c68df223\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-22T20:10:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.124.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.179\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.0.179\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-22T20:10:43Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/22/23 20:10:48.981
    Mar 22 20:10:48.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 replace -f -'
    Mar 22 20:10:50.446: INFO: stderr: ""
    Mar 22 20:10:50.446: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/22/23 20:10:50.446
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Mar 22 20:10:50.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7992 delete pods e2e-test-httpd-pod'
    Mar 22 20:10:52.760: INFO: stderr: ""
    Mar 22 20:10:52.760: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7992" for this suite. 03/22/23 20:10:52.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:52.794
Mar 22 20:10:52.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 20:10:52.797
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:52.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:52.822
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/22/23 20:10:52.84
STEP: Wait for the Deployment to create new ReplicaSet 03/22/23 20:10:52.848
STEP: delete the deployment 03/22/23 20:10:52.853
STEP: wait for all rs to be garbage collected 03/22/23 20:10:52.872
STEP: expected 0 pods, got 2 pods 03/22/23 20:10:52.889
STEP: Gathering metrics 03/22/23 20:10:53.403
W0322 20:10:53.414256      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 20:10:53.414: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3242" for this suite. 03/22/23 20:10:53.42
------------------------------
• [0.634 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:52.794
    Mar 22 20:10:52.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 20:10:52.797
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:52.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:52.822
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/22/23 20:10:52.84
    STEP: Wait for the Deployment to create new ReplicaSet 03/22/23 20:10:52.848
    STEP: delete the deployment 03/22/23 20:10:52.853
    STEP: wait for all rs to be garbage collected 03/22/23 20:10:52.872
    STEP: expected 0 pods, got 2 pods 03/22/23 20:10:52.889
    STEP: Gathering metrics 03/22/23 20:10:53.403
    W0322 20:10:53.414256      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 20:10:53.414: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3242" for this suite. 03/22/23 20:10:53.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:53.431
Mar 22 20:10:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename endpointslice 03/22/23 20:10:53.435
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:53.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:53.456
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Mar 22 20:10:53.476: INFO: Endpoints addresses: [100.65.20.184] , ports: [443]
Mar 22 20:10:53.476: INFO: EndpointSlices addresses: [100.65.20.184] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:53.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1187" for this suite. 03/22/23 20:10:53.482
------------------------------
• [0.059 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:53.431
    Mar 22 20:10:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename endpointslice 03/22/23 20:10:53.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:53.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:53.456
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Mar 22 20:10:53.476: INFO: Endpoints addresses: [100.65.20.184] , ports: [443]
    Mar 22 20:10:53.476: INFO: EndpointSlices addresses: [100.65.20.184] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:53.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1187" for this suite. 03/22/23 20:10:53.482
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:53.495
Mar 22 20:10:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:10:53.497
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:53.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:53.52
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 03/22/23 20:10:53.527
Mar 22 20:10:53.541: INFO: Waiting up to 5m0s for pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3" in namespace "emptydir-8490" to be "Succeeded or Failed"
Mar 22 20:10:53.548: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.766348ms
Mar 22 20:10:55.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012670701s
Mar 22 20:10:57.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012748858s
STEP: Saw pod success 03/22/23 20:10:57.554
Mar 22 20:10:57.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3" satisfied condition "Succeeded or Failed"
Mar 22 20:10:57.560: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 container test-container: <nil>
STEP: delete the pod 03/22/23 20:10:57.572
Mar 22 20:10:57.584: INFO: Waiting for pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 to disappear
Mar 22 20:10:57.588: INFO: Pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:10:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8490" for this suite. 03/22/23 20:10:57.596
------------------------------
• [4.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:53.495
    Mar 22 20:10:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:10:53.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:53.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:53.52
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/22/23 20:10:53.527
    Mar 22 20:10:53.541: INFO: Waiting up to 5m0s for pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3" in namespace "emptydir-8490" to be "Succeeded or Failed"
    Mar 22 20:10:53.548: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.766348ms
    Mar 22 20:10:55.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012670701s
    Mar 22 20:10:57.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012748858s
    STEP: Saw pod success 03/22/23 20:10:57.554
    Mar 22 20:10:57.554: INFO: Pod "pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3" satisfied condition "Succeeded or Failed"
    Mar 22 20:10:57.560: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:10:57.572
    Mar 22 20:10:57.584: INFO: Waiting for pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 to disappear
    Mar 22 20:10:57.588: INFO: Pod pod-a7334ca1-1f59-4cc5-b902-c47e18d406d3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:10:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8490" for this suite. 03/22/23 20:10:57.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:10:57.622
Mar 22 20:10:57.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:10:57.624
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:57.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:57.645
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/22/23 20:10:57.654
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local;sleep 1; done
 03/22/23 20:10:57.662
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local;sleep 1; done
 03/22/23 20:10:57.663
STEP: creating a pod to probe DNS 03/22/23 20:10:57.663
STEP: submitting the pod to kubernetes 03/22/23 20:10:57.663
Mar 22 20:10:57.674: INFO: Waiting up to 15m0s for pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b" in namespace "dns-7900" to be "running"
Mar 22 20:10:57.681: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615504ms
Mar 22 20:10:59.691: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017234963s
Mar 22 20:11:01.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013127227s
Mar 22 20:11:03.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012964794s
Mar 22 20:11:05.704: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029680677s
Mar 22 20:11:07.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013034773s
Mar 22 20:11:09.688: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Running", Reason="", readiness=true. Elapsed: 12.014485346s
Mar 22 20:11:09.688: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:11:09.688
STEP: looking for the results for each expected name from probers 03/22/23 20:11:09.693
Mar 22 20:11:09.737: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.747: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.760: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.773: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.793: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.801: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.812: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.822: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
Mar 22 20:11:09.822: INFO: Lookups using dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local]

Mar 22 20:11:14.938: INFO: DNS probes using dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b succeeded

STEP: deleting the pod 03/22/23 20:11:14.938
STEP: deleting the test headless service 03/22/23 20:11:14.957
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:14.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7900" for this suite. 03/22/23 20:11:14.974
------------------------------
• [SLOW TEST] [17.362 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:10:57.622
    Mar 22 20:10:57.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:10:57.624
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:10:57.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:10:57.645
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/22/23 20:10:57.654
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local;sleep 1; done
     03/22/23 20:10:57.662
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7900.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local;sleep 1; done
     03/22/23 20:10:57.663
    STEP: creating a pod to probe DNS 03/22/23 20:10:57.663
    STEP: submitting the pod to kubernetes 03/22/23 20:10:57.663
    Mar 22 20:10:57.674: INFO: Waiting up to 15m0s for pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b" in namespace "dns-7900" to be "running"
    Mar 22 20:10:57.681: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615504ms
    Mar 22 20:10:59.691: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017234963s
    Mar 22 20:11:01.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013127227s
    Mar 22 20:11:03.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012964794s
    Mar 22 20:11:05.704: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029680677s
    Mar 22 20:11:07.687: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013034773s
    Mar 22 20:11:09.688: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b": Phase="Running", Reason="", readiness=true. Elapsed: 12.014485346s
    Mar 22 20:11:09.688: INFO: Pod "dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:11:09.688
    STEP: looking for the results for each expected name from probers 03/22/23 20:11:09.693
    Mar 22 20:11:09.737: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.747: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.760: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.773: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.793: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.801: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.812: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.822: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local from pod dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b: the server could not find the requested resource (get pods dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b)
    Mar 22 20:11:09.822: INFO: Lookups using dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7900.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7900.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7900.svc.cluster.local jessie_udp@dns-test-service-2.dns-7900.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7900.svc.cluster.local]

    Mar 22 20:11:14.938: INFO: DNS probes using dns-7900/dns-test-8a09c427-8f71-4fdb-96a9-d44b9850365b succeeded

    STEP: deleting the pod 03/22/23 20:11:14.938
    STEP: deleting the test headless service 03/22/23 20:11:14.957
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:14.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7900" for this suite. 03/22/23 20:11:14.974
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:14.984
Mar 22 20:11:14.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 20:11:14.986
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:15.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:15.017
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Mar 22 20:11:15.034: INFO: Waiting up to 2m0s for pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" in namespace "var-expansion-9127" to be "container 0 failed with reason CreateContainerConfigError"
Mar 22 20:11:15.039: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664599ms
Mar 22 20:11:17.045: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01086139s
Mar 22 20:11:17.045: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 22 20:11:17.045: INFO: Deleting pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" in namespace "var-expansion-9127"
Mar 22 20:11:17.054: INFO: Wait up to 5m0s for pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:19.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9127" for this suite. 03/22/23 20:11:19.074
------------------------------
• [4.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:14.984
    Mar 22 20:11:14.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 20:11:14.986
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:15.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:15.017
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Mar 22 20:11:15.034: INFO: Waiting up to 2m0s for pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" in namespace "var-expansion-9127" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 22 20:11:15.039: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664599ms
    Mar 22 20:11:17.045: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01086139s
    Mar 22 20:11:17.045: INFO: Pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 22 20:11:17.045: INFO: Deleting pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" in namespace "var-expansion-9127"
    Mar 22 20:11:17.054: INFO: Wait up to 5m0s for pod "var-expansion-255c7f15-a253-4d01-b477-02acb5655393" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:19.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9127" for this suite. 03/22/23 20:11:19.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:19.088
Mar 22 20:11:19.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:11:19.09
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:19.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:19.111
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 03/22/23 20:11:19.12
Mar 22 20:11:19.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 22 20:11:19.339: INFO: stderr: ""
Mar 22 20:11:19.339: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 03/22/23 20:11:19.339
Mar 22 20:11:19.339: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 22 20:11:19.339: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6229" to be "running and ready, or succeeded"
Mar 22 20:11:19.344: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599373ms
Mar 22 20:11:19.344: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pool-v7t41yxh0-q56kk' to be 'Running' but was 'Pending'
Mar 22 20:11:21.348: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00914978s
Mar 22 20:11:21.348: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 22 20:11:21.348: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/22/23 20:11:21.348
Mar 22 20:11:21.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator'
Mar 22 20:11:21.488: INFO: stderr: ""
Mar 22 20:11:21.488: INFO: stdout: "I0322 20:11:20.347589       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/hz5 256\nI0322 20:11:20.547815       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/scl 247\nI0322 20:11:20.748624       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/z7v 468\nI0322 20:11:20.948059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/hhl 349\nI0322 20:11:21.148484       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/9dpd 597\nI0322 20:11:21.347823       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8c4 405\n"
STEP: limiting log lines 03/22/23 20:11:21.488
Mar 22 20:11:21.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --tail=1'
Mar 22 20:11:21.624: INFO: stderr: ""
Mar 22 20:11:21.624: INFO: stdout: "I0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\n"
Mar 22 20:11:21.624: INFO: got output "I0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\n"
STEP: limiting log bytes 03/22/23 20:11:21.624
Mar 22 20:11:21.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --limit-bytes=1'
Mar 22 20:11:21.775: INFO: stderr: ""
Mar 22 20:11:21.775: INFO: stdout: "I"
Mar 22 20:11:21.776: INFO: got output "I"
STEP: exposing timestamps 03/22/23 20:11:21.776
Mar 22 20:11:21.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 22 20:11:21.907: INFO: stderr: ""
Mar 22 20:11:21.907: INFO: stdout: "2023-03-22T20:11:21.748841164Z I0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\n"
Mar 22 20:11:21.907: INFO: got output "2023-03-22T20:11:21.748841164Z I0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\n"
STEP: restricting to a time range 03/22/23 20:11:21.907
Mar 22 20:11:24.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --since=1s'
Mar 22 20:11:24.535: INFO: stderr: ""
Mar 22 20:11:24.535: INFO: stdout: "I0322 20:11:23.548546       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/98k 373\nI0322 20:11:23.747852       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/d8r 571\nI0322 20:11:23.948259       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rhp 227\nI0322 20:11:24.147893       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7xv 419\nI0322 20:11:24.348309       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rtmg 443\n"
Mar 22 20:11:24.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --since=24h'
Mar 22 20:11:24.672: INFO: stderr: ""
Mar 22 20:11:24.672: INFO: stdout: "I0322 20:11:20.347589       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/hz5 256\nI0322 20:11:20.547815       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/scl 247\nI0322 20:11:20.748624       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/z7v 468\nI0322 20:11:20.948059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/hhl 349\nI0322 20:11:21.148484       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/9dpd 597\nI0322 20:11:21.347823       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8c4 405\nI0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\nI0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\nI0322 20:11:21.948167       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/lkgt 452\nI0322 20:11:22.148718       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/wv9 319\nI0322 20:11:22.348207       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/8c6 237\nI0322 20:11:22.547631       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cgfv 200\nI0322 20:11:22.748134       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/nft4 236\nI0322 20:11:22.948614       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/2s5x 303\nI0322 20:11:23.147844       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/nj8 202\nI0322 20:11:23.348125       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/jrw 260\nI0322 20:11:23.548546       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/98k 373\nI0322 20:11:23.747852       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/d8r 571\nI0322 20:11:23.948259       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rhp 227\nI0322 20:11:24.147893       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7xv 419\nI0322 20:11:24.348309       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rtmg 443\nI0322 20:11:24.547674       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/wblj 465\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Mar 22 20:11:24.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 delete pod logs-generator'
Mar 22 20:11:25.960: INFO: stderr: ""
Mar 22 20:11:25.960: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:25.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6229" for this suite. 03/22/23 20:11:25.966
------------------------------
• [SLOW TEST] [6.884 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:19.088
    Mar 22 20:11:19.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:11:19.09
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:19.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:19.111
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 03/22/23 20:11:19.12
    Mar 22 20:11:19.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 22 20:11:19.339: INFO: stderr: ""
    Mar 22 20:11:19.339: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 03/22/23 20:11:19.339
    Mar 22 20:11:19.339: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 22 20:11:19.339: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6229" to be "running and ready, or succeeded"
    Mar 22 20:11:19.344: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599373ms
    Mar 22 20:11:19.344: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pool-v7t41yxh0-q56kk' to be 'Running' but was 'Pending'
    Mar 22 20:11:21.348: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00914978s
    Mar 22 20:11:21.348: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 22 20:11:21.348: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/22/23 20:11:21.348
    Mar 22 20:11:21.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator'
    Mar 22 20:11:21.488: INFO: stderr: ""
    Mar 22 20:11:21.488: INFO: stdout: "I0322 20:11:20.347589       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/hz5 256\nI0322 20:11:20.547815       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/scl 247\nI0322 20:11:20.748624       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/z7v 468\nI0322 20:11:20.948059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/hhl 349\nI0322 20:11:21.148484       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/9dpd 597\nI0322 20:11:21.347823       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8c4 405\n"
    STEP: limiting log lines 03/22/23 20:11:21.488
    Mar 22 20:11:21.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --tail=1'
    Mar 22 20:11:21.624: INFO: stderr: ""
    Mar 22 20:11:21.624: INFO: stdout: "I0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\n"
    Mar 22 20:11:21.624: INFO: got output "I0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\n"
    STEP: limiting log bytes 03/22/23 20:11:21.624
    Mar 22 20:11:21.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --limit-bytes=1'
    Mar 22 20:11:21.775: INFO: stderr: ""
    Mar 22 20:11:21.775: INFO: stdout: "I"
    Mar 22 20:11:21.776: INFO: got output "I"
    STEP: exposing timestamps 03/22/23 20:11:21.776
    Mar 22 20:11:21.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 22 20:11:21.907: INFO: stderr: ""
    Mar 22 20:11:21.907: INFO: stdout: "2023-03-22T20:11:21.748841164Z I0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\n"
    Mar 22 20:11:21.907: INFO: got output "2023-03-22T20:11:21.748841164Z I0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\n"
    STEP: restricting to a time range 03/22/23 20:11:21.907
    Mar 22 20:11:24.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --since=1s'
    Mar 22 20:11:24.535: INFO: stderr: ""
    Mar 22 20:11:24.535: INFO: stdout: "I0322 20:11:23.548546       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/98k 373\nI0322 20:11:23.747852       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/d8r 571\nI0322 20:11:23.948259       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rhp 227\nI0322 20:11:24.147893       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7xv 419\nI0322 20:11:24.348309       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rtmg 443\n"
    Mar 22 20:11:24.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 logs logs-generator logs-generator --since=24h'
    Mar 22 20:11:24.672: INFO: stderr: ""
    Mar 22 20:11:24.672: INFO: stdout: "I0322 20:11:20.347589       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/hz5 256\nI0322 20:11:20.547815       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/scl 247\nI0322 20:11:20.748624       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/z7v 468\nI0322 20:11:20.948059       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/hhl 349\nI0322 20:11:21.148484       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/9dpd 597\nI0322 20:11:21.347823       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/8c4 405\nI0322 20:11:21.548209       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/cs2 224\nI0322 20:11:21.748687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/qhr8 315\nI0322 20:11:21.948167       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/lkgt 452\nI0322 20:11:22.148718       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/wv9 319\nI0322 20:11:22.348207       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/8c6 237\nI0322 20:11:22.547631       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cgfv 200\nI0322 20:11:22.748134       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/nft4 236\nI0322 20:11:22.948614       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/2s5x 303\nI0322 20:11:23.147844       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/nj8 202\nI0322 20:11:23.348125       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/jrw 260\nI0322 20:11:23.548546       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/98k 373\nI0322 20:11:23.747852       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/d8r 571\nI0322 20:11:23.948259       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/rhp 227\nI0322 20:11:24.147893       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7xv 419\nI0322 20:11:24.348309       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rtmg 443\nI0322 20:11:24.547674       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/wblj 465\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Mar 22 20:11:24.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6229 delete pod logs-generator'
    Mar 22 20:11:25.960: INFO: stderr: ""
    Mar 22 20:11:25.960: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:25.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6229" for this suite. 03/22/23 20:11:25.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:25.977
Mar 22 20:11:25.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 20:11:25.979
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:25.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:26.006
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Mar 22 20:11:26.017: INFO: Got root ca configmap in namespace "svcaccounts-2963"
Mar 22 20:11:26.023: INFO: Deleted root ca configmap in namespace "svcaccounts-2963"
STEP: waiting for a new root ca configmap created 03/22/23 20:11:26.524
Mar 22 20:11:26.529: INFO: Recreated root ca configmap in namespace "svcaccounts-2963"
Mar 22 20:11:26.538: INFO: Updated root ca configmap in namespace "svcaccounts-2963"
STEP: waiting for the root ca configmap reconciled 03/22/23 20:11:27.039
Mar 22 20:11:27.045: INFO: Reconciled root ca configmap in namespace "svcaccounts-2963"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:27.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2963" for this suite. 03/22/23 20:11:27.053
------------------------------
• [1.085 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:25.977
    Mar 22 20:11:25.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 20:11:25.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:25.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:26.006
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Mar 22 20:11:26.017: INFO: Got root ca configmap in namespace "svcaccounts-2963"
    Mar 22 20:11:26.023: INFO: Deleted root ca configmap in namespace "svcaccounts-2963"
    STEP: waiting for a new root ca configmap created 03/22/23 20:11:26.524
    Mar 22 20:11:26.529: INFO: Recreated root ca configmap in namespace "svcaccounts-2963"
    Mar 22 20:11:26.538: INFO: Updated root ca configmap in namespace "svcaccounts-2963"
    STEP: waiting for the root ca configmap reconciled 03/22/23 20:11:27.039
    Mar 22 20:11:27.045: INFO: Reconciled root ca configmap in namespace "svcaccounts-2963"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:27.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2963" for this suite. 03/22/23 20:11:27.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:27.065
Mar 22 20:11:27.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-runtime 03/22/23 20:11:27.066
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:27.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:27.089
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/22/23 20:11:27.107
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/22/23 20:11:43.219
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/22/23 20:11:43.223
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/22/23 20:11:43.232
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/22/23 20:11:43.232
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/22/23 20:11:43.268
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/22/23 20:11:46.293
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/22/23 20:11:48.309
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/22/23 20:11:48.315
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/22/23 20:11:48.316
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/22/23 20:11:48.34
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/22/23 20:11:49.351
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/22/23 20:11:52.385
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/22/23 20:11:52.399
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/22/23 20:11:52.399
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:52.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7677" for this suite. 03/22/23 20:11:52.438
------------------------------
• [SLOW TEST] [25.391 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:27.065
    Mar 22 20:11:27.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-runtime 03/22/23 20:11:27.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:27.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:27.089
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/22/23 20:11:27.107
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/22/23 20:11:43.219
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/22/23 20:11:43.223
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/22/23 20:11:43.232
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/22/23 20:11:43.232
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/22/23 20:11:43.268
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/22/23 20:11:46.293
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/22/23 20:11:48.309
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/22/23 20:11:48.315
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/22/23 20:11:48.316
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/22/23 20:11:48.34
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/22/23 20:11:49.351
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/22/23 20:11:52.385
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/22/23 20:11:52.399
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/22/23 20:11:52.399
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:52.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7677" for this suite. 03/22/23 20:11:52.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:52.461
Mar 22 20:11:52.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 20:11:52.462
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:52.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:52.487
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-kdm2k" 03/22/23 20:11:52.494
Mar 22 20:11:52.517: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-kdm2k-2445" 03/22/23 20:11:52.517
Mar 22 20:11:52.528: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-kdm2k-2445" 03/22/23 20:11:52.528
Mar 22 20:11:52.539: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:52.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4476" for this suite. 03/22/23 20:11:52.546
STEP: Destroying namespace "e2e-ns-kdm2k-2445" for this suite. 03/22/23 20:11:52.556
------------------------------
• [0.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:52.461
    Mar 22 20:11:52.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 20:11:52.462
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:52.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:52.487
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-kdm2k" 03/22/23 20:11:52.494
    Mar 22 20:11:52.517: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-kdm2k-2445" 03/22/23 20:11:52.517
    Mar 22 20:11:52.528: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-kdm2k-2445" 03/22/23 20:11:52.528
    Mar 22 20:11:52.539: INFO: Namespace "e2e-ns-kdm2k-2445" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:52.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4476" for this suite. 03/22/23 20:11:52.546
    STEP: Destroying namespace "e2e-ns-kdm2k-2445" for this suite. 03/22/23 20:11:52.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:52.569
Mar 22 20:11:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sysctl 03/22/23 20:11:52.571
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:52.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:52.607
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/22/23 20:11:52.619
STEP: Watching for error events or started pod 03/22/23 20:11:52.63
STEP: Waiting for pod completion 03/22/23 20:11:54.637
Mar 22 20:11:54.637: INFO: Waiting up to 3m0s for pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2" in namespace "sysctl-3834" to be "completed"
Mar 22 20:11:54.642: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970818ms
Mar 22 20:11:56.648: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011264748s
Mar 22 20:11:56.649: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/22/23 20:11:56.653
STEP: Getting logs from the pod 03/22/23 20:11:56.654
STEP: Checking that the sysctl is actually updated 03/22/23 20:11:56.675
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:11:56.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3834" for this suite. 03/22/23 20:11:56.682
------------------------------
• [4.123 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:52.569
    Mar 22 20:11:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sysctl 03/22/23 20:11:52.571
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:52.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:52.607
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/22/23 20:11:52.619
    STEP: Watching for error events or started pod 03/22/23 20:11:52.63
    STEP: Waiting for pod completion 03/22/23 20:11:54.637
    Mar 22 20:11:54.637: INFO: Waiting up to 3m0s for pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2" in namespace "sysctl-3834" to be "completed"
    Mar 22 20:11:54.642: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970818ms
    Mar 22 20:11:56.648: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011264748s
    Mar 22 20:11:56.649: INFO: Pod "sysctl-76478f0e-cc06-4757-8a9d-ab0e6698f0a2" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/22/23 20:11:56.653
    STEP: Getting logs from the pod 03/22/23 20:11:56.654
    STEP: Checking that the sysctl is actually updated 03/22/23 20:11:56.675
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:11:56.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3834" for this suite. 03/22/23 20:11:56.682
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:11:56.692
Mar 22 20:11:56.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename proxy 03/22/23 20:11:56.694
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:56.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:56.722
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/22/23 20:11:56.741
STEP: creating replication controller proxy-service-bf9w6 in namespace proxy-9970 03/22/23 20:11:56.741
I0322 20:11:56.752616      18 runners.go:193] Created replication controller with name: proxy-service-bf9w6, namespace: proxy-9970, replica count: 1
I0322 20:11:57.803819      18 runners.go:193] proxy-service-bf9w6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0322 20:11:58.803990      18 runners.go:193] proxy-service-bf9w6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:11:58.810: INFO: setup took 2.081077935s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/22/23 20:11:58.81
Mar 22 20:11:58.858: INFO: (0) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 47.674302ms)
Mar 22 20:11:58.859: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 48.223934ms)
Mar 22 20:11:58.859: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 48.428202ms)
Mar 22 20:11:58.861: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 50.594667ms)
Mar 22 20:11:58.861: INFO: (0) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 51.05144ms)
Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 51.155431ms)
Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 51.666505ms)
Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 51.122074ms)
Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 51.100329ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 59.628149ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 59.626674ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 59.763039ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 59.80519ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 59.911654ms)
Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 59.759026ms)
Mar 22 20:11:58.875: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 64.58568ms)
Mar 22 20:11:58.885: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 10.308505ms)
Mar 22 20:11:58.887: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 11.362379ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.043723ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.947725ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.862166ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.251688ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.435748ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.112852ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.337565ms)
Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 11.949874ms)
Mar 22 20:11:58.889: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.672412ms)
Mar 22 20:11:58.891: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.843027ms)
Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 17.184714ms)
Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 17.154135ms)
Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 17.368501ms)
Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 17.351536ms)
Mar 22 20:11:58.905: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 10.772267ms)
Mar 22 20:11:58.905: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 8.493057ms)
Mar 22 20:11:58.906: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 9.772694ms)
Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.987663ms)
Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.125233ms)
Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.482562ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.626263ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.93091ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.397543ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 14.826563ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.424702ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.760992ms)
Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.647643ms)
Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 16.9068ms)
Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 15.364997ms)
Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 15.181972ms)
Mar 22 20:11:58.922: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.388384ms)
Mar 22 20:11:58.922: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 9.446282ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 16.676951ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.492441ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.000161ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.624976ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 15.119784ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 15.471301ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.859881ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 14.934488ms)
Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 17.11617ms)
Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 14.750875ms)
Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 16.798609ms)
Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 16.45394ms)
Mar 22 20:11:58.931: INFO: (3) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 17.261648ms)
Mar 22 20:11:58.931: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 16.974989ms)
Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.917483ms)
Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 10.576654ms)
Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.176253ms)
Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.116232ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.83471ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.112828ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.60961ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.303729ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.374969ms)
Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.076161ms)
Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 13.030589ms)
Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.811379ms)
Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 12.864697ms)
Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 18.568942ms)
Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.405714ms)
Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.542043ms)
Mar 22 20:11:58.961: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 8.829249ms)
Mar 22 20:11:58.965: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 13.402777ms)
Mar 22 20:11:58.969: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 17.070626ms)
Mar 22 20:11:58.969: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 16.946786ms)
Mar 22 20:11:58.970: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 18.165094ms)
Mar 22 20:11:58.970: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.083591ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 18.083415ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 18.077908ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 18.386774ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.268394ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 18.183891ms)
Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 18.406475ms)
Mar 22 20:11:58.973: INFO: (5) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 20.343549ms)
Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.860248ms)
Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 23.021077ms)
Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 23.131173ms)
Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 11.704591ms)
Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.310572ms)
Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.247167ms)
Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.86888ms)
Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 11.787181ms)
Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.426978ms)
Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 19.020464ms)
Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 18.640412ms)
Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 19.628875ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 19.436882ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 19.274844ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 19.435489ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.485169ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 19.263157ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.514003ms)
Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.743128ms)
Mar 22 20:11:59.007: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 10.452349ms)
Mar 22 20:11:59.007: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 10.745725ms)
Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 19.193897ms)
Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 19.677142ms)
Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 18.882116ms)
Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.91249ms)
Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 20.016648ms)
Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 20.204886ms)
Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.695052ms)
Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 20.15875ms)
Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 20.243441ms)
Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 20.528544ms)
Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.431684ms)
Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 21.899614ms)
Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 21.65754ms)
Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 21.844626ms)
Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.090022ms)
Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 9.825383ms)
Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 10.232394ms)
Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 10.624708ms)
Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 10.321932ms)
Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.055359ms)
Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 11.440967ms)
Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.541613ms)
Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 11.732446ms)
Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.369801ms)
Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.000983ms)
Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 12.48897ms)
Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.932394ms)
Mar 22 20:11:59.034: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.652357ms)
Mar 22 20:11:59.034: INFO: (8) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 14.839751ms)
Mar 22 20:11:59.035: INFO: (8) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 15.504921ms)
Mar 22 20:11:59.051: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.549039ms)
Mar 22 20:11:59.051: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 15.641696ms)
Mar 22 20:11:59.057: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 21.148983ms)
Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 21.362321ms)
Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 22.183153ms)
Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 22.012276ms)
Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 22.664489ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 22.711351ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 23.391736ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 22.850835ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 23.361894ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 23.221723ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 23.230659ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 23.722889ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 23.260921ms)
Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.954856ms)
Mar 22 20:11:59.072: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.11899ms)
Mar 22 20:11:59.072: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 10.839901ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 14.696581ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.256685ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.487102ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.218176ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.658019ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.743993ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 13.001591ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 14.566452ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.220805ms)
Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.499167ms)
Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.633348ms)
Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 18.853204ms)
Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 19.092597ms)
Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 18.709416ms)
Mar 22 20:11:59.091: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.126733ms)
Mar 22 20:11:59.091: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.286747ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.651499ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.281784ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.626597ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 12.533977ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.254435ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.353494ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.379271ms)
Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.300818ms)
Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 17.839143ms)
Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 18.041233ms)
Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.278077ms)
Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 19.084021ms)
Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 19.226463ms)
Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.31056ms)
Mar 22 20:11:59.110: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 8.823569ms)
Mar 22 20:11:59.110: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 9.901057ms)
Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.251985ms)
Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 10.678781ms)
Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.807643ms)
Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.378164ms)
Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.553951ms)
Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 11.153416ms)
Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 11.90377ms)
Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 11.819593ms)
Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 13.276307ms)
Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.738309ms)
Mar 22 20:11:59.114: INFO: (12) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.838413ms)
Mar 22 20:11:59.114: INFO: (12) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.022448ms)
Mar 22 20:11:59.120: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 19.822876ms)
Mar 22 20:11:59.124: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 23.705831ms)
Mar 22 20:11:59.135: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 9.052637ms)
Mar 22 20:11:59.135: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 9.876174ms)
Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.510501ms)
Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.16356ms)
Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 10.973687ms)
Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.931774ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 10.975991ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 11.532177ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 11.939938ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.296061ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.382415ms)
Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.370381ms)
Mar 22 20:11:59.138: INFO: (13) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 12.783352ms)
Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.84703ms)
Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.141997ms)
Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.890418ms)
Mar 22 20:11:59.149: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 8.971849ms)
Mar 22 20:11:59.156: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.262869ms)
Mar 22 20:11:59.161: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 19.731483ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 20.115893ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 20.517924ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 20.012156ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 20.007125ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 20.077575ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 20.978017ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.899597ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 20.210243ms)
Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 21.484361ms)
Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.367177ms)
Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 24.128382ms)
Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 24.748435ms)
Mar 22 20:11:59.169: INFO: (14) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 26.758568ms)
Mar 22 20:11:59.180: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 10.26878ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.745005ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.930856ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.297396ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 12.434583ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.755127ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 13.204431ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.860786ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.995428ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.743078ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 13.404772ms)
Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.472413ms)
Mar 22 20:11:59.185: INFO: (15) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 13.381037ms)
Mar 22 20:11:59.186: INFO: (15) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 14.917143ms)
Mar 22 20:11:59.186: INFO: (15) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 15.282902ms)
Mar 22 20:11:59.190: INFO: (15) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.5995ms)
Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.632456ms)
Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 15.193212ms)
Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 16.279479ms)
Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 15.262269ms)
Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 15.646752ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 23.653415ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 23.350969ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 23.872825ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 25.415129ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 24.018266ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.93945ms)
Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 23.715487ms)
Mar 22 20:11:59.220: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 28.500266ms)
Mar 22 20:11:59.220: INFO: (16) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 28.840307ms)
Mar 22 20:11:59.226: INFO: (16) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 34.639488ms)
Mar 22 20:11:59.243: INFO: (16) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 50.735771ms)
Mar 22 20:11:59.259: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 14.508784ms)
Mar 22 20:11:59.259: INFO: (17) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.896639ms)
Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 15.497228ms)
Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 15.119229ms)
Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 15.189915ms)
Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 16.441259ms)
Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 16.489112ms)
Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 17.447497ms)
Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 16.578073ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 23.710553ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 23.368854ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 24.621705ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 24.202896ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 24.120564ms)
Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 23.835719ms)
Mar 22 20:11:59.275: INFO: (17) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 31.232094ms)
Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.862585ms)
Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.350165ms)
Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 12.679845ms)
Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.917116ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.736377ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.031068ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 13.124322ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 13.335241ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.822975ms)
Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 13.129752ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 28.21707ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 28.831924ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 29.428059ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 28.592805ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 28.573693ms)
Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 28.61305ms)
Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.780194ms)
Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.381448ms)
Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 14.372478ms)
Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 15.168324ms)
Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 24.449009ms)
Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 24.330375ms)
Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.949016ms)
Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 24.427912ms)
Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 24.241641ms)
Mar 22 20:11:59.332: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 24.842075ms)
Mar 22 20:11:59.332: INFO: (19) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 24.864908ms)
Mar 22 20:11:59.333: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 26.083849ms)
Mar 22 20:11:59.335: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 28.249135ms)
Mar 22 20:11:59.337: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 30.118021ms)
Mar 22 20:11:59.341: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 34.117916ms)
Mar 22 20:11:59.341: INFO: (19) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 33.928274ms)
STEP: deleting ReplicationController proxy-service-bf9w6 in namespace proxy-9970, will wait for the garbage collector to delete the pods 03/22/23 20:11:59.341
Mar 22 20:11:59.409: INFO: Deleting ReplicationController proxy-service-bf9w6 took: 7.263912ms
Mar 22 20:11:59.510: INFO: Terminating ReplicationController proxy-service-bf9w6 pods took: 101.285091ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 22 20:12:01.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9970" for this suite. 03/22/23 20:12:01.219
------------------------------
• [4.534 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:11:56.692
    Mar 22 20:11:56.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename proxy 03/22/23 20:11:56.694
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:11:56.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:11:56.722
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/22/23 20:11:56.741
    STEP: creating replication controller proxy-service-bf9w6 in namespace proxy-9970 03/22/23 20:11:56.741
    I0322 20:11:56.752616      18 runners.go:193] Created replication controller with name: proxy-service-bf9w6, namespace: proxy-9970, replica count: 1
    I0322 20:11:57.803819      18 runners.go:193] proxy-service-bf9w6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0322 20:11:58.803990      18 runners.go:193] proxy-service-bf9w6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:11:58.810: INFO: setup took 2.081077935s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/22/23 20:11:58.81
    Mar 22 20:11:58.858: INFO: (0) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 47.674302ms)
    Mar 22 20:11:58.859: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 48.223934ms)
    Mar 22 20:11:58.859: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 48.428202ms)
    Mar 22 20:11:58.861: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 50.594667ms)
    Mar 22 20:11:58.861: INFO: (0) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 51.05144ms)
    Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 51.155431ms)
    Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 51.666505ms)
    Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 51.122074ms)
    Mar 22 20:11:58.862: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 51.100329ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 59.628149ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 59.626674ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 59.763039ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 59.80519ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 59.911654ms)
    Mar 22 20:11:58.870: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 59.759026ms)
    Mar 22 20:11:58.875: INFO: (0) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 64.58568ms)
    Mar 22 20:11:58.885: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 10.308505ms)
    Mar 22 20:11:58.887: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 11.362379ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.043723ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.947725ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.862166ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.251688ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.435748ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.112852ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.337565ms)
    Mar 22 20:11:58.888: INFO: (1) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 11.949874ms)
    Mar 22 20:11:58.889: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.672412ms)
    Mar 22 20:11:58.891: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.843027ms)
    Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 17.184714ms)
    Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 17.154135ms)
    Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 17.368501ms)
    Mar 22 20:11:58.893: INFO: (1) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 17.351536ms)
    Mar 22 20:11:58.905: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 10.772267ms)
    Mar 22 20:11:58.905: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 8.493057ms)
    Mar 22 20:11:58.906: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 9.772694ms)
    Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.987663ms)
    Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.125233ms)
    Mar 22 20:11:58.908: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.482562ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.626263ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.93091ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.397543ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 14.826563ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.424702ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.760992ms)
    Mar 22 20:11:58.909: INFO: (2) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.647643ms)
    Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 16.9068ms)
    Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 15.364997ms)
    Mar 22 20:11:58.911: INFO: (2) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 15.181972ms)
    Mar 22 20:11:58.922: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.388384ms)
    Mar 22 20:11:58.922: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 9.446282ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 16.676951ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.492441ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.000161ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.624976ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 15.119784ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 15.471301ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.859881ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 14.934488ms)
    Mar 22 20:11:58.929: INFO: (3) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 17.11617ms)
    Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 14.750875ms)
    Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 16.798609ms)
    Mar 22 20:11:58.930: INFO: (3) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 16.45394ms)
    Mar 22 20:11:58.931: INFO: (3) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 17.261648ms)
    Mar 22 20:11:58.931: INFO: (3) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 16.974989ms)
    Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.917483ms)
    Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 10.576654ms)
    Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.176253ms)
    Mar 22 20:11:58.944: INFO: (4) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.116232ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.83471ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.112828ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.60961ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.303729ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.374969ms)
    Mar 22 20:11:58.945: INFO: (4) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.076161ms)
    Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 13.030589ms)
    Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.811379ms)
    Mar 22 20:11:58.946: INFO: (4) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 12.864697ms)
    Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 18.568942ms)
    Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.405714ms)
    Mar 22 20:11:58.951: INFO: (4) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.542043ms)
    Mar 22 20:11:58.961: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 8.829249ms)
    Mar 22 20:11:58.965: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 13.402777ms)
    Mar 22 20:11:58.969: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 17.070626ms)
    Mar 22 20:11:58.969: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 16.946786ms)
    Mar 22 20:11:58.970: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 18.165094ms)
    Mar 22 20:11:58.970: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.083591ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 18.083415ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 18.077908ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 18.386774ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 18.268394ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 18.183891ms)
    Mar 22 20:11:58.971: INFO: (5) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 18.406475ms)
    Mar 22 20:11:58.973: INFO: (5) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 20.343549ms)
    Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.860248ms)
    Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 23.021077ms)
    Mar 22 20:11:58.975: INFO: (5) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 23.131173ms)
    Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 11.704591ms)
    Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.310572ms)
    Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.247167ms)
    Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.86888ms)
    Mar 22 20:11:58.988: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 11.787181ms)
    Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.426978ms)
    Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 19.020464ms)
    Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 18.640412ms)
    Mar 22 20:11:58.995: INFO: (6) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 19.628875ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 19.436882ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 19.274844ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 19.435489ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.485169ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 19.263157ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.514003ms)
    Mar 22 20:11:58.996: INFO: (6) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.743128ms)
    Mar 22 20:11:59.007: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 10.452349ms)
    Mar 22 20:11:59.007: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 10.745725ms)
    Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 19.193897ms)
    Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 19.677142ms)
    Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 18.882116ms)
    Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 19.91249ms)
    Mar 22 20:11:59.016: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 20.016648ms)
    Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 20.204886ms)
    Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.695052ms)
    Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 20.15875ms)
    Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 20.243441ms)
    Mar 22 20:11:59.017: INFO: (7) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 20.528544ms)
    Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.431684ms)
    Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 21.899614ms)
    Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 21.65754ms)
    Mar 22 20:11:59.019: INFO: (7) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 21.844626ms)
    Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.090022ms)
    Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 9.825383ms)
    Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 10.232394ms)
    Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 10.624708ms)
    Mar 22 20:11:59.030: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 10.321932ms)
    Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.055359ms)
    Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 11.440967ms)
    Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.541613ms)
    Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 11.732446ms)
    Mar 22 20:11:59.031: INFO: (8) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.369801ms)
    Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.000983ms)
    Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 12.48897ms)
    Mar 22 20:11:59.032: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.932394ms)
    Mar 22 20:11:59.034: INFO: (8) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.652357ms)
    Mar 22 20:11:59.034: INFO: (8) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 14.839751ms)
    Mar 22 20:11:59.035: INFO: (8) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 15.504921ms)
    Mar 22 20:11:59.051: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.549039ms)
    Mar 22 20:11:59.051: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 15.641696ms)
    Mar 22 20:11:59.057: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 21.148983ms)
    Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 21.362321ms)
    Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 22.183153ms)
    Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 22.012276ms)
    Mar 22 20:11:59.058: INFO: (9) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 22.664489ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 22.711351ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 23.391736ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 22.850835ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 23.361894ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 23.221723ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 23.230659ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 23.722889ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 23.260921ms)
    Mar 22 20:11:59.059: INFO: (9) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 22.954856ms)
    Mar 22 20:11:59.072: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 11.11899ms)
    Mar 22 20:11:59.072: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 10.839901ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 14.696581ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 14.256685ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.487102ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.218176ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.658019ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.743993ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 13.001591ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 14.566452ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.220805ms)
    Mar 22 20:11:59.075: INFO: (10) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.499167ms)
    Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.633348ms)
    Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 18.853204ms)
    Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 19.092597ms)
    Mar 22 20:11:59.079: INFO: (10) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 18.709416ms)
    Mar 22 20:11:59.091: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.126733ms)
    Mar 22 20:11:59.091: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.286747ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.651499ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.281784ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.626597ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 12.533977ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.254435ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.353494ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.379271ms)
    Mar 22 20:11:59.092: INFO: (11) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 12.300818ms)
    Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 17.839143ms)
    Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 18.041233ms)
    Mar 22 20:11:59.098: INFO: (11) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.278077ms)
    Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 19.084021ms)
    Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 19.226463ms)
    Mar 22 20:11:59.099: INFO: (11) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.31056ms)
    Mar 22 20:11:59.110: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 8.823569ms)
    Mar 22 20:11:59.110: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 9.901057ms)
    Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.251985ms)
    Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 10.678781ms)
    Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 11.807643ms)
    Mar 22 20:11:59.111: INFO: (12) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 11.378164ms)
    Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.553951ms)
    Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 11.153416ms)
    Mar 22 20:11:59.112: INFO: (12) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 11.90377ms)
    Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 11.819593ms)
    Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 13.276307ms)
    Mar 22 20:11:59.113: INFO: (12) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 12.738309ms)
    Mar 22 20:11:59.114: INFO: (12) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.838413ms)
    Mar 22 20:11:59.114: INFO: (12) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.022448ms)
    Mar 22 20:11:59.120: INFO: (12) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 19.822876ms)
    Mar 22 20:11:59.124: INFO: (12) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 23.705831ms)
    Mar 22 20:11:59.135: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 9.052637ms)
    Mar 22 20:11:59.135: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 9.876174ms)
    Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 10.510501ms)
    Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.16356ms)
    Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 10.973687ms)
    Mar 22 20:11:59.136: INFO: (13) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 11.931774ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 10.975991ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 11.532177ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 11.939938ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 12.296061ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.382415ms)
    Mar 22 20:11:59.137: INFO: (13) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 11.370381ms)
    Mar 22 20:11:59.138: INFO: (13) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 12.783352ms)
    Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 13.84703ms)
    Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 14.141997ms)
    Mar 22 20:11:59.140: INFO: (13) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.890418ms)
    Mar 22 20:11:59.149: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 8.971849ms)
    Mar 22 20:11:59.156: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 14.262869ms)
    Mar 22 20:11:59.161: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 19.731483ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 20.115893ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 20.517924ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 20.012156ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 20.007125ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 20.077575ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 20.978017ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 19.899597ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 20.210243ms)
    Mar 22 20:11:59.162: INFO: (14) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 21.484361ms)
    Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.367177ms)
    Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 24.128382ms)
    Mar 22 20:11:59.165: INFO: (14) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 24.748435ms)
    Mar 22 20:11:59.169: INFO: (14) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 26.758568ms)
    Mar 22 20:11:59.180: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 10.26878ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.745005ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.930856ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 12.297396ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 12.434583ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.755127ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 13.204431ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 12.860786ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 12.995428ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 12.743078ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 13.404772ms)
    Mar 22 20:11:59.184: INFO: (15) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.472413ms)
    Mar 22 20:11:59.185: INFO: (15) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 13.381037ms)
    Mar 22 20:11:59.186: INFO: (15) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 14.917143ms)
    Mar 22 20:11:59.186: INFO: (15) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 15.282902ms)
    Mar 22 20:11:59.190: INFO: (15) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 18.5995ms)
    Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 14.632456ms)
    Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 15.193212ms)
    Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 16.279479ms)
    Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 15.262269ms)
    Mar 22 20:11:59.206: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 15.646752ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 23.653415ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 23.350969ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 23.872825ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 25.415129ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 24.018266ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.93945ms)
    Mar 22 20:11:59.215: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 23.715487ms)
    Mar 22 20:11:59.220: INFO: (16) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 28.500266ms)
    Mar 22 20:11:59.220: INFO: (16) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 28.840307ms)
    Mar 22 20:11:59.226: INFO: (16) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 34.639488ms)
    Mar 22 20:11:59.243: INFO: (16) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 50.735771ms)
    Mar 22 20:11:59.259: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 14.508784ms)
    Mar 22 20:11:59.259: INFO: (17) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.896639ms)
    Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 15.497228ms)
    Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 15.119229ms)
    Mar 22 20:11:59.260: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 15.189915ms)
    Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 16.441259ms)
    Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 16.489112ms)
    Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 17.447497ms)
    Mar 22 20:11:59.261: INFO: (17) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 16.578073ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 23.710553ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 23.368854ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 24.621705ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 24.202896ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 24.120564ms)
    Mar 22 20:11:59.268: INFO: (17) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 23.835719ms)
    Mar 22 20:11:59.275: INFO: (17) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 31.232094ms)
    Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 12.862585ms)
    Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.350165ms)
    Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 12.679845ms)
    Mar 22 20:11:59.289: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 12.917116ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 12.736377ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 13.031068ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 13.124322ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 13.335241ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 13.822975ms)
    Mar 22 20:11:59.290: INFO: (18) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 13.129752ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 28.21707ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 28.831924ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 29.428059ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 28.592805ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 28.573693ms)
    Mar 22 20:11:59.305: INFO: (18) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 28.61305ms)
    Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 13.780194ms)
    Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname1/proxy/: tls baz (200; 14.381448ms)
    Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname2/proxy/: bar (200; 14.372478ms)
    Mar 22 20:11:59.321: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:462/proxy/: tls qux (200; 15.168324ms)
    Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname1/proxy/: foo (200; 24.449009ms)
    Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/services/proxy-service-bf9w6:portname2/proxy/: bar (200; 24.330375ms)
    Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 23.949016ms)
    Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/http:proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">... (200; 24.427912ms)
    Mar 22 20:11:59.331: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:160/proxy/: foo (200; 24.241641ms)
    Mar 22 20:11:59.332: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:460/proxy/: tls baz (200; 24.842075ms)
    Mar 22 20:11:59.332: INFO: (19) /api/v1/namespaces/proxy-9970/services/https:proxy-service-bf9w6:tlsportname2/proxy/: tls qux (200; 24.864908ms)
    Mar 22 20:11:59.333: INFO: (19) /api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/https:proxy-service-bf9w6-8trjs:443/proxy/tlsrewritem... (200; 26.083849ms)
    Mar 22 20:11:59.335: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs/proxy/rewriteme">test</a> (200; 28.249135ms)
    Mar 22 20:11:59.337: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:1080/proxy/rewriteme">test<... (200; 30.118021ms)
    Mar 22 20:11:59.341: INFO: (19) /api/v1/namespaces/proxy-9970/pods/proxy-service-bf9w6-8trjs:162/proxy/: bar (200; 34.117916ms)
    Mar 22 20:11:59.341: INFO: (19) /api/v1/namespaces/proxy-9970/services/http:proxy-service-bf9w6:portname1/proxy/: foo (200; 33.928274ms)
    STEP: deleting ReplicationController proxy-service-bf9w6 in namespace proxy-9970, will wait for the garbage collector to delete the pods 03/22/23 20:11:59.341
    Mar 22 20:11:59.409: INFO: Deleting ReplicationController proxy-service-bf9w6 took: 7.263912ms
    Mar 22 20:11:59.510: INFO: Terminating ReplicationController proxy-service-bf9w6 pods took: 101.285091ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:12:01.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9970" for this suite. 03/22/23 20:12:01.219
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:12:01.228
Mar 22 20:12:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename cronjob 03/22/23 20:12:01.229
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:12:01.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:12:01.254
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/22/23 20:12:01.269
STEP: Ensuring a job is scheduled 03/22/23 20:12:01.278
STEP: Ensuring exactly one is scheduled 03/22/23 20:13:01.283
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/22/23 20:13:01.288
STEP: Ensuring the job is replaced with a new one 03/22/23 20:13:01.293
STEP: Removing cronjob 03/22/23 20:14:01.3
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9408" for this suite. 03/22/23 20:14:01.314
------------------------------
• [SLOW TEST] [120.093 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:12:01.228
    Mar 22 20:12:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename cronjob 03/22/23 20:12:01.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:12:01.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:12:01.254
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/22/23 20:12:01.269
    STEP: Ensuring a job is scheduled 03/22/23 20:12:01.278
    STEP: Ensuring exactly one is scheduled 03/22/23 20:13:01.283
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/22/23 20:13:01.288
    STEP: Ensuring the job is replaced with a new one 03/22/23 20:13:01.293
    STEP: Removing cronjob 03/22/23 20:14:01.3
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9408" for this suite. 03/22/23 20:14:01.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:01.322
Mar 22 20:14:01.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:14:01.324
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:01.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:01.35
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4724-delete-me 03/22/23 20:14:01.365
STEP: Waiting for the RuntimeClass to disappear 03/22/23 20:14:01.372
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:01.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4724" for this suite. 03/22/23 20:14:01.391
------------------------------
• [0.078 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:01.322
    Mar 22 20:14:01.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:14:01.324
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:01.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:01.35
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4724-delete-me 03/22/23 20:14:01.365
    STEP: Waiting for the RuntimeClass to disappear 03/22/23 20:14:01.372
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:01.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4724" for this suite. 03/22/23 20:14:01.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:01.4
Mar 22 20:14:01.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:14:01.401
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:01.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:01.425
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 03/22/23 20:14:01.431
Mar 22 20:14:01.441: INFO: Waiting up to 5m0s for pod "pod-fmcf9" in namespace "pods-8602" to be "running"
Mar 22 20:14:01.447: INFO: Pod "pod-fmcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295026ms
Mar 22 20:14:03.452: INFO: Pod "pod-fmcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011791266s
Mar 22 20:14:05.453: INFO: Pod "pod-fmcf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.012732593s
Mar 22 20:14:05.454: INFO: Pod "pod-fmcf9" satisfied condition "running"
STEP: patching /status 03/22/23 20:14:05.455
Mar 22 20:14:05.467: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8602" for this suite. 03/22/23 20:14:05.474
------------------------------
• [4.082 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:01.4
    Mar 22 20:14:01.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:14:01.401
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:01.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:01.425
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 03/22/23 20:14:01.431
    Mar 22 20:14:01.441: INFO: Waiting up to 5m0s for pod "pod-fmcf9" in namespace "pods-8602" to be "running"
    Mar 22 20:14:01.447: INFO: Pod "pod-fmcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295026ms
    Mar 22 20:14:03.452: INFO: Pod "pod-fmcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011791266s
    Mar 22 20:14:05.453: INFO: Pod "pod-fmcf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.012732593s
    Mar 22 20:14:05.454: INFO: Pod "pod-fmcf9" satisfied condition "running"
    STEP: patching /status 03/22/23 20:14:05.455
    Mar 22 20:14:05.467: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8602" for this suite. 03/22/23 20:14:05.474
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:05.49
Mar 22 20:14:05.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 20:14:05.492
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:05.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:05.516
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 22 20:14:05.535: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 22 20:14:10.546: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 20:14:10.546
Mar 22 20:14:10.547: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 22 20:14:12.552: INFO: Creating deployment "test-rollover-deployment"
Mar 22 20:14:12.571: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 22 20:14:14.583: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 22 20:14:14.594: INFO: Ensure that both replica sets have 1 created replica
Mar 22 20:14:14.603: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 22 20:14:14.620: INFO: Updating deployment test-rollover-deployment
Mar 22 20:14:14.620: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 22 20:14:16.629: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 22 20:14:16.646: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 22 20:14:16.656: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 20:14:16.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:14:18.668: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 20:14:18.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:14:20.669: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 20:14:20.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:14:22.675: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 20:14:22.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:14:24.668: INFO: all replica sets need to contain the pod-template-hash label
Mar 22 20:14:24.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:14:26.668: INFO: 
Mar 22 20:14:26.668: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 20:14:26.680: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2036  e0c0487e-813a-459d-b875-8c6c2fc18bec 11833 2 2023-03-22 20:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022cd078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 20:14:12 +0000 UTC,LastTransitionTime:2023-03-22 20:14:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-22 20:14:26 +0000 UTC,LastTransitionTime:2023-03-22 20:14:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 20:14:26.685: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2036  605cd805-7842-4fa5-a1c2-15c163f43dfe 11822 2 2023-03-22 20:14:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287b97 0xc001287b98}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001287c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:14:26.686: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 22 20:14:26.686: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2036  e953e790-64ae-4d48-a3f9-9801d2a38b1b 11832 2 2023-03-22 20:14:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287a67 0xc001287a68}] [] [{e2e.test Update apps/v1 2023-03-22 20:14:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001287b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:14:26.686: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2036  8c156c44-4c54-4b38-aaac-f6c4ef67e6c4 11757 2 2023-03-22 20:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287cb7 0xc001287cb8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001287d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:14:26.691: INFO: Pod "test-rollover-deployment-6c6df9974f-sjfdq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-sjfdq test-rollover-deployment-6c6df9974f- deployment-2036  d28c5cee-be74-4247-ba6e-bb07f270a5d3 11779 0 2023-03-22 20:14:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 605cd805-7842-4fa5-a1c2-15c163f43dfe 0xc0028f42b7 0xc0028f42b8}] [] [{kube-controller-manager Update v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"605cd805-7842-4fa5-a1c2-15c163f43dfe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq7lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq7lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.86,StartTime:2023-03-22 20:14:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6c53f8787ba7bd3fe08422d351c2869f9f63bd4e27b1995d3e6ce653270e9438,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:26.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2036" for this suite. 03/22/23 20:14:26.697
------------------------------
• [SLOW TEST] [21.215 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:05.49
    Mar 22 20:14:05.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 20:14:05.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:05.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:05.516
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 22 20:14:05.535: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 22 20:14:10.546: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 20:14:10.546
    Mar 22 20:14:10.547: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 22 20:14:12.552: INFO: Creating deployment "test-rollover-deployment"
    Mar 22 20:14:12.571: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 22 20:14:14.583: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 22 20:14:14.594: INFO: Ensure that both replica sets have 1 created replica
    Mar 22 20:14:14.603: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 22 20:14:14.620: INFO: Updating deployment test-rollover-deployment
    Mar 22 20:14:14.620: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 22 20:14:16.629: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 22 20:14:16.646: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 22 20:14:16.656: INFO: all replica sets need to contain the pod-template-hash label
    Mar 22 20:14:16.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:14:18.668: INFO: all replica sets need to contain the pod-template-hash label
    Mar 22 20:14:18.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:14:20.669: INFO: all replica sets need to contain the pod-template-hash label
    Mar 22 20:14:20.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:14:22.675: INFO: all replica sets need to contain the pod-template-hash label
    Mar 22 20:14:22.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:14:24.668: INFO: all replica sets need to contain the pod-template-hash label
    Mar 22 20:14:24.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 14, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:14:26.668: INFO: 
    Mar 22 20:14:26.668: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 20:14:26.680: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2036  e0c0487e-813a-459d-b875-8c6c2fc18bec 11833 2 2023-03-22 20:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022cd078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 20:14:12 +0000 UTC,LastTransitionTime:2023-03-22 20:14:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-22 20:14:26 +0000 UTC,LastTransitionTime:2023-03-22 20:14:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 22 20:14:26.685: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2036  605cd805-7842-4fa5-a1c2-15c163f43dfe 11822 2 2023-03-22 20:14:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287b97 0xc001287b98}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001287c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:14:26.686: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 22 20:14:26.686: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2036  e953e790-64ae-4d48-a3f9-9801d2a38b1b 11832 2 2023-03-22 20:14:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287a67 0xc001287a68}] [] [{e2e.test Update apps/v1 2023-03-22 20:14:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001287b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:14:26.686: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2036  8c156c44-4c54-4b38-aaac-f6c4ef67e6c4 11757 2 2023-03-22 20:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e0c0487e-813a-459d-b875-8c6c2fc18bec 0xc001287cb7 0xc001287cb8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0c0487e-813a-459d-b875-8c6c2fc18bec\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001287d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:14:26.691: INFO: Pod "test-rollover-deployment-6c6df9974f-sjfdq" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-sjfdq test-rollover-deployment-6c6df9974f- deployment-2036  d28c5cee-be74-4247-ba6e-bb07f270a5d3 11779 0 2023-03-22 20:14:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 605cd805-7842-4fa5-a1c2-15c163f43dfe 0xc0028f42b7 0xc0028f42b8}] [] [{kube-controller-manager Update v1 2023-03-22 20:14:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"605cd805-7842-4fa5-a1c2-15c163f43dfe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:14:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq7lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq7lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:14:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.86,StartTime:2023-03-22 20:14:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:14:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6c53f8787ba7bd3fe08422d351c2869f9f63bd4e27b1995d3e6ce653270e9438,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:26.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2036" for this suite. 03/22/23 20:14:26.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:26.714
Mar 22 20:14:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:14:26.716
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:26.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:26.748
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Mar 22 20:14:26.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-5098 version'
Mar 22 20:14:26.868: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 22 20:14:26.869: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:26.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5098" for this suite. 03/22/23 20:14:26.874
------------------------------
• [0.169 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:26.714
    Mar 22 20:14:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:14:26.716
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:26.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:26.748
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Mar 22 20:14:26.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-5098 version'
    Mar 22 20:14:26.868: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 22 20:14:26.869: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:26.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5098" for this suite. 03/22/23 20:14:26.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:26.883
Mar 22 20:14:26.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:14:26.885
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:26.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:26.908
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 03/22/23 20:14:26.916
Mar 22 20:14:26.916: INFO: Creating e2e-svc-a-lk77w
Mar 22 20:14:26.929: INFO: Creating e2e-svc-b-bbhrd
Mar 22 20:14:26.948: INFO: Creating e2e-svc-c-29n6c
STEP: deleting service collection 03/22/23 20:14:26.964
Mar 22 20:14:26.992: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:26.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8392" for this suite. 03/22/23 20:14:26.999
------------------------------
• [0.127 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:26.883
    Mar 22 20:14:26.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:14:26.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:26.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:26.908
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 03/22/23 20:14:26.916
    Mar 22 20:14:26.916: INFO: Creating e2e-svc-a-lk77w
    Mar 22 20:14:26.929: INFO: Creating e2e-svc-b-bbhrd
    Mar 22 20:14:26.948: INFO: Creating e2e-svc-c-29n6c
    STEP: deleting service collection 03/22/23 20:14:26.964
    Mar 22 20:14:26.992: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:26.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8392" for this suite. 03/22/23 20:14:26.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:27.026
Mar 22 20:14:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:14:27.027
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:27.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:27.053
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:14:27.101
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:14:27.111
Mar 22 20:14:27.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:27.126: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:14:28.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:28.140: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:14:29.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:14:29.141: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/22/23 20:14:29.146
Mar 22 20:14:29.152: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/22/23 20:14:29.153
Mar 22 20:14:29.172: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/22/23 20:14:29.173
Mar 22 20:14:29.178: INFO: Observed &DaemonSet event: ADDED
Mar 22 20:14:29.178: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.183: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.183: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.184: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.185: INFO: Found daemon set daemon-set in namespace daemonsets-3189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 22 20:14:29.186: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/22/23 20:14:29.186
STEP: watching for the daemon set status to be patched 03/22/23 20:14:29.2
Mar 22 20:14:29.231: INFO: Observed &DaemonSet event: ADDED
Mar 22 20:14:29.232: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.233: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.233: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.234: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.235: INFO: Observed daemon set daemon-set in namespace daemonsets-3189 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 22 20:14:29.235: INFO: Observed &DaemonSet event: MODIFIED
Mar 22 20:14:29.236: INFO: Found daemon set daemon-set in namespace daemonsets-3189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 22 20:14:29.236: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:14:29.24
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3189, will wait for the garbage collector to delete the pods 03/22/23 20:14:29.24
Mar 22 20:14:29.305: INFO: Deleting DaemonSet.extensions daemon-set took: 8.819679ms
Mar 22 20:14:29.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.411692ms
Mar 22 20:14:31.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:31.811: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 20:14:31.815: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11944"},"items":null}

Mar 22 20:14:31.819: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11945"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3189" for this suite. 03/22/23 20:14:31.859
------------------------------
• [4.842 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:27.026
    Mar 22 20:14:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:14:27.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:27.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:27.053
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:14:27.101
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:14:27.111
    Mar 22 20:14:27.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:27.126: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:14:28.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:28.140: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:14:29.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:14:29.141: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/22/23 20:14:29.146
    Mar 22 20:14:29.152: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/22/23 20:14:29.153
    Mar 22 20:14:29.172: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/22/23 20:14:29.173
    Mar 22 20:14:29.178: INFO: Observed &DaemonSet event: ADDED
    Mar 22 20:14:29.178: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.183: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.183: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.184: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.185: INFO: Found daemon set daemon-set in namespace daemonsets-3189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 22 20:14:29.186: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/22/23 20:14:29.186
    STEP: watching for the daemon set status to be patched 03/22/23 20:14:29.2
    Mar 22 20:14:29.231: INFO: Observed &DaemonSet event: ADDED
    Mar 22 20:14:29.232: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.233: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.233: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.234: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.235: INFO: Observed daemon set daemon-set in namespace daemonsets-3189 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 22 20:14:29.235: INFO: Observed &DaemonSet event: MODIFIED
    Mar 22 20:14:29.236: INFO: Found daemon set daemon-set in namespace daemonsets-3189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 22 20:14:29.236: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:14:29.24
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3189, will wait for the garbage collector to delete the pods 03/22/23 20:14:29.24
    Mar 22 20:14:29.305: INFO: Deleting DaemonSet.extensions daemon-set took: 8.819679ms
    Mar 22 20:14:29.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.411692ms
    Mar 22 20:14:31.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:31.811: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 20:14:31.815: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11944"},"items":null}

    Mar 22 20:14:31.819: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11945"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3189" for this suite. 03/22/23 20:14:31.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:31.87
Mar 22 20:14:31.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:14:31.872
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:31.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:31.907
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-e1450fbf-7696-4339-b410-a54b69ed4f95 03/22/23 20:14:31.919
STEP: Creating a pod to test consume secrets 03/22/23 20:14:31.943
Mar 22 20:14:31.954: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09" in namespace "projected-7416" to be "Succeeded or Failed"
Mar 22 20:14:31.960: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696918ms
Mar 22 20:14:33.967: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011507787s
Mar 22 20:14:35.967: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011703725s
STEP: Saw pod success 03/22/23 20:14:35.967
Mar 22 20:14:35.968: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09" satisfied condition "Succeeded or Failed"
Mar 22 20:14:35.978: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:14:36.023
Mar 22 20:14:36.038: INFO: Waiting for pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 to disappear
Mar 22 20:14:36.042: INFO: Pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:36.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7416" for this suite. 03/22/23 20:14:36.049
------------------------------
• [4.186 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:31.87
    Mar 22 20:14:31.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:14:31.872
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:31.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:31.907
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-e1450fbf-7696-4339-b410-a54b69ed4f95 03/22/23 20:14:31.919
    STEP: Creating a pod to test consume secrets 03/22/23 20:14:31.943
    Mar 22 20:14:31.954: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09" in namespace "projected-7416" to be "Succeeded or Failed"
    Mar 22 20:14:31.960: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696918ms
    Mar 22 20:14:33.967: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011507787s
    Mar 22 20:14:35.967: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011703725s
    STEP: Saw pod success 03/22/23 20:14:35.967
    Mar 22 20:14:35.968: INFO: Pod "pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09" satisfied condition "Succeeded or Failed"
    Mar 22 20:14:35.978: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:14:36.023
    Mar 22 20:14:36.038: INFO: Waiting for pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 to disappear
    Mar 22 20:14:36.042: INFO: Pod pod-projected-secrets-464ad69f-96cd-4186-b151-f178e886af09 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:36.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7416" for this suite. 03/22/23 20:14:36.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:36.069
Mar 22 20:14:36.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename endpointslice 03/22/23 20:14:36.071
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:36.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:36.102
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 03/22/23 20:14:36.109
STEP: getting /apis/discovery.k8s.io 03/22/23 20:14:36.126
STEP: getting /apis/discovery.k8s.iov1 03/22/23 20:14:36.129
STEP: creating 03/22/23 20:14:36.132
STEP: getting 03/22/23 20:14:36.161
STEP: listing 03/22/23 20:14:36.164
STEP: watching 03/22/23 20:14:36.17
Mar 22 20:14:36.171: INFO: starting watch
STEP: cluster-wide listing 03/22/23 20:14:36.175
STEP: cluster-wide watching 03/22/23 20:14:36.179
Mar 22 20:14:36.180: INFO: starting watch
STEP: patching 03/22/23 20:14:36.183
STEP: updating 03/22/23 20:14:36.191
Mar 22 20:14:36.206: INFO: waiting for watch events with expected annotations
Mar 22 20:14:36.206: INFO: saw patched and updated annotations
STEP: deleting 03/22/23 20:14:36.208
STEP: deleting a collection 03/22/23 20:14:36.223
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:36.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9653" for this suite. 03/22/23 20:14:36.265
------------------------------
• [0.219 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:36.069
    Mar 22 20:14:36.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename endpointslice 03/22/23 20:14:36.071
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:36.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:36.102
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 03/22/23 20:14:36.109
    STEP: getting /apis/discovery.k8s.io 03/22/23 20:14:36.126
    STEP: getting /apis/discovery.k8s.iov1 03/22/23 20:14:36.129
    STEP: creating 03/22/23 20:14:36.132
    STEP: getting 03/22/23 20:14:36.161
    STEP: listing 03/22/23 20:14:36.164
    STEP: watching 03/22/23 20:14:36.17
    Mar 22 20:14:36.171: INFO: starting watch
    STEP: cluster-wide listing 03/22/23 20:14:36.175
    STEP: cluster-wide watching 03/22/23 20:14:36.179
    Mar 22 20:14:36.180: INFO: starting watch
    STEP: patching 03/22/23 20:14:36.183
    STEP: updating 03/22/23 20:14:36.191
    Mar 22 20:14:36.206: INFO: waiting for watch events with expected annotations
    Mar 22 20:14:36.206: INFO: saw patched and updated annotations
    STEP: deleting 03/22/23 20:14:36.208
    STEP: deleting a collection 03/22/23 20:14:36.223
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:36.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9653" for this suite. 03/22/23 20:14:36.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:36.289
Mar 22 20:14:36.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:14:36.291
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:36.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:36.318
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Mar 22 20:14:36.358: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:14:36.366
Mar 22 20:14:36.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:36.380: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:14:37.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:37.393: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:14:38.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:14:38.393: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/22/23 20:14:38.412
STEP: Check that daemon pods images are updated. 03/22/23 20:14:38.429
Mar 22 20:14:38.435: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:38.435: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:39.453: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:39.453: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:40.457: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:40.457: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:41.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:41.452: INFO: Pod daemon-set-r722f is not available
Mar 22 20:14:41.453: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:42.460: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:43.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:43.452: INFO: Pod daemon-set-hhq7m is not available
Mar 22 20:14:44.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 22 20:14:44.452: INFO: Pod daemon-set-hhq7m is not available
Mar 22 20:14:46.453: INFO: Pod daemon-set-zgmtf is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/22/23 20:14:46.459
Mar 22 20:14:46.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 20:14:46.471: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:14:47.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:14:47.489: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:14:47.531
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3464, will wait for the garbage collector to delete the pods 03/22/23 20:14:47.531
Mar 22 20:14:47.597: INFO: Deleting DaemonSet.extensions daemon-set took: 11.701874ms
Mar 22 20:14:47.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.801256ms
Mar 22 20:14:49.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:14:49.906: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 20:14:49.911: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12261"},"items":null}

Mar 22 20:14:49.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12261"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3464" for this suite. 03/22/23 20:14:49.957
------------------------------
• [SLOW TEST] [13.678 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:36.289
    Mar 22 20:14:36.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:14:36.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:36.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:36.318
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Mar 22 20:14:36.358: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:14:36.366
    Mar 22 20:14:36.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:36.380: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:14:37.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:37.393: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:14:38.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:14:38.393: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/22/23 20:14:38.412
    STEP: Check that daemon pods images are updated. 03/22/23 20:14:38.429
    Mar 22 20:14:38.435: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:38.435: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:39.453: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:39.453: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:40.457: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:40.457: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:41.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:41.452: INFO: Pod daemon-set-r722f is not available
    Mar 22 20:14:41.453: INFO: Wrong image for pod: daemon-set-xtpwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:42.460: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:43.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:43.452: INFO: Pod daemon-set-hhq7m is not available
    Mar 22 20:14:44.452: INFO: Wrong image for pod: daemon-set-29gwk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 22 20:14:44.452: INFO: Pod daemon-set-hhq7m is not available
    Mar 22 20:14:46.453: INFO: Pod daemon-set-zgmtf is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/22/23 20:14:46.459
    Mar 22 20:14:46.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 20:14:46.471: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:14:47.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:14:47.489: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:14:47.531
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3464, will wait for the garbage collector to delete the pods 03/22/23 20:14:47.531
    Mar 22 20:14:47.597: INFO: Deleting DaemonSet.extensions daemon-set took: 11.701874ms
    Mar 22 20:14:47.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.801256ms
    Mar 22 20:14:49.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:14:49.906: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 20:14:49.911: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12261"},"items":null}

    Mar 22 20:14:49.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12261"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3464" for this suite. 03/22/23 20:14:49.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:49.968
Mar 22 20:14:49.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption 03/22/23 20:14:49.969
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:49.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:50.003
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 03/22/23 20:14:50.012
STEP: Waiting for the pdb to be processed 03/22/23 20:14:50.021
STEP: First trying to evict a pod which shouldn't be evictable 03/22/23 20:14:50.042
STEP: Waiting for all pods to be running 03/22/23 20:14:50.042
Mar 22 20:14:50.048: INFO: pods: 1 < 3
STEP: locating a running pod 03/22/23 20:14:52.065
STEP: Updating the pdb to allow a pod to be evicted 03/22/23 20:14:52.076
STEP: Waiting for the pdb to be processed 03/22/23 20:14:52.1
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/22/23 20:14:52.105
STEP: Waiting for all pods to be running 03/22/23 20:14:52.105
STEP: Waiting for the pdb to observed all healthy pods 03/22/23 20:14:52.114
STEP: Patching the pdb to disallow a pod to be evicted 03/22/23 20:14:52.148
STEP: Waiting for the pdb to be processed 03/22/23 20:14:52.176
STEP: Waiting for all pods to be running 03/22/23 20:14:52.18
Mar 22 20:14:52.185: INFO: running pods: 2 < 3
STEP: locating a running pod 03/22/23 20:14:54.192
STEP: Deleting the pdb to allow a pod to be evicted 03/22/23 20:14:54.203
STEP: Waiting for the pdb to be deleted 03/22/23 20:14:54.211
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/22/23 20:14:54.216
STEP: Waiting for all pods to be running 03/22/23 20:14:54.216
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:14:54.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9599" for this suite. 03/22/23 20:14:54.258
------------------------------
• [4.297 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:49.968
    Mar 22 20:14:49.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption 03/22/23 20:14:49.969
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:49.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:50.003
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 03/22/23 20:14:50.012
    STEP: Waiting for the pdb to be processed 03/22/23 20:14:50.021
    STEP: First trying to evict a pod which shouldn't be evictable 03/22/23 20:14:50.042
    STEP: Waiting for all pods to be running 03/22/23 20:14:50.042
    Mar 22 20:14:50.048: INFO: pods: 1 < 3
    STEP: locating a running pod 03/22/23 20:14:52.065
    STEP: Updating the pdb to allow a pod to be evicted 03/22/23 20:14:52.076
    STEP: Waiting for the pdb to be processed 03/22/23 20:14:52.1
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/22/23 20:14:52.105
    STEP: Waiting for all pods to be running 03/22/23 20:14:52.105
    STEP: Waiting for the pdb to observed all healthy pods 03/22/23 20:14:52.114
    STEP: Patching the pdb to disallow a pod to be evicted 03/22/23 20:14:52.148
    STEP: Waiting for the pdb to be processed 03/22/23 20:14:52.176
    STEP: Waiting for all pods to be running 03/22/23 20:14:52.18
    Mar 22 20:14:52.185: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/22/23 20:14:54.192
    STEP: Deleting the pdb to allow a pod to be evicted 03/22/23 20:14:54.203
    STEP: Waiting for the pdb to be deleted 03/22/23 20:14:54.211
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/22/23 20:14:54.216
    STEP: Waiting for all pods to be running 03/22/23 20:14:54.216
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:14:54.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9599" for this suite. 03/22/23 20:14:54.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:14:54.281
Mar 22 20:14:54.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename subpath 03/22/23 20:14:54.283
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:54.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:54.319
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/22/23 20:14:54.329
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-dsr8 03/22/23 20:14:54.342
STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:14:54.342
Mar 22 20:14:54.352: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dsr8" in namespace "subpath-2324" to be "Succeeded or Failed"
Mar 22 20:14:54.360: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.293651ms
Mar 22 20:14:56.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013091525s
Mar 22 20:14:58.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 4.013514061s
Mar 22 20:15:00.373: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 6.020508677s
Mar 22 20:15:02.368: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 8.015362984s
Mar 22 20:15:04.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 10.013306247s
Mar 22 20:15:06.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 12.014373646s
Mar 22 20:15:08.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 14.014682187s
Mar 22 20:15:10.371: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 16.017882489s
Mar 22 20:15:12.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 18.013336172s
Mar 22 20:15:14.369: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 20.016917035s
Mar 22 20:15:16.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=false. Elapsed: 22.014138011s
Mar 22 20:15:18.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013268495s
STEP: Saw pod success 03/22/23 20:15:18.366
Mar 22 20:15:18.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8" satisfied condition "Succeeded or Failed"
Mar 22 20:15:18.371: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-downwardapi-dsr8 container test-container-subpath-downwardapi-dsr8: <nil>
STEP: delete the pod 03/22/23 20:15:18.385
Mar 22 20:15:18.402: INFO: Waiting for pod pod-subpath-test-downwardapi-dsr8 to disappear
Mar 22 20:15:18.407: INFO: Pod pod-subpath-test-downwardapi-dsr8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dsr8 03/22/23 20:15:18.407
Mar 22 20:15:18.407: INFO: Deleting pod "pod-subpath-test-downwardapi-dsr8" in namespace "subpath-2324"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:18.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2324" for this suite. 03/22/23 20:15:18.418
------------------------------
• [SLOW TEST] [24.148 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:14:54.281
    Mar 22 20:14:54.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename subpath 03/22/23 20:14:54.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:14:54.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:14:54.319
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/22/23 20:14:54.329
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-dsr8 03/22/23 20:14:54.342
    STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:14:54.342
    Mar 22 20:14:54.352: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dsr8" in namespace "subpath-2324" to be "Succeeded or Failed"
    Mar 22 20:14:54.360: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.293651ms
    Mar 22 20:14:56.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013091525s
    Mar 22 20:14:58.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 4.013514061s
    Mar 22 20:15:00.373: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 6.020508677s
    Mar 22 20:15:02.368: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 8.015362984s
    Mar 22 20:15:04.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 10.013306247s
    Mar 22 20:15:06.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 12.014373646s
    Mar 22 20:15:08.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 14.014682187s
    Mar 22 20:15:10.371: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 16.017882489s
    Mar 22 20:15:12.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 18.013336172s
    Mar 22 20:15:14.369: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=true. Elapsed: 20.016917035s
    Mar 22 20:15:16.367: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Running", Reason="", readiness=false. Elapsed: 22.014138011s
    Mar 22 20:15:18.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013268495s
    STEP: Saw pod success 03/22/23 20:15:18.366
    Mar 22 20:15:18.366: INFO: Pod "pod-subpath-test-downwardapi-dsr8" satisfied condition "Succeeded or Failed"
    Mar 22 20:15:18.371: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-downwardapi-dsr8 container test-container-subpath-downwardapi-dsr8: <nil>
    STEP: delete the pod 03/22/23 20:15:18.385
    Mar 22 20:15:18.402: INFO: Waiting for pod pod-subpath-test-downwardapi-dsr8 to disappear
    Mar 22 20:15:18.407: INFO: Pod pod-subpath-test-downwardapi-dsr8 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-dsr8 03/22/23 20:15:18.407
    Mar 22 20:15:18.407: INFO: Deleting pod "pod-subpath-test-downwardapi-dsr8" in namespace "subpath-2324"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:18.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2324" for this suite. 03/22/23 20:15:18.418
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:18.431
Mar 22 20:15:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:15:18.433
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:18.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:18.456
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-8359 03/22/23 20:15:18.462
STEP: creating service affinity-clusterip-transition in namespace services-8359 03/22/23 20:15:18.463
STEP: creating replication controller affinity-clusterip-transition in namespace services-8359 03/22/23 20:15:18.476
I0322 20:15:18.486158      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8359, replica count: 3
I0322 20:15:21.541277      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:15:21.550: INFO: Creating new exec pod
Mar 22 20:15:21.558: INFO: Waiting up to 5m0s for pod "execpod-affinitynr6wg" in namespace "services-8359" to be "running"
Mar 22 20:15:21.564: INFO: Pod "execpod-affinitynr6wg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732451ms
Mar 22 20:15:23.570: INFO: Pod "execpod-affinitynr6wg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011028659s
Mar 22 20:15:23.570: INFO: Pod "execpod-affinitynr6wg" satisfied condition "running"
Mar 22 20:15:24.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Mar 22 20:15:24.861: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 22 20:15:24.861: INFO: stdout: ""
Mar 22 20:15:24.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c nc -v -z -w 2 10.245.206.175 80'
Mar 22 20:15:25.207: INFO: stderr: "+ nc -v -z -w 2 10.245.206.175 80\nConnection to 10.245.206.175 80 port [tcp/http] succeeded!\n"
Mar 22 20:15:25.207: INFO: stdout: ""
Mar 22 20:15:25.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.206.175:80/ ; done'
Mar 22 20:15:25.820: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n"
Mar 22 20:15:25.820: INFO: stdout: "\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-lnmxz"
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:25.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.206.175:80/ ; done'
Mar 22 20:15:26.361: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n"
Mar 22 20:15:26.361: INFO: stdout: "\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz"
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
Mar 22 20:15:26.361: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8359, will wait for the garbage collector to delete the pods 03/22/23 20:15:26.379
Mar 22 20:15:26.444: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.240269ms
Mar 22 20:15:26.545: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.074758ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:28.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8359" for this suite. 03/22/23 20:15:28.069
------------------------------
• [SLOW TEST] [9.646 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:18.431
    Mar 22 20:15:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:15:18.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:18.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:18.456
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-8359 03/22/23 20:15:18.462
    STEP: creating service affinity-clusterip-transition in namespace services-8359 03/22/23 20:15:18.463
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8359 03/22/23 20:15:18.476
    I0322 20:15:18.486158      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8359, replica count: 3
    I0322 20:15:21.541277      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:15:21.550: INFO: Creating new exec pod
    Mar 22 20:15:21.558: INFO: Waiting up to 5m0s for pod "execpod-affinitynr6wg" in namespace "services-8359" to be "running"
    Mar 22 20:15:21.564: INFO: Pod "execpod-affinitynr6wg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732451ms
    Mar 22 20:15:23.570: INFO: Pod "execpod-affinitynr6wg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011028659s
    Mar 22 20:15:23.570: INFO: Pod "execpod-affinitynr6wg" satisfied condition "running"
    Mar 22 20:15:24.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Mar 22 20:15:24.861: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 22 20:15:24.861: INFO: stdout: ""
    Mar 22 20:15:24.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c nc -v -z -w 2 10.245.206.175 80'
    Mar 22 20:15:25.207: INFO: stderr: "+ nc -v -z -w 2 10.245.206.175 80\nConnection to 10.245.206.175 80 port [tcp/http] succeeded!\n"
    Mar 22 20:15:25.207: INFO: stdout: ""
    Mar 22 20:15:25.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.206.175:80/ ; done'
    Mar 22 20:15:25.820: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n"
    Mar 22 20:15:25.820: INFO: stdout: "\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-xkzkk\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-l6b7d\naffinity-clusterip-transition-lnmxz"
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-xkzkk
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-l6b7d
    Mar 22 20:15:25.820: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:25.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-8359 exec execpod-affinitynr6wg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.206.175:80/ ; done'
    Mar 22 20:15:26.361: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.206.175:80/\n"
    Mar 22 20:15:26.361: INFO: stdout: "\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz\naffinity-clusterip-transition-lnmxz"
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Received response from host: affinity-clusterip-transition-lnmxz
    Mar 22 20:15:26.361: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8359, will wait for the garbage collector to delete the pods 03/22/23 20:15:26.379
    Mar 22 20:15:26.444: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.240269ms
    Mar 22 20:15:26.545: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.074758ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:28.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8359" for this suite. 03/22/23 20:15:28.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:28.077
Mar 22 20:15:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:15:28.078
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:28.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:28.101
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:15:28.108
Mar 22 20:15:28.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196" in namespace "downward-api-7290" to be "Succeeded or Failed"
Mar 22 20:15:28.129: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536607ms
Mar 22 20:15:30.137: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01193111s
Mar 22 20:15:32.141: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016071356s
STEP: Saw pod success 03/22/23 20:15:32.141
Mar 22 20:15:32.141: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196" satisfied condition "Succeeded or Failed"
Mar 22 20:15:32.146: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 container client-container: <nil>
STEP: delete the pod 03/22/23 20:15:32.159
Mar 22 20:15:32.172: INFO: Waiting for pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 to disappear
Mar 22 20:15:32.177: INFO: Pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7290" for this suite. 03/22/23 20:15:32.185
------------------------------
• [4.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:28.077
    Mar 22 20:15:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:15:28.078
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:28.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:28.101
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:15:28.108
    Mar 22 20:15:28.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196" in namespace "downward-api-7290" to be "Succeeded or Failed"
    Mar 22 20:15:28.129: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536607ms
    Mar 22 20:15:30.137: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01193111s
    Mar 22 20:15:32.141: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016071356s
    STEP: Saw pod success 03/22/23 20:15:32.141
    Mar 22 20:15:32.141: INFO: Pod "downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196" satisfied condition "Succeeded or Failed"
    Mar 22 20:15:32.146: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:15:32.159
    Mar 22 20:15:32.172: INFO: Waiting for pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 to disappear
    Mar 22 20:15:32.177: INFO: Pod downwardapi-volume-fda3a6db-e2f9-4783-8eee-76bf5f3dd196 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7290" for this suite. 03/22/23 20:15:32.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:32.201
Mar 22 20:15:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:15:32.202
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:32.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:32.227
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:15:32.25
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:15:33.652
STEP: Deploying the webhook pod 03/22/23 20:15:33.663
STEP: Wait for the deployment to be ready 03/22/23 20:15:33.678
Mar 22 20:15:33.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:15:35.729
STEP: Verifying the service has paired with the endpoint 03/22/23 20:15:35.751
Mar 22 20:15:36.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 03/22/23 20:15:36.76
STEP: create a pod that should be denied by the webhook 03/22/23 20:15:36.81
STEP: create a pod that causes the webhook to hang 03/22/23 20:15:36.876
STEP: create a configmap that should be denied by the webhook 03/22/23 20:15:46.888
STEP: create a configmap that should be admitted by the webhook 03/22/23 20:15:46.94
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/22/23 20:15:46.982
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/22/23 20:15:46.999
STEP: create a namespace that bypass the webhook 03/22/23 20:15:47.012
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/22/23 20:15:47.022
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:47.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6934" for this suite. 03/22/23 20:15:47.16
STEP: Destroying namespace "webhook-6934-markers" for this suite. 03/22/23 20:15:47.168
------------------------------
• [SLOW TEST] [14.976 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:32.201
    Mar 22 20:15:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:15:32.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:32.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:32.227
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:15:32.25
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:15:33.652
    STEP: Deploying the webhook pod 03/22/23 20:15:33.663
    STEP: Wait for the deployment to be ready 03/22/23 20:15:33.678
    Mar 22 20:15:33.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:15:35.729
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:15:35.751
    Mar 22 20:15:36.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 03/22/23 20:15:36.76
    STEP: create a pod that should be denied by the webhook 03/22/23 20:15:36.81
    STEP: create a pod that causes the webhook to hang 03/22/23 20:15:36.876
    STEP: create a configmap that should be denied by the webhook 03/22/23 20:15:46.888
    STEP: create a configmap that should be admitted by the webhook 03/22/23 20:15:46.94
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/22/23 20:15:46.982
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/22/23 20:15:46.999
    STEP: create a namespace that bypass the webhook 03/22/23 20:15:47.012
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/22/23 20:15:47.022
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:47.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6934" for this suite. 03/22/23 20:15:47.16
    STEP: Destroying namespace "webhook-6934-markers" for this suite. 03/22/23 20:15:47.168
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:47.184
Mar 22 20:15:47.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 20:15:47.185
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:47.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:47.233
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 22 20:15:47.293: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 20:15:47.293
Mar 22 20:15:47.293: INFO: Waiting up to 5m0s for pod "test-rs-q7c5h" in namespace "replicaset-2036" to be "running"
Mar 22 20:15:47.311: INFO: Pod "test-rs-q7c5h": Phase="Pending", Reason="", readiness=false. Elapsed: 17.829844ms
Mar 22 20:15:49.321: INFO: Pod "test-rs-q7c5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.027118947s
Mar 22 20:15:49.321: INFO: Pod "test-rs-q7c5h" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  03/22/23 20:15:49.321
Mar 22 20:15:49.341: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/22/23 20:15:49.341
W0322 20:15:49.354344      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 22 20:15:49.363: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
Mar 22 20:15:49.368: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
Mar 22 20:15:49.390: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
Mar 22 20:15:49.411: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
Mar 22 20:15:51.104: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 2, AvailableReplicas 2
Mar 22 20:15:51.121: INFO: observed Replicaset test-rs in namespace replicaset-2036 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:51.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2036" for this suite. 03/22/23 20:15:51.132
------------------------------
• [3.959 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:47.184
    Mar 22 20:15:47.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 20:15:47.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:47.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:47.233
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 22 20:15:47.293: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 20:15:47.293
    Mar 22 20:15:47.293: INFO: Waiting up to 5m0s for pod "test-rs-q7c5h" in namespace "replicaset-2036" to be "running"
    Mar 22 20:15:47.311: INFO: Pod "test-rs-q7c5h": Phase="Pending", Reason="", readiness=false. Elapsed: 17.829844ms
    Mar 22 20:15:49.321: INFO: Pod "test-rs-q7c5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.027118947s
    Mar 22 20:15:49.321: INFO: Pod "test-rs-q7c5h" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  03/22/23 20:15:49.321
    Mar 22 20:15:49.341: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/22/23 20:15:49.341
    W0322 20:15:49.354344      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 22 20:15:49.363: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
    Mar 22 20:15:49.368: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
    Mar 22 20:15:49.390: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
    Mar 22 20:15:49.411: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 1, AvailableReplicas 1
    Mar 22 20:15:51.104: INFO: observed ReplicaSet test-rs in namespace replicaset-2036 with ReadyReplicas 2, AvailableReplicas 2
    Mar 22 20:15:51.121: INFO: observed Replicaset test-rs in namespace replicaset-2036 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:51.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2036" for this suite. 03/22/23 20:15:51.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:51.148
Mar 22 20:15:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:15:51.15
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:51.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:51.181
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 03/22/23 20:15:51.194
Mar 22 20:15:51.204: INFO: Waiting up to 5m0s for pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da" in namespace "emptydir-2770" to be "Succeeded or Failed"
Mar 22 20:15:51.220: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Pending", Reason="", readiness=false. Elapsed: 15.452031ms
Mar 22 20:15:53.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022463948s
Mar 22 20:15:55.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022568425s
STEP: Saw pod success 03/22/23 20:15:55.227
Mar 22 20:15:55.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da" satisfied condition "Succeeded or Failed"
Mar 22 20:15:55.231: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da container test-container: <nil>
STEP: delete the pod 03/22/23 20:15:55.241
Mar 22 20:15:55.256: INFO: Waiting for pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da to disappear
Mar 22 20:15:55.263: INFO: Pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2770" for this suite. 03/22/23 20:15:55.27
------------------------------
• [4.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:51.148
    Mar 22 20:15:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:15:51.15
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:51.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:51.181
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/22/23 20:15:51.194
    Mar 22 20:15:51.204: INFO: Waiting up to 5m0s for pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da" in namespace "emptydir-2770" to be "Succeeded or Failed"
    Mar 22 20:15:51.220: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Pending", Reason="", readiness=false. Elapsed: 15.452031ms
    Mar 22 20:15:53.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022463948s
    Mar 22 20:15:55.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022568425s
    STEP: Saw pod success 03/22/23 20:15:55.227
    Mar 22 20:15:55.227: INFO: Pod "pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da" satisfied condition "Succeeded or Failed"
    Mar 22 20:15:55.231: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da container test-container: <nil>
    STEP: delete the pod 03/22/23 20:15:55.241
    Mar 22 20:15:55.256: INFO: Waiting for pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da to disappear
    Mar 22 20:15:55.263: INFO: Pod pod-eb537cdb-cf86-447e-88f5-7a6c9376c0da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2770" for this suite. 03/22/23 20:15:55.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:55.293
Mar 22 20:15:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context-test 03/22/23 20:15:55.295
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:55.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:55.319
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Mar 22 20:15:55.337: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7" in namespace "security-context-test-8450" to be "Succeeded or Failed"
Mar 22 20:15:55.345: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.979213ms
Mar 22 20:15:57.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014435839s
Mar 22 20:15:59.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014247815s
Mar 22 20:15:59.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 20:15:59.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8450" for this suite. 03/22/23 20:15:59.359
------------------------------
• [4.075 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:55.293
    Mar 22 20:15:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context-test 03/22/23 20:15:55.295
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:55.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:55.319
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Mar 22 20:15:55.337: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7" in namespace "security-context-test-8450" to be "Succeeded or Failed"
    Mar 22 20:15:55.345: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.979213ms
    Mar 22 20:15:57.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014435839s
    Mar 22 20:15:59.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014247815s
    Mar 22 20:15:59.352: INFO: Pod "busybox-readonly-false-994412da-e598-49c4-be1b-f083e1fbe6f7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:15:59.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8450" for this suite. 03/22/23 20:15:59.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:15:59.375
Mar 22 20:15:59.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:15:59.376
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:59.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:59.412
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-806 03/22/23 20:15:59.422
STEP: creating service affinity-nodeport-transition in namespace services-806 03/22/23 20:15:59.422
STEP: creating replication controller affinity-nodeport-transition in namespace services-806 03/22/23 20:15:59.439
I0322 20:15:59.455600      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-806, replica count: 3
I0322 20:16:02.509287      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:16:02.524: INFO: Creating new exec pod
Mar 22 20:16:02.533: INFO: Waiting up to 5m0s for pod "execpod-affinityz4q2w" in namespace "services-806" to be "running"
Mar 22 20:16:02.559: INFO: Pod "execpod-affinityz4q2w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.432092ms
Mar 22 20:16:04.565: INFO: Pod "execpod-affinityz4q2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.023463593s
Mar 22 20:16:04.565: INFO: Pod "execpod-affinityz4q2w" satisfied condition "running"
Mar 22 20:16:05.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Mar 22 20:16:05.930: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 22 20:16:05.930: INFO: stdout: ""
Mar 22 20:16:05.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.245.218.180 80'
Mar 22 20:16:06.343: INFO: stderr: "+ nc -v -z -w 2 10.245.218.180 80\nConnection to 10.245.218.180 80 port [tcp/http] succeeded!\n"
Mar 22 20:16:06.343: INFO: stdout: ""
Mar 22 20:16:06.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.124.0.3 32044'
Mar 22 20:16:06.677: INFO: stderr: "+ nc -v -z -w 2 10.124.0.3 32044\nConnection to 10.124.0.3 32044 port [tcp/*] succeeded!\n"
Mar 22 20:16:06.677: INFO: stdout: ""
Mar 22 20:16:06.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 32044'
Mar 22 20:16:07.103: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 32044\nConnection to 10.124.0.2 32044 port [tcp/*] succeeded!\n"
Mar 22 20:16:07.103: INFO: stdout: ""
Mar 22 20:16:07.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32044/ ; done'
Mar 22 20:16:07.793: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n"
Mar 22 20:16:07.794: INFO: stdout: "\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-c68vx"
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
Mar 22 20:16:07.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32044/ ; done'
Mar 22 20:16:08.391: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n"
Mar 22 20:16:08.391: INFO: stdout: "\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq"
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
Mar 22 20:16:08.391: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-806, will wait for the garbage collector to delete the pods 03/22/23 20:16:08.404
Mar 22 20:16:08.471: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.187192ms
Mar 22 20:16:08.572: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.80508ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:16:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-806" for this suite. 03/22/23 20:16:10.417
------------------------------
• [SLOW TEST] [11.051 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:15:59.375
    Mar 22 20:15:59.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:15:59.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:15:59.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:15:59.412
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-806 03/22/23 20:15:59.422
    STEP: creating service affinity-nodeport-transition in namespace services-806 03/22/23 20:15:59.422
    STEP: creating replication controller affinity-nodeport-transition in namespace services-806 03/22/23 20:15:59.439
    I0322 20:15:59.455600      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-806, replica count: 3
    I0322 20:16:02.509287      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:16:02.524: INFO: Creating new exec pod
    Mar 22 20:16:02.533: INFO: Waiting up to 5m0s for pod "execpod-affinityz4q2w" in namespace "services-806" to be "running"
    Mar 22 20:16:02.559: INFO: Pod "execpod-affinityz4q2w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.432092ms
    Mar 22 20:16:04.565: INFO: Pod "execpod-affinityz4q2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.023463593s
    Mar 22 20:16:04.565: INFO: Pod "execpod-affinityz4q2w" satisfied condition "running"
    Mar 22 20:16:05.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Mar 22 20:16:05.930: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 22 20:16:05.930: INFO: stdout: ""
    Mar 22 20:16:05.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.245.218.180 80'
    Mar 22 20:16:06.343: INFO: stderr: "+ nc -v -z -w 2 10.245.218.180 80\nConnection to 10.245.218.180 80 port [tcp/http] succeeded!\n"
    Mar 22 20:16:06.343: INFO: stdout: ""
    Mar 22 20:16:06.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.124.0.3 32044'
    Mar 22 20:16:06.677: INFO: stderr: "+ nc -v -z -w 2 10.124.0.3 32044\nConnection to 10.124.0.3 32044 port [tcp/*] succeeded!\n"
    Mar 22 20:16:06.677: INFO: stdout: ""
    Mar 22 20:16:06.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 32044'
    Mar 22 20:16:07.103: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 32044\nConnection to 10.124.0.2 32044 port [tcp/*] succeeded!\n"
    Mar 22 20:16:07.103: INFO: stdout: ""
    Mar 22 20:16:07.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32044/ ; done'
    Mar 22 20:16:07.793: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n"
    Mar 22 20:16:07.794: INFO: stdout: "\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-c68vx\naffinity-nodeport-transition-d4ld9\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-c68vx"
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-d4ld9
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:07.794: INFO: Received response from host: affinity-nodeport-transition-c68vx
    Mar 22 20:16:07.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-806 exec execpod-affinityz4q2w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32044/ ; done'
    Mar 22 20:16:08.391: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32044/\n"
    Mar 22 20:16:08.391: INFO: stdout: "\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq\naffinity-nodeport-transition-6d4pq"
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Received response from host: affinity-nodeport-transition-6d4pq
    Mar 22 20:16:08.391: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-806, will wait for the garbage collector to delete the pods 03/22/23 20:16:08.404
    Mar 22 20:16:08.471: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.187192ms
    Mar 22 20:16:08.572: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.80508ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:16:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-806" for this suite. 03/22/23 20:16:10.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:16:10.427
Mar 22 20:16:10.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:16:10.428
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:10.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:10.463
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 03/22/23 20:16:10.474
STEP: Getting a ResourceQuota 03/22/23 20:16:10.481
STEP: Updating a ResourceQuota 03/22/23 20:16:10.488
STEP: Verifying a ResourceQuota was modified 03/22/23 20:16:10.496
STEP: Deleting a ResourceQuota 03/22/23 20:16:10.502
STEP: Verifying the deleted ResourceQuota 03/22/23 20:16:10.51
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:16:10.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1314" for this suite. 03/22/23 20:16:10.521
------------------------------
• [0.103 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:16:10.427
    Mar 22 20:16:10.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:16:10.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:10.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:10.463
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 03/22/23 20:16:10.474
    STEP: Getting a ResourceQuota 03/22/23 20:16:10.481
    STEP: Updating a ResourceQuota 03/22/23 20:16:10.488
    STEP: Verifying a ResourceQuota was modified 03/22/23 20:16:10.496
    STEP: Deleting a ResourceQuota 03/22/23 20:16:10.502
    STEP: Verifying the deleted ResourceQuota 03/22/23 20:16:10.51
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:16:10.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1314" for this suite. 03/22/23 20:16:10.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:16:10.531
Mar 22 20:16:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename init-container 03/22/23 20:16:10.533
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:10.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:10.561
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 03/22/23 20:16:10.568
Mar 22 20:16:10.568: INFO: PodSpec: initContainers in spec.initContainers
Mar 22 20:16:49.392: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8c91dc84-c8fe-47bd-a440-3e1637ed3e77", GenerateName:"", Namespace:"init-container-6694", SelfLink:"", UID:"64e64c56-f7de-44bf-8188-ea730071e2e5", ResourceVersion:"13505", Generation:0, CreationTimestamp:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"568317757"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008de060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 22, 20, 16, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008de090), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-cvnwp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0054040e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004ac80e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pool-v7t41yxh0-q56kk", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004b6e000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004ac8160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004ac8180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004ac8188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004ac818c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001730050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.124.0.4", PodIP:"10.244.0.175", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.175"}}, StartTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b6e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b6e150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://04f158df7e04cc103f3a380d2d02891a6611d7dc9d6a54ed31a60fc26c1fe86f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005404180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005404140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004ac820f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:16:49.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6694" for this suite. 03/22/23 20:16:49.405
------------------------------
• [SLOW TEST] [38.889 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:16:10.531
    Mar 22 20:16:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename init-container 03/22/23 20:16:10.533
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:10.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:10.561
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 03/22/23 20:16:10.568
    Mar 22 20:16:10.568: INFO: PodSpec: initContainers in spec.initContainers
    Mar 22 20:16:49.392: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8c91dc84-c8fe-47bd-a440-3e1637ed3e77", GenerateName:"", Namespace:"init-container-6694", SelfLink:"", UID:"64e64c56-f7de-44bf-8188-ea730071e2e5", ResourceVersion:"13505", Generation:0, CreationTimestamp:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"568317757"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008de060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 22, 20, 16, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008de090), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-cvnwp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0054040e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-cvnwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004ac80e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pool-v7t41yxh0-q56kk", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004b6e000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004ac8160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004ac8180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004ac8188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004ac818c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001730050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.124.0.4", PodIP:"10.244.0.175", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.175"}}, StartTime:time.Date(2023, time.March, 22, 20, 16, 10, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b6e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b6e150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://04f158df7e04cc103f3a380d2d02891a6611d7dc9d6a54ed31a60fc26c1fe86f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005404180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005404140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004ac820f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:16:49.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6694" for this suite. 03/22/23 20:16:49.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:16:49.443
Mar 22 20:16:49.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename watch 03/22/23 20:16:49.445
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:49.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:49.476
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/22/23 20:16:49.484
STEP: creating a new configmap 03/22/23 20:16:49.487
STEP: modifying the configmap once 03/22/23 20:16:49.494
STEP: changing the label value of the configmap 03/22/23 20:16:49.506
STEP: Expecting to observe a delete notification for the watched object 03/22/23 20:16:49.522
Mar 22 20:16:49.522: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13510 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:16:49.523: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13511 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:16:49.523: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13512 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/22/23 20:16:49.524
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/22/23 20:16:49.54
STEP: changing the label value of the configmap back 03/22/23 20:16:59.542
STEP: modifying the configmap a third time 03/22/23 20:16:59.555
STEP: deleting the configmap 03/22/23 20:16:59.573
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/22/23 20:16:59.58
Mar 22 20:16:59.580: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13570 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:16:59.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13571 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:16:59.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13572 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:16:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4740" for this suite. 03/22/23 20:16:59.598
------------------------------
• [SLOW TEST] [10.162 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:16:49.443
    Mar 22 20:16:49.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename watch 03/22/23 20:16:49.445
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:49.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:49.476
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/22/23 20:16:49.484
    STEP: creating a new configmap 03/22/23 20:16:49.487
    STEP: modifying the configmap once 03/22/23 20:16:49.494
    STEP: changing the label value of the configmap 03/22/23 20:16:49.506
    STEP: Expecting to observe a delete notification for the watched object 03/22/23 20:16:49.522
    Mar 22 20:16:49.522: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13510 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:16:49.523: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13511 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:16:49.523: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13512 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/22/23 20:16:49.524
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/22/23 20:16:49.54
    STEP: changing the label value of the configmap back 03/22/23 20:16:59.542
    STEP: modifying the configmap a third time 03/22/23 20:16:59.555
    STEP: deleting the configmap 03/22/23 20:16:59.573
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/22/23 20:16:59.58
    Mar 22 20:16:59.580: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13570 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:16:59.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13571 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:16:59.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4740  a06792f1-bfa5-46f4-bcbd-b4d775ed0696 13572 0 2023-03-22 20:16:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-22 20:16:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:16:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4740" for this suite. 03/22/23 20:16:59.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:16:59.617
Mar 22 20:16:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:16:59.619
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:59.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:59.643
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:16:59.652
Mar 22 20:16:59.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 22 20:16:59.852: INFO: stderr: ""
Mar 22 20:16:59.854: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/22/23 20:16:59.854
Mar 22 20:16:59.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Mar 22 20:17:00.280: INFO: stderr: ""
Mar 22 20:17:00.280: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:17:00.28
Mar 22 20:17:00.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 delete pods e2e-test-httpd-pod'
Mar 22 20:17:02.471: INFO: stderr: ""
Mar 22 20:17:02.471: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:02.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3072" for this suite. 03/22/23 20:17:02.48
------------------------------
• [2.871 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:16:59.617
    Mar 22 20:16:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:16:59.619
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:16:59.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:16:59.643
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:16:59.652
    Mar 22 20:16:59.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 22 20:16:59.852: INFO: stderr: ""
    Mar 22 20:16:59.854: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/22/23 20:16:59.854
    Mar 22 20:16:59.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Mar 22 20:17:00.280: INFO: stderr: ""
    Mar 22 20:17:00.280: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 20:17:00.28
    Mar 22 20:17:00.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3072 delete pods e2e-test-httpd-pod'
    Mar 22 20:17:02.471: INFO: stderr: ""
    Mar 22 20:17:02.471: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:02.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3072" for this suite. 03/22/23 20:17:02.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:02.493
Mar 22 20:17:02.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-webhook 03/22/23 20:17:02.495
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:02.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:02.529
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/22/23 20:17:02.535
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/22/23 20:17:02.926
STEP: Deploying the custom resource conversion webhook pod 03/22/23 20:17:02.936
STEP: Wait for the deployment to be ready 03/22/23 20:17:02.953
Mar 22 20:17:02.975: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:17:04.994
STEP: Verifying the service has paired with the endpoint 03/22/23 20:17:05.013
Mar 22 20:17:06.014: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 22 20:17:06.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Creating a v1 custom resource 03/22/23 20:17:08.68
STEP: v2 custom resource should be converted 03/22/23 20:17:08.688
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:09.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6454" for this suite. 03/22/23 20:17:09.295
------------------------------
• [SLOW TEST] [6.827 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:02.493
    Mar 22 20:17:02.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-webhook 03/22/23 20:17:02.495
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:02.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:02.529
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/22/23 20:17:02.535
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/22/23 20:17:02.926
    STEP: Deploying the custom resource conversion webhook pod 03/22/23 20:17:02.936
    STEP: Wait for the deployment to be ready 03/22/23 20:17:02.953
    Mar 22 20:17:02.975: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:17:04.994
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:17:05.013
    Mar 22 20:17:06.014: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 22 20:17:06.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Creating a v1 custom resource 03/22/23 20:17:08.68
    STEP: v2 custom resource should be converted 03/22/23 20:17:08.688
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:09.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6454" for this suite. 03/22/23 20:17:09.295
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:09.32
Mar 22 20:17:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:17:09.321
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:09.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:09.367
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 03/22/23 20:17:09.378
STEP: submitting the pod to kubernetes 03/22/23 20:17:09.378
Mar 22 20:17:09.389: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" in namespace "pods-7815" to be "running and ready"
Mar 22 20:17:09.399: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.697612ms
Mar 22 20:17:09.404: INFO: The phase of Pod pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:17:11.412: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022687519s
Mar 22 20:17:11.412: INFO: The phase of Pod pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f is Running (Ready = true)
Mar 22 20:17:11.412: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/22/23 20:17:11.418
STEP: updating the pod 03/22/23 20:17:11.423
Mar 22 20:17:11.941: INFO: Successfully updated pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f"
Mar 22 20:17:11.941: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" in namespace "pods-7815" to be "terminated with reason DeadlineExceeded"
Mar 22 20:17:11.946: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 4.269313ms
Mar 22 20:17:13.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011390615s
Mar 22 20:17:15.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=false. Elapsed: 4.011792352s
Mar 22 20:17:17.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01097194s
Mar 22 20:17:17.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:17.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7815" for this suite. 03/22/23 20:17:17.96
------------------------------
• [SLOW TEST] [8.650 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:09.32
    Mar 22 20:17:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:17:09.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:09.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:09.367
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 03/22/23 20:17:09.378
    STEP: submitting the pod to kubernetes 03/22/23 20:17:09.378
    Mar 22 20:17:09.389: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" in namespace "pods-7815" to be "running and ready"
    Mar 22 20:17:09.399: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.697612ms
    Mar 22 20:17:09.404: INFO: The phase of Pod pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:17:11.412: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022687519s
    Mar 22 20:17:11.412: INFO: The phase of Pod pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f is Running (Ready = true)
    Mar 22 20:17:11.412: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/22/23 20:17:11.418
    STEP: updating the pod 03/22/23 20:17:11.423
    Mar 22 20:17:11.941: INFO: Successfully updated pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f"
    Mar 22 20:17:11.941: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" in namespace "pods-7815" to be "terminated with reason DeadlineExceeded"
    Mar 22 20:17:11.946: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 4.269313ms
    Mar 22 20:17:13.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011390615s
    Mar 22 20:17:15.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Running", Reason="", readiness=false. Elapsed: 4.011792352s
    Mar 22 20:17:17.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.01097194s
    Mar 22 20:17:17.953: INFO: Pod "pod-update-activedeadlineseconds-80ff068d-2047-4c4a-b7df-c410e3be890f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:17.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7815" for this suite. 03/22/23 20:17:17.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:17.983
Mar 22 20:17:17.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:17:17.985
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:18.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:18.007
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/22/23 20:17:18.015
Mar 22 20:17:18.025: INFO: Waiting up to 5m0s for pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da" in namespace "emptydir-5912" to be "Succeeded or Failed"
Mar 22 20:17:18.030: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872546ms
Mar 22 20:17:20.037: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012675163s
Mar 22 20:17:22.036: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0116343s
STEP: Saw pod success 03/22/23 20:17:22.036
Mar 22 20:17:22.037: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da" satisfied condition "Succeeded or Failed"
Mar 22 20:17:22.042: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da container test-container: <nil>
STEP: delete the pod 03/22/23 20:17:22.054
Mar 22 20:17:22.070: INFO: Waiting for pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da to disappear
Mar 22 20:17:22.074: INFO: Pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:22.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5912" for this suite. 03/22/23 20:17:22.081
------------------------------
• [4.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:17.983
    Mar 22 20:17:17.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:17:17.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:18.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:18.007
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/22/23 20:17:18.015
    Mar 22 20:17:18.025: INFO: Waiting up to 5m0s for pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da" in namespace "emptydir-5912" to be "Succeeded or Failed"
    Mar 22 20:17:18.030: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872546ms
    Mar 22 20:17:20.037: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012675163s
    Mar 22 20:17:22.036: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0116343s
    STEP: Saw pod success 03/22/23 20:17:22.036
    Mar 22 20:17:22.037: INFO: Pod "pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da" satisfied condition "Succeeded or Failed"
    Mar 22 20:17:22.042: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da container test-container: <nil>
    STEP: delete the pod 03/22/23 20:17:22.054
    Mar 22 20:17:22.070: INFO: Waiting for pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da to disappear
    Mar 22 20:17:22.074: INFO: Pod pod-d0504b2f-c75e-4f04-aba5-4afd7609a9da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:22.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5912" for this suite. 03/22/23 20:17:22.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:22.098
Mar 22 20:17:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:17:22.099
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:22.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:22.138
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-2c276294-e758-490e-9d2e-9d35c6242e57 03/22/23 20:17:22.145
STEP: Creating a pod to test consume configMaps 03/22/23 20:17:22.153
Mar 22 20:17:22.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6" in namespace "projected-3718" to be "Succeeded or Failed"
Mar 22 20:17:22.168: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628614ms
Mar 22 20:17:24.189: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025849058s
Mar 22 20:17:26.183: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01951548s
STEP: Saw pod success 03/22/23 20:17:26.183
Mar 22 20:17:26.184: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6" satisfied condition "Succeeded or Failed"
Mar 22 20:17:26.188: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:17:26.199
Mar 22 20:17:26.212: INFO: Waiting for pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 to disappear
Mar 22 20:17:26.217: INFO: Pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:26.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3718" for this suite. 03/22/23 20:17:26.223
------------------------------
• [4.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:22.098
    Mar 22 20:17:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:17:22.099
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:22.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:22.138
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-2c276294-e758-490e-9d2e-9d35c6242e57 03/22/23 20:17:22.145
    STEP: Creating a pod to test consume configMaps 03/22/23 20:17:22.153
    Mar 22 20:17:22.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6" in namespace "projected-3718" to be "Succeeded or Failed"
    Mar 22 20:17:22.168: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628614ms
    Mar 22 20:17:24.189: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025849058s
    Mar 22 20:17:26.183: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01951548s
    STEP: Saw pod success 03/22/23 20:17:26.183
    Mar 22 20:17:26.184: INFO: Pod "pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6" satisfied condition "Succeeded or Failed"
    Mar 22 20:17:26.188: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:17:26.199
    Mar 22 20:17:26.212: INFO: Waiting for pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 to disappear
    Mar 22 20:17:26.217: INFO: Pod pod-projected-configmaps-ab1d1576-b9fe-429d-bd00-dbc545c5b4a6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:26.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3718" for this suite. 03/22/23 20:17:26.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:26.239
Mar 22 20:17:26.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename containers 03/22/23 20:17:26.24
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:26.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:26.271
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 03/22/23 20:17:26.284
Mar 22 20:17:26.293: INFO: Waiting up to 5m0s for pod "client-containers-1d46e295-c792-4030-b942-52c29f297147" in namespace "containers-9466" to be "Succeeded or Failed"
Mar 22 20:17:26.298: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607259ms
Mar 22 20:17:28.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011105836s
Mar 22 20:17:30.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534499s
STEP: Saw pod success 03/22/23 20:17:30.304
Mar 22 20:17:30.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147" satisfied condition "Succeeded or Failed"
Mar 22 20:17:30.308: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-1d46e295-c792-4030-b942-52c29f297147 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:17:30.32
Mar 22 20:17:30.340: INFO: Waiting for pod client-containers-1d46e295-c792-4030-b942-52c29f297147 to disappear
Mar 22 20:17:30.345: INFO: Pod client-containers-1d46e295-c792-4030-b942-52c29f297147 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:30.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9466" for this suite. 03/22/23 20:17:30.351
------------------------------
• [4.120 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:26.239
    Mar 22 20:17:26.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename containers 03/22/23 20:17:26.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:26.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:26.271
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 03/22/23 20:17:26.284
    Mar 22 20:17:26.293: INFO: Waiting up to 5m0s for pod "client-containers-1d46e295-c792-4030-b942-52c29f297147" in namespace "containers-9466" to be "Succeeded or Failed"
    Mar 22 20:17:26.298: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607259ms
    Mar 22 20:17:28.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011105836s
    Mar 22 20:17:30.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534499s
    STEP: Saw pod success 03/22/23 20:17:30.304
    Mar 22 20:17:30.304: INFO: Pod "client-containers-1d46e295-c792-4030-b942-52c29f297147" satisfied condition "Succeeded or Failed"
    Mar 22 20:17:30.308: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-1d46e295-c792-4030-b942-52c29f297147 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:17:30.32
    Mar 22 20:17:30.340: INFO: Waiting for pod client-containers-1d46e295-c792-4030-b942-52c29f297147 to disappear
    Mar 22 20:17:30.345: INFO: Pod client-containers-1d46e295-c792-4030-b942-52c29f297147 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:30.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9466" for this suite. 03/22/23 20:17:30.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:30.36
Mar 22 20:17:30.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:17:30.361
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:30.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:30.389
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-ffb97359-0177-4e90-9bc6-0a3b92e6a8c1 03/22/23 20:17:30.435
STEP: Creating a pod to test consume secrets 03/22/23 20:17:30.444
Mar 22 20:17:30.453: INFO: Waiting up to 5m0s for pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0" in namespace "secrets-5344" to be "Succeeded or Failed"
Mar 22 20:17:30.458: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750506ms
Mar 22 20:17:32.465: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418863s
Mar 22 20:17:34.467: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014181587s
STEP: Saw pod success 03/22/23 20:17:34.467
Mar 22 20:17:34.467: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0" satisfied condition "Succeeded or Failed"
Mar 22 20:17:34.476: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:17:34.487
Mar 22 20:17:34.506: INFO: Waiting for pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 to disappear
Mar 22 20:17:34.509: INFO: Pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5344" for this suite. 03/22/23 20:17:34.514
STEP: Destroying namespace "secret-namespace-3313" for this suite. 03/22/23 20:17:34.529
------------------------------
• [4.177 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:30.36
    Mar 22 20:17:30.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:17:30.361
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:30.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:30.389
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-ffb97359-0177-4e90-9bc6-0a3b92e6a8c1 03/22/23 20:17:30.435
    STEP: Creating a pod to test consume secrets 03/22/23 20:17:30.444
    Mar 22 20:17:30.453: INFO: Waiting up to 5m0s for pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0" in namespace "secrets-5344" to be "Succeeded or Failed"
    Mar 22 20:17:30.458: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750506ms
    Mar 22 20:17:32.465: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418863s
    Mar 22 20:17:34.467: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014181587s
    STEP: Saw pod success 03/22/23 20:17:34.467
    Mar 22 20:17:34.467: INFO: Pod "pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0" satisfied condition "Succeeded or Failed"
    Mar 22 20:17:34.476: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:17:34.487
    Mar 22 20:17:34.506: INFO: Waiting for pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 to disappear
    Mar 22 20:17:34.509: INFO: Pod pod-secrets-410366a3-3aee-430e-ae5e-1e74eaab9ef0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5344" for this suite. 03/22/23 20:17:34.514
    STEP: Destroying namespace "secret-namespace-3313" for this suite. 03/22/23 20:17:34.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:34.538
Mar 22 20:17:34.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:17:34.539
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:34.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:34.562
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 03/22/23 20:17:34.57
Mar 22 20:17:34.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 create -f -'
Mar 22 20:17:35.996: INFO: stderr: ""
Mar 22 20:17:35.996: INFO: stdout: "pod/pause created\n"
Mar 22 20:17:35.996: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 22 20:17:35.996: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1213" to be "running and ready"
Mar 22 20:17:36.004: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28261ms
Mar 22 20:17:36.004: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pool-v7t41yxh0-q56kk' to be 'Running' but was 'Pending'
Mar 22 20:17:38.011: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015227173s
Mar 22 20:17:38.011: INFO: Pod "pause" satisfied condition "running and ready"
Mar 22 20:17:38.011: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 03/22/23 20:17:38.011
Mar 22 20:17:38.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 label pods pause testing-label=testing-label-value'
Mar 22 20:17:38.160: INFO: stderr: ""
Mar 22 20:17:38.160: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/22/23 20:17:38.16
Mar 22 20:17:38.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pod pause -L testing-label'
Mar 22 20:17:38.291: INFO: stderr: ""
Mar 22 20:17:38.291: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/22/23 20:17:38.291
Mar 22 20:17:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 label pods pause testing-label-'
Mar 22 20:17:38.416: INFO: stderr: ""
Mar 22 20:17:38.416: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/22/23 20:17:38.416
Mar 22 20:17:38.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pod pause -L testing-label'
Mar 22 20:17:38.528: INFO: stderr: ""
Mar 22 20:17:38.528: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 03/22/23 20:17:38.529
Mar 22 20:17:38.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 delete --grace-period=0 --force -f -'
Mar 22 20:17:38.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 20:17:38.631: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 22 20:17:38.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get rc,svc -l name=pause --no-headers'
Mar 22 20:17:38.787: INFO: stderr: "No resources found in kubectl-1213 namespace.\n"
Mar 22 20:17:38.787: INFO: stdout: ""
Mar 22 20:17:38.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 20:17:38.905: INFO: stderr: ""
Mar 22 20:17:38.905: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:17:38.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1213" for this suite. 03/22/23 20:17:38.917
------------------------------
• [4.388 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:34.538
    Mar 22 20:17:34.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:17:34.539
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:34.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:34.562
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 03/22/23 20:17:34.57
    Mar 22 20:17:34.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 create -f -'
    Mar 22 20:17:35.996: INFO: stderr: ""
    Mar 22 20:17:35.996: INFO: stdout: "pod/pause created\n"
    Mar 22 20:17:35.996: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 22 20:17:35.996: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1213" to be "running and ready"
    Mar 22 20:17:36.004: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28261ms
    Mar 22 20:17:36.004: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pool-v7t41yxh0-q56kk' to be 'Running' but was 'Pending'
    Mar 22 20:17:38.011: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015227173s
    Mar 22 20:17:38.011: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 22 20:17:38.011: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 03/22/23 20:17:38.011
    Mar 22 20:17:38.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 label pods pause testing-label=testing-label-value'
    Mar 22 20:17:38.160: INFO: stderr: ""
    Mar 22 20:17:38.160: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/22/23 20:17:38.16
    Mar 22 20:17:38.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pod pause -L testing-label'
    Mar 22 20:17:38.291: INFO: stderr: ""
    Mar 22 20:17:38.291: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/22/23 20:17:38.291
    Mar 22 20:17:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 label pods pause testing-label-'
    Mar 22 20:17:38.416: INFO: stderr: ""
    Mar 22 20:17:38.416: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/22/23 20:17:38.416
    Mar 22 20:17:38.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pod pause -L testing-label'
    Mar 22 20:17:38.528: INFO: stderr: ""
    Mar 22 20:17:38.528: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 03/22/23 20:17:38.529
    Mar 22 20:17:38.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 delete --grace-period=0 --force -f -'
    Mar 22 20:17:38.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 20:17:38.631: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 22 20:17:38.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get rc,svc -l name=pause --no-headers'
    Mar 22 20:17:38.787: INFO: stderr: "No resources found in kubectl-1213 namespace.\n"
    Mar 22 20:17:38.787: INFO: stdout: ""
    Mar 22 20:17:38.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-1213 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 22 20:17:38.905: INFO: stderr: ""
    Mar 22 20:17:38.905: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:17:38.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1213" for this suite. 03/22/23 20:17:38.917
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:17:38.926
Mar 22 20:17:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename cronjob 03/22/23 20:17:38.927
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:38.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:38.947
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/22/23 20:17:38.955
STEP: Ensuring more than one job is running at a time 03/22/23 20:17:38.963
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/22/23 20:19:00.971
STEP: Removing cronjob 03/22/23 20:19:00.979
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 22 20:19:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9249" for this suite. 03/22/23 20:19:01.001
------------------------------
• [SLOW TEST] [82.089 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:17:38.926
    Mar 22 20:17:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename cronjob 03/22/23 20:17:38.927
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:17:38.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:17:38.947
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/22/23 20:17:38.955
    STEP: Ensuring more than one job is running at a time 03/22/23 20:17:38.963
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/22/23 20:19:00.971
    STEP: Removing cronjob 03/22/23 20:19:00.979
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:19:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9249" for this suite. 03/22/23 20:19:01.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:19:01.017
Mar 22 20:19:01.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 20:19:01.018
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:01.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:01.054
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b in namespace container-probe-6159 03/22/23 20:19:01.063
Mar 22 20:19:01.072: INFO: Waiting up to 5m0s for pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b" in namespace "container-probe-6159" to be "not pending"
Mar 22 20:19:01.079: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.968963ms
Mar 22 20:19:03.085: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013083181s
Mar 22 20:19:03.085: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b" satisfied condition "not pending"
Mar 22 20:19:03.085: INFO: Started pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b in namespace container-probe-6159
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:19:03.085
Mar 22 20:19:03.089: INFO: Initial restart count of pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b is 0
Mar 22 20:19:53.272: INFO: Restart count of pod container-probe-6159/busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b is now 1 (50.182597286s elapsed)
STEP: deleting the pod 03/22/23 20:19:53.272
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 20:19:53.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6159" for this suite. 03/22/23 20:19:53.301
------------------------------
• [SLOW TEST] [52.292 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:19:01.017
    Mar 22 20:19:01.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 20:19:01.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:01.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:01.054
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b in namespace container-probe-6159 03/22/23 20:19:01.063
    Mar 22 20:19:01.072: INFO: Waiting up to 5m0s for pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b" in namespace "container-probe-6159" to be "not pending"
    Mar 22 20:19:01.079: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.968963ms
    Mar 22 20:19:03.085: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013083181s
    Mar 22 20:19:03.085: INFO: Pod "busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b" satisfied condition "not pending"
    Mar 22 20:19:03.085: INFO: Started pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b in namespace container-probe-6159
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:19:03.085
    Mar 22 20:19:03.089: INFO: Initial restart count of pod busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b is 0
    Mar 22 20:19:53.272: INFO: Restart count of pod container-probe-6159/busybox-ce7e21e3-2eea-4e38-81fc-7faffd1f265b is now 1 (50.182597286s elapsed)
    STEP: deleting the pod 03/22/23 20:19:53.272
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:19:53.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6159" for this suite. 03/22/23 20:19:53.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:19:53.313
Mar 22 20:19:53.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:19:53.315
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:53.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:53.347
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-87cb0a17-9476-4a42-8ffb-ac1523101ec7 03/22/23 20:19:53.355
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:19:53.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9991" for this suite. 03/22/23 20:19:53.367
------------------------------
• [0.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:19:53.313
    Mar 22 20:19:53.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:19:53.315
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:53.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:53.347
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-87cb0a17-9476-4a42-8ffb-ac1523101ec7 03/22/23 20:19:53.355
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:19:53.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9991" for this suite. 03/22/23 20:19:53.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:19:53.379
Mar 22 20:19:53.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:19:53.381
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:53.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:53.407
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 22 20:19:53.426: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e" in namespace "kubelet-test-8441" to be "running and ready"
Mar 22 20:19:53.431: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.094816ms
Mar 22 20:19:53.431: INFO: The phase of Pod busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:19:55.437: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e": Phase="Running", Reason="", readiness=true. Elapsed: 2.01023778s
Mar 22 20:19:55.437: INFO: The phase of Pod busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e is Running (Ready = true)
Mar 22 20:19:55.437: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:19:55.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8441" for this suite. 03/22/23 20:19:55.509
------------------------------
• [2.138 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:19:53.379
    Mar 22 20:19:53.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:19:53.381
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:53.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:53.407
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 22 20:19:53.426: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e" in namespace "kubelet-test-8441" to be "running and ready"
    Mar 22 20:19:53.431: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.094816ms
    Mar 22 20:19:53.431: INFO: The phase of Pod busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:19:55.437: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e": Phase="Running", Reason="", readiness=true. Elapsed: 2.01023778s
    Mar 22 20:19:55.437: INFO: The phase of Pod busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e is Running (Ready = true)
    Mar 22 20:19:55.437: INFO: Pod "busybox-readonly-fs9471b4b0-5883-4a76-8f32-963f20e0353e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:19:55.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8441" for this suite. 03/22/23 20:19:55.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:19:55.523
Mar 22 20:19:55.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename cronjob 03/22/23 20:19:55.524
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:55.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:55.547
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/22/23 20:19:55.554
STEP: Ensuring a job is scheduled 03/22/23 20:19:55.562
STEP: Ensuring exactly one is scheduled 03/22/23 20:20:01.569
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/22/23 20:20:01.579
STEP: Ensuring no more jobs are scheduled 03/22/23 20:20:01.587
STEP: Removing cronjob 03/22/23 20:25:01.618
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 22 20:25:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5688" for this suite. 03/22/23 20:25:01.65
------------------------------
• [SLOW TEST] [306.137 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:19:55.523
    Mar 22 20:19:55.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename cronjob 03/22/23 20:19:55.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:19:55.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:19:55.547
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/22/23 20:19:55.554
    STEP: Ensuring a job is scheduled 03/22/23 20:19:55.562
    STEP: Ensuring exactly one is scheduled 03/22/23 20:20:01.569
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/22/23 20:20:01.579
    STEP: Ensuring no more jobs are scheduled 03/22/23 20:20:01.587
    STEP: Removing cronjob 03/22/23 20:25:01.618
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:25:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5688" for this suite. 03/22/23 20:25:01.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:25:01.667
Mar 22 20:25:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:25:01.679
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:25:01.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:25:01.71
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5930 03/22/23 20:25:01.727
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[] 03/22/23 20:25:01.743
Mar 22 20:25:01.762: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 22 20:25:02.789: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5930 03/22/23 20:25:02.789
Mar 22 20:25:02.800: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5930" to be "running and ready"
Mar 22 20:25:02.806: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743807ms
Mar 22 20:25:02.806: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:25:04.814: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013864459s
Mar 22 20:25:04.814: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:25:06.814: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.014386691s
Mar 22 20:25:06.814: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 22 20:25:06.814: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod1:[100]] 03/22/23 20:25:06.821
Mar 22 20:25:06.853: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5930 03/22/23 20:25:06.853
Mar 22 20:25:06.862: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5930" to be "running and ready"
Mar 22 20:25:06.867: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086748ms
Mar 22 20:25:06.867: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:25:08.876: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013868579s
Mar 22 20:25:08.876: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 22 20:25:08.876: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod1:[100] pod2:[101]] 03/22/23 20:25:08.88
Mar 22 20:25:08.900: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/22/23 20:25:08.9
Mar 22 20:25:08.900: INFO: Creating new exec pod
Mar 22 20:25:08.909: INFO: Waiting up to 5m0s for pod "execpod6stw2" in namespace "services-5930" to be "running"
Mar 22 20:25:08.917: INFO: Pod "execpod6stw2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.973953ms
Mar 22 20:25:10.923: INFO: Pod "execpod6stw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014561933s
Mar 22 20:25:10.923: INFO: Pod "execpod6stw2" satisfied condition "running"
Mar 22 20:25:11.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Mar 22 20:25:12.293: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 22 20:25:12.293: INFO: stdout: ""
Mar 22 20:25:12.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 10.245.225.189 80'
Mar 22 20:25:12.656: INFO: stderr: "+ nc -v -z -w 2 10.245.225.189 80\nConnection to 10.245.225.189 80 port [tcp/http] succeeded!\n"
Mar 22 20:25:12.656: INFO: stdout: ""
Mar 22 20:25:12.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Mar 22 20:25:12.997: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 22 20:25:12.997: INFO: stdout: ""
Mar 22 20:25:12.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 10.245.225.189 81'
Mar 22 20:25:13.344: INFO: stderr: "+ nc -v -z -w 2 10.245.225.189 81\nConnection to 10.245.225.189 81 port [tcp/*] succeeded!\n"
Mar 22 20:25:13.344: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5930 03/22/23 20:25:13.344
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod2:[101]] 03/22/23 20:25:13.37
Mar 22 20:25:13.396: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5930 03/22/23 20:25:13.396
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[] 03/22/23 20:25:13.411
Mar 22 20:25:13.443: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:25:13.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5930" for this suite. 03/22/23 20:25:13.474
------------------------------
• [SLOW TEST] [11.820 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:25:01.667
    Mar 22 20:25:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:25:01.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:25:01.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:25:01.71
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5930 03/22/23 20:25:01.727
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[] 03/22/23 20:25:01.743
    Mar 22 20:25:01.762: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar 22 20:25:02.789: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5930 03/22/23 20:25:02.789
    Mar 22 20:25:02.800: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5930" to be "running and ready"
    Mar 22 20:25:02.806: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743807ms
    Mar 22 20:25:02.806: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:25:04.814: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013864459s
    Mar 22 20:25:04.814: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:25:06.814: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.014386691s
    Mar 22 20:25:06.814: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 22 20:25:06.814: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod1:[100]] 03/22/23 20:25:06.821
    Mar 22 20:25:06.853: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5930 03/22/23 20:25:06.853
    Mar 22 20:25:06.862: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5930" to be "running and ready"
    Mar 22 20:25:06.867: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086748ms
    Mar 22 20:25:06.867: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:25:08.876: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013868579s
    Mar 22 20:25:08.876: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 22 20:25:08.876: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod1:[100] pod2:[101]] 03/22/23 20:25:08.88
    Mar 22 20:25:08.900: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/22/23 20:25:08.9
    Mar 22 20:25:08.900: INFO: Creating new exec pod
    Mar 22 20:25:08.909: INFO: Waiting up to 5m0s for pod "execpod6stw2" in namespace "services-5930" to be "running"
    Mar 22 20:25:08.917: INFO: Pod "execpod6stw2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.973953ms
    Mar 22 20:25:10.923: INFO: Pod "execpod6stw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014561933s
    Mar 22 20:25:10.923: INFO: Pod "execpod6stw2" satisfied condition "running"
    Mar 22 20:25:11.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Mar 22 20:25:12.293: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 22 20:25:12.293: INFO: stdout: ""
    Mar 22 20:25:12.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 10.245.225.189 80'
    Mar 22 20:25:12.656: INFO: stderr: "+ nc -v -z -w 2 10.245.225.189 80\nConnection to 10.245.225.189 80 port [tcp/http] succeeded!\n"
    Mar 22 20:25:12.656: INFO: stdout: ""
    Mar 22 20:25:12.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Mar 22 20:25:12.997: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 22 20:25:12.997: INFO: stdout: ""
    Mar 22 20:25:12.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-5930 exec execpod6stw2 -- /bin/sh -x -c nc -v -z -w 2 10.245.225.189 81'
    Mar 22 20:25:13.344: INFO: stderr: "+ nc -v -z -w 2 10.245.225.189 81\nConnection to 10.245.225.189 81 port [tcp/*] succeeded!\n"
    Mar 22 20:25:13.344: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5930 03/22/23 20:25:13.344
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[pod2:[101]] 03/22/23 20:25:13.37
    Mar 22 20:25:13.396: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5930 03/22/23 20:25:13.396
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5930 to expose endpoints map[] 03/22/23 20:25:13.411
    Mar 22 20:25:13.443: INFO: successfully validated that service multi-endpoint-test in namespace services-5930 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:25:13.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5930" for this suite. 03/22/23 20:25:13.474
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:25:13.496
Mar 22 20:25:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 20:25:13.498
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:25:13.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:25:13.532
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf in namespace container-probe-4154 03/22/23 20:25:13.541
Mar 22 20:25:13.560: INFO: Waiting up to 5m0s for pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf" in namespace "container-probe-4154" to be "not pending"
Mar 22 20:25:13.569: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.82974ms
Mar 22 20:25:15.576: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933881s
Mar 22 20:25:15.577: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf" satisfied condition "not pending"
Mar 22 20:25:15.577: INFO: Started pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf in namespace container-probe-4154
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:25:15.577
Mar 22 20:25:15.581: INFO: Initial restart count of pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf is 0
STEP: deleting the pod 03/22/23 20:29:16.421
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:16.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4154" for this suite. 03/22/23 20:29:16.44
------------------------------
• [SLOW TEST] [242.954 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:25:13.496
    Mar 22 20:25:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 20:25:13.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:25:13.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:25:13.532
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf in namespace container-probe-4154 03/22/23 20:25:13.541
    Mar 22 20:25:13.560: INFO: Waiting up to 5m0s for pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf" in namespace "container-probe-4154" to be "not pending"
    Mar 22 20:25:13.569: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.82974ms
    Mar 22 20:25:15.576: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933881s
    Mar 22 20:25:15.577: INFO: Pod "test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf" satisfied condition "not pending"
    Mar 22 20:25:15.577: INFO: Started pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf in namespace container-probe-4154
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:25:15.577
    Mar 22 20:25:15.581: INFO: Initial restart count of pod test-webserver-2b9f898a-ef6e-4aa4-be5b-4c3ec1ea21bf is 0
    STEP: deleting the pod 03/22/23 20:29:16.421
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:16.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4154" for this suite. 03/22/23 20:29:16.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:16.459
Mar 22 20:29:16.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:29:16.461
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:16.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:16.494
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:29:16.539
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:29:16.547
Mar 22 20:29:16.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:29:16.557: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:29:17.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:29:17.570: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:29:18.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:29:18.570: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/22/23 20:29:18.575
STEP: DeleteCollection of the DaemonSets 03/22/23 20:29:18.58
STEP: Verify that ReplicaSets have been deleted 03/22/23 20:29:18.589
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Mar 22 20:29:18.607: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17161"},"items":null}

Mar 22 20:29:18.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17161"},"items":[{"metadata":{"name":"daemon-set-9vv8p","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"31326acb-6c73-4b87-854e-f3f87326a2ab","resourceVersion":"17159","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zk4gr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zk4gr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56kh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56kh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.3","podIP":"10.244.0.81","podIPs":[{"ip":"10.244.0.81"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8a17516d21f964dc91c6332dbd567e26aa7be6e060748ce772193e47146b75e2","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gk7ng","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"1cbd5901-fbe9-456b-b188-8ad00b6b5b23","resourceVersion":"17158","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dwpjm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dwpjm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56k5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56k5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.2","podIP":"10.244.1.36","podIPs":[{"ip":"10.244.1.36"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3a9513e5c7a5571c02d72cdc16e021d92a359d3417ecb71bc7578638a8c0cd8e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mwtb7","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"18939fe1-b6dd-4b92-b620-8f01f9bacf31","resourceVersion":"17160","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nnlsz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nnlsz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56kk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56kk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.4","podIP":"10.244.0.166","podIPs":[{"ip":"10.244.0.166"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://530955c115f9b79bba675b0bb8f3161916ef54f0d28614e110f75a6458842336","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7383" for this suite. 03/22/23 20:29:18.635
------------------------------
• [2.186 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:16.459
    Mar 22 20:29:16.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:29:16.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:16.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:16.494
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:29:16.539
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:29:16.547
    Mar 22 20:29:16.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:29:16.557: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:29:17.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:29:17.570: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:29:18.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:29:18.570: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/22/23 20:29:18.575
    STEP: DeleteCollection of the DaemonSets 03/22/23 20:29:18.58
    STEP: Verify that ReplicaSets have been deleted 03/22/23 20:29:18.589
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Mar 22 20:29:18.607: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17161"},"items":null}

    Mar 22 20:29:18.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17161"},"items":[{"metadata":{"name":"daemon-set-9vv8p","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"31326acb-6c73-4b87-854e-f3f87326a2ab","resourceVersion":"17159","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zk4gr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zk4gr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56kh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56kh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.3","podIP":"10.244.0.81","podIPs":[{"ip":"10.244.0.81"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8a17516d21f964dc91c6332dbd567e26aa7be6e060748ce772193e47146b75e2","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gk7ng","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"1cbd5901-fbe9-456b-b188-8ad00b6b5b23","resourceVersion":"17158","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dwpjm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dwpjm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56k5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56k5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.2","podIP":"10.244.1.36","podIPs":[{"ip":"10.244.1.36"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3a9513e5c7a5571c02d72cdc16e021d92a359d3417ecb71bc7578638a8c0cd8e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mwtb7","generateName":"daemon-set-","namespace":"daemonsets-7383","uid":"18939fe1-b6dd-4b92-b620-8f01f9bacf31","resourceVersion":"17160","creationTimestamp":"2023-03-22T20:29:16Z","deletionTimestamp":"2023-03-22T20:29:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dea49ce5-6ed4-4b02-8d11-cd4d7c4bf9c8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-22T20:29:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nnlsz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nnlsz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pool-v7t41yxh0-q56kk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pool-v7t41yxh0-q56kk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-22T20:29:16Z"}],"hostIP":"10.124.0.4","podIP":"10.244.0.166","podIPs":[{"ip":"10.244.0.166"}],"startTime":"2023-03-22T20:29:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-22T20:29:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://530955c115f9b79bba675b0bb8f3161916ef54f0d28614e110f75a6458842336","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7383" for this suite. 03/22/23 20:29:18.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:18.655
Mar 22 20:29:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:29:18.657
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:18.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:18.676
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-2083/secret-test-e94b52f0-9224-45b2-9a3d-a0de3dc8beca 03/22/23 20:29:18.681
STEP: Creating a pod to test consume secrets 03/22/23 20:29:18.687
Mar 22 20:29:18.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554" in namespace "secrets-2083" to be "Succeeded or Failed"
Mar 22 20:29:18.701: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289894ms
Mar 22 20:29:20.712: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016427579s
Mar 22 20:29:22.706: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010196888s
STEP: Saw pod success 03/22/23 20:29:22.706
Mar 22 20:29:22.706: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554" satisfied condition "Succeeded or Failed"
Mar 22 20:29:22.710: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 container env-test: <nil>
STEP: delete the pod 03/22/23 20:29:22.75
Mar 22 20:29:22.763: INFO: Waiting for pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 to disappear
Mar 22 20:29:22.767: INFO: Pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:22.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2083" for this suite. 03/22/23 20:29:22.774
------------------------------
• [4.126 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:18.655
    Mar 22 20:29:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:29:18.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:18.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:18.676
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-2083/secret-test-e94b52f0-9224-45b2-9a3d-a0de3dc8beca 03/22/23 20:29:18.681
    STEP: Creating a pod to test consume secrets 03/22/23 20:29:18.687
    Mar 22 20:29:18.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554" in namespace "secrets-2083" to be "Succeeded or Failed"
    Mar 22 20:29:18.701: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289894ms
    Mar 22 20:29:20.712: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016427579s
    Mar 22 20:29:22.706: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010196888s
    STEP: Saw pod success 03/22/23 20:29:22.706
    Mar 22 20:29:22.706: INFO: Pod "pod-configmaps-013f4301-0e14-447e-8830-4d7845636554" satisfied condition "Succeeded or Failed"
    Mar 22 20:29:22.710: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 container env-test: <nil>
    STEP: delete the pod 03/22/23 20:29:22.75
    Mar 22 20:29:22.763: INFO: Waiting for pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 to disappear
    Mar 22 20:29:22.767: INFO: Pod pod-configmaps-013f4301-0e14-447e-8830-4d7845636554 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:22.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2083" for this suite. 03/22/23 20:29:22.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:22.791
Mar 22 20:29:22.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename watch 03/22/23 20:29:22.793
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:22.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:22.813
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/22/23 20:29:22.82
STEP: creating a new configmap 03/22/23 20:29:22.823
STEP: modifying the configmap once 03/22/23 20:29:22.829
STEP: closing the watch once it receives two notifications 03/22/23 20:29:22.841
Mar 22 20:29:22.841: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17225 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:29:22.842: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17226 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/22/23 20:29:22.843
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/22/23 20:29:22.852
STEP: deleting the configmap 03/22/23 20:29:22.856
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/22/23 20:29:22.862
Mar 22 20:29:22.862: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17227 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:29:22.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17228 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:22.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3712" for this suite. 03/22/23 20:29:22.87
------------------------------
• [0.088 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:22.791
    Mar 22 20:29:22.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename watch 03/22/23 20:29:22.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:22.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:22.813
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/22/23 20:29:22.82
    STEP: creating a new configmap 03/22/23 20:29:22.823
    STEP: modifying the configmap once 03/22/23 20:29:22.829
    STEP: closing the watch once it receives two notifications 03/22/23 20:29:22.841
    Mar 22 20:29:22.841: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17225 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:29:22.842: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17226 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/22/23 20:29:22.843
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/22/23 20:29:22.852
    STEP: deleting the configmap 03/22/23 20:29:22.856
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/22/23 20:29:22.862
    Mar 22 20:29:22.862: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17227 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:29:22.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3712  097483d3-30a2-40a9-98eb-3e1d77c5a093 17228 0 2023-03-22 20:29:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-22 20:29:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:22.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3712" for this suite. 03/22/23 20:29:22.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:22.88
Mar 22 20:29:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename prestop 03/22/23 20:29:22.882
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:22.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:22.912
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4239 03/22/23 20:29:22.919
STEP: Waiting for pods to come up. 03/22/23 20:29:22.928
Mar 22 20:29:22.929: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4239" to be "running"
Mar 22 20:29:22.933: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230931ms
Mar 22 20:29:24.939: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010184445s
Mar 22 20:29:24.939: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4239 03/22/23 20:29:24.943
Mar 22 20:29:24.951: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4239" to be "running"
Mar 22 20:29:24.956: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397586ms
Mar 22 20:29:26.963: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011832842s
Mar 22 20:29:26.963: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/22/23 20:29:26.963
Mar 22 20:29:32.005: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/22/23 20:29:32.005
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4239" for this suite. 03/22/23 20:29:32.027
------------------------------
• [SLOW TEST] [9.158 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:22.88
    Mar 22 20:29:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename prestop 03/22/23 20:29:22.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:22.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:22.912
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4239 03/22/23 20:29:22.919
    STEP: Waiting for pods to come up. 03/22/23 20:29:22.928
    Mar 22 20:29:22.929: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4239" to be "running"
    Mar 22 20:29:22.933: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230931ms
    Mar 22 20:29:24.939: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010184445s
    Mar 22 20:29:24.939: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4239 03/22/23 20:29:24.943
    Mar 22 20:29:24.951: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4239" to be "running"
    Mar 22 20:29:24.956: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397586ms
    Mar 22 20:29:26.963: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011832842s
    Mar 22 20:29:26.963: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/22/23 20:29:26.963
    Mar 22 20:29:32.005: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/22/23 20:29:32.005
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4239" for this suite. 03/22/23 20:29:32.027
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:32.043
Mar 22 20:29:32.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:29:32.044
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:32.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:32.077
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5334" for this suite. 03/22/23 20:29:36.121
------------------------------
• [4.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:32.043
    Mar 22 20:29:32.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:29:32.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:32.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:32.077
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5334" for this suite. 03/22/23 20:29:36.121
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:36.141
Mar 22 20:29:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:29:36.142
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:36.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:36.192
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 03/22/23 20:29:36.203
Mar 22 20:29:36.203: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6286 proxy --unix-socket=/tmp/kubectl-proxy-unix867551250/test'
STEP: retrieving proxy /api/ output 03/22/23 20:29:36.368
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6286" for this suite. 03/22/23 20:29:36.381
------------------------------
• [0.248 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:36.141
    Mar 22 20:29:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:29:36.142
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:36.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:36.192
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 03/22/23 20:29:36.203
    Mar 22 20:29:36.203: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6286 proxy --unix-socket=/tmp/kubectl-proxy-unix867551250/test'
    STEP: retrieving proxy /api/ output 03/22/23 20:29:36.368
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6286" for this suite. 03/22/23 20:29:36.381
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:36.39
Mar 22 20:29:36.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 20:29:36.391
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:36.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:36.426
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Mar 22 20:29:36.440: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/22/23 20:29:36.456
STEP: Checking rc "condition-test" has the desired failure condition set 03/22/23 20:29:36.465
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/22/23 20:29:37.487
Mar 22 20:29:37.500: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/22/23 20:29:37.501
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9545" for this suite. 03/22/23 20:29:37.512
------------------------------
• [1.131 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:36.39
    Mar 22 20:29:36.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 20:29:36.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:36.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:36.426
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Mar 22 20:29:36.440: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/22/23 20:29:36.456
    STEP: Checking rc "condition-test" has the desired failure condition set 03/22/23 20:29:36.465
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/22/23 20:29:37.487
    Mar 22 20:29:37.500: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/22/23 20:29:37.501
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9545" for this suite. 03/22/23 20:29:37.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:37.524
Mar 22 20:29:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:29:37.525
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:37.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:37.566
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Mar 22 20:29:37.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 20:29:39.188
Mar 22 20:29:39.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 create -f -'
Mar 22 20:29:40.063: INFO: stderr: ""
Mar 22 20:29:40.075: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 22 20:29:40.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 delete e2e-test-crd-publish-openapi-8156-crds test-cr'
Mar 22 20:29:40.249: INFO: stderr: ""
Mar 22 20:29:40.249: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 22 20:29:40.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 apply -f -'
Mar 22 20:29:40.705: INFO: stderr: ""
Mar 22 20:29:40.705: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 22 20:29:40.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 delete e2e-test-crd-publish-openapi-8156-crds test-cr'
Mar 22 20:29:40.907: INFO: stderr: ""
Mar 22 20:29:40.907: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/22/23 20:29:40.907
Mar 22 20:29:40.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 explain e2e-test-crd-publish-openapi-8156-crds'
Mar 22 20:29:41.319: INFO: stderr: ""
Mar 22 20:29:41.319: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8156-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:43.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5813" for this suite. 03/22/23 20:29:43.426
------------------------------
• [SLOW TEST] [5.909 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:37.524
    Mar 22 20:29:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:29:37.525
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:37.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:37.566
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Mar 22 20:29:37.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 20:29:39.188
    Mar 22 20:29:39.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 create -f -'
    Mar 22 20:29:40.063: INFO: stderr: ""
    Mar 22 20:29:40.075: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 22 20:29:40.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 delete e2e-test-crd-publish-openapi-8156-crds test-cr'
    Mar 22 20:29:40.249: INFO: stderr: ""
    Mar 22 20:29:40.249: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 22 20:29:40.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 apply -f -'
    Mar 22 20:29:40.705: INFO: stderr: ""
    Mar 22 20:29:40.705: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 22 20:29:40.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 --namespace=crd-publish-openapi-5813 delete e2e-test-crd-publish-openapi-8156-crds test-cr'
    Mar 22 20:29:40.907: INFO: stderr: ""
    Mar 22 20:29:40.907: INFO: stdout: "e2e-test-crd-publish-openapi-8156-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/22/23 20:29:40.907
    Mar 22 20:29:40.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5813 explain e2e-test-crd-publish-openapi-8156-crds'
    Mar 22 20:29:41.319: INFO: stderr: ""
    Mar 22 20:29:41.319: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8156-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:43.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5813" for this suite. 03/22/23 20:29:43.426
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:43.435
Mar 22 20:29:43.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:29:43.437
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:43.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:43.458
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/22/23 20:29:43.464
Mar 22 20:29:43.476: INFO: Waiting up to 5m0s for pod "pod-db67472d-49a8-47b3-aa7d-980bee497690" in namespace "emptydir-9754" to be "Succeeded or Failed"
Mar 22 20:29:43.481: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272668ms
Mar 22 20:29:45.495: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018437692s
Mar 22 20:29:47.487: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010220239s
STEP: Saw pod success 03/22/23 20:29:47.487
Mar 22 20:29:47.487: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690" satisfied condition "Succeeded or Failed"
Mar 22 20:29:47.492: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-db67472d-49a8-47b3-aa7d-980bee497690 container test-container: <nil>
STEP: delete the pod 03/22/23 20:29:47.504
Mar 22 20:29:47.517: INFO: Waiting for pod pod-db67472d-49a8-47b3-aa7d-980bee497690 to disappear
Mar 22 20:29:47.523: INFO: Pod pod-db67472d-49a8-47b3-aa7d-980bee497690 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9754" for this suite. 03/22/23 20:29:47.53
------------------------------
• [4.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:43.435
    Mar 22 20:29:43.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:29:43.437
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:43.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:43.458
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/22/23 20:29:43.464
    Mar 22 20:29:43.476: INFO: Waiting up to 5m0s for pod "pod-db67472d-49a8-47b3-aa7d-980bee497690" in namespace "emptydir-9754" to be "Succeeded or Failed"
    Mar 22 20:29:43.481: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272668ms
    Mar 22 20:29:45.495: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018437692s
    Mar 22 20:29:47.487: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010220239s
    STEP: Saw pod success 03/22/23 20:29:47.487
    Mar 22 20:29:47.487: INFO: Pod "pod-db67472d-49a8-47b3-aa7d-980bee497690" satisfied condition "Succeeded or Failed"
    Mar 22 20:29:47.492: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-db67472d-49a8-47b3-aa7d-980bee497690 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:29:47.504
    Mar 22 20:29:47.517: INFO: Waiting for pod pod-db67472d-49a8-47b3-aa7d-980bee497690 to disappear
    Mar 22 20:29:47.523: INFO: Pod pod-db67472d-49a8-47b3-aa7d-980bee497690 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9754" for this suite. 03/22/23 20:29:47.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:47.54
Mar 22 20:29:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:29:47.541
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:47.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:47.565
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 03/22/23 20:29:47.576
STEP: watching for the Service to be added 03/22/23 20:29:47.591
Mar 22 20:29:47.598: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 22 20:29:47.598: INFO: Service test-service-drxwt created
STEP: Getting /status 03/22/23 20:29:47.598
Mar 22 20:29:47.607: INFO: Service test-service-drxwt has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/22/23 20:29:47.607
STEP: watching for the Service to be patched 03/22/23 20:29:47.617
Mar 22 20:29:47.625: INFO: observed Service test-service-drxwt in namespace services-1647 with annotations: map[] & LoadBalancer: {[]}
Mar 22 20:29:47.625: INFO: Found Service test-service-drxwt in namespace services-1647 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 22 20:29:47.625: INFO: Service test-service-drxwt has service status patched
STEP: updating the ServiceStatus 03/22/23 20:29:47.625
Mar 22 20:29:47.636: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/22/23 20:29:47.636
Mar 22 20:29:47.639: INFO: Observed Service test-service-drxwt in namespace services-1647 with annotations: map[] & Conditions: {[]}
Mar 22 20:29:47.639: INFO: Observed event: &Service{ObjectMeta:{test-service-drxwt  services-1647  96129e9a-3ed2-45e3-a1d2-85533ceeed64 17552 0 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.245.189.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.245.189.4],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 22 20:29:47.640: INFO: Found Service test-service-drxwt in namespace services-1647 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 22 20:29:47.640: INFO: Service test-service-drxwt has service status updated
STEP: patching the service 03/22/23 20:29:47.64
STEP: watching for the Service to be patched 03/22/23 20:29:47.652
Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
Mar 22 20:29:47.657: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service:patched test-service-static:true]
Mar 22 20:29:47.657: INFO: Service test-service-drxwt patched
STEP: deleting the service 03/22/23 20:29:47.657
STEP: watching for the Service to be deleted 03/22/23 20:29:47.676
Mar 22 20:29:47.680: INFO: Observed event: ADDED
Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
Mar 22 20:29:47.680: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 22 20:29:47.680: INFO: Service test-service-drxwt deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1647" for this suite. 03/22/23 20:29:47.694
------------------------------
• [0.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:47.54
    Mar 22 20:29:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:29:47.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:47.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:47.565
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 03/22/23 20:29:47.576
    STEP: watching for the Service to be added 03/22/23 20:29:47.591
    Mar 22 20:29:47.598: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 22 20:29:47.598: INFO: Service test-service-drxwt created
    STEP: Getting /status 03/22/23 20:29:47.598
    Mar 22 20:29:47.607: INFO: Service test-service-drxwt has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/22/23 20:29:47.607
    STEP: watching for the Service to be patched 03/22/23 20:29:47.617
    Mar 22 20:29:47.625: INFO: observed Service test-service-drxwt in namespace services-1647 with annotations: map[] & LoadBalancer: {[]}
    Mar 22 20:29:47.625: INFO: Found Service test-service-drxwt in namespace services-1647 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 22 20:29:47.625: INFO: Service test-service-drxwt has service status patched
    STEP: updating the ServiceStatus 03/22/23 20:29:47.625
    Mar 22 20:29:47.636: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/22/23 20:29:47.636
    Mar 22 20:29:47.639: INFO: Observed Service test-service-drxwt in namespace services-1647 with annotations: map[] & Conditions: {[]}
    Mar 22 20:29:47.639: INFO: Observed event: &Service{ObjectMeta:{test-service-drxwt  services-1647  96129e9a-3ed2-45e3-a1d2-85533ceeed64 17552 0 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.245.189.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.245.189.4],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 22 20:29:47.640: INFO: Found Service test-service-drxwt in namespace services-1647 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 22 20:29:47.640: INFO: Service test-service-drxwt has service status updated
    STEP: patching the service 03/22/23 20:29:47.64
    STEP: watching for the Service to be patched 03/22/23 20:29:47.652
    Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
    Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
    Mar 22 20:29:47.656: INFO: observed Service test-service-drxwt in namespace services-1647 with labels: map[test-service-static:true]
    Mar 22 20:29:47.657: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service:patched test-service-static:true]
    Mar 22 20:29:47.657: INFO: Service test-service-drxwt patched
    STEP: deleting the service 03/22/23 20:29:47.657
    STEP: watching for the Service to be deleted 03/22/23 20:29:47.676
    Mar 22 20:29:47.680: INFO: Observed event: ADDED
    Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
    Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
    Mar 22 20:29:47.680: INFO: Observed event: MODIFIED
    Mar 22 20:29:47.680: INFO: Found Service test-service-drxwt in namespace services-1647 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 22 20:29:47.680: INFO: Service test-service-drxwt deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1647" for this suite. 03/22/23 20:29:47.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:47.712
Mar 22 20:29:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 20:29:47.714
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:47.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:47.741
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 22 20:29:47.750: INFO: Creating simple deployment test-new-deployment
Mar 22 20:29:47.768: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 03/22/23 20:29:49.784
STEP: updating a scale subresource 03/22/23 20:29:49.788
STEP: verifying the deployment Spec.Replicas was modified 03/22/23 20:29:49.799
STEP: Patch a scale subresource 03/22/23 20:29:49.813
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 20:29:49.835: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3429  0e12257b-15ef-49ff-a207-081d1da7f4b3 17611 3 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056f7cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-22 20:29:49 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 20:29:49 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 20:29:49.850: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3429  c55c656e-cac3-4303-8256-42b89873c38c 17607 3 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0e12257b-15ef-49ff-a207-081d1da7f4b3 0xc004f1a107 0xc004f1a108}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e12257b-15ef-49ff-a207-081d1da7f4b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1a198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:29:49.861: INFO: Pod "test-new-deployment-7f5969cbc7-5mwq5" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5mwq5 test-new-deployment-7f5969cbc7- deployment-3429  dc0693f0-2ed6-4038-9e1a-e5d58ae66fa5 17608 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a587 0xc004f1a588}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6psc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6psc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 20:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-65d56" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-65d56 test-new-deployment-7f5969cbc7- deployment-3429  c1453704-cc9a-4ee9-9f5c-e4139a96dd16 17617 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a740 0xc004f1a741}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djsvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djsvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-7t6bq" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7t6bq test-new-deployment-7f5969cbc7- deployment-3429  92f97e3f-3f25-436f-99fb-5bd7190d79e1 17593 0 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a8a0 0xc004f1a8a1}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgrw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgrw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.43,StartTime:2023-03-22 20:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:29:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf1c445c30340e85e9fff537223cdad43a944303526437b69971d47d4456fa25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-xxgm4" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xxgm4 test-new-deployment-7f5969cbc7- deployment-3429  03fafa43-c430-47e6-a1f6-130b740c44c1 17614 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1aa90 0xc004f1aa91}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttxv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttxv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:49.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3429" for this suite. 03/22/23 20:29:49.871
------------------------------
• [2.168 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:47.712
    Mar 22 20:29:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 20:29:47.714
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:47.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:47.741
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 22 20:29:47.750: INFO: Creating simple deployment test-new-deployment
    Mar 22 20:29:47.768: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 03/22/23 20:29:49.784
    STEP: updating a scale subresource 03/22/23 20:29:49.788
    STEP: verifying the deployment Spec.Replicas was modified 03/22/23 20:29:49.799
    STEP: Patch a scale subresource 03/22/23 20:29:49.813
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 20:29:49.835: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3429  0e12257b-15ef-49ff-a207-081d1da7f4b3 17611 3 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056f7cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-22 20:29:49 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 20:29:49 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 22 20:29:49.850: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3429  c55c656e-cac3-4303-8256-42b89873c38c 17607 3 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0e12257b-15ef-49ff-a207-081d1da7f4b3 0xc004f1a107 0xc004f1a108}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e12257b-15ef-49ff-a207-081d1da7f4b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1a198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:29:49.861: INFO: Pod "test-new-deployment-7f5969cbc7-5mwq5" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5mwq5 test-new-deployment-7f5969cbc7- deployment-3429  dc0693f0-2ed6-4038-9e1a-e5d58ae66fa5 17608 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a587 0xc004f1a588}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6psc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6psc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 20:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-65d56" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-65d56 test-new-deployment-7f5969cbc7- deployment-3429  c1453704-cc9a-4ee9-9f5c-e4139a96dd16 17617 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a740 0xc004f1a741}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djsvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djsvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-7t6bq" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7t6bq test-new-deployment-7f5969cbc7- deployment-3429  92f97e3f-3f25-436f-99fb-5bd7190d79e1 17593 0 2023-03-22 20:29:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1a8a0 0xc004f1a8a1}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgrw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgrw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.43,StartTime:2023-03-22 20:29:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:29:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf1c445c30340e85e9fff537223cdad43a944303526437b69971d47d4456fa25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 20:29:49.862: INFO: Pod "test-new-deployment-7f5969cbc7-xxgm4" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xxgm4 test-new-deployment-7f5969cbc7- deployment-3429  03fafa43-c430-47e6-a1f6-130b740c44c1 17614 0 2023-03-22 20:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c55c656e-cac3-4303-8256-42b89873c38c 0xc004f1aa90 0xc004f1aa91}] [] [{kube-controller-manager Update v1 2023-03-22 20:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c55c656e-cac3-4303-8256-42b89873c38c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttxv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttxv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:49.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3429" for this suite. 03/22/23 20:29:49.871
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:49.884
Mar 22 20:29:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:29:49.885
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:49.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:49.906
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-f6bc20db-4ba7-4df0-b32e-37fa7464f562 03/22/23 20:29:49.911
STEP: Creating a pod to test consume configMaps 03/22/23 20:29:49.917
Mar 22 20:29:49.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b" in namespace "configmap-9039" to be "Succeeded or Failed"
Mar 22 20:29:49.932: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.835483ms
Mar 22 20:29:51.938: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013378677s
Mar 22 20:29:53.939: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01434538s
STEP: Saw pod success 03/22/23 20:29:53.939
Mar 22 20:29:53.939: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b" satisfied condition "Succeeded or Failed"
Mar 22 20:29:53.943: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:29:53.954
Mar 22 20:29:53.967: INFO: Waiting for pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b to disappear
Mar 22 20:29:53.971: INFO: Pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:29:53.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9039" for this suite. 03/22/23 20:29:53.978
------------------------------
• [4.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:49.884
    Mar 22 20:29:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:29:49.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:49.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:49.906
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-f6bc20db-4ba7-4df0-b32e-37fa7464f562 03/22/23 20:29:49.911
    STEP: Creating a pod to test consume configMaps 03/22/23 20:29:49.917
    Mar 22 20:29:49.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b" in namespace "configmap-9039" to be "Succeeded or Failed"
    Mar 22 20:29:49.932: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.835483ms
    Mar 22 20:29:51.938: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013378677s
    Mar 22 20:29:53.939: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01434538s
    STEP: Saw pod success 03/22/23 20:29:53.939
    Mar 22 20:29:53.939: INFO: Pod "pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b" satisfied condition "Succeeded or Failed"
    Mar 22 20:29:53.943: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:29:53.954
    Mar 22 20:29:53.967: INFO: Waiting for pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b to disappear
    Mar 22 20:29:53.971: INFO: Pod pod-configmaps-fcc9bf32-2455-4152-9ccc-d0acd244407b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:29:53.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9039" for this suite. 03/22/23 20:29:53.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:29:53.99
Mar 22 20:29:53.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:29:53.991
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:54.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:54.017
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-df232e4d-6ab7-4d48-9a04-32f039b9a150 03/22/23 20:29:54.028
STEP: Creating secret with name s-test-opt-upd-abc8abf6-9379-4dfe-a96a-46e449135280 03/22/23 20:29:54.037
STEP: Creating the pod 03/22/23 20:29:54.044
Mar 22 20:29:54.053: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236" in namespace "projected-4366" to be "running and ready"
Mar 22 20:29:54.059: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29591ms
Mar 22 20:29:54.060: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:29:56.065: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011343413s
Mar 22 20:29:56.066: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:29:58.066: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Running", Reason="", readiness=true. Elapsed: 4.012022138s
Mar 22 20:29:58.067: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Running (Ready = true)
Mar 22 20:29:58.067: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-df232e4d-6ab7-4d48-9a04-32f039b9a150 03/22/23 20:29:58.129
STEP: Updating secret s-test-opt-upd-abc8abf6-9379-4dfe-a96a-46e449135280 03/22/23 20:29:58.136
STEP: Creating secret with name s-test-opt-create-3bd816b6-35d8-4907-a1ec-bfa740d7d048 03/22/23 20:29:58.144
STEP: waiting to observe update in volume 03/22/23 20:29:58.15
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 20:31:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4366" for this suite. 03/22/23 20:31:12.831
------------------------------
• [SLOW TEST] [78.850 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:29:53.99
    Mar 22 20:29:53.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:29:53.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:29:54.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:29:54.017
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-df232e4d-6ab7-4d48-9a04-32f039b9a150 03/22/23 20:29:54.028
    STEP: Creating secret with name s-test-opt-upd-abc8abf6-9379-4dfe-a96a-46e449135280 03/22/23 20:29:54.037
    STEP: Creating the pod 03/22/23 20:29:54.044
    Mar 22 20:29:54.053: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236" in namespace "projected-4366" to be "running and ready"
    Mar 22 20:29:54.059: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29591ms
    Mar 22 20:29:54.060: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:29:56.065: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011343413s
    Mar 22 20:29:56.066: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:29:58.066: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236": Phase="Running", Reason="", readiness=true. Elapsed: 4.012022138s
    Mar 22 20:29:58.067: INFO: The phase of Pod pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236 is Running (Ready = true)
    Mar 22 20:29:58.067: INFO: Pod "pod-projected-secrets-df8de9a3-8132-47df-9d54-f8c913c82236" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-df232e4d-6ab7-4d48-9a04-32f039b9a150 03/22/23 20:29:58.129
    STEP: Updating secret s-test-opt-upd-abc8abf6-9379-4dfe-a96a-46e449135280 03/22/23 20:29:58.136
    STEP: Creating secret with name s-test-opt-create-3bd816b6-35d8-4907-a1ec-bfa740d7d048 03/22/23 20:29:58.144
    STEP: waiting to observe update in volume 03/22/23 20:29:58.15
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:31:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4366" for this suite. 03/22/23 20:31:12.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:31:12.849
Mar 22 20:31:12.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename cronjob 03/22/23 20:31:12.851
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:12.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:12.877
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/22/23 20:31:12.883
STEP: creating 03/22/23 20:31:12.884
STEP: getting 03/22/23 20:31:12.893
STEP: listing 03/22/23 20:31:12.897
STEP: watching 03/22/23 20:31:12.902
Mar 22 20:31:12.902: INFO: starting watch
STEP: cluster-wide listing 03/22/23 20:31:12.905
STEP: cluster-wide watching 03/22/23 20:31:12.91
Mar 22 20:31:12.910: INFO: starting watch
STEP: patching 03/22/23 20:31:12.914
STEP: updating 03/22/23 20:31:12.923
Mar 22 20:31:12.933: INFO: waiting for watch events with expected annotations
Mar 22 20:31:12.933: INFO: saw patched and updated annotations
STEP: patching /status 03/22/23 20:31:12.934
STEP: updating /status 03/22/23 20:31:12.952
STEP: get /status 03/22/23 20:31:12.966
STEP: deleting 03/22/23 20:31:12.974
STEP: deleting a collection 03/22/23 20:31:12.991
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 22 20:31:13.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4861" for this suite. 03/22/23 20:31:13.012
------------------------------
• [0.172 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:31:12.849
    Mar 22 20:31:12.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename cronjob 03/22/23 20:31:12.851
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:12.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:12.877
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/22/23 20:31:12.883
    STEP: creating 03/22/23 20:31:12.884
    STEP: getting 03/22/23 20:31:12.893
    STEP: listing 03/22/23 20:31:12.897
    STEP: watching 03/22/23 20:31:12.902
    Mar 22 20:31:12.902: INFO: starting watch
    STEP: cluster-wide listing 03/22/23 20:31:12.905
    STEP: cluster-wide watching 03/22/23 20:31:12.91
    Mar 22 20:31:12.910: INFO: starting watch
    STEP: patching 03/22/23 20:31:12.914
    STEP: updating 03/22/23 20:31:12.923
    Mar 22 20:31:12.933: INFO: waiting for watch events with expected annotations
    Mar 22 20:31:12.933: INFO: saw patched and updated annotations
    STEP: patching /status 03/22/23 20:31:12.934
    STEP: updating /status 03/22/23 20:31:12.952
    STEP: get /status 03/22/23 20:31:12.966
    STEP: deleting 03/22/23 20:31:12.974
    STEP: deleting a collection 03/22/23 20:31:12.991
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:31:13.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4861" for this suite. 03/22/23 20:31:13.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:31:13.022
Mar 22 20:31:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename subpath 03/22/23 20:31:13.024
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:13.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:13.063
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/22/23 20:31:13.073
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-wzcj 03/22/23 20:31:13.09
STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:31:13.09
Mar 22 20:31:13.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wzcj" in namespace "subpath-9597" to be "Succeeded or Failed"
Mar 22 20:31:13.112: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870185ms
Mar 22 20:31:15.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012508037s
Mar 22 20:31:17.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 4.01307202s
Mar 22 20:31:19.131: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 6.025881126s
Mar 22 20:31:21.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 8.012670418s
Mar 22 20:31:23.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012244198s
Mar 22 20:31:25.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012069163s
Mar 22 20:31:27.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 14.01328556s
Mar 22 20:31:29.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 16.012336737s
Mar 22 20:31:31.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 18.012862107s
Mar 22 20:31:33.132: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 20.027566142s
Mar 22 20:31:35.121: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=false. Elapsed: 22.016567927s
Mar 22 20:31:37.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013242022s
STEP: Saw pod success 03/22/23 20:31:37.118
Mar 22 20:31:37.119: INFO: Pod "pod-subpath-test-projected-wzcj" satisfied condition "Succeeded or Failed"
Mar 22 20:31:37.123: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod pod-subpath-test-projected-wzcj container test-container-subpath-projected-wzcj: <nil>
STEP: delete the pod 03/22/23 20:31:37.177
Mar 22 20:31:37.192: INFO: Waiting for pod pod-subpath-test-projected-wzcj to disappear
Mar 22 20:31:37.196: INFO: Pod pod-subpath-test-projected-wzcj no longer exists
STEP: Deleting pod pod-subpath-test-projected-wzcj 03/22/23 20:31:37.196
Mar 22 20:31:37.197: INFO: Deleting pod "pod-subpath-test-projected-wzcj" in namespace "subpath-9597"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 22 20:31:37.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9597" for this suite. 03/22/23 20:31:37.212
------------------------------
• [SLOW TEST] [24.202 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:31:13.022
    Mar 22 20:31:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename subpath 03/22/23 20:31:13.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:13.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:13.063
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/22/23 20:31:13.073
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-wzcj 03/22/23 20:31:13.09
    STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:31:13.09
    Mar 22 20:31:13.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wzcj" in namespace "subpath-9597" to be "Succeeded or Failed"
    Mar 22 20:31:13.112: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870185ms
    Mar 22 20:31:15.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012508037s
    Mar 22 20:31:17.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 4.01307202s
    Mar 22 20:31:19.131: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 6.025881126s
    Mar 22 20:31:21.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 8.012670418s
    Mar 22 20:31:23.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012244198s
    Mar 22 20:31:25.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012069163s
    Mar 22 20:31:27.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 14.01328556s
    Mar 22 20:31:29.117: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 16.012336737s
    Mar 22 20:31:31.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 18.012862107s
    Mar 22 20:31:33.132: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=true. Elapsed: 20.027566142s
    Mar 22 20:31:35.121: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Running", Reason="", readiness=false. Elapsed: 22.016567927s
    Mar 22 20:31:37.118: INFO: Pod "pod-subpath-test-projected-wzcj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013242022s
    STEP: Saw pod success 03/22/23 20:31:37.118
    Mar 22 20:31:37.119: INFO: Pod "pod-subpath-test-projected-wzcj" satisfied condition "Succeeded or Failed"
    Mar 22 20:31:37.123: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod pod-subpath-test-projected-wzcj container test-container-subpath-projected-wzcj: <nil>
    STEP: delete the pod 03/22/23 20:31:37.177
    Mar 22 20:31:37.192: INFO: Waiting for pod pod-subpath-test-projected-wzcj to disappear
    Mar 22 20:31:37.196: INFO: Pod pod-subpath-test-projected-wzcj no longer exists
    STEP: Deleting pod pod-subpath-test-projected-wzcj 03/22/23 20:31:37.196
    Mar 22 20:31:37.197: INFO: Deleting pod "pod-subpath-test-projected-wzcj" in namespace "subpath-9597"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:31:37.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9597" for this suite. 03/22/23 20:31:37.212
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:31:37.227
Mar 22 20:31:37.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:31:37.229
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:37.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:37.25
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-b2cc2dd3-759f-4960-82d4-fda78c7d8f45 03/22/23 20:31:37.257
STEP: Creating a pod to test consume secrets 03/22/23 20:31:37.267
Mar 22 20:31:37.278: INFO: Waiting up to 5m0s for pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231" in namespace "secrets-9448" to be "Succeeded or Failed"
Mar 22 20:31:37.285: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826002ms
Mar 22 20:31:39.292: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014306526s
Mar 22 20:31:41.292: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01472661s
STEP: Saw pod success 03/22/23 20:31:41.293
Mar 22 20:31:41.293: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231" satisfied condition "Succeeded or Failed"
Mar 22 20:31:41.297: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:31:41.31
Mar 22 20:31:41.329: INFO: Waiting for pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 to disappear
Mar 22 20:31:41.332: INFO: Pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:31:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9448" for this suite. 03/22/23 20:31:41.34
------------------------------
• [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:31:37.227
    Mar 22 20:31:37.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:31:37.229
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:37.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:37.25
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-b2cc2dd3-759f-4960-82d4-fda78c7d8f45 03/22/23 20:31:37.257
    STEP: Creating a pod to test consume secrets 03/22/23 20:31:37.267
    Mar 22 20:31:37.278: INFO: Waiting up to 5m0s for pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231" in namespace "secrets-9448" to be "Succeeded or Failed"
    Mar 22 20:31:37.285: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826002ms
    Mar 22 20:31:39.292: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014306526s
    Mar 22 20:31:41.292: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01472661s
    STEP: Saw pod success 03/22/23 20:31:41.293
    Mar 22 20:31:41.293: INFO: Pod "pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231" satisfied condition "Succeeded or Failed"
    Mar 22 20:31:41.297: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:31:41.31
    Mar 22 20:31:41.329: INFO: Waiting for pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 to disappear
    Mar 22 20:31:41.332: INFO: Pod pod-secrets-5b0efcd1-fa13-475e-b1da-c3e41b53b231 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:31:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9448" for this suite. 03/22/23 20:31:41.34
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:31:41.352
Mar 22 20:31:41.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename subpath 03/22/23 20:31:41.354
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:41.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:41.382
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/22/23 20:31:41.392
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-8zms 03/22/23 20:31:41.415
STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:31:41.416
Mar 22 20:31:41.424: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8zms" in namespace "subpath-2190" to be "Succeeded or Failed"
Mar 22 20:31:41.433: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Pending", Reason="", readiness=false. Elapsed: 7.958081ms
Mar 22 20:31:43.445: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 2.019763346s
Mar 22 20:31:45.442: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 4.017268671s
Mar 22 20:31:47.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 6.013882335s
Mar 22 20:31:49.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 8.013112046s
Mar 22 20:31:51.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 10.014512206s
Mar 22 20:31:53.437: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 12.012524832s
Mar 22 20:31:55.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 14.013420033s
Mar 22 20:31:57.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 16.013186521s
Mar 22 20:31:59.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 18.013204332s
Mar 22 20:32:01.445: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 20.020417011s
Mar 22 20:32:03.444: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=false. Elapsed: 22.019036396s
Mar 22 20:32:05.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014373889s
STEP: Saw pod success 03/22/23 20:32:05.439
Mar 22 20:32:05.439: INFO: Pod "pod-subpath-test-secret-8zms" satisfied condition "Succeeded or Failed"
Mar 22 20:32:05.444: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-secret-8zms container test-container-subpath-secret-8zms: <nil>
STEP: delete the pod 03/22/23 20:32:05.457
Mar 22 20:32:05.470: INFO: Waiting for pod pod-subpath-test-secret-8zms to disappear
Mar 22 20:32:05.474: INFO: Pod pod-subpath-test-secret-8zms no longer exists
STEP: Deleting pod pod-subpath-test-secret-8zms 03/22/23 20:32:05.474
Mar 22 20:32:05.474: INFO: Deleting pod "pod-subpath-test-secret-8zms" in namespace "subpath-2190"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2190" for this suite. 03/22/23 20:32:05.483
------------------------------
• [SLOW TEST] [24.139 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:31:41.352
    Mar 22 20:31:41.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename subpath 03/22/23 20:31:41.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:31:41.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:31:41.382
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/22/23 20:31:41.392
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-8zms 03/22/23 20:31:41.415
    STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:31:41.416
    Mar 22 20:31:41.424: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8zms" in namespace "subpath-2190" to be "Succeeded or Failed"
    Mar 22 20:31:41.433: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Pending", Reason="", readiness=false. Elapsed: 7.958081ms
    Mar 22 20:31:43.445: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 2.019763346s
    Mar 22 20:31:45.442: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 4.017268671s
    Mar 22 20:31:47.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 6.013882335s
    Mar 22 20:31:49.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 8.013112046s
    Mar 22 20:31:51.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 10.014512206s
    Mar 22 20:31:53.437: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 12.012524832s
    Mar 22 20:31:55.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 14.013420033s
    Mar 22 20:31:57.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 16.013186521s
    Mar 22 20:31:59.438: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 18.013204332s
    Mar 22 20:32:01.445: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=true. Elapsed: 20.020417011s
    Mar 22 20:32:03.444: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Running", Reason="", readiness=false. Elapsed: 22.019036396s
    Mar 22 20:32:05.439: INFO: Pod "pod-subpath-test-secret-8zms": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014373889s
    STEP: Saw pod success 03/22/23 20:32:05.439
    Mar 22 20:32:05.439: INFO: Pod "pod-subpath-test-secret-8zms" satisfied condition "Succeeded or Failed"
    Mar 22 20:32:05.444: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-secret-8zms container test-container-subpath-secret-8zms: <nil>
    STEP: delete the pod 03/22/23 20:32:05.457
    Mar 22 20:32:05.470: INFO: Waiting for pod pod-subpath-test-secret-8zms to disappear
    Mar 22 20:32:05.474: INFO: Pod pod-subpath-test-secret-8zms no longer exists
    STEP: Deleting pod pod-subpath-test-secret-8zms 03/22/23 20:32:05.474
    Mar 22 20:32:05.474: INFO: Deleting pod "pod-subpath-test-secret-8zms" in namespace "subpath-2190"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2190" for this suite. 03/22/23 20:32:05.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:05.497
Mar 22 20:32:05.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:32:05.498
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:05.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:05.52
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/22/23 20:32:05.527
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/22/23 20:32:05.534
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/22/23 20:32:05.534
STEP: creating a pod to probe DNS 03/22/23 20:32:05.535
STEP: submitting the pod to kubernetes 03/22/23 20:32:05.535
Mar 22 20:32:05.545: INFO: Waiting up to 15m0s for pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0" in namespace "dns-1535" to be "running"
Mar 22 20:32:05.554: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.460361ms
Mar 22 20:32:07.561: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015589466s
Mar 22 20:32:07.561: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:32:07.561
STEP: looking for the results for each expected name from probers 03/22/23 20:32:07.567
Mar 22 20:32:07.629: INFO: DNS probes using dns-1535/dns-test-cf078817-2008-4f56-9213-a0650c4533c0 succeeded

STEP: deleting the pod 03/22/23 20:32:07.639
STEP: deleting the test headless service 03/22/23 20:32:07.652
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1535" for this suite. 03/22/23 20:32:07.68
------------------------------
• [2.191 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:05.497
    Mar 22 20:32:05.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:32:05.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:05.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:05.52
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/22/23 20:32:05.527
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/22/23 20:32:05.534
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1535.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/22/23 20:32:05.534
    STEP: creating a pod to probe DNS 03/22/23 20:32:05.535
    STEP: submitting the pod to kubernetes 03/22/23 20:32:05.535
    Mar 22 20:32:05.545: INFO: Waiting up to 15m0s for pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0" in namespace "dns-1535" to be "running"
    Mar 22 20:32:05.554: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.460361ms
    Mar 22 20:32:07.561: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015589466s
    Mar 22 20:32:07.561: INFO: Pod "dns-test-cf078817-2008-4f56-9213-a0650c4533c0" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:32:07.561
    STEP: looking for the results for each expected name from probers 03/22/23 20:32:07.567
    Mar 22 20:32:07.629: INFO: DNS probes using dns-1535/dns-test-cf078817-2008-4f56-9213-a0650c4533c0 succeeded

    STEP: deleting the pod 03/22/23 20:32:07.639
    STEP: deleting the test headless service 03/22/23 20:32:07.652
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1535" for this suite. 03/22/23 20:32:07.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:07.696
Mar 22 20:32:07.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 20:32:07.697
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:07.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:07.716
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 22 20:32:07.763: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"04cd226a-807f-4f5c-9374-2e9533e7a463", Controller:(*bool)(0xc003f03d06), BlockOwnerDeletion:(*bool)(0xc003f03d07)}}
Mar 22 20:32:07.773: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4ec77585-1e73-4772-92b3-9c085fd59359", Controller:(*bool)(0xc003f03f86), BlockOwnerDeletion:(*bool)(0xc003f03f87)}}
Mar 22 20:32:07.782: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3c0ffb22-1bca-4460-93f3-c8fd5777f967", Controller:(*bool)(0xc003092a56), BlockOwnerDeletion:(*bool)(0xc003092a57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8883" for this suite. 03/22/23 20:32:12.801
------------------------------
• [SLOW TEST] [5.115 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:07.696
    Mar 22 20:32:07.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 20:32:07.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:07.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:07.716
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 22 20:32:07.763: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"04cd226a-807f-4f5c-9374-2e9533e7a463", Controller:(*bool)(0xc003f03d06), BlockOwnerDeletion:(*bool)(0xc003f03d07)}}
    Mar 22 20:32:07.773: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4ec77585-1e73-4772-92b3-9c085fd59359", Controller:(*bool)(0xc003f03f86), BlockOwnerDeletion:(*bool)(0xc003f03f87)}}
    Mar 22 20:32:07.782: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3c0ffb22-1bca-4460-93f3-c8fd5777f967", Controller:(*bool)(0xc003092a56), BlockOwnerDeletion:(*bool)(0xc003092a57)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8883" for this suite. 03/22/23 20:32:12.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:12.812
Mar 22 20:32:12.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:32:12.813
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:12.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:12.834
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:32:12.856
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:13.818
STEP: Deploying the webhook pod 03/22/23 20:32:13.833
STEP: Wait for the deployment to be ready 03/22/23 20:32:13.849
Mar 22 20:32:13.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:32:15.879
STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:15.892
Mar 22 20:32:16.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/22/23 20:32:16.897
STEP: create a configmap that should be updated by the webhook 03/22/23 20:32:16.943
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4674" for this suite. 03/22/23 20:32:17.086
STEP: Destroying namespace "webhook-4674-markers" for this suite. 03/22/23 20:32:17.096
------------------------------
• [4.292 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:12.812
    Mar 22 20:32:12.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:32:12.813
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:12.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:12.834
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:32:12.856
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:13.818
    STEP: Deploying the webhook pod 03/22/23 20:32:13.833
    STEP: Wait for the deployment to be ready 03/22/23 20:32:13.849
    Mar 22 20:32:13.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:32:15.879
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:15.892
    Mar 22 20:32:16.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/22/23 20:32:16.897
    STEP: create a configmap that should be updated by the webhook 03/22/23 20:32:16.943
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4674" for this suite. 03/22/23 20:32:17.086
    STEP: Destroying namespace "webhook-4674-markers" for this suite. 03/22/23 20:32:17.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:17.11
Mar 22 20:32:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:32:17.111
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:17.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:17.133
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:32:17.185
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:17.729
STEP: Deploying the webhook pod 03/22/23 20:32:17.736
STEP: Wait for the deployment to be ready 03/22/23 20:32:17.753
Mar 22 20:32:17.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:32:19.783
STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:19.796
Mar 22 20:32:20.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/22/23 20:32:20.804
STEP: create a namespace for the webhook 03/22/23 20:32:20.868
STEP: create a configmap should be unconditionally rejected by the webhook 03/22/23 20:32:20.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:20.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3611" for this suite. 03/22/23 20:32:20.994
STEP: Destroying namespace "webhook-3611-markers" for this suite. 03/22/23 20:32:21.02
------------------------------
• [3.927 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:17.11
    Mar 22 20:32:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:32:17.111
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:17.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:17.133
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:32:17.185
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:17.729
    STEP: Deploying the webhook pod 03/22/23 20:32:17.736
    STEP: Wait for the deployment to be ready 03/22/23 20:32:17.753
    Mar 22 20:32:17.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:32:19.783
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:19.796
    Mar 22 20:32:20.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/22/23 20:32:20.804
    STEP: create a namespace for the webhook 03/22/23 20:32:20.868
    STEP: create a configmap should be unconditionally rejected by the webhook 03/22/23 20:32:20.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:20.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3611" for this suite. 03/22/23 20:32:20.994
    STEP: Destroying namespace "webhook-3611-markers" for this suite. 03/22/23 20:32:21.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:21.056
Mar 22 20:32:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:32:21.057
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:21.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:21.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:32:21.123
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:21.768
STEP: Deploying the webhook pod 03/22/23 20:32:21.79
STEP: Wait for the deployment to be ready 03/22/23 20:32:21.804
Mar 22 20:32:21.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:32:23.835
STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:23.849
Mar 22 20:32:24.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Mar 22 20:32:24.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-505-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 20:32:25.38
STEP: Creating a custom resource while v1 is storage version 03/22/23 20:32:25.423
STEP: Patching Custom Resource Definition to set v2 as storage 03/22/23 20:32:27.53
STEP: Patching the custom resource while v2 is storage version 03/22/23 20:32:27.55
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6170" for this suite. 03/22/23 20:32:28.192
STEP: Destroying namespace "webhook-6170-markers" for this suite. 03/22/23 20:32:28.199
------------------------------
• [SLOW TEST] [7.152 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:21.056
    Mar 22 20:32:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:32:21.057
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:21.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:21.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:32:21.123
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:32:21.768
    STEP: Deploying the webhook pod 03/22/23 20:32:21.79
    STEP: Wait for the deployment to be ready 03/22/23 20:32:21.804
    Mar 22 20:32:21.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:32:23.835
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:32:23.849
    Mar 22 20:32:24.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Mar 22 20:32:24.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-505-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 20:32:25.38
    STEP: Creating a custom resource while v1 is storage version 03/22/23 20:32:25.423
    STEP: Patching Custom Resource Definition to set v2 as storage 03/22/23 20:32:27.53
    STEP: Patching the custom resource while v2 is storage version 03/22/23 20:32:27.55
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6170" for this suite. 03/22/23 20:32:28.192
    STEP: Destroying namespace "webhook-6170-markers" for this suite. 03/22/23 20:32:28.199
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:28.213
Mar 22 20:32:28.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/22/23 20:32:28.215
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:28.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:28.243
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/22/23 20:32:28.261
STEP: Creating hostNetwork=false pod 03/22/23 20:32:28.261
Mar 22 20:32:28.271: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5436" to be "running and ready"
Mar 22 20:32:28.284: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.931595ms
Mar 22 20:32:28.284: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:32:30.290: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018143034s
Mar 22 20:32:30.290: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 22 20:32:30.290: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/22/23 20:32:30.294
Mar 22 20:32:30.307: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5436" to be "running and ready"
Mar 22 20:32:30.312: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942743ms
Mar 22 20:32:30.312: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:32:32.318: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764944s
Mar 22 20:32:32.318: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 22 20:32:32.318: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/22/23 20:32:32.322
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/22/23 20:32:32.323
Mar 22 20:32:32.323: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:32.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:32.324: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:32.324: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 22 20:32:32.463: INFO: Exec stderr: ""
Mar 22 20:32:32.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:32.464: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:32.465: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 22 20:32:32.614: INFO: Exec stderr: ""
Mar 22 20:32:32.615: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:32.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:32.616: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:32.616: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 22 20:32:32.772: INFO: Exec stderr: ""
Mar 22 20:32:32.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:32.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:32.775: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:32.775: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 22 20:32:32.900: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/22/23 20:32:32.9
Mar 22 20:32:32.901: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:32.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:32.902: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:32.902: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 22 20:32:33.052: INFO: Exec stderr: ""
Mar 22 20:32:33.052: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:33.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:33.053: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:33.053: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 22 20:32:33.185: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/22/23 20:32:33.185
Mar 22 20:32:33.185: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:33.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:33.187: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:33.187: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 22 20:32:33.323: INFO: Exec stderr: ""
Mar 22 20:32:33.323: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:33.324: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:33.324: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 22 20:32:33.452: INFO: Exec stderr: ""
Mar 22 20:32:33.452: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:33.453: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:33.454: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 22 20:32:33.600: INFO: Exec stderr: ""
Mar 22 20:32:33.600: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:32:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:32:33.601: INFO: ExecWithOptions: Clientset creation
Mar 22 20:32:33.601: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 22 20:32:33.742: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:33.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5436" for this suite. 03/22/23 20:32:33.748
------------------------------
• [SLOW TEST] [5.549 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:28.213
    Mar 22 20:32:28.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/22/23 20:32:28.215
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:28.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:28.243
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/22/23 20:32:28.261
    STEP: Creating hostNetwork=false pod 03/22/23 20:32:28.261
    Mar 22 20:32:28.271: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5436" to be "running and ready"
    Mar 22 20:32:28.284: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.931595ms
    Mar 22 20:32:28.284: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:32:30.290: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018143034s
    Mar 22 20:32:30.290: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 22 20:32:30.290: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/22/23 20:32:30.294
    Mar 22 20:32:30.307: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5436" to be "running and ready"
    Mar 22 20:32:30.312: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942743ms
    Mar 22 20:32:30.312: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:32:32.318: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764944s
    Mar 22 20:32:32.318: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 22 20:32:32.318: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/22/23 20:32:32.322
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/22/23 20:32:32.323
    Mar 22 20:32:32.323: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:32.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:32.324: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:32.324: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 22 20:32:32.463: INFO: Exec stderr: ""
    Mar 22 20:32:32.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:32.464: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:32.465: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 22 20:32:32.614: INFO: Exec stderr: ""
    Mar 22 20:32:32.615: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:32.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:32.616: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:32.616: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 22 20:32:32.772: INFO: Exec stderr: ""
    Mar 22 20:32:32.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:32.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:32.775: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:32.775: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 22 20:32:32.900: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/22/23 20:32:32.9
    Mar 22 20:32:32.901: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:32.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:32.902: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:32.902: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 22 20:32:33.052: INFO: Exec stderr: ""
    Mar 22 20:32:33.052: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:33.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:33.053: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:33.053: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 22 20:32:33.185: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/22/23 20:32:33.185
    Mar 22 20:32:33.185: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:33.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:33.187: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:33.187: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 22 20:32:33.323: INFO: Exec stderr: ""
    Mar 22 20:32:33.323: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:33.324: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:33.324: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 22 20:32:33.452: INFO: Exec stderr: ""
    Mar 22 20:32:33.452: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:33.453: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:33.454: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 22 20:32:33.600: INFO: Exec stderr: ""
    Mar 22 20:32:33.600: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5436 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:32:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:32:33.601: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:32:33.601: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5436/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 22 20:32:33.742: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:33.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5436" for this suite. 03/22/23 20:32:33.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:33.771
Mar 22 20:32:33.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename events 03/22/23 20:32:33.773
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:33.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:33.795
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/22/23 20:32:33.801
Mar 22 20:32:33.808: INFO: created test-event-1
Mar 22 20:32:33.814: INFO: created test-event-2
Mar 22 20:32:33.821: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/22/23 20:32:33.821
STEP: delete collection of events 03/22/23 20:32:33.829
Mar 22 20:32:33.829: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/22/23 20:32:33.849
Mar 22 20:32:33.850: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:33.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3572" for this suite. 03/22/23 20:32:33.859
------------------------------
• [0.099 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:33.771
    Mar 22 20:32:33.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename events 03/22/23 20:32:33.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:33.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:33.795
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/22/23 20:32:33.801
    Mar 22 20:32:33.808: INFO: created test-event-1
    Mar 22 20:32:33.814: INFO: created test-event-2
    Mar 22 20:32:33.821: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/22/23 20:32:33.821
    STEP: delete collection of events 03/22/23 20:32:33.829
    Mar 22 20:32:33.829: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/22/23 20:32:33.849
    Mar 22 20:32:33.850: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:33.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3572" for this suite. 03/22/23 20:32:33.859
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:33.87
Mar 22 20:32:33.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:32:33.873
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:33.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:33.904
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 03/22/23 20:32:33.911
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/22/23 20:32:33.919
STEP: patching the secret 03/22/23 20:32:33.929
STEP: deleting the secret using a LabelSelector 03/22/23 20:32:33.94
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/22/23 20:32:33.948
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:33.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4840" for this suite. 03/22/23 20:32:33.96
------------------------------
• [0.098 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:33.87
    Mar 22 20:32:33.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:32:33.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:33.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:33.904
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 03/22/23 20:32:33.911
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/22/23 20:32:33.919
    STEP: patching the secret 03/22/23 20:32:33.929
    STEP: deleting the secret using a LabelSelector 03/22/23 20:32:33.94
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/22/23 20:32:33.948
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:33.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4840" for this suite. 03/22/23 20:32:33.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:33.979
Mar 22 20:32:33.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir-wrapper 03/22/23 20:32:33.981
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:34.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:34.019
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 22 20:32:34.059: INFO: Waiting up to 5m0s for pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2" in namespace "emptydir-wrapper-2064" to be "running and ready"
Mar 22 20:32:34.065: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.403897ms
Mar 22 20:32:34.065: INFO: The phase of Pod pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:32:36.071: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012046534s
Mar 22 20:32:36.072: INFO: The phase of Pod pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2 is Running (Ready = true)
Mar 22 20:32:36.072: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/22/23 20:32:36.076
STEP: Cleaning up the configmap 03/22/23 20:32:36.083
STEP: Cleaning up the pod 03/22/23 20:32:36.089
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:36.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2064" for this suite. 03/22/23 20:32:36.109
------------------------------
• [2.136 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:33.979
    Mar 22 20:32:33.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir-wrapper 03/22/23 20:32:33.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:34.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:34.019
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 22 20:32:34.059: INFO: Waiting up to 5m0s for pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2" in namespace "emptydir-wrapper-2064" to be "running and ready"
    Mar 22 20:32:34.065: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.403897ms
    Mar 22 20:32:34.065: INFO: The phase of Pod pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:32:36.071: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012046534s
    Mar 22 20:32:36.072: INFO: The phase of Pod pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2 is Running (Ready = true)
    Mar 22 20:32:36.072: INFO: Pod "pod-secrets-3a68c737-3dd3-44d4-b070-48a10be91ae2" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/22/23 20:32:36.076
    STEP: Cleaning up the configmap 03/22/23 20:32:36.083
    STEP: Cleaning up the pod 03/22/23 20:32:36.089
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:36.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2064" for this suite. 03/22/23 20:32:36.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:36.122
Mar 22 20:32:36.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:32:36.128
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.155
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1381" for this suite. 03/22/23 20:32:36.172
------------------------------
• [0.059 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:36.122
    Mar 22 20:32:36.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:32:36.128
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.155
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1381" for this suite. 03/22/23 20:32:36.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:36.188
Mar 22 20:32:36.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 20:32:36.19
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.214
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 03/22/23 20:32:36.22
Mar 22 20:32:36.227: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/22/23 20:32:36.227
Mar 22 20:32:36.235: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/22/23 20:32:36.235
Mar 22 20:32:36.254: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:36.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2830" for this suite. 03/22/23 20:32:36.261
------------------------------
• [0.085 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:36.188
    Mar 22 20:32:36.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 20:32:36.19
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.214
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 03/22/23 20:32:36.22
    Mar 22 20:32:36.227: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/22/23 20:32:36.227
    Mar 22 20:32:36.235: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/22/23 20:32:36.235
    Mar 22 20:32:36.254: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:36.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2830" for this suite. 03/22/23 20:32:36.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:36.276
Mar 22 20:32:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:32:36.278
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.299
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Mar 22 20:32:36.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: creating the pod 03/22/23 20:32:36.308
STEP: submitting the pod to kubernetes 03/22/23 20:32:36.308
Mar 22 20:32:36.319: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e" in namespace "pods-9097" to be "running and ready"
Mar 22 20:32:36.333: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.704197ms
Mar 22 20:32:36.333: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:32:38.339: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019361742s
Mar 22 20:32:38.339: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:32:40.340: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.020529457s
Mar 22 20:32:40.340: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Running (Ready = true)
Mar 22 20:32:40.340: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:40.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9097" for this suite. 03/22/23 20:32:40.505
------------------------------
• [4.237 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:36.276
    Mar 22 20:32:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:32:36.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:36.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:36.299
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Mar 22 20:32:36.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: creating the pod 03/22/23 20:32:36.308
    STEP: submitting the pod to kubernetes 03/22/23 20:32:36.308
    Mar 22 20:32:36.319: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e" in namespace "pods-9097" to be "running and ready"
    Mar 22 20:32:36.333: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.704197ms
    Mar 22 20:32:36.333: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:32:38.339: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019361742s
    Mar 22 20:32:38.339: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:32:40.340: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.020529457s
    Mar 22 20:32:40.340: INFO: The phase of Pod pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e is Running (Ready = true)
    Mar 22 20:32:40.340: INFO: Pod "pod-exec-websocket-967301b6-c248-4b42-9bd1-9e8e48322d5e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:40.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9097" for this suite. 03/22/23 20:32:40.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:40.519
Mar 22 20:32:40.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 20:32:40.521
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:40.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:40.547
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/22/23 20:32:40.554
Mar 22 20:32:40.564: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 22 20:32:45.571: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 20:32:45.572
STEP: getting scale subresource 03/22/23 20:32:45.572
STEP: updating a scale subresource 03/22/23 20:32:45.576
STEP: verifying the replicaset Spec.Replicas was modified 03/22/23 20:32:45.583
STEP: Patch a scale subresource 03/22/23 20:32:45.589
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:32:45.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4214" for this suite. 03/22/23 20:32:45.636
------------------------------
• [SLOW TEST] [5.127 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:40.519
    Mar 22 20:32:40.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 20:32:40.521
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:40.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:40.547
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/22/23 20:32:40.554
    Mar 22 20:32:40.564: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 22 20:32:45.571: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 20:32:45.572
    STEP: getting scale subresource 03/22/23 20:32:45.572
    STEP: updating a scale subresource 03/22/23 20:32:45.576
    STEP: verifying the replicaset Spec.Replicas was modified 03/22/23 20:32:45.583
    STEP: Patch a scale subresource 03/22/23 20:32:45.589
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:32:45.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4214" for this suite. 03/22/23 20:32:45.636
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:32:45.655
Mar 22 20:32:45.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename taint-multiple-pods 03/22/23 20:32:45.658
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:45.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:45.683
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Mar 22 20:32:45.688: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 20:33:45.732: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Mar 22 20:33:45.737: INFO: Starting informer...
STEP: Starting pods... 03/22/23 20:33:45.737
Mar 22 20:33:45.959: INFO: Pod1 is running on pool-v7t41yxh0-q56kk. Tainting Node
Mar 22 20:33:46.174: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3125" to be "running"
Mar 22 20:33:46.178: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381612ms
Mar 22 20:33:48.184: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009999436s
Mar 22 20:33:48.184: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 22 20:33:48.184: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3125" to be "running"
Mar 22 20:33:48.188: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.411752ms
Mar 22 20:33:48.188: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 22 20:33:48.188: INFO: Pod2 is running on pool-v7t41yxh0-q56kk. Tainting Node
STEP: Trying to apply a taint on the Node 03/22/23 20:33:48.188
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:33:48.205
STEP: Waiting for Pod1 and Pod2 to be deleted 03/22/23 20:33:48.209
Mar 22 20:33:54.258: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 22 20:34:14.329: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:34:14.364
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:14.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-3125" for this suite. 03/22/23 20:34:14.376
------------------------------
• [SLOW TEST] [88.734 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:32:45.655
    Mar 22 20:32:45.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename taint-multiple-pods 03/22/23 20:32:45.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:32:45.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:32:45.683
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Mar 22 20:32:45.688: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 20:33:45.732: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Mar 22 20:33:45.737: INFO: Starting informer...
    STEP: Starting pods... 03/22/23 20:33:45.737
    Mar 22 20:33:45.959: INFO: Pod1 is running on pool-v7t41yxh0-q56kk. Tainting Node
    Mar 22 20:33:46.174: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3125" to be "running"
    Mar 22 20:33:46.178: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381612ms
    Mar 22 20:33:48.184: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009999436s
    Mar 22 20:33:48.184: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 22 20:33:48.184: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3125" to be "running"
    Mar 22 20:33:48.188: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.411752ms
    Mar 22 20:33:48.188: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 22 20:33:48.188: INFO: Pod2 is running on pool-v7t41yxh0-q56kk. Tainting Node
    STEP: Trying to apply a taint on the Node 03/22/23 20:33:48.188
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:33:48.205
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/22/23 20:33:48.209
    Mar 22 20:33:54.258: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 22 20:34:14.329: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:34:14.364
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:14.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-3125" for this suite. 03/22/23 20:34:14.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:14.391
Mar 22 20:34:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:34:14.393
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:14.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:14.414
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9075 03/22/23 20:34:14.42
STEP: changing the ExternalName service to type=NodePort 03/22/23 20:34:14.427
STEP: creating replication controller externalname-service in namespace services-9075 03/22/23 20:34:14.451
I0322 20:34:14.457406      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9075, replica count: 2
I0322 20:34:17.509205      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:34:17.509: INFO: Creating new exec pod
Mar 22 20:34:17.526: INFO: Waiting up to 5m0s for pod "execpodpplr8" in namespace "services-9075" to be "running"
Mar 22 20:34:17.539: INFO: Pod "execpodpplr8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.711189ms
Mar 22 20:34:19.547: INFO: Pod "execpodpplr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.020396151s
Mar 22 20:34:19.547: INFO: Pod "execpodpplr8" satisfied condition "running"
Mar 22 20:34:20.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 22 20:34:20.875: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 22 20:34:20.875: INFO: stdout: ""
Mar 22 20:34:20.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.245.150.24 80'
Mar 22 20:34:21.173: INFO: stderr: "+ nc -v -z -w 2 10.245.150.24 80\nConnection to 10.245.150.24 80 port [tcp/http] succeeded!\n"
Mar 22 20:34:21.173: INFO: stdout: ""
Mar 22 20:34:21.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 31396'
Mar 22 20:34:21.467: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 31396\nConnection to 10.124.0.2 31396 port [tcp/*] succeeded!\n"
Mar 22 20:34:21.467: INFO: stdout: ""
Mar 22 20:34:21.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 31396'
Mar 22 20:34:21.773: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 31396\nConnection to 10.124.0.4 31396 port [tcp/*] succeeded!\n"
Mar 22 20:34:21.773: INFO: stdout: ""
Mar 22 20:34:21.773: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:21.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9075" for this suite. 03/22/23 20:34:21.808
------------------------------
• [SLOW TEST] [7.433 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:14.391
    Mar 22 20:34:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:34:14.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:14.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:14.414
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9075 03/22/23 20:34:14.42
    STEP: changing the ExternalName service to type=NodePort 03/22/23 20:34:14.427
    STEP: creating replication controller externalname-service in namespace services-9075 03/22/23 20:34:14.451
    I0322 20:34:14.457406      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9075, replica count: 2
    I0322 20:34:17.509205      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:34:17.509: INFO: Creating new exec pod
    Mar 22 20:34:17.526: INFO: Waiting up to 5m0s for pod "execpodpplr8" in namespace "services-9075" to be "running"
    Mar 22 20:34:17.539: INFO: Pod "execpodpplr8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.711189ms
    Mar 22 20:34:19.547: INFO: Pod "execpodpplr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.020396151s
    Mar 22 20:34:19.547: INFO: Pod "execpodpplr8" satisfied condition "running"
    Mar 22 20:34:20.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 22 20:34:20.875: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 22 20:34:20.875: INFO: stdout: ""
    Mar 22 20:34:20.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.245.150.24 80'
    Mar 22 20:34:21.173: INFO: stderr: "+ nc -v -z -w 2 10.245.150.24 80\nConnection to 10.245.150.24 80 port [tcp/http] succeeded!\n"
    Mar 22 20:34:21.173: INFO: stdout: ""
    Mar 22 20:34:21.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 31396'
    Mar 22 20:34:21.467: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 31396\nConnection to 10.124.0.2 31396 port [tcp/*] succeeded!\n"
    Mar 22 20:34:21.467: INFO: stdout: ""
    Mar 22 20:34:21.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-9075 exec execpodpplr8 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 31396'
    Mar 22 20:34:21.773: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 31396\nConnection to 10.124.0.4 31396 port [tcp/*] succeeded!\n"
    Mar 22 20:34:21.773: INFO: stdout: ""
    Mar 22 20:34:21.773: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:21.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9075" for this suite. 03/22/23 20:34:21.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:21.826
Mar 22 20:34:21.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:34:21.83
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:21.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:21.852
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 03/22/23 20:34:21.859
STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:34:21.867
STEP: Creating a ResourceQuota with not best effort scope 03/22/23 20:34:23.879
STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:34:23.888
STEP: Creating a best-effort pod 03/22/23 20:34:25.893
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/22/23 20:34:25.91
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/22/23 20:34:27.917
STEP: Deleting the pod 03/22/23 20:34:29.924
STEP: Ensuring resource quota status released the pod usage 03/22/23 20:34:29.941
STEP: Creating a not best-effort pod 03/22/23 20:34:31.951
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/22/23 20:34:31.965
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/22/23 20:34:33.972
STEP: Deleting the pod 03/22/23 20:34:35.982
STEP: Ensuring resource quota status released the pod usage 03/22/23 20:34:35.998
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:38.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-777" for this suite. 03/22/23 20:34:38.009
------------------------------
• [SLOW TEST] [16.192 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:21.826
    Mar 22 20:34:21.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:34:21.83
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:21.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:21.852
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 03/22/23 20:34:21.859
    STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:34:21.867
    STEP: Creating a ResourceQuota with not best effort scope 03/22/23 20:34:23.879
    STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:34:23.888
    STEP: Creating a best-effort pod 03/22/23 20:34:25.893
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/22/23 20:34:25.91
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/22/23 20:34:27.917
    STEP: Deleting the pod 03/22/23 20:34:29.924
    STEP: Ensuring resource quota status released the pod usage 03/22/23 20:34:29.941
    STEP: Creating a not best-effort pod 03/22/23 20:34:31.951
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/22/23 20:34:31.965
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/22/23 20:34:33.972
    STEP: Deleting the pod 03/22/23 20:34:35.982
    STEP: Ensuring resource quota status released the pod usage 03/22/23 20:34:35.998
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:38.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-777" for this suite. 03/22/23 20:34:38.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:38.023
Mar 22 20:34:38.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename containers 03/22/23 20:34:38.024
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:38.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:38.044
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 03/22/23 20:34:38.05
Mar 22 20:34:38.059: INFO: Waiting up to 5m0s for pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464" in namespace "containers-9195" to be "Succeeded or Failed"
Mar 22 20:34:38.068: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Pending", Reason="", readiness=false. Elapsed: 9.276192ms
Mar 22 20:34:40.073: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014469629s
Mar 22 20:34:42.073: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014872313s
STEP: Saw pod success 03/22/23 20:34:42.074
Mar 22 20:34:42.074: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464" satisfied condition "Succeeded or Failed"
Mar 22 20:34:42.079: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:34:42.129
Mar 22 20:34:42.143: INFO: Waiting for pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 to disappear
Mar 22 20:34:42.147: INFO: Pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:42.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9195" for this suite. 03/22/23 20:34:42.153
------------------------------
• [4.143 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:38.023
    Mar 22 20:34:38.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename containers 03/22/23 20:34:38.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:38.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:38.044
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 03/22/23 20:34:38.05
    Mar 22 20:34:38.059: INFO: Waiting up to 5m0s for pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464" in namespace "containers-9195" to be "Succeeded or Failed"
    Mar 22 20:34:38.068: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Pending", Reason="", readiness=false. Elapsed: 9.276192ms
    Mar 22 20:34:40.073: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014469629s
    Mar 22 20:34:42.073: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014872313s
    STEP: Saw pod success 03/22/23 20:34:42.074
    Mar 22 20:34:42.074: INFO: Pod "client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464" satisfied condition "Succeeded or Failed"
    Mar 22 20:34:42.079: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:34:42.129
    Mar 22 20:34:42.143: INFO: Waiting for pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 to disappear
    Mar 22 20:34:42.147: INFO: Pod client-containers-24c0c98a-b9f7-4b3e-8441-8d577fe9e464 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:42.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9195" for this suite. 03/22/23 20:34:42.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:42.183
Mar 22 20:34:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:34:42.185
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:42.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:42.211
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:34:42.234
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:34:42.716
STEP: Deploying the webhook pod 03/22/23 20:34:42.725
STEP: Wait for the deployment to be ready 03/22/23 20:34:42.74
Mar 22 20:34:42.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:34:44.77
STEP: Verifying the service has paired with the endpoint 03/22/23 20:34:44.786
Mar 22 20:34:45.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 03/22/23 20:34:45.793
STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.845
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/22/23 20:34:45.876
STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.89
STEP: Patching a validating webhook configuration's rules to include the create operation 03/22/23 20:34:45.902
STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.912
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:45.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2199" for this suite. 03/22/23 20:34:45.99
STEP: Destroying namespace "webhook-2199-markers" for this suite. 03/22/23 20:34:46.004
------------------------------
• [3.837 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:42.183
    Mar 22 20:34:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:34:42.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:42.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:42.211
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:34:42.234
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:34:42.716
    STEP: Deploying the webhook pod 03/22/23 20:34:42.725
    STEP: Wait for the deployment to be ready 03/22/23 20:34:42.74
    Mar 22 20:34:42.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:34:44.77
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:34:44.786
    Mar 22 20:34:45.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 03/22/23 20:34:45.793
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.845
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/22/23 20:34:45.876
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.89
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/22/23 20:34:45.902
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:34:45.912
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:45.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2199" for this suite. 03/22/23 20:34:45.99
    STEP: Destroying namespace "webhook-2199-markers" for this suite. 03/22/23 20:34:46.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:46.03
Mar 22 20:34:46.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:34:46.031
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.053
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 03/22/23 20:34:46.06
Mar 22 20:34:46.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3893 api-versions'
Mar 22 20:34:46.199: INFO: stderr: ""
Mar 22 20:34:46.199: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:46.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3893" for this suite. 03/22/23 20:34:46.205
------------------------------
• [0.183 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:46.03
    Mar 22 20:34:46.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:34:46.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.053
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 03/22/23 20:34:46.06
    Mar 22 20:34:46.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3893 api-versions'
    Mar 22 20:34:46.199: INFO: stderr: ""
    Mar 22 20:34:46.199: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:46.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3893" for this suite. 03/22/23 20:34:46.205
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:46.213
Mar 22 20:34:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:34:46.214
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.234
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:46.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-506" for this suite. 03/22/23 20:34:46.306
------------------------------
• [0.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:46.213
    Mar 22 20:34:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:34:46.214
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.234
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:46.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-506" for this suite. 03/22/23 20:34:46.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:46.321
Mar 22 20:34:46.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename limitrange 03/22/23 20:34:46.322
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.355
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 03/22/23 20:34:46.361
STEP: Setting up watch 03/22/23 20:34:46.361
STEP: Submitting a LimitRange 03/22/23 20:34:46.466
STEP: Verifying LimitRange creation was observed 03/22/23 20:34:46.478
STEP: Fetching the LimitRange to ensure it has proper values 03/22/23 20:34:46.478
Mar 22 20:34:46.483: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 22 20:34:46.483: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/22/23 20:34:46.483
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/22/23 20:34:46.491
Mar 22 20:34:46.499: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 22 20:34:46.500: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/22/23 20:34:46.5
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/22/23 20:34:46.511
Mar 22 20:34:46.516: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 22 20:34:46.517: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/22/23 20:34:46.517
STEP: Failing to create a Pod with more than max resources 03/22/23 20:34:46.52
STEP: Updating a LimitRange 03/22/23 20:34:46.524
STEP: Verifying LimitRange updating is effective 03/22/23 20:34:46.531
STEP: Creating a Pod with less than former min resources 03/22/23 20:34:48.538
STEP: Failing to create a Pod with more than max resources 03/22/23 20:34:48.546
STEP: Deleting a LimitRange 03/22/23 20:34:48.549
STEP: Verifying the LimitRange was deleted 03/22/23 20:34:48.557
Mar 22 20:34:53.563: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/22/23 20:34:53.563
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-287" for this suite. 03/22/23 20:34:53.579
------------------------------
• [SLOW TEST] [7.277 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:46.321
    Mar 22 20:34:46.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename limitrange 03/22/23 20:34:46.322
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:46.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:46.355
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 03/22/23 20:34:46.361
    STEP: Setting up watch 03/22/23 20:34:46.361
    STEP: Submitting a LimitRange 03/22/23 20:34:46.466
    STEP: Verifying LimitRange creation was observed 03/22/23 20:34:46.478
    STEP: Fetching the LimitRange to ensure it has proper values 03/22/23 20:34:46.478
    Mar 22 20:34:46.483: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 22 20:34:46.483: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/22/23 20:34:46.483
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/22/23 20:34:46.491
    Mar 22 20:34:46.499: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 22 20:34:46.500: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/22/23 20:34:46.5
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/22/23 20:34:46.511
    Mar 22 20:34:46.516: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 22 20:34:46.517: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/22/23 20:34:46.517
    STEP: Failing to create a Pod with more than max resources 03/22/23 20:34:46.52
    STEP: Updating a LimitRange 03/22/23 20:34:46.524
    STEP: Verifying LimitRange updating is effective 03/22/23 20:34:46.531
    STEP: Creating a Pod with less than former min resources 03/22/23 20:34:48.538
    STEP: Failing to create a Pod with more than max resources 03/22/23 20:34:48.546
    STEP: Deleting a LimitRange 03/22/23 20:34:48.549
    STEP: Verifying the LimitRange was deleted 03/22/23 20:34:48.557
    Mar 22 20:34:53.563: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/22/23 20:34:53.563
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-287" for this suite. 03/22/23 20:34:53.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:53.601
Mar 22 20:34:53.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:34:53.602
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:53.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:53.622
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-9129da76-970b-4bf7-85f6-0a4296bfcb18 03/22/23 20:34:53.633
STEP: Creating configMap with name cm-test-opt-upd-0735d4cc-ff6b-4dc7-9038-fd5b133b24f4 03/22/23 20:34:53.639
STEP: Creating the pod 03/22/23 20:34:53.646
Mar 22 20:34:53.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d" in namespace "configmap-1895" to be "running and ready"
Mar 22 20:34:53.663: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685895ms
Mar 22 20:34:53.678: INFO: The phase of Pod pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:34:55.685: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.028811339s
Mar 22 20:34:55.685: INFO: The phase of Pod pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d is Running (Ready = true)
Mar 22 20:34:55.685: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9129da76-970b-4bf7-85f6-0a4296bfcb18 03/22/23 20:34:55.723
STEP: Updating configmap cm-test-opt-upd-0735d4cc-ff6b-4dc7-9038-fd5b133b24f4 03/22/23 20:34:55.733
STEP: Creating configMap with name cm-test-opt-create-fefe725a-bee6-4051-be64-8db6895af213 03/22/23 20:34:55.741
STEP: waiting to observe update in volume 03/22/23 20:34:55.748
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:34:59.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1895" for this suite. 03/22/23 20:34:59.821
------------------------------
• [SLOW TEST] [6.229 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:53.601
    Mar 22 20:34:53.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:34:53.602
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:53.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:53.622
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-9129da76-970b-4bf7-85f6-0a4296bfcb18 03/22/23 20:34:53.633
    STEP: Creating configMap with name cm-test-opt-upd-0735d4cc-ff6b-4dc7-9038-fd5b133b24f4 03/22/23 20:34:53.639
    STEP: Creating the pod 03/22/23 20:34:53.646
    Mar 22 20:34:53.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d" in namespace "configmap-1895" to be "running and ready"
    Mar 22 20:34:53.663: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685895ms
    Mar 22 20:34:53.678: INFO: The phase of Pod pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:34:55.685: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.028811339s
    Mar 22 20:34:55.685: INFO: The phase of Pod pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d is Running (Ready = true)
    Mar 22 20:34:55.685: INFO: Pod "pod-configmaps-df45012b-0702-4c36-a93a-38c7cafcf98d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9129da76-970b-4bf7-85f6-0a4296bfcb18 03/22/23 20:34:55.723
    STEP: Updating configmap cm-test-opt-upd-0735d4cc-ff6b-4dc7-9038-fd5b133b24f4 03/22/23 20:34:55.733
    STEP: Creating configMap with name cm-test-opt-create-fefe725a-bee6-4051-be64-8db6895af213 03/22/23 20:34:55.741
    STEP: waiting to observe update in volume 03/22/23 20:34:55.748
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:34:59.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1895" for this suite. 03/22/23 20:34:59.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:34:59.834
Mar 22 20:34:59.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:34:59.835
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:59.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:59.86
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-539c1786-a377-4549-ac77-227ca3746b91 03/22/23 20:34:59.878
STEP: Creating secret with name s-test-opt-upd-e0addc8b-5fb8-4309-99be-82670daff8b7 03/22/23 20:34:59.895
STEP: Creating the pod 03/22/23 20:34:59.902
Mar 22 20:34:59.920: INFO: Waiting up to 5m0s for pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62" in namespace "secrets-7734" to be "running and ready"
Mar 22 20:34:59.926: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138274ms
Mar 22 20:34:59.926: INFO: The phase of Pod pod-secrets-81de6713-d74a-4382-a650-f091423f5d62 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:35:01.933: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62": Phase="Running", Reason="", readiness=true. Elapsed: 2.012910374s
Mar 22 20:35:01.933: INFO: The phase of Pod pod-secrets-81de6713-d74a-4382-a650-f091423f5d62 is Running (Ready = true)
Mar 22 20:35:01.933: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-539c1786-a377-4549-ac77-227ca3746b91 03/22/23 20:35:02.044
STEP: Updating secret s-test-opt-upd-e0addc8b-5fb8-4309-99be-82670daff8b7 03/22/23 20:35:02.057
STEP: Creating secret with name s-test-opt-create-08271d79-e243-47f5-be24-1f12b34a02c8 03/22/23 20:35:02.065
STEP: waiting to observe update in volume 03/22/23 20:35:02.074
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:04.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7734" for this suite. 03/22/23 20:35:04.146
------------------------------
• [4.322 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:34:59.834
    Mar 22 20:34:59.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:34:59.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:34:59.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:34:59.86
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-539c1786-a377-4549-ac77-227ca3746b91 03/22/23 20:34:59.878
    STEP: Creating secret with name s-test-opt-upd-e0addc8b-5fb8-4309-99be-82670daff8b7 03/22/23 20:34:59.895
    STEP: Creating the pod 03/22/23 20:34:59.902
    Mar 22 20:34:59.920: INFO: Waiting up to 5m0s for pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62" in namespace "secrets-7734" to be "running and ready"
    Mar 22 20:34:59.926: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138274ms
    Mar 22 20:34:59.926: INFO: The phase of Pod pod-secrets-81de6713-d74a-4382-a650-f091423f5d62 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:35:01.933: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62": Phase="Running", Reason="", readiness=true. Elapsed: 2.012910374s
    Mar 22 20:35:01.933: INFO: The phase of Pod pod-secrets-81de6713-d74a-4382-a650-f091423f5d62 is Running (Ready = true)
    Mar 22 20:35:01.933: INFO: Pod "pod-secrets-81de6713-d74a-4382-a650-f091423f5d62" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-539c1786-a377-4549-ac77-227ca3746b91 03/22/23 20:35:02.044
    STEP: Updating secret s-test-opt-upd-e0addc8b-5fb8-4309-99be-82670daff8b7 03/22/23 20:35:02.057
    STEP: Creating secret with name s-test-opt-create-08271d79-e243-47f5-be24-1f12b34a02c8 03/22/23 20:35:02.065
    STEP: waiting to observe update in volume 03/22/23 20:35:02.074
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:04.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7734" for this suite. 03/22/23 20:35:04.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:04.157
Mar 22 20:35:04.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 20:35:04.158
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:04.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:04.186
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8953 03/22/23 20:35:04.194
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-8953 03/22/23 20:35:04.204
Mar 22 20:35:04.215: INFO: Found 0 stateful pods, waiting for 1
Mar 22 20:35:14.222: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/22/23 20:35:14.231
STEP: updating a scale subresource 03/22/23 20:35:14.235
STEP: verifying the statefulset Spec.Replicas was modified 03/22/23 20:35:14.244
STEP: Patch a scale subresource 03/22/23 20:35:14.252
STEP: verifying the statefulset Spec.Replicas was modified 03/22/23 20:35:14.266
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 20:35:14.270: INFO: Deleting all statefulset in ns statefulset-8953
Mar 22 20:35:14.275: INFO: Scaling statefulset ss to 0
Mar 22 20:35:24.300: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:35:24.323: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8953" for this suite. 03/22/23 20:35:24.348
------------------------------
• [SLOW TEST] [20.202 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:04.157
    Mar 22 20:35:04.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 20:35:04.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:04.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:04.186
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8953 03/22/23 20:35:04.194
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-8953 03/22/23 20:35:04.204
    Mar 22 20:35:04.215: INFO: Found 0 stateful pods, waiting for 1
    Mar 22 20:35:14.222: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/22/23 20:35:14.231
    STEP: updating a scale subresource 03/22/23 20:35:14.235
    STEP: verifying the statefulset Spec.Replicas was modified 03/22/23 20:35:14.244
    STEP: Patch a scale subresource 03/22/23 20:35:14.252
    STEP: verifying the statefulset Spec.Replicas was modified 03/22/23 20:35:14.266
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 20:35:14.270: INFO: Deleting all statefulset in ns statefulset-8953
    Mar 22 20:35:14.275: INFO: Scaling statefulset ss to 0
    Mar 22 20:35:24.300: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:35:24.323: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8953" for this suite. 03/22/23 20:35:24.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:24.363
Mar 22 20:35:24.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:35:24.365
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:24.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:24.388
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 03/22/23 20:35:24.394
Mar 22 20:35:24.404: INFO: Waiting up to 5m0s for pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8" in namespace "downward-api-569" to be "Succeeded or Failed"
Mar 22 20:35:24.410: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.129619ms
Mar 22 20:35:26.415: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010652523s
Mar 22 20:35:28.416: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011762484s
STEP: Saw pod success 03/22/23 20:35:28.416
Mar 22 20:35:28.417: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8" satisfied condition "Succeeded or Failed"
Mar 22 20:35:28.421: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 container dapi-container: <nil>
STEP: delete the pod 03/22/23 20:35:28.433
Mar 22 20:35:28.449: INFO: Waiting for pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 to disappear
Mar 22 20:35:28.453: INFO: Pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:28.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-569" for this suite. 03/22/23 20:35:28.459
------------------------------
• [4.106 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:24.363
    Mar 22 20:35:24.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:35:24.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:24.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:24.388
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 03/22/23 20:35:24.394
    Mar 22 20:35:24.404: INFO: Waiting up to 5m0s for pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8" in namespace "downward-api-569" to be "Succeeded or Failed"
    Mar 22 20:35:24.410: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.129619ms
    Mar 22 20:35:26.415: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010652523s
    Mar 22 20:35:28.416: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011762484s
    STEP: Saw pod success 03/22/23 20:35:28.416
    Mar 22 20:35:28.417: INFO: Pod "downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8" satisfied condition "Succeeded or Failed"
    Mar 22 20:35:28.421: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 20:35:28.433
    Mar 22 20:35:28.449: INFO: Waiting for pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 to disappear
    Mar 22 20:35:28.453: INFO: Pod downward-api-395ed26d-21b9-4ab0-a489-58db81d498f8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:28.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-569" for this suite. 03/22/23 20:35:28.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:28.475
Mar 22 20:35:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:35:28.476
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:28.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:28.498
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1336 03/22/23 20:35:28.503
STEP: creating replication controller nodeport-test in namespace services-1336 03/22/23 20:35:28.522
I0322 20:35:28.531536      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1336, replica count: 2
I0322 20:35:31.583581      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:35:31.583: INFO: Creating new exec pod
Mar 22 20:35:31.594: INFO: Waiting up to 5m0s for pod "execpod5jlmb" in namespace "services-1336" to be "running"
Mar 22 20:35:31.601: INFO: Pod "execpod5jlmb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.761218ms
Mar 22 20:35:33.607: INFO: Pod "execpod5jlmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013523793s
Mar 22 20:35:33.607: INFO: Pod "execpod5jlmb" satisfied condition "running"
Mar 22 20:35:34.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Mar 22 20:35:34.931: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 22 20:35:34.931: INFO: stdout: ""
Mar 22 20:35:34.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.245.141.43 80'
Mar 22 20:35:35.176: INFO: stderr: "+ nc -v -z -w 2 10.245.141.43 80\nConnection to 10.245.141.43 80 port [tcp/http] succeeded!\n"
Mar 22 20:35:35.176: INFO: stdout: ""
Mar 22 20:35:35.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.124.0.3 31577'
Mar 22 20:35:35.562: INFO: stderr: "+ nc -v -z -w 2 10.124.0.3 31577\nConnection to 10.124.0.3 31577 port [tcp/*] succeeded!\n"
Mar 22 20:35:35.562: INFO: stdout: ""
Mar 22 20:35:35.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 31577'
Mar 22 20:35:35.835: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 31577\nConnection to 10.124.0.4 31577 port [tcp/*] succeeded!\n"
Mar 22 20:35:35.835: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1336" for this suite. 03/22/23 20:35:35.841
------------------------------
• [SLOW TEST] [7.374 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:28.475
    Mar 22 20:35:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:35:28.476
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:28.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:28.498
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1336 03/22/23 20:35:28.503
    STEP: creating replication controller nodeport-test in namespace services-1336 03/22/23 20:35:28.522
    I0322 20:35:28.531536      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1336, replica count: 2
    I0322 20:35:31.583581      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:35:31.583: INFO: Creating new exec pod
    Mar 22 20:35:31.594: INFO: Waiting up to 5m0s for pod "execpod5jlmb" in namespace "services-1336" to be "running"
    Mar 22 20:35:31.601: INFO: Pod "execpod5jlmb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.761218ms
    Mar 22 20:35:33.607: INFO: Pod "execpod5jlmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013523793s
    Mar 22 20:35:33.607: INFO: Pod "execpod5jlmb" satisfied condition "running"
    Mar 22 20:35:34.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Mar 22 20:35:34.931: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 22 20:35:34.931: INFO: stdout: ""
    Mar 22 20:35:34.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.245.141.43 80'
    Mar 22 20:35:35.176: INFO: stderr: "+ nc -v -z -w 2 10.245.141.43 80\nConnection to 10.245.141.43 80 port [tcp/http] succeeded!\n"
    Mar 22 20:35:35.176: INFO: stdout: ""
    Mar 22 20:35:35.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.124.0.3 31577'
    Mar 22 20:35:35.562: INFO: stderr: "+ nc -v -z -w 2 10.124.0.3 31577\nConnection to 10.124.0.3 31577 port [tcp/*] succeeded!\n"
    Mar 22 20:35:35.562: INFO: stdout: ""
    Mar 22 20:35:35.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1336 exec execpod5jlmb -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 31577'
    Mar 22 20:35:35.835: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 31577\nConnection to 10.124.0.4 31577 port [tcp/*] succeeded!\n"
    Mar 22 20:35:35.835: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1336" for this suite. 03/22/23 20:35:35.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:35.85
Mar 22 20:35:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename podtemplate 03/22/23 20:35:35.853
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:35.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:35.874
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:35.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2890" for this suite. 03/22/23 20:35:35.933
------------------------------
• [0.090 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:35.85
    Mar 22 20:35:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename podtemplate 03/22/23 20:35:35.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:35.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:35.874
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:35.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2890" for this suite. 03/22/23 20:35:35.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:35.948
Mar 22 20:35:35.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 20:35:35.949
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:35.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:35.971
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Mar 22 20:35:35.988: INFO: Waiting up to 5m0s for pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a" in namespace "container-probe-8939" to be "running and ready"
Mar 22 20:35:35.993: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.490156ms
Mar 22 20:35:35.994: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:35:38.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.011663603s
Mar 22 20:35:38.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:40.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.01205872s
Mar 22 20:35:40.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:42.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.01173305s
Mar 22 20:35:42.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:44.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 8.01281751s
Mar 22 20:35:44.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:46.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 10.012049665s
Mar 22 20:35:46.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:48.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 12.011828818s
Mar 22 20:35:48.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:50.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 14.013086663s
Mar 22 20:35:50.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:52.006: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 16.01847541s
Mar 22 20:35:52.007: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:54.002: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 18.013657255s
Mar 22 20:35:54.002: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:56.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 20.012957468s
Mar 22 20:35:56.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
Mar 22 20:35:58.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=true. Elapsed: 22.011935836s
Mar 22 20:35:58.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = true)
Mar 22 20:35:58.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a" satisfied condition "running and ready"
Mar 22 20:35:58.005: INFO: Container started at 2023-03-22 20:35:36 +0000 UTC, pod became ready at 2023-03-22 20:35:56 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8939" for this suite. 03/22/23 20:35:58.011
------------------------------
• [SLOW TEST] [22.070 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:35.948
    Mar 22 20:35:35.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 20:35:35.949
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:35.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:35.971
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Mar 22 20:35:35.988: INFO: Waiting up to 5m0s for pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a" in namespace "container-probe-8939" to be "running and ready"
    Mar 22 20:35:35.993: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.490156ms
    Mar 22 20:35:35.994: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:35:38.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.011663603s
    Mar 22 20:35:38.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:40.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.01205872s
    Mar 22 20:35:40.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:42.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.01173305s
    Mar 22 20:35:42.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:44.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 8.01281751s
    Mar 22 20:35:44.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:46.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 10.012049665s
    Mar 22 20:35:46.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:48.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 12.011828818s
    Mar 22 20:35:48.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:50.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 14.013086663s
    Mar 22 20:35:50.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:52.006: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 16.01847541s
    Mar 22 20:35:52.007: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:54.002: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 18.013657255s
    Mar 22 20:35:54.002: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:56.001: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=false. Elapsed: 20.012957468s
    Mar 22 20:35:56.001: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = false)
    Mar 22 20:35:58.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a": Phase="Running", Reason="", readiness=true. Elapsed: 22.011935836s
    Mar 22 20:35:58.000: INFO: The phase of Pod test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a is Running (Ready = true)
    Mar 22 20:35:58.000: INFO: Pod "test-webserver-b921a719-c368-472b-a598-8f6b7e532b8a" satisfied condition "running and ready"
    Mar 22 20:35:58.005: INFO: Container started at 2023-03-22 20:35:36 +0000 UTC, pod became ready at 2023-03-22 20:35:56 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8939" for this suite. 03/22/23 20:35:58.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:58.023
Mar 22 20:35:58.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:35:58.025
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:58.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:58.047
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 22 20:35:58.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3012" for this suite. 03/22/23 20:35:58.065
------------------------------
• [0.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:58.023
    Mar 22 20:35:58.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename runtimeclass 03/22/23 20:35:58.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:58.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:58.047
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:35:58.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3012" for this suite. 03/22/23 20:35:58.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:35:58.075
Mar 22 20:35:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 20:35:58.077
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:58.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:58.096
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/22/23 20:35:58.106
STEP: Verify that the required pods have come up. 03/22/23 20:35:58.113
Mar 22 20:35:58.117: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 22 20:36:03.123: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 20:36:03.123
STEP: Getting /status 03/22/23 20:36:03.123
Mar 22 20:36:03.128: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/22/23 20:36:03.128
Mar 22 20:36:03.142: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/22/23 20:36:03.142
Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: ADDED
Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.147: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.147: INFO: Found replicaset test-rs in namespace replicaset-6388 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 22 20:36:03.147: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/22/23 20:36:03.148
Mar 22 20:36:03.148: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 22 20:36:03.156: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/22/23 20:36:03.156
Mar 22 20:36:03.160: INFO: Observed &ReplicaSet event: ADDED
Mar 22 20:36:03.161: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.161: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.162: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.162: INFO: Observed replicaset test-rs in namespace replicaset-6388 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 22 20:36:03.162: INFO: Observed &ReplicaSet event: MODIFIED
Mar 22 20:36:03.163: INFO: Found replicaset test-rs in namespace replicaset-6388 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 22 20:36:03.163: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:03.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6388" for this suite. 03/22/23 20:36:03.169
------------------------------
• [SLOW TEST] [5.101 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:35:58.075
    Mar 22 20:35:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 20:35:58.077
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:35:58.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:35:58.096
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/22/23 20:35:58.106
    STEP: Verify that the required pods have come up. 03/22/23 20:35:58.113
    Mar 22 20:35:58.117: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 22 20:36:03.123: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 20:36:03.123
    STEP: Getting /status 03/22/23 20:36:03.123
    Mar 22 20:36:03.128: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/22/23 20:36:03.128
    Mar 22 20:36:03.142: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/22/23 20:36:03.142
    Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: ADDED
    Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.146: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.147: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.147: INFO: Found replicaset test-rs in namespace replicaset-6388 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 22 20:36:03.147: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/22/23 20:36:03.148
    Mar 22 20:36:03.148: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 22 20:36:03.156: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/22/23 20:36:03.156
    Mar 22 20:36:03.160: INFO: Observed &ReplicaSet event: ADDED
    Mar 22 20:36:03.161: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.161: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.162: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.162: INFO: Observed replicaset test-rs in namespace replicaset-6388 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 22 20:36:03.162: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 22 20:36:03.163: INFO: Found replicaset test-rs in namespace replicaset-6388 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 22 20:36:03.163: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:03.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6388" for this suite. 03/22/23 20:36:03.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:03.177
Mar 22 20:36:03.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption 03/22/23 20:36:03.18
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.202
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:03.208
Mar 22 20:36:03.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption-2 03/22/23 20:36:03.209
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.228
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.25
STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.264
STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.279
STEP: listing a collection of PDBs across all namespaces 03/22/23 20:36:03.284
STEP: listing a collection of PDBs in namespace disruption-6523 03/22/23 20:36:03.29
STEP: deleting a collection of PDBs 03/22/23 20:36:03.295
STEP: Waiting for the PDB collection to be deleted 03/22/23 20:36:03.307
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:03.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:03.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3480" for this suite. 03/22/23 20:36:03.322
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6523" for this suite. 03/22/23 20:36:03.329
------------------------------
• [0.159 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:03.177
    Mar 22 20:36:03.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption 03/22/23 20:36:03.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.202
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:03.208
    Mar 22 20:36:03.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption-2 03/22/23 20:36:03.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.228
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.25
    STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.264
    STEP: Waiting for the pdb to be processed 03/22/23 20:36:03.279
    STEP: listing a collection of PDBs across all namespaces 03/22/23 20:36:03.284
    STEP: listing a collection of PDBs in namespace disruption-6523 03/22/23 20:36:03.29
    STEP: deleting a collection of PDBs 03/22/23 20:36:03.295
    STEP: Waiting for the PDB collection to be deleted 03/22/23 20:36:03.307
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:03.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:03.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3480" for this suite. 03/22/23 20:36:03.322
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6523" for this suite. 03/22/23 20:36:03.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:03.348
Mar 22 20:36:03.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:36:03.35
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.372
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:36:03.387
Mar 22 20:36:03.396: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3" in namespace "projected-4224" to be "Succeeded or Failed"
Mar 22 20:36:03.404: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478772ms
Mar 22 20:36:05.410: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012567231s
Mar 22 20:36:07.409: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012138813s
STEP: Saw pod success 03/22/23 20:36:07.409
Mar 22 20:36:07.410: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3" satisfied condition "Succeeded or Failed"
Mar 22 20:36:07.415: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 container client-container: <nil>
STEP: delete the pod 03/22/23 20:36:07.435
Mar 22 20:36:07.450: INFO: Waiting for pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 to disappear
Mar 22 20:36:07.454: INFO: Pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:07.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4224" for this suite. 03/22/23 20:36:07.46
------------------------------
• [4.121 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:03.348
    Mar 22 20:36:03.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:36:03.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:03.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:03.372
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:36:03.387
    Mar 22 20:36:03.396: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3" in namespace "projected-4224" to be "Succeeded or Failed"
    Mar 22 20:36:03.404: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478772ms
    Mar 22 20:36:05.410: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012567231s
    Mar 22 20:36:07.409: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012138813s
    STEP: Saw pod success 03/22/23 20:36:07.409
    Mar 22 20:36:07.410: INFO: Pod "downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3" satisfied condition "Succeeded or Failed"
    Mar 22 20:36:07.415: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:36:07.435
    Mar 22 20:36:07.450: INFO: Waiting for pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 to disappear
    Mar 22 20:36:07.454: INFO: Pod downwardapi-volume-7c2e6a8b-2dee-4925-a40d-62c0402e8ce3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:07.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4224" for this suite. 03/22/23 20:36:07.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:07.471
Mar 22 20:36:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename watch 03/22/23 20:36:07.473
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:07.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:07.5
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/22/23 20:36:07.508
STEP: modifying the configmap once 03/22/23 20:36:07.515
STEP: modifying the configmap a second time 03/22/23 20:36:07.527
STEP: deleting the configmap 03/22/23 20:36:07.538
STEP: creating a watch on configmaps from the resource version returned by the first update 03/22/23 20:36:07.546
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/22/23 20:36:07.549
Mar 22 20:36:07.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-726  832a27ae-6101-4763-9972-49c700968274 20798 0 2023-03-22 20:36:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-22 20:36:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:36:07.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-726  832a27ae-6101-4763-9972-49c700968274 20799 0 2023-03-22 20:36:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-22 20:36:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:07.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-726" for this suite. 03/22/23 20:36:07.557
------------------------------
• [0.093 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:07.471
    Mar 22 20:36:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename watch 03/22/23 20:36:07.473
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:07.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:07.5
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/22/23 20:36:07.508
    STEP: modifying the configmap once 03/22/23 20:36:07.515
    STEP: modifying the configmap a second time 03/22/23 20:36:07.527
    STEP: deleting the configmap 03/22/23 20:36:07.538
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/22/23 20:36:07.546
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/22/23 20:36:07.549
    Mar 22 20:36:07.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-726  832a27ae-6101-4763-9972-49c700968274 20798 0 2023-03-22 20:36:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-22 20:36:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:36:07.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-726  832a27ae-6101-4763-9972-49c700968274 20799 0 2023-03-22 20:36:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-22 20:36:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:07.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-726" for this suite. 03/22/23 20:36:07.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:07.567
Mar 22 20:36:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:36:07.569
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:07.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:07.597
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Mar 22 20:36:07.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 create -f -'
Mar 22 20:36:08.528: INFO: stderr: ""
Mar 22 20:36:08.528: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 22 20:36:08.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 create -f -'
Mar 22 20:36:08.861: INFO: stderr: ""
Mar 22 20:36:08.861: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/22/23 20:36:08.861
Mar 22 20:36:09.876: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:36:09.876: INFO: Found 1 / 1
Mar 22 20:36:09.876: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 22 20:36:09.904: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:36:09.905: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 20:36:09.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe pod agnhost-primary-lfwcn'
Mar 22 20:36:10.182: INFO: stderr: ""
Mar 22 20:36:10.182: INFO: stdout: "Name:             agnhost-primary-lfwcn\nNamespace:        kubectl-50\nPriority:         0\nService Account:  default\nNode:             pool-v7t41yxh0-q56kk/10.124.0.4\nStart Time:       Wed, 22 Mar 2023 20:36:08 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.0.245\nIPs:\n  IP:           10.244.0.245\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://786a3f18c5aee430d02cb75d01c2e6d31a6843881091c83f1ffbeae3f102eced\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 22 Mar 2023 20:36:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7j2p6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-7j2p6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-50/agnhost-primary-lfwcn to pool-v7t41yxh0-q56kk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 22 20:36:10.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe rc agnhost-primary'
Mar 22 20:36:10.340: INFO: stderr: ""
Mar 22 20:36:10.340: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-50\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lfwcn\n"
Mar 22 20:36:10.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe service agnhost-primary'
Mar 22 20:36:10.494: INFO: stderr: ""
Mar 22 20:36:10.494: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-50\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.245.241.47\nIPs:               10.245.241.47\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.0.245:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 22 20:36:10.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe node pool-v7t41yxh0-q56k5'
Mar 22 20:36:10.666: INFO: stderr: ""
Mar 22 20:36:10.667: INFO: stdout: "Name:               pool-v7t41yxh0-q56k5\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s-2vcpu-4gb\n                    beta.kubernetes.io/os=linux\n                    doks.digitalocean.com/managed=true\n                    doks.digitalocean.com/node-id=6f9ecd98-7c6f-4f68-98f3-600ee158e3bb\n                    doks.digitalocean.com/node-pool=pool-v7t41yxh0\n                    doks.digitalocean.com/node-pool-id=17eebe5f-8577-4b1f-9c17-4be7a9a4f4bc\n                    doks.digitalocean.com/version=1.26.3-do.0\n                    failure-domain.beta.kubernetes.io/region=sfo3\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pool-v7t41yxh0-q56k5\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=s-2vcpu-4gb\n                    region=sfo3\n                    topology.kubernetes.io/region=sfo3\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.124.0.2\n                    csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"346815183\"}\n                    io.cilium.network.ipv4-cilium-host: 10.244.1.107\n                    io.cilium.network.ipv4-health-ip: 10.244.1.83\n                    io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 22 Mar 2023 19:41:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pool-v7t41yxh0-q56k5\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 22 Mar 2023 20:36:01 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 22 Mar 2023 19:41:48 +0000   Wed, 22 Mar 2023 19:41:48 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.124.0.2\n  Hostname:    pool-v7t41yxh0-q56k5\n  ExternalIP:  147.182.228.236\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      82394940Ki\n  hugepages-2Mi:          0\n  memory:                 4023932Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1900m\n  ephemeral-storage:      75935176579\n  hugepages-2Mi:          0\n  memory:                 3110Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 cd044ab1ee654f0fb4f78967c200d11b\n  System UUID:                cd044ab1-ee65-4f0f-b4f7-8967c200d11b\n  Boot ID:                    4b742010-459c-4fa9-a2e5-b73395e9339e\n  Kernel Version:             6.0.0-0.deb11.6-amd64\n  OS Image:                   Debian GNU/Linux 11 (bullseye)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nProviderID:                   digitalocean://346815183\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-nkg6k                                               300m (15%)    0 (0%)      300Mi (9%)       0 (0%)         54m\n  kube-system                 coredns-9765d8f5f-k57jv                                    100m (5%)     0 (0%)      150M (4%)        150M (4%)      54m\n  kube-system                 cpc-bridge-proxy-mvx4m                                     100m (5%)     0 (0%)      75Mi (2%)        0 (0%)         54m\n  kube-system                 csi-do-node-q2wlc                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 do-node-agent-4mgkz                                        102m (5%)     102m (5%)   80Mi (2%)        300Mi (9%)     53m\n  kube-system                 konnectivity-agent-s74g2                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 kube-proxy-p8crx                                           0 (0%)        0 (0%)      125Mi (4%)       0 (0%)         54m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    602m (31%)       102m (5%)\n  memory                 758174080 (23%)  464572800 (14%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 54m                kube-proxy             \n  Normal   Starting                 54m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      54m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  54m                kubelet                Updated Node Allocatable limit across pods\n  Normal   Synced                   54m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode           54m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n  Normal   NodeReady                54m                kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeReady\n  Normal   RegisteredNode           39m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n  Normal   RegisteredNode           16m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n"
Mar 22 20:36:10.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe namespace kubectl-50'
Mar 22 20:36:10.797: INFO: stderr: ""
Mar 22 20:36:10.797: INFO: stdout: "Name:         kubectl-50\nLabels:       e2e-framework=kubectl\n              e2e-run=e69c15cc-2126-4a9e-901a-fc3a13da8e2f\n              kubernetes.io/metadata.name=kubectl-50\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:10.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-50" for this suite. 03/22/23 20:36:10.802
------------------------------
• [3.244 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:07.567
    Mar 22 20:36:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:36:07.569
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:07.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:07.597
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Mar 22 20:36:07.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 create -f -'
    Mar 22 20:36:08.528: INFO: stderr: ""
    Mar 22 20:36:08.528: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 22 20:36:08.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 create -f -'
    Mar 22 20:36:08.861: INFO: stderr: ""
    Mar 22 20:36:08.861: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/22/23 20:36:08.861
    Mar 22 20:36:09.876: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:36:09.876: INFO: Found 1 / 1
    Mar 22 20:36:09.876: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 22 20:36:09.904: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:36:09.905: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 22 20:36:09.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe pod agnhost-primary-lfwcn'
    Mar 22 20:36:10.182: INFO: stderr: ""
    Mar 22 20:36:10.182: INFO: stdout: "Name:             agnhost-primary-lfwcn\nNamespace:        kubectl-50\nPriority:         0\nService Account:  default\nNode:             pool-v7t41yxh0-q56kk/10.124.0.4\nStart Time:       Wed, 22 Mar 2023 20:36:08 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.0.245\nIPs:\n  IP:           10.244.0.245\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://786a3f18c5aee430d02cb75d01c2e6d31a6843881091c83f1ffbeae3f102eced\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 22 Mar 2023 20:36:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7j2p6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-7j2p6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-50/agnhost-primary-lfwcn to pool-v7t41yxh0-q56kk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Mar 22 20:36:10.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe rc agnhost-primary'
    Mar 22 20:36:10.340: INFO: stderr: ""
    Mar 22 20:36:10.340: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-50\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lfwcn\n"
    Mar 22 20:36:10.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe service agnhost-primary'
    Mar 22 20:36:10.494: INFO: stderr: ""
    Mar 22 20:36:10.494: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-50\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.245.241.47\nIPs:               10.245.241.47\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.0.245:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 22 20:36:10.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe node pool-v7t41yxh0-q56k5'
    Mar 22 20:36:10.666: INFO: stderr: ""
    Mar 22 20:36:10.667: INFO: stdout: "Name:               pool-v7t41yxh0-q56k5\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s-2vcpu-4gb\n                    beta.kubernetes.io/os=linux\n                    doks.digitalocean.com/managed=true\n                    doks.digitalocean.com/node-id=6f9ecd98-7c6f-4f68-98f3-600ee158e3bb\n                    doks.digitalocean.com/node-pool=pool-v7t41yxh0\n                    doks.digitalocean.com/node-pool-id=17eebe5f-8577-4b1f-9c17-4be7a9a4f4bc\n                    doks.digitalocean.com/version=1.26.3-do.0\n                    failure-domain.beta.kubernetes.io/region=sfo3\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pool-v7t41yxh0-q56k5\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=s-2vcpu-4gb\n                    region=sfo3\n                    topology.kubernetes.io/region=sfo3\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.124.0.2\n                    csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"346815183\"}\n                    io.cilium.network.ipv4-cilium-host: 10.244.1.107\n                    io.cilium.network.ipv4-health-ip: 10.244.1.83\n                    io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 22 Mar 2023 19:41:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pool-v7t41yxh0-q56k5\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 22 Mar 2023 20:36:01 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 22 Mar 2023 19:41:48 +0000   Wed, 22 Mar 2023 19:41:48 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 22 Mar 2023 20:34:47 +0000   Wed, 22 Mar 2023 19:41:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.124.0.2\n  Hostname:    pool-v7t41yxh0-q56k5\n  ExternalIP:  147.182.228.236\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      82394940Ki\n  hugepages-2Mi:          0\n  memory:                 4023932Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1900m\n  ephemeral-storage:      75935176579\n  hugepages-2Mi:          0\n  memory:                 3110Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 cd044ab1ee654f0fb4f78967c200d11b\n  System UUID:                cd044ab1-ee65-4f0f-b4f7-8967c200d11b\n  Boot ID:                    4b742010-459c-4fa9-a2e5-b73395e9339e\n  Kernel Version:             6.0.0-0.deb11.6-amd64\n  OS Image:                   Debian GNU/Linux 11 (bullseye)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.14\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nProviderID:                   digitalocean://346815183\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-nkg6k                                               300m (15%)    0 (0%)      300Mi (9%)       0 (0%)         54m\n  kube-system                 coredns-9765d8f5f-k57jv                                    100m (5%)     0 (0%)      150M (4%)        150M (4%)      54m\n  kube-system                 cpc-bridge-proxy-mvx4m                                     100m (5%)     0 (0%)      75Mi (2%)        0 (0%)         54m\n  kube-system                 csi-do-node-q2wlc                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 do-node-agent-4mgkz                                        102m (5%)     102m (5%)   80Mi (2%)        300Mi (9%)     53m\n  kube-system                 konnectivity-agent-s74g2                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 kube-proxy-p8crx                                           0 (0%)        0 (0%)      125Mi (4%)       0 (0%)         54m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    602m (31%)       102m (5%)\n  memory                 758174080 (23%)  464572800 (14%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 54m                kube-proxy             \n  Normal   Starting                 54m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      54m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     54m (x2 over 54m)  kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  54m                kubelet                Updated Node Allocatable limit across pods\n  Normal   Synced                   54m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode           54m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n  Normal   NodeReady                54m                kubelet                Node pool-v7t41yxh0-q56k5 status is now: NodeReady\n  Normal   RegisteredNode           39m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n  Normal   RegisteredNode           16m                node-controller        Node pool-v7t41yxh0-q56k5 event: Registered Node pool-v7t41yxh0-q56k5 in Controller\n"
    Mar 22 20:36:10.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-50 describe namespace kubectl-50'
    Mar 22 20:36:10.797: INFO: stderr: ""
    Mar 22 20:36:10.797: INFO: stdout: "Name:         kubectl-50\nLabels:       e2e-framework=kubectl\n              e2e-run=e69c15cc-2126-4a9e-901a-fc3a13da8e2f\n              kubernetes.io/metadata.name=kubectl-50\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:10.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-50" for this suite. 03/22/23 20:36:10.802
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:10.812
Mar 22 20:36:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:36:10.813
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:10.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:10.837
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 03/22/23 20:36:10.843
Mar 22 20:36:10.853: INFO: Waiting up to 5m0s for pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33" in namespace "downward-api-4796" to be "running and ready"
Mar 22 20:36:10.860: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962613ms
Mar 22 20:36:10.860: INFO: The phase of Pod labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:36:12.867: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33": Phase="Running", Reason="", readiness=true. Elapsed: 2.013806312s
Mar 22 20:36:12.867: INFO: The phase of Pod labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33 is Running (Ready = true)
Mar 22 20:36:12.867: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33" satisfied condition "running and ready"
Mar 22 20:36:13.403: INFO: Successfully updated pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:17.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4796" for this suite. 03/22/23 20:36:17.456
------------------------------
• [SLOW TEST] [6.651 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:10.812
    Mar 22 20:36:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:36:10.813
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:10.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:10.837
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 03/22/23 20:36:10.843
    Mar 22 20:36:10.853: INFO: Waiting up to 5m0s for pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33" in namespace "downward-api-4796" to be "running and ready"
    Mar 22 20:36:10.860: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962613ms
    Mar 22 20:36:10.860: INFO: The phase of Pod labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:36:12.867: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33": Phase="Running", Reason="", readiness=true. Elapsed: 2.013806312s
    Mar 22 20:36:12.867: INFO: The phase of Pod labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33 is Running (Ready = true)
    Mar 22 20:36:12.867: INFO: Pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33" satisfied condition "running and ready"
    Mar 22 20:36:13.403: INFO: Successfully updated pod "labelsupdate3ebdeab8-7e86-403c-b245-86664105bc33"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:17.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4796" for this suite. 03/22/23 20:36:17.456
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:17.463
Mar 22 20:36:17.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:36:17.465
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:17.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:17.49
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 03/22/23 20:36:17.497
STEP: fetching the ConfigMap 03/22/23 20:36:17.504
STEP: patching the ConfigMap 03/22/23 20:36:17.508
STEP: listing all ConfigMaps in all namespaces with a label selector 03/22/23 20:36:17.517
STEP: deleting the ConfigMap by collection with a label selector 03/22/23 20:36:17.524
STEP: listing all ConfigMaps in test namespace 03/22/23 20:36:17.546
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:17.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6115" for this suite. 03/22/23 20:36:17.569
------------------------------
• [0.121 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:17.463
    Mar 22 20:36:17.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:36:17.465
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:17.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:17.49
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 03/22/23 20:36:17.497
    STEP: fetching the ConfigMap 03/22/23 20:36:17.504
    STEP: patching the ConfigMap 03/22/23 20:36:17.508
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/22/23 20:36:17.517
    STEP: deleting the ConfigMap by collection with a label selector 03/22/23 20:36:17.524
    STEP: listing all ConfigMaps in test namespace 03/22/23 20:36:17.546
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:17.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6115" for this suite. 03/22/23 20:36:17.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:17.586
Mar 22 20:36:17.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 20:36:17.588
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:17.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:17.61
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 03/22/23 20:36:17.615
STEP: Ensuring job reaches completions 03/22/23 20:36:17.628
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6823" for this suite. 03/22/23 20:36:29.644
------------------------------
• [SLOW TEST] [12.067 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:17.586
    Mar 22 20:36:17.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 20:36:17.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:17.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:17.61
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 03/22/23 20:36:17.615
    STEP: Ensuring job reaches completions 03/22/23 20:36:17.628
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6823" for this suite. 03/22/23 20:36:29.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:29.655
Mar 22 20:36:29.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 20:36:29.656
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:29.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:29.677
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 03/22/23 20:36:29.683
Mar 22 20:36:29.698: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9991" to be "running and ready"
Mar 22 20:36:29.703: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735685ms
Mar 22 20:36:29.703: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:36:31.709: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010368632s
Mar 22 20:36:31.709: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 22 20:36:31.709: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/22/23 20:36:31.713
STEP: Then the orphan pod is adopted 03/22/23 20:36:31.72
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9991" for this suite. 03/22/23 20:36:32.756
------------------------------
• [3.110 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:29.655
    Mar 22 20:36:29.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 20:36:29.656
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:29.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:29.677
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/22/23 20:36:29.683
    Mar 22 20:36:29.698: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9991" to be "running and ready"
    Mar 22 20:36:29.703: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735685ms
    Mar 22 20:36:29.703: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:36:31.709: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010368632s
    Mar 22 20:36:31.709: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 22 20:36:31.709: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/22/23 20:36:31.713
    STEP: Then the orphan pod is adopted 03/22/23 20:36:31.72
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9991" for this suite. 03/22/23 20:36:32.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:32.765
Mar 22 20:36:32.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 20:36:32.768
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:32.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:32.789
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 03/22/23 20:36:32.795
Mar 22 20:36:32.804: INFO: Waiting up to 5m0s for pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5" in namespace "var-expansion-8875" to be "Succeeded or Failed"
Mar 22 20:36:32.810: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101752ms
Mar 22 20:36:34.824: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020048009s
Mar 22 20:36:36.817: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012817119s
STEP: Saw pod success 03/22/23 20:36:36.817
Mar 22 20:36:36.817: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5" satisfied condition "Succeeded or Failed"
Mar 22 20:36:36.821: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 container dapi-container: <nil>
STEP: delete the pod 03/22/23 20:36:36.833
Mar 22 20:36:36.851: INFO: Waiting for pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 to disappear
Mar 22 20:36:36.855: INFO: Pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 20:36:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8875" for this suite. 03/22/23 20:36:36.862
------------------------------
• [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:32.765
    Mar 22 20:36:32.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 20:36:32.768
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:32.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:32.789
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 03/22/23 20:36:32.795
    Mar 22 20:36:32.804: INFO: Waiting up to 5m0s for pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5" in namespace "var-expansion-8875" to be "Succeeded or Failed"
    Mar 22 20:36:32.810: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101752ms
    Mar 22 20:36:34.824: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020048009s
    Mar 22 20:36:36.817: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012817119s
    STEP: Saw pod success 03/22/23 20:36:36.817
    Mar 22 20:36:36.817: INFO: Pod "var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5" satisfied condition "Succeeded or Failed"
    Mar 22 20:36:36.821: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 20:36:36.833
    Mar 22 20:36:36.851: INFO: Waiting for pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 to disappear
    Mar 22 20:36:36.855: INFO: Pod var-expansion-e36a3ee9-2db5-47eb-977c-70228af418a5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:36:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8875" for this suite. 03/22/23 20:36:36.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:36:36.877
Mar 22 20:36:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename taint-single-pod 03/22/23 20:36:36.878
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:36.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:36.9
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Mar 22 20:36:36.907: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 20:37:36.947: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Mar 22 20:37:36.952: INFO: Starting informer...
STEP: Starting pod... 03/22/23 20:37:36.952
Mar 22 20:37:37.169: INFO: Pod is running on pool-v7t41yxh0-q56kk. Tainting Node
STEP: Trying to apply a taint on the Node 03/22/23 20:37:37.169
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:37:37.187
STEP: Waiting short time to make sure Pod is queued for deletion 03/22/23 20:37:37.192
Mar 22 20:37:37.192: INFO: Pod wasn't evicted. Proceeding
Mar 22 20:37:37.192: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:37:37.21
STEP: Waiting some time to make sure that toleration time passed. 03/22/23 20:37:37.214
Mar 22 20:38:52.215: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:38:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-671" for this suite. 03/22/23 20:38:52.221
------------------------------
• [SLOW TEST] [135.352 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:36:36.877
    Mar 22 20:36:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename taint-single-pod 03/22/23 20:36:36.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:36:36.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:36:36.9
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Mar 22 20:36:36.907: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 20:37:36.947: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Mar 22 20:37:36.952: INFO: Starting informer...
    STEP: Starting pod... 03/22/23 20:37:36.952
    Mar 22 20:37:37.169: INFO: Pod is running on pool-v7t41yxh0-q56kk. Tainting Node
    STEP: Trying to apply a taint on the Node 03/22/23 20:37:37.169
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:37:37.187
    STEP: Waiting short time to make sure Pod is queued for deletion 03/22/23 20:37:37.192
    Mar 22 20:37:37.192: INFO: Pod wasn't evicted. Proceeding
    Mar 22 20:37:37.192: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/22/23 20:37:37.21
    STEP: Waiting some time to make sure that toleration time passed. 03/22/23 20:37:37.214
    Mar 22 20:38:52.215: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:38:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-671" for this suite. 03/22/23 20:38:52.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:38:52.233
Mar 22 20:38:52.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-runtime 03/22/23 20:38:52.234
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:38:52.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:38:52.264
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 03/22/23 20:38:52.272
STEP: wait for the container to reach Succeeded 03/22/23 20:38:52.284
STEP: get the container status 03/22/23 20:38:56.326
STEP: the container should be terminated 03/22/23 20:38:56.331
STEP: the termination message should be set 03/22/23 20:38:56.331
Mar 22 20:38:56.332: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/22/23 20:38:56.332
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 22 20:38:56.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6097" for this suite. 03/22/23 20:38:56.361
------------------------------
• [4.136 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:38:52.233
    Mar 22 20:38:52.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-runtime 03/22/23 20:38:52.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:38:52.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:38:52.264
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 03/22/23 20:38:52.272
    STEP: wait for the container to reach Succeeded 03/22/23 20:38:52.284
    STEP: get the container status 03/22/23 20:38:56.326
    STEP: the container should be terminated 03/22/23 20:38:56.331
    STEP: the termination message should be set 03/22/23 20:38:56.331
    Mar 22 20:38:56.332: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/22/23 20:38:56.332
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:38:56.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6097" for this suite. 03/22/23 20:38:56.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:38:56.375
Mar 22 20:38:56.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 20:38:56.378
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:38:56.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:38:56.398
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Mar 22 20:38:56.413: INFO: Waiting up to 2m0s for pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" in namespace "var-expansion-5211" to be "container 0 failed with reason CreateContainerConfigError"
Mar 22 20:38:56.418: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714454ms
Mar 22 20:38:58.424: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01036729s
Mar 22 20:38:58.428: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 22 20:38:58.428: INFO: Deleting pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" in namespace "var-expansion-5211"
Mar 22 20:38:58.438: INFO: Wait up to 5m0s for pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:00.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5211" for this suite. 03/22/23 20:39:00.454
------------------------------
• [4.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:38:56.375
    Mar 22 20:38:56.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 20:38:56.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:38:56.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:38:56.398
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Mar 22 20:38:56.413: INFO: Waiting up to 2m0s for pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" in namespace "var-expansion-5211" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 22 20:38:56.418: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714454ms
    Mar 22 20:38:58.424: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01036729s
    Mar 22 20:38:58.428: INFO: Pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 22 20:38:58.428: INFO: Deleting pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" in namespace "var-expansion-5211"
    Mar 22 20:38:58.438: INFO: Wait up to 5m0s for pod "var-expansion-c8064da7-02f1-44ad-8227-46326999059f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:00.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5211" for this suite. 03/22/23 20:39:00.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:00.467
Mar 22 20:39:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename controllerrevisions 03/22/23 20:39:00.468
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:00.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:00.493
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-7br9v-daemon-set" 03/22/23 20:39:00.526
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:39:00.536
Mar 22 20:39:00.556: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
Mar 22 20:39:00.556: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:39:01.577: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
Mar 22 20:39:01.577: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:39:02.570: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 2
Mar 22 20:39:02.571: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 20:39:03.572: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 3
Mar 22 20:39:03.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-7br9v-daemon-set
STEP: Confirm DaemonSet "e2e-7br9v-daemon-set" successfully created with "daemonset-name=e2e-7br9v-daemon-set" label 03/22/23 20:39:03.578
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7br9v-daemon-set" 03/22/23 20:39:03.589
Mar 22 20:39:03.600: INFO: Located ControllerRevision: "e2e-7br9v-daemon-set-6868f969dc"
STEP: Patching ControllerRevision "e2e-7br9v-daemon-set-6868f969dc" 03/22/23 20:39:03.617
Mar 22 20:39:03.628: INFO: e2e-7br9v-daemon-set-6868f969dc has been patched
STEP: Create a new ControllerRevision 03/22/23 20:39:03.629
Mar 22 20:39:03.637: INFO: Created ControllerRevision: e2e-7br9v-daemon-set-699dd4c9
STEP: Confirm that there are two ControllerRevisions 03/22/23 20:39:03.648
Mar 22 20:39:03.649: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 22 20:39:03.664: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-7br9v-daemon-set-6868f969dc" 03/22/23 20:39:03.664
STEP: Confirm that there is only one ControllerRevision 03/22/23 20:39:03.678
Mar 22 20:39:03.678: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 22 20:39:03.681: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-7br9v-daemon-set-699dd4c9" 03/22/23 20:39:03.69
Mar 22 20:39:03.701: INFO: e2e-7br9v-daemon-set-699dd4c9 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/22/23 20:39:03.701
W0322 20:39:03.718030      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/22/23 20:39:03.718
Mar 22 20:39:03.718: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 22 20:39:04.723: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 22 20:39:04.728: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7br9v-daemon-set-699dd4c9=updated" 03/22/23 20:39:04.728
STEP: Confirm that there is only one ControllerRevision 03/22/23 20:39:04.74
Mar 22 20:39:04.740: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 22 20:39:04.744: INFO: Found 1 ControllerRevisions
Mar 22 20:39:04.748: INFO: ControllerRevision "e2e-7br9v-daemon-set-688689d6d6" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-7br9v-daemon-set" 03/22/23 20:39:04.754
STEP: deleting DaemonSet.extensions e2e-7br9v-daemon-set in namespace controllerrevisions-4174, will wait for the garbage collector to delete the pods 03/22/23 20:39:04.755
Mar 22 20:39:04.817: INFO: Deleting DaemonSet.extensions e2e-7br9v-daemon-set took: 7.798038ms
Mar 22 20:39:04.919: INFO: Terminating DaemonSet.extensions e2e-7br9v-daemon-set pods took: 101.426408ms
Mar 22 20:39:05.733: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
Mar 22 20:39:05.734: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7br9v-daemon-set
Mar 22 20:39:05.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22033"},"items":null}

Mar 22 20:39:05.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22034"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:05.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4174" for this suite. 03/22/23 20:39:05.796
------------------------------
• [SLOW TEST] [5.338 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:00.467
    Mar 22 20:39:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename controllerrevisions 03/22/23 20:39:00.468
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:00.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:00.493
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-7br9v-daemon-set" 03/22/23 20:39:00.526
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:39:00.536
    Mar 22 20:39:00.556: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
    Mar 22 20:39:00.556: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:39:01.577: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
    Mar 22 20:39:01.577: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:39:02.570: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 2
    Mar 22 20:39:02.571: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 20:39:03.572: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 3
    Mar 22 20:39:03.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-7br9v-daemon-set
    STEP: Confirm DaemonSet "e2e-7br9v-daemon-set" successfully created with "daemonset-name=e2e-7br9v-daemon-set" label 03/22/23 20:39:03.578
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7br9v-daemon-set" 03/22/23 20:39:03.589
    Mar 22 20:39:03.600: INFO: Located ControllerRevision: "e2e-7br9v-daemon-set-6868f969dc"
    STEP: Patching ControllerRevision "e2e-7br9v-daemon-set-6868f969dc" 03/22/23 20:39:03.617
    Mar 22 20:39:03.628: INFO: e2e-7br9v-daemon-set-6868f969dc has been patched
    STEP: Create a new ControllerRevision 03/22/23 20:39:03.629
    Mar 22 20:39:03.637: INFO: Created ControllerRevision: e2e-7br9v-daemon-set-699dd4c9
    STEP: Confirm that there are two ControllerRevisions 03/22/23 20:39:03.648
    Mar 22 20:39:03.649: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 22 20:39:03.664: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-7br9v-daemon-set-6868f969dc" 03/22/23 20:39:03.664
    STEP: Confirm that there is only one ControllerRevision 03/22/23 20:39:03.678
    Mar 22 20:39:03.678: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 22 20:39:03.681: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-7br9v-daemon-set-699dd4c9" 03/22/23 20:39:03.69
    Mar 22 20:39:03.701: INFO: e2e-7br9v-daemon-set-699dd4c9 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/22/23 20:39:03.701
    W0322 20:39:03.718030      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/22/23 20:39:03.718
    Mar 22 20:39:03.718: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 22 20:39:04.723: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 22 20:39:04.728: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7br9v-daemon-set-699dd4c9=updated" 03/22/23 20:39:04.728
    STEP: Confirm that there is only one ControllerRevision 03/22/23 20:39:04.74
    Mar 22 20:39:04.740: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 22 20:39:04.744: INFO: Found 1 ControllerRevisions
    Mar 22 20:39:04.748: INFO: ControllerRevision "e2e-7br9v-daemon-set-688689d6d6" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-7br9v-daemon-set" 03/22/23 20:39:04.754
    STEP: deleting DaemonSet.extensions e2e-7br9v-daemon-set in namespace controllerrevisions-4174, will wait for the garbage collector to delete the pods 03/22/23 20:39:04.755
    Mar 22 20:39:04.817: INFO: Deleting DaemonSet.extensions e2e-7br9v-daemon-set took: 7.798038ms
    Mar 22 20:39:04.919: INFO: Terminating DaemonSet.extensions e2e-7br9v-daemon-set pods took: 101.426408ms
    Mar 22 20:39:05.733: INFO: Number of nodes with available pods controlled by daemonset e2e-7br9v-daemon-set: 0
    Mar 22 20:39:05.734: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7br9v-daemon-set
    Mar 22 20:39:05.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22033"},"items":null}

    Mar 22 20:39:05.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22034"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:05.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4174" for this suite. 03/22/23 20:39:05.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:05.806
Mar 22 20:39:05.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:39:05.81
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:05.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:05.862
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-b006820e-8b2d-4378-acc7-f7de31cbe600 03/22/23 20:39:05.869
STEP: Creating a pod to test consume configMaps 03/22/23 20:39:05.882
Mar 22 20:39:05.904: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2" in namespace "projected-5247" to be "Succeeded or Failed"
Mar 22 20:39:05.911: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.781328ms
Mar 22 20:39:07.917: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012385533s
Mar 22 20:39:09.916: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012120456s
STEP: Saw pod success 03/22/23 20:39:09.916
Mar 22 20:39:09.916: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2" satisfied condition "Succeeded or Failed"
Mar 22 20:39:09.920: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:39:09.963
Mar 22 20:39:09.976: INFO: Waiting for pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 to disappear
Mar 22 20:39:09.979: INFO: Pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:09.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5247" for this suite. 03/22/23 20:39:09.99
------------------------------
• [4.206 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:05.806
    Mar 22 20:39:05.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:39:05.81
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:05.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:05.862
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-b006820e-8b2d-4378-acc7-f7de31cbe600 03/22/23 20:39:05.869
    STEP: Creating a pod to test consume configMaps 03/22/23 20:39:05.882
    Mar 22 20:39:05.904: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2" in namespace "projected-5247" to be "Succeeded or Failed"
    Mar 22 20:39:05.911: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.781328ms
    Mar 22 20:39:07.917: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012385533s
    Mar 22 20:39:09.916: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012120456s
    STEP: Saw pod success 03/22/23 20:39:09.916
    Mar 22 20:39:09.916: INFO: Pod "pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2" satisfied condition "Succeeded or Failed"
    Mar 22 20:39:09.920: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:39:09.963
    Mar 22 20:39:09.976: INFO: Waiting for pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 to disappear
    Mar 22 20:39:09.979: INFO: Pod pod-projected-configmaps-e1ac27e0-1dd2-45b1-a934-7765c99c3ec2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:09.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5247" for this suite. 03/22/23 20:39:09.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:10.014
Mar 22 20:39:10.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename init-container 03/22/23 20:39:10.016
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:10.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:10.036
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 03/22/23 20:39:10.04
Mar 22 20:39:10.040: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:15.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-312" for this suite. 03/22/23 20:39:15.631
------------------------------
• [SLOW TEST] [5.625 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:10.014
    Mar 22 20:39:10.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename init-container 03/22/23 20:39:10.016
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:10.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:10.036
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 03/22/23 20:39:10.04
    Mar 22 20:39:10.040: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:15.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-312" for this suite. 03/22/23 20:39:15.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:15.641
Mar 22 20:39:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 20:39:15.642
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:15.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:15.663
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/22/23 20:39:15.67
STEP: Wait for the Deployment to create new ReplicaSet 03/22/23 20:39:15.677
STEP: delete the deployment 03/22/23 20:39:15.794
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/22/23 20:39:15.803
STEP: Gathering metrics 03/22/23 20:39:16.336
W0322 20:39:16.348113      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 20:39:16.348: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:16.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1955" for this suite. 03/22/23 20:39:16.354
------------------------------
• [0.722 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:15.641
    Mar 22 20:39:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 20:39:15.642
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:15.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:15.663
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/22/23 20:39:15.67
    STEP: Wait for the Deployment to create new ReplicaSet 03/22/23 20:39:15.677
    STEP: delete the deployment 03/22/23 20:39:15.794
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/22/23 20:39:15.803
    STEP: Gathering metrics 03/22/23 20:39:16.336
    W0322 20:39:16.348113      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 20:39:16.348: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:16.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1955" for this suite. 03/22/23 20:39:16.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:16.365
Mar 22 20:39:16.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:39:16.367
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:16.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:16.412
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:39:16.419
Mar 22 20:39:16.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5" in namespace "downward-api-5125" to be "Succeeded or Failed"
Mar 22 20:39:16.438: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289692ms
Mar 22 20:39:18.445: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013407759s
Mar 22 20:39:20.449: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017206107s
STEP: Saw pod success 03/22/23 20:39:20.449
Mar 22 20:39:20.449: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5" satisfied condition "Succeeded or Failed"
Mar 22 20:39:20.453: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 container client-container: <nil>
STEP: delete the pod 03/22/23 20:39:20.466
Mar 22 20:39:20.480: INFO: Waiting for pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 to disappear
Mar 22 20:39:20.488: INFO: Pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:20.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5125" for this suite. 03/22/23 20:39:20.494
------------------------------
• [4.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:16.365
    Mar 22 20:39:16.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:39:16.367
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:16.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:16.412
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:39:16.419
    Mar 22 20:39:16.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5" in namespace "downward-api-5125" to be "Succeeded or Failed"
    Mar 22 20:39:16.438: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289692ms
    Mar 22 20:39:18.445: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013407759s
    Mar 22 20:39:20.449: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017206107s
    STEP: Saw pod success 03/22/23 20:39:20.449
    Mar 22 20:39:20.449: INFO: Pod "downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5" satisfied condition "Succeeded or Failed"
    Mar 22 20:39:20.453: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:39:20.466
    Mar 22 20:39:20.480: INFO: Waiting for pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 to disappear
    Mar 22 20:39:20.488: INFO: Pod downwardapi-volume-8c88a130-c014-4ea5-9c8c-66ffb86047b5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:20.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5125" for this suite. 03/22/23 20:39:20.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:20.515
Mar 22 20:39:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 20:39:20.516
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:20.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:20.539
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 03/22/23 20:39:20.552
STEP: waiting for RC to be added 03/22/23 20:39:20.561
STEP: waiting for available Replicas 03/22/23 20:39:20.561
STEP: patching ReplicationController 03/22/23 20:39:21.659
STEP: waiting for RC to be modified 03/22/23 20:39:21.672
STEP: patching ReplicationController status 03/22/23 20:39:21.672
STEP: waiting for RC to be modified 03/22/23 20:39:21.683
STEP: waiting for available Replicas 03/22/23 20:39:21.684
STEP: fetching ReplicationController status 03/22/23 20:39:21.694
STEP: patching ReplicationController scale 03/22/23 20:39:21.698
STEP: waiting for RC to be modified 03/22/23 20:39:21.707
STEP: waiting for ReplicationController's scale to be the max amount 03/22/23 20:39:21.707
STEP: fetching ReplicationController; ensuring that it's patched 03/22/23 20:39:23.452
STEP: updating ReplicationController status 03/22/23 20:39:23.456
STEP: waiting for RC to be modified 03/22/23 20:39:23.465
STEP: listing all ReplicationControllers 03/22/23 20:39:23.466
STEP: checking that ReplicationController has expected values 03/22/23 20:39:23.48
STEP: deleting ReplicationControllers by collection 03/22/23 20:39:23.48
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/22/23 20:39:23.493
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:23.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6825" for this suite. 03/22/23 20:39:23.536
------------------------------
• [3.031 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:20.515
    Mar 22 20:39:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 20:39:20.516
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:20.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:20.539
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 03/22/23 20:39:20.552
    STEP: waiting for RC to be added 03/22/23 20:39:20.561
    STEP: waiting for available Replicas 03/22/23 20:39:20.561
    STEP: patching ReplicationController 03/22/23 20:39:21.659
    STEP: waiting for RC to be modified 03/22/23 20:39:21.672
    STEP: patching ReplicationController status 03/22/23 20:39:21.672
    STEP: waiting for RC to be modified 03/22/23 20:39:21.683
    STEP: waiting for available Replicas 03/22/23 20:39:21.684
    STEP: fetching ReplicationController status 03/22/23 20:39:21.694
    STEP: patching ReplicationController scale 03/22/23 20:39:21.698
    STEP: waiting for RC to be modified 03/22/23 20:39:21.707
    STEP: waiting for ReplicationController's scale to be the max amount 03/22/23 20:39:21.707
    STEP: fetching ReplicationController; ensuring that it's patched 03/22/23 20:39:23.452
    STEP: updating ReplicationController status 03/22/23 20:39:23.456
    STEP: waiting for RC to be modified 03/22/23 20:39:23.465
    STEP: listing all ReplicationControllers 03/22/23 20:39:23.466
    STEP: checking that ReplicationController has expected values 03/22/23 20:39:23.48
    STEP: deleting ReplicationControllers by collection 03/22/23 20:39:23.48
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/22/23 20:39:23.493
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:23.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6825" for this suite. 03/22/23 20:39:23.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:23.552
Mar 22 20:39:23.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename subpath 03/22/23 20:39:23.554
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:23.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:23.593
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/22/23 20:39:23.599
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-xcdh 03/22/23 20:39:23.614
STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:39:23.614
Mar 22 20:39:23.626: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xcdh" in namespace "subpath-2576" to be "Succeeded or Failed"
Mar 22 20:39:23.632: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701826ms
Mar 22 20:39:25.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01082949s
Mar 22 20:39:27.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 4.011803304s
Mar 22 20:39:29.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 6.013002363s
Mar 22 20:39:31.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 8.012505713s
Mar 22 20:39:33.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 10.011850476s
Mar 22 20:39:35.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 12.012739922s
Mar 22 20:39:37.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 14.011570286s
Mar 22 20:39:39.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 16.010811575s
Mar 22 20:39:41.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 18.011007455s
Mar 22 20:39:43.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 20.011312041s
Mar 22 20:39:45.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=false. Elapsed: 22.01101462s
Mar 22 20:39:47.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011069033s
STEP: Saw pod success 03/22/23 20:39:47.638
Mar 22 20:39:47.638: INFO: Pod "pod-subpath-test-configmap-xcdh" satisfied condition "Succeeded or Failed"
Mar 22 20:39:47.642: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-configmap-xcdh container test-container-subpath-configmap-xcdh: <nil>
STEP: delete the pod 03/22/23 20:39:47.653
Mar 22 20:39:47.664: INFO: Waiting for pod pod-subpath-test-configmap-xcdh to disappear
Mar 22 20:39:47.668: INFO: Pod pod-subpath-test-configmap-xcdh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xcdh 03/22/23 20:39:47.668
Mar 22 20:39:47.669: INFO: Deleting pod "pod-subpath-test-configmap-xcdh" in namespace "subpath-2576"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:47.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2576" for this suite. 03/22/23 20:39:47.68
------------------------------
• [SLOW TEST] [24.143 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:23.552
    Mar 22 20:39:23.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename subpath 03/22/23 20:39:23.554
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:23.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:23.593
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/22/23 20:39:23.599
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-xcdh 03/22/23 20:39:23.614
    STEP: Creating a pod to test atomic-volume-subpath 03/22/23 20:39:23.614
    Mar 22 20:39:23.626: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xcdh" in namespace "subpath-2576" to be "Succeeded or Failed"
    Mar 22 20:39:23.632: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701826ms
    Mar 22 20:39:25.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01082949s
    Mar 22 20:39:27.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 4.011803304s
    Mar 22 20:39:29.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 6.013002363s
    Mar 22 20:39:31.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 8.012505713s
    Mar 22 20:39:33.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 10.011850476s
    Mar 22 20:39:35.639: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 12.012739922s
    Mar 22 20:39:37.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 14.011570286s
    Mar 22 20:39:39.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 16.010811575s
    Mar 22 20:39:41.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 18.011007455s
    Mar 22 20:39:43.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=true. Elapsed: 20.011312041s
    Mar 22 20:39:45.637: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Running", Reason="", readiness=false. Elapsed: 22.01101462s
    Mar 22 20:39:47.638: INFO: Pod "pod-subpath-test-configmap-xcdh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011069033s
    STEP: Saw pod success 03/22/23 20:39:47.638
    Mar 22 20:39:47.638: INFO: Pod "pod-subpath-test-configmap-xcdh" satisfied condition "Succeeded or Failed"
    Mar 22 20:39:47.642: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-configmap-xcdh container test-container-subpath-configmap-xcdh: <nil>
    STEP: delete the pod 03/22/23 20:39:47.653
    Mar 22 20:39:47.664: INFO: Waiting for pod pod-subpath-test-configmap-xcdh to disappear
    Mar 22 20:39:47.668: INFO: Pod pod-subpath-test-configmap-xcdh no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xcdh 03/22/23 20:39:47.668
    Mar 22 20:39:47.669: INFO: Deleting pod "pod-subpath-test-configmap-xcdh" in namespace "subpath-2576"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:47.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2576" for this suite. 03/22/23 20:39:47.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:47.696
Mar 22 20:39:47.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename events 03/22/23 20:39:47.698
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:47.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:47.715
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/22/23 20:39:47.722
STEP: listing all events in all namespaces 03/22/23 20:39:47.729
STEP: patching the test event 03/22/23 20:39:47.737
STEP: fetching the test event 03/22/23 20:39:47.744
STEP: updating the test event 03/22/23 20:39:47.748
STEP: getting the test event 03/22/23 20:39:47.767
STEP: deleting the test event 03/22/23 20:39:47.771
STEP: listing all events in all namespaces 03/22/23 20:39:47.779
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1071" for this suite. 03/22/23 20:39:47.792
------------------------------
• [0.104 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:47.696
    Mar 22 20:39:47.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename events 03/22/23 20:39:47.698
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:47.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:47.715
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/22/23 20:39:47.722
    STEP: listing all events in all namespaces 03/22/23 20:39:47.729
    STEP: patching the test event 03/22/23 20:39:47.737
    STEP: fetching the test event 03/22/23 20:39:47.744
    STEP: updating the test event 03/22/23 20:39:47.748
    STEP: getting the test event 03/22/23 20:39:47.767
    STEP: deleting the test event 03/22/23 20:39:47.771
    STEP: listing all events in all namespaces 03/22/23 20:39:47.779
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1071" for this suite. 03/22/23 20:39:47.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:47.801
Mar 22 20:39:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 20:39:47.802
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:47.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:47.822
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/22/23 20:39:47.829
Mar 22 20:39:47.837: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-876" to be "running and ready"
Mar 22 20:39:47.842: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.162377ms
Mar 22 20:39:47.843: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:39:49.849: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011558753s
Mar 22 20:39:49.849: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 22 20:39:49.849: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/22/23 20:39:49.853
STEP: Then the orphan pod is adopted 03/22/23 20:39:49.859
STEP: When the matched label of one of its pods change 03/22/23 20:39:50.87
Mar 22 20:39:50.875: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/22/23 20:39:50.893
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-876" for this suite. 03/22/23 20:39:51.916
------------------------------
• [4.125 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:47.801
    Mar 22 20:39:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 20:39:47.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:47.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:47.822
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/22/23 20:39:47.829
    Mar 22 20:39:47.837: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-876" to be "running and ready"
    Mar 22 20:39:47.842: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.162377ms
    Mar 22 20:39:47.843: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:39:49.849: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011558753s
    Mar 22 20:39:49.849: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 22 20:39:49.849: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/22/23 20:39:49.853
    STEP: Then the orphan pod is adopted 03/22/23 20:39:49.859
    STEP: When the matched label of one of its pods change 03/22/23 20:39:50.87
    Mar 22 20:39:50.875: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/22/23 20:39:50.893
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-876" for this suite. 03/22/23 20:39:51.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:51.933
Mar 22 20:39:51.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename csiinlinevolumes 03/22/23 20:39:51.934
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:51.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:51.961
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 03/22/23 20:39:51.973
STEP: getting 03/22/23 20:39:52.007
STEP: listing in namespace 03/22/23 20:39:52.012
STEP: patching 03/22/23 20:39:52.028
STEP: deleting 03/22/23 20:39:52.038
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4622" for this suite. 03/22/23 20:39:52.072
------------------------------
• [0.148 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:51.933
    Mar 22 20:39:51.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename csiinlinevolumes 03/22/23 20:39:51.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:51.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:51.961
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 03/22/23 20:39:51.973
    STEP: getting 03/22/23 20:39:52.007
    STEP: listing in namespace 03/22/23 20:39:52.012
    STEP: patching 03/22/23 20:39:52.028
    STEP: deleting 03/22/23 20:39:52.038
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4622" for this suite. 03/22/23 20:39:52.072
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:52.081
Mar 22 20:39:52.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:39:52.085
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:52.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:52.127
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/22/23 20:39:52.143
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/22/23 20:39:52.144
STEP: creating a pod to probe DNS 03/22/23 20:39:52.144
STEP: submitting the pod to kubernetes 03/22/23 20:39:52.144
Mar 22 20:39:52.163: INFO: Waiting up to 15m0s for pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3" in namespace "dns-281" to be "running"
Mar 22 20:39:52.170: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.824551ms
Mar 22 20:39:54.176: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013062759s
Mar 22 20:39:54.177: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:39:54.177
STEP: looking for the results for each expected name from probers 03/22/23 20:39:54.193
Mar 22 20:39:54.250: INFO: DNS probes using dns-281/dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3 succeeded

STEP: deleting the pod 03/22/23 20:39:54.25
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:39:54.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-281" for this suite. 03/22/23 20:39:54.271
------------------------------
• [2.198 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:52.081
    Mar 22 20:39:52.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:39:52.085
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:52.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:52.127
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/22/23 20:39:52.143
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/22/23 20:39:52.144
    STEP: creating a pod to probe DNS 03/22/23 20:39:52.144
    STEP: submitting the pod to kubernetes 03/22/23 20:39:52.144
    Mar 22 20:39:52.163: INFO: Waiting up to 15m0s for pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3" in namespace "dns-281" to be "running"
    Mar 22 20:39:52.170: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.824551ms
    Mar 22 20:39:54.176: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013062759s
    Mar 22 20:39:54.177: INFO: Pod "dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:39:54.177
    STEP: looking for the results for each expected name from probers 03/22/23 20:39:54.193
    Mar 22 20:39:54.250: INFO: DNS probes using dns-281/dns-test-de966f87-9b05-4bb7-89ee-9cd107032aa3 succeeded

    STEP: deleting the pod 03/22/23 20:39:54.25
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:39:54.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-281" for this suite. 03/22/23 20:39:54.271
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:39:54.279
Mar 22 20:39:54.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename ephemeral-containers-test 03/22/23 20:39:54.281
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:54.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:54.304
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/22/23 20:39:54.312
Mar 22 20:39:54.320: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4889" to be "running and ready"
Mar 22 20:39:54.330: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34771ms
Mar 22 20:39:54.330: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:39:56.347: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026765565s
Mar 22 20:39:56.347: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 22 20:39:56.347: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/22/23 20:39:56.351
Mar 22 20:39:56.365: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4889" to be "container debugger running"
Mar 22 20:39:56.370: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.247312ms
Mar 22 20:39:58.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010810834s
Mar 22 20:40:00.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010854838s
Mar 22 20:40:00.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/22/23 20:40:00.376
Mar 22 20:40:00.377: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4889 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:40:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:40:00.378: INFO: ExecWithOptions: Clientset creation
Mar 22 20:40:00.378: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/ephemeral-containers-test-4889/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 22 20:40:00.609: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:40:00.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4889" for this suite. 03/22/23 20:40:00.627
------------------------------
• [SLOW TEST] [6.357 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:39:54.279
    Mar 22 20:39:54.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/22/23 20:39:54.281
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:39:54.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:39:54.304
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/22/23 20:39:54.312
    Mar 22 20:39:54.320: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4889" to be "running and ready"
    Mar 22 20:39:54.330: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34771ms
    Mar 22 20:39:54.330: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:39:56.347: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026765565s
    Mar 22 20:39:56.347: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 22 20:39:56.347: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/22/23 20:39:56.351
    Mar 22 20:39:56.365: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4889" to be "container debugger running"
    Mar 22 20:39:56.370: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.247312ms
    Mar 22 20:39:58.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010810834s
    Mar 22 20:40:00.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010854838s
    Mar 22 20:40:00.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/22/23 20:40:00.376
    Mar 22 20:40:00.377: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4889 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:40:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:40:00.378: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:40:00.378: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/ephemeral-containers-test-4889/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 22 20:40:00.609: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:40:00.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4889" for this suite. 03/22/23 20:40:00.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:40:00.643
Mar 22 20:40:00.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:40:00.645
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:40:00.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:40:00.674
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-339e7f3c-9753-4e5b-aa57-199007084d20 03/22/23 20:40:00.694
STEP: Creating the pod 03/22/23 20:40:00.7
Mar 22 20:40:00.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60" in namespace "configmap-8144" to be "running and ready"
Mar 22 20:40:00.720: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.736965ms
Mar 22 20:40:00.720: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:40:02.727: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016098088s
Mar 22 20:40:02.730: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:40:04.726: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Running", Reason="", readiness=true. Elapsed: 4.014857021s
Mar 22 20:40:04.726: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Running (Ready = true)
Mar 22 20:40:04.726: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-339e7f3c-9753-4e5b-aa57-199007084d20 03/22/23 20:40:04.747
STEP: waiting to observe update in volume 03/22/23 20:40:04.755
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8144" for this suite. 03/22/23 20:41:27.532
------------------------------
• [SLOW TEST] [86.898 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:40:00.643
    Mar 22 20:40:00.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:40:00.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:40:00.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:40:00.674
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-339e7f3c-9753-4e5b-aa57-199007084d20 03/22/23 20:40:00.694
    STEP: Creating the pod 03/22/23 20:40:00.7
    Mar 22 20:40:00.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60" in namespace "configmap-8144" to be "running and ready"
    Mar 22 20:40:00.720: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.736965ms
    Mar 22 20:40:00.720: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:40:02.727: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016098088s
    Mar 22 20:40:02.730: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:40:04.726: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60": Phase="Running", Reason="", readiness=true. Elapsed: 4.014857021s
    Mar 22 20:40:04.726: INFO: The phase of Pod pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60 is Running (Ready = true)
    Mar 22 20:40:04.726: INFO: Pod "pod-configmaps-68317834-cdc6-43c8-a5b0-106ae1116d60" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-339e7f3c-9753-4e5b-aa57-199007084d20 03/22/23 20:40:04.747
    STEP: waiting to observe update in volume 03/22/23 20:40:04.755
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8144" for this suite. 03/22/23 20:41:27.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:27.547
Mar 22 20:41:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 20:41:27.548
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:27.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:27.569
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/22/23 20:41:27.574
STEP: Verify that the required pods have come up 03/22/23 20:41:27.582
Mar 22 20:41:27.586: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 22 20:41:32.591: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/22/23 20:41:32.591
Mar 22 20:41:32.596: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/22/23 20:41:32.596
STEP: DeleteCollection of the ReplicaSets 03/22/23 20:41:32.6
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/22/23 20:41:32.611
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-539" for this suite. 03/22/23 20:41:32.621
------------------------------
• [SLOW TEST] [5.095 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:27.547
    Mar 22 20:41:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 20:41:27.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:27.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:27.569
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/22/23 20:41:27.574
    STEP: Verify that the required pods have come up 03/22/23 20:41:27.582
    Mar 22 20:41:27.586: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 22 20:41:32.591: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/22/23 20:41:32.591
    Mar 22 20:41:32.596: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/22/23 20:41:32.596
    STEP: DeleteCollection of the ReplicaSets 03/22/23 20:41:32.6
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/22/23 20:41:32.611
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-539" for this suite. 03/22/23 20:41:32.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:32.644
Mar 22 20:41:32.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:41:32.646
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:32.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:32.675
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 03/22/23 20:41:32.688
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:32.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3148" for this suite. 03/22/23 20:41:32.704
------------------------------
• [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:32.644
    Mar 22 20:41:32.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:41:32.646
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:32.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:32.675
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 03/22/23 20:41:32.688
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:32.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3148" for this suite. 03/22/23 20:41:32.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:32.719
Mar 22 20:41:32.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:41:32.721
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:32.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:32.751
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:41:32.785
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:41:33.634
STEP: Deploying the webhook pod 03/22/23 20:41:33.644
STEP: Wait for the deployment to be ready 03/22/23 20:41:33.658
Mar 22 20:41:33.670: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:41:35.685
STEP: Verifying the service has paired with the endpoint 03/22/23 20:41:35.697
Mar 22 20:41:36.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/22/23 20:41:36.71
STEP: create a pod that should be updated by the webhook 03/22/23 20:41:36.774
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6666" for this suite. 03/22/23 20:41:36.899
STEP: Destroying namespace "webhook-6666-markers" for this suite. 03/22/23 20:41:36.91
------------------------------
• [4.200 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:32.719
    Mar 22 20:41:32.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:41:32.721
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:32.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:32.751
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:41:32.785
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:41:33.634
    STEP: Deploying the webhook pod 03/22/23 20:41:33.644
    STEP: Wait for the deployment to be ready 03/22/23 20:41:33.658
    Mar 22 20:41:33.670: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:41:35.685
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:41:35.697
    Mar 22 20:41:36.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/22/23 20:41:36.71
    STEP: create a pod that should be updated by the webhook 03/22/23 20:41:36.774
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6666" for this suite. 03/22/23 20:41:36.899
    STEP: Destroying namespace "webhook-6666-markers" for this suite. 03/22/23 20:41:36.91
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:36.919
Mar 22 20:41:36.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:41:36.92
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:36.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:36.955
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:41:36.98
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:41:38.284
STEP: Deploying the webhook pod 03/22/23 20:41:38.298
STEP: Wait for the deployment to be ready 03/22/23 20:41:38.316
Mar 22 20:41:38.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:41:40.341
STEP: Verifying the service has paired with the endpoint 03/22/23 20:41:40.354
Mar 22 20:41:41.355: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 03/22/23 20:41:41.442
STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:41:41.517
STEP: Deleting the collection of validation webhooks 03/22/23 20:41:41.597
STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:41:41.641
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:41.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2468" for this suite. 03/22/23 20:41:41.707
STEP: Destroying namespace "webhook-2468-markers" for this suite. 03/22/23 20:41:41.721
------------------------------
• [4.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:36.919
    Mar 22 20:41:36.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:41:36.92
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:36.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:36.955
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:41:36.98
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:41:38.284
    STEP: Deploying the webhook pod 03/22/23 20:41:38.298
    STEP: Wait for the deployment to be ready 03/22/23 20:41:38.316
    Mar 22 20:41:38.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:41:40.341
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:41:40.354
    Mar 22 20:41:41.355: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 03/22/23 20:41:41.442
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:41:41.517
    STEP: Deleting the collection of validation webhooks 03/22/23 20:41:41.597
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/22/23 20:41:41.641
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:41.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2468" for this suite. 03/22/23 20:41:41.707
    STEP: Destroying namespace "webhook-2468-markers" for this suite. 03/22/23 20:41:41.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:41.74
Mar 22 20:41:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:41:41.741
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:41.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:41.772
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 03/22/23 20:41:41.781
Mar 22 20:41:41.782: INFO: namespace kubectl-7537
Mar 22 20:41:41.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 create -f -'
Mar 22 20:41:42.309: INFO: stderr: ""
Mar 22 20:41:42.309: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/22/23 20:41:42.309
Mar 22 20:41:43.315: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:41:43.315: INFO: Found 1 / 1
Mar 22 20:41:43.315: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 22 20:41:43.319: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 22 20:41:43.319: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 22 20:41:43.319: INFO: wait on agnhost-primary startup in kubectl-7537 
Mar 22 20:41:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 logs agnhost-primary-5cstn agnhost-primary'
Mar 22 20:41:43.528: INFO: stderr: ""
Mar 22 20:41:43.528: INFO: stdout: "Paused\n"
STEP: exposing RC 03/22/23 20:41:43.528
Mar 22 20:41:43.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 22 20:41:43.668: INFO: stderr: ""
Mar 22 20:41:43.668: INFO: stdout: "service/rm2 exposed\n"
Mar 22 20:41:43.671: INFO: Service rm2 in namespace kubectl-7537 found.
STEP: exposing service 03/22/23 20:41:45.68
Mar 22 20:41:45.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 22 20:41:45.853: INFO: stderr: ""
Mar 22 20:41:45.853: INFO: stdout: "service/rm3 exposed\n"
Mar 22 20:41:45.857: INFO: Service rm3 in namespace kubectl-7537 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:47.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7537" for this suite. 03/22/23 20:41:47.874
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:41.74
    Mar 22 20:41:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:41:41.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:41.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:41.772
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 03/22/23 20:41:41.781
    Mar 22 20:41:41.782: INFO: namespace kubectl-7537
    Mar 22 20:41:41.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 create -f -'
    Mar 22 20:41:42.309: INFO: stderr: ""
    Mar 22 20:41:42.309: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/22/23 20:41:42.309
    Mar 22 20:41:43.315: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:41:43.315: INFO: Found 1 / 1
    Mar 22 20:41:43.315: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 22 20:41:43.319: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 22 20:41:43.319: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 22 20:41:43.319: INFO: wait on agnhost-primary startup in kubectl-7537 
    Mar 22 20:41:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 logs agnhost-primary-5cstn agnhost-primary'
    Mar 22 20:41:43.528: INFO: stderr: ""
    Mar 22 20:41:43.528: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/22/23 20:41:43.528
    Mar 22 20:41:43.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 22 20:41:43.668: INFO: stderr: ""
    Mar 22 20:41:43.668: INFO: stdout: "service/rm2 exposed\n"
    Mar 22 20:41:43.671: INFO: Service rm2 in namespace kubectl-7537 found.
    STEP: exposing service 03/22/23 20:41:45.68
    Mar 22 20:41:45.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7537 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 22 20:41:45.853: INFO: stderr: ""
    Mar 22 20:41:45.853: INFO: stdout: "service/rm3 exposed\n"
    Mar 22 20:41:45.857: INFO: Service rm3 in namespace kubectl-7537 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:47.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7537" for this suite. 03/22/23 20:41:47.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:47.883
Mar 22 20:41:47.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename csiinlinevolumes 03/22/23 20:41:47.884
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:47.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:47.902
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 03/22/23 20:41:47.91
STEP: getting 03/22/23 20:41:47.929
STEP: listing 03/22/23 20:41:47.942
STEP: deleting 03/22/23 20:41:47.947
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:47.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5283" for this suite. 03/22/23 20:41:47.973
------------------------------
• [0.102 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:47.883
    Mar 22 20:41:47.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename csiinlinevolumes 03/22/23 20:41:47.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:47.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:47.902
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 03/22/23 20:41:47.91
    STEP: getting 03/22/23 20:41:47.929
    STEP: listing 03/22/23 20:41:47.942
    STEP: deleting 03/22/23 20:41:47.947
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:47.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5283" for this suite. 03/22/23 20:41:47.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:47.985
Mar 22 20:41:47.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:41:47.987
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:48.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:48.011
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:41:48.049
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:41:48.056
Mar 22 20:41:48.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:41:48.070: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:49.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:41:49.102: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:50.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:41:50.083: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/22/23 20:41:50.087
Mar 22 20:41:50.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 20:41:50.113: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:51.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 20:41:51.124: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:52.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 20:41:52.125: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:53.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 20:41:53.124: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:41:54.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:41:54.126: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:41:54.129
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9121, will wait for the garbage collector to delete the pods 03/22/23 20:41:54.129
Mar 22 20:41:54.192: INFO: Deleting DaemonSet.extensions daemon-set took: 8.166269ms
Mar 22 20:41:54.296: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.308249ms
Mar 22 20:41:57.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:41:57.011: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 20:41:57.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23591"},"items":null}

Mar 22 20:41:57.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:57.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9121" for this suite. 03/22/23 20:41:57.06
------------------------------
• [SLOW TEST] [9.086 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:47.985
    Mar 22 20:41:47.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:41:47.987
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:48.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:48.011
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 03/22/23 20:41:48.049
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 20:41:48.056
    Mar 22 20:41:48.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:41:48.070: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:49.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:41:49.102: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:50.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:41:50.083: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/22/23 20:41:50.087
    Mar 22 20:41:50.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 20:41:50.113: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:51.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 20:41:51.124: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:52.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 20:41:52.125: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:53.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 20:41:53.124: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:41:54.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:41:54.126: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:41:54.129
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9121, will wait for the garbage collector to delete the pods 03/22/23 20:41:54.129
    Mar 22 20:41:54.192: INFO: Deleting DaemonSet.extensions daemon-set took: 8.166269ms
    Mar 22 20:41:54.296: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.308249ms
    Mar 22 20:41:57.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:41:57.011: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 20:41:57.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23591"},"items":null}

    Mar 22 20:41:57.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23591"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:57.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9121" for this suite. 03/22/23 20:41:57.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:57.072
Mar 22 20:41:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename certificates 03/22/23 20:41:57.073
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:57.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:57.095
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/22/23 20:41:59.027
STEP: getting /apis/certificates.k8s.io 03/22/23 20:41:59.041
STEP: getting /apis/certificates.k8s.io/v1 03/22/23 20:41:59.046
STEP: creating 03/22/23 20:41:59.049
STEP: getting 03/22/23 20:41:59.073
STEP: listing 03/22/23 20:41:59.096
STEP: watching 03/22/23 20:41:59.101
Mar 22 20:41:59.101: INFO: starting watch
STEP: patching 03/22/23 20:41:59.104
STEP: updating 03/22/23 20:41:59.115
Mar 22 20:41:59.123: INFO: waiting for watch events with expected annotations
Mar 22 20:41:59.123: INFO: saw patched and updated annotations
STEP: getting /approval 03/22/23 20:41:59.123
STEP: patching /approval 03/22/23 20:41:59.127
STEP: updating /approval 03/22/23 20:41:59.135
STEP: getting /status 03/22/23 20:41:59.148
STEP: patching /status 03/22/23 20:41:59.153
STEP: updating /status 03/22/23 20:41:59.161
STEP: deleting 03/22/23 20:41:59.184
STEP: deleting a collection 03/22/23 20:41:59.22
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:59.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-8846" for this suite. 03/22/23 20:41:59.248
------------------------------
• [2.195 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:57.072
    Mar 22 20:41:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename certificates 03/22/23 20:41:57.073
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:57.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:57.095
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/22/23 20:41:59.027
    STEP: getting /apis/certificates.k8s.io 03/22/23 20:41:59.041
    STEP: getting /apis/certificates.k8s.io/v1 03/22/23 20:41:59.046
    STEP: creating 03/22/23 20:41:59.049
    STEP: getting 03/22/23 20:41:59.073
    STEP: listing 03/22/23 20:41:59.096
    STEP: watching 03/22/23 20:41:59.101
    Mar 22 20:41:59.101: INFO: starting watch
    STEP: patching 03/22/23 20:41:59.104
    STEP: updating 03/22/23 20:41:59.115
    Mar 22 20:41:59.123: INFO: waiting for watch events with expected annotations
    Mar 22 20:41:59.123: INFO: saw patched and updated annotations
    STEP: getting /approval 03/22/23 20:41:59.123
    STEP: patching /approval 03/22/23 20:41:59.127
    STEP: updating /approval 03/22/23 20:41:59.135
    STEP: getting /status 03/22/23 20:41:59.148
    STEP: patching /status 03/22/23 20:41:59.153
    STEP: updating /status 03/22/23 20:41:59.161
    STEP: deleting 03/22/23 20:41:59.184
    STEP: deleting a collection 03/22/23 20:41:59.22
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:59.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-8846" for this suite. 03/22/23 20:41:59.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:59.271
Mar 22 20:41:59.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename tables 03/22/23 20:41:59.272
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:59.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:59.291
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Mar 22 20:41:59.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-564" for this suite. 03/22/23 20:41:59.316
------------------------------
• [0.054 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:59.271
    Mar 22 20:41:59.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename tables 03/22/23 20:41:59.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:59.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:59.291
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:41:59.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-564" for this suite. 03/22/23 20:41:59.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:41:59.325
Mar 22 20:41:59.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename watch 03/22/23 20:41:59.327
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:59.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:59.348
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/22/23 20:41:59.354
STEP: creating a watch on configmaps with label B 03/22/23 20:41:59.356
STEP: creating a watch on configmaps with label A or B 03/22/23 20:41:59.359
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.361
Mar 22 20:41:59.368: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23630 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:41:59.368: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23630 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.368
Mar 22 20:41:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23631 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:41:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23631 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/22/23 20:41:59.385
Mar 22 20:41:59.396: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23632 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:41:59.396: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23632 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.397
Mar 22 20:41:59.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23633 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:41:59.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23633 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/22/23 20:41:59.404
Mar 22 20:41:59.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23634 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:41:59.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23634 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/22/23 20:42:09.415
Mar 22 20:42:09.426: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23714 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 22 20:42:09.426: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23714 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:42:19.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-232" for this suite. 03/22/23 20:42:19.433
------------------------------
• [SLOW TEST] [20.115 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:41:59.325
    Mar 22 20:41:59.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename watch 03/22/23 20:41:59.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:41:59.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:41:59.348
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/22/23 20:41:59.354
    STEP: creating a watch on configmaps with label B 03/22/23 20:41:59.356
    STEP: creating a watch on configmaps with label A or B 03/22/23 20:41:59.359
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.361
    Mar 22 20:41:59.368: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23630 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:41:59.368: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23630 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.368
    Mar 22 20:41:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23631 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:41:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23631 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/22/23 20:41:59.385
    Mar 22 20:41:59.396: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23632 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:41:59.396: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23632 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/22/23 20:41:59.397
    Mar 22 20:41:59.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23633 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:41:59.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-232  29202555-d59c-484d-8176-1c12bb1bc7cf 23633 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/22/23 20:41:59.404
    Mar 22 20:41:59.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23634 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:41:59.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23634 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/22/23 20:42:09.415
    Mar 22 20:42:09.426: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23714 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 22 20:42:09.426: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-232  d6c28cfb-d020-4822-8786-3ebaa12029ca 23714 0 2023-03-22 20:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-22 20:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:42:19.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-232" for this suite. 03/22/23 20:42:19.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:42:19.441
Mar 22 20:42:19.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename init-container 03/22/23 20:42:19.443
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:19.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:19.473
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 03/22/23 20:42:19.484
Mar 22 20:42:19.484: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:42:23.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7801" for this suite. 03/22/23 20:42:23.475
------------------------------
• [4.046 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:42:19.441
    Mar 22 20:42:19.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename init-container 03/22/23 20:42:19.443
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:19.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:19.473
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 03/22/23 20:42:19.484
    Mar 22 20:42:19.484: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:42:23.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7801" for this suite. 03/22/23 20:42:23.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:42:23.49
Mar 22 20:42:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:42:23.492
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:23.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:23.514
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/22/23 20:42:23.521
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/22/23 20:42:23.521
STEP: creating a pod to probe /etc/hosts 03/22/23 20:42:23.522
STEP: submitting the pod to kubernetes 03/22/23 20:42:23.522
Mar 22 20:42:23.545: INFO: Waiting up to 15m0s for pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a" in namespace "dns-2120" to be "running"
Mar 22 20:42:23.567: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.444412ms
Mar 22 20:42:25.576: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031246411s
Mar 22 20:42:25.576: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:42:25.576
STEP: looking for the results for each expected name from probers 03/22/23 20:42:25.58
Mar 22 20:42:25.645: INFO: DNS probes using dns-2120/dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a succeeded

STEP: deleting the pod 03/22/23 20:42:25.645
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:42:25.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2120" for this suite. 03/22/23 20:42:25.666
------------------------------
• [2.186 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:42:23.49
    Mar 22 20:42:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:42:23.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:23.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:23.514
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/22/23 20:42:23.521
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/22/23 20:42:23.521
    STEP: creating a pod to probe /etc/hosts 03/22/23 20:42:23.522
    STEP: submitting the pod to kubernetes 03/22/23 20:42:23.522
    Mar 22 20:42:23.545: INFO: Waiting up to 15m0s for pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a" in namespace "dns-2120" to be "running"
    Mar 22 20:42:23.567: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.444412ms
    Mar 22 20:42:25.576: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031246411s
    Mar 22 20:42:25.576: INFO: Pod "dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:42:25.576
    STEP: looking for the results for each expected name from probers 03/22/23 20:42:25.58
    Mar 22 20:42:25.645: INFO: DNS probes using dns-2120/dns-test-37da4616-123a-4de7-a70d-4e9613f46b7a succeeded

    STEP: deleting the pod 03/22/23 20:42:25.645
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:42:25.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2120" for this suite. 03/22/23 20:42:25.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:42:25.684
Mar 22 20:42:25.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:42:25.686
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:25.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:25.708
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 03/22/23 20:42:25.715
STEP: Creating a ResourceQuota 03/22/23 20:42:30.723
STEP: Ensuring resource quota status is calculated 03/22/23 20:42:30.729
STEP: Creating a ReplicaSet 03/22/23 20:42:32.735
STEP: Ensuring resource quota status captures replicaset creation 03/22/23 20:42:32.749
STEP: Deleting a ReplicaSet 03/22/23 20:42:34.754
STEP: Ensuring resource quota status released usage 03/22/23 20:42:34.762
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:42:36.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6057" for this suite. 03/22/23 20:42:36.775
------------------------------
• [SLOW TEST] [11.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:42:25.684
    Mar 22 20:42:25.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:42:25.686
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:25.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:25.708
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 03/22/23 20:42:25.715
    STEP: Creating a ResourceQuota 03/22/23 20:42:30.723
    STEP: Ensuring resource quota status is calculated 03/22/23 20:42:30.729
    STEP: Creating a ReplicaSet 03/22/23 20:42:32.735
    STEP: Ensuring resource quota status captures replicaset creation 03/22/23 20:42:32.749
    STEP: Deleting a ReplicaSet 03/22/23 20:42:34.754
    STEP: Ensuring resource quota status released usage 03/22/23 20:42:34.762
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:42:36.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6057" for this suite. 03/22/23 20:42:36.775
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:42:36.792
Mar 22 20:42:36.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 20:42:36.793
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:36.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:36.817
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9480 03/22/23 20:42:36.828
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9480 03/22/23 20:42:36.84
Mar 22 20:42:36.853: INFO: Found 0 stateful pods, waiting for 1
Mar 22 20:42:46.858: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/22/23 20:42:46.871
STEP: Getting /status 03/22/23 20:42:46.884
Mar 22 20:42:46.889: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/22/23 20:42:46.889
Mar 22 20:42:46.900: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/22/23 20:42:46.9
Mar 22 20:42:46.905: INFO: Observed &StatefulSet event: ADDED
Mar 22 20:42:46.905: INFO: Found Statefulset ss in namespace statefulset-9480 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 22 20:42:46.905: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/22/23 20:42:46.905
Mar 22 20:42:46.905: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 22 20:42:46.915: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/22/23 20:42:46.915
Mar 22 20:42:46.920: INFO: Observed &StatefulSet event: ADDED
Mar 22 20:42:46.920: INFO: Observed Statefulset ss in namespace statefulset-9480 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 22 20:42:46.920: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 20:42:46.920: INFO: Deleting all statefulset in ns statefulset-9480
Mar 22 20:42:46.924: INFO: Scaling statefulset ss to 0
Mar 22 20:42:56.949: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:42:56.953: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:42:56.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9480" for this suite. 03/22/23 20:42:56.976
------------------------------
• [SLOW TEST] [20.191 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:42:36.792
    Mar 22 20:42:36.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 20:42:36.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:36.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:36.817
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9480 03/22/23 20:42:36.828
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9480 03/22/23 20:42:36.84
    Mar 22 20:42:36.853: INFO: Found 0 stateful pods, waiting for 1
    Mar 22 20:42:46.858: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/22/23 20:42:46.871
    STEP: Getting /status 03/22/23 20:42:46.884
    Mar 22 20:42:46.889: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/22/23 20:42:46.889
    Mar 22 20:42:46.900: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/22/23 20:42:46.9
    Mar 22 20:42:46.905: INFO: Observed &StatefulSet event: ADDED
    Mar 22 20:42:46.905: INFO: Found Statefulset ss in namespace statefulset-9480 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 22 20:42:46.905: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/22/23 20:42:46.905
    Mar 22 20:42:46.905: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 22 20:42:46.915: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/22/23 20:42:46.915
    Mar 22 20:42:46.920: INFO: Observed &StatefulSet event: ADDED
    Mar 22 20:42:46.920: INFO: Observed Statefulset ss in namespace statefulset-9480 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 22 20:42:46.920: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 20:42:46.920: INFO: Deleting all statefulset in ns statefulset-9480
    Mar 22 20:42:46.924: INFO: Scaling statefulset ss to 0
    Mar 22 20:42:56.949: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:42:56.953: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:42:56.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9480" for this suite. 03/22/23 20:42:56.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:42:56.994
Mar 22 20:42:56.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 20:42:56.996
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:57.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:57.024
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 03/22/23 20:42:57.031
Mar 22 20:42:57.040: INFO: Waiting up to 5m0s for pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5" in namespace "var-expansion-2684" to be "Succeeded or Failed"
Mar 22 20:42:57.048: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203217ms
Mar 22 20:42:59.058: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017640229s
Mar 22 20:43:01.054: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01340151s
STEP: Saw pod success 03/22/23 20:43:01.054
Mar 22 20:43:01.054: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5" satisfied condition "Succeeded or Failed"
Mar 22 20:43:01.063: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 container dapi-container: <nil>
STEP: delete the pod 03/22/23 20:43:01.076
Mar 22 20:43:01.099: INFO: Waiting for pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 to disappear
Mar 22 20:43:01.104: INFO: Pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:01.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2684" for this suite. 03/22/23 20:43:01.111
------------------------------
• [4.124 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:42:56.994
    Mar 22 20:42:56.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 20:42:56.996
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:42:57.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:42:57.024
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 03/22/23 20:42:57.031
    Mar 22 20:42:57.040: INFO: Waiting up to 5m0s for pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5" in namespace "var-expansion-2684" to be "Succeeded or Failed"
    Mar 22 20:42:57.048: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203217ms
    Mar 22 20:42:59.058: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017640229s
    Mar 22 20:43:01.054: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01340151s
    STEP: Saw pod success 03/22/23 20:43:01.054
    Mar 22 20:43:01.054: INFO: Pod "var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5" satisfied condition "Succeeded or Failed"
    Mar 22 20:43:01.063: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 20:43:01.076
    Mar 22 20:43:01.099: INFO: Waiting for pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 to disappear
    Mar 22 20:43:01.104: INFO: Pod var-expansion-6302199c-aa0a-45cf-b054-b5778dab9de5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:01.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2684" for this suite. 03/22/23 20:43:01.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:01.122
Mar 22 20:43:01.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:43:01.124
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:01.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:01.153
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 03/22/23 20:43:01.164
Mar 22 20:43:01.177: INFO: Waiting up to 5m0s for pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3" in namespace "emptydir-6759" to be "Succeeded or Failed"
Mar 22 20:43:01.181: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163529ms
Mar 22 20:43:03.188: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010780824s
Mar 22 20:43:05.188: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011236081s
STEP: Saw pod success 03/22/23 20:43:05.189
Mar 22 20:43:05.189: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3" satisfied condition "Succeeded or Failed"
Mar 22 20:43:05.193: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 container test-container: <nil>
STEP: delete the pod 03/22/23 20:43:05.204
Mar 22 20:43:05.217: INFO: Waiting for pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 to disappear
Mar 22 20:43:05.220: INFO: Pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:05.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6759" for this suite. 03/22/23 20:43:05.227
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:01.122
    Mar 22 20:43:01.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:43:01.124
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:01.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:01.153
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/22/23 20:43:01.164
    Mar 22 20:43:01.177: INFO: Waiting up to 5m0s for pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3" in namespace "emptydir-6759" to be "Succeeded or Failed"
    Mar 22 20:43:01.181: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163529ms
    Mar 22 20:43:03.188: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010780824s
    Mar 22 20:43:05.188: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011236081s
    STEP: Saw pod success 03/22/23 20:43:05.189
    Mar 22 20:43:05.189: INFO: Pod "pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3" satisfied condition "Succeeded or Failed"
    Mar 22 20:43:05.193: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:43:05.204
    Mar 22 20:43:05.217: INFO: Waiting for pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 to disappear
    Mar 22 20:43:05.220: INFO: Pod pod-c8a9a37e-fdc3-4685-aa0a-8da786117fc3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:05.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6759" for this suite. 03/22/23 20:43:05.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:05.238
Mar 22 20:43:05.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 20:43:05.24
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:05.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:05.261
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Mar 22 20:43:05.310: INFO: Create a RollingUpdate DaemonSet
Mar 22 20:43:05.318: INFO: Check that daemon pods launch on every node of the cluster
Mar 22 20:43:05.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:43:05.331: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:43:06.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:43:06.347: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 20:43:07.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 20:43:07.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 22 20:43:07.345: INFO: Update the DaemonSet to trigger a rollout
Mar 22 20:43:07.362: INFO: Updating DaemonSet daemon-set
Mar 22 20:43:10.384: INFO: Roll back the DaemonSet before rollout is complete
Mar 22 20:43:10.397: INFO: Updating DaemonSet daemon-set
Mar 22 20:43:10.397: INFO: Make sure DaemonSet rollback is complete
Mar 22 20:43:10.402: INFO: Wrong image for pod: daemon-set-8vxb5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Mar 22 20:43:10.402: INFO: Pod daemon-set-8vxb5 is not available
Mar 22 20:43:13.415: INFO: Pod daemon-set-m8wcb is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:43:13.428
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6634, will wait for the garbage collector to delete the pods 03/22/23 20:43:13.428
Mar 22 20:43:13.490: INFO: Deleting DaemonSet.extensions daemon-set took: 7.858824ms
Mar 22 20:43:13.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848626ms
Mar 22 20:43:14.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 20:43:14.897: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 20:43:14.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24300"},"items":null}

Mar 22 20:43:14.905: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24300"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:14.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6634" for this suite. 03/22/23 20:43:14.939
------------------------------
• [SLOW TEST] [9.709 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:05.238
    Mar 22 20:43:05.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 20:43:05.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:05.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:05.261
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Mar 22 20:43:05.310: INFO: Create a RollingUpdate DaemonSet
    Mar 22 20:43:05.318: INFO: Check that daemon pods launch on every node of the cluster
    Mar 22 20:43:05.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:43:05.331: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:43:06.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:43:06.347: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 20:43:07.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 20:43:07.345: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 22 20:43:07.345: INFO: Update the DaemonSet to trigger a rollout
    Mar 22 20:43:07.362: INFO: Updating DaemonSet daemon-set
    Mar 22 20:43:10.384: INFO: Roll back the DaemonSet before rollout is complete
    Mar 22 20:43:10.397: INFO: Updating DaemonSet daemon-set
    Mar 22 20:43:10.397: INFO: Make sure DaemonSet rollback is complete
    Mar 22 20:43:10.402: INFO: Wrong image for pod: daemon-set-8vxb5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Mar 22 20:43:10.402: INFO: Pod daemon-set-8vxb5 is not available
    Mar 22 20:43:13.415: INFO: Pod daemon-set-m8wcb is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 20:43:13.428
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6634, will wait for the garbage collector to delete the pods 03/22/23 20:43:13.428
    Mar 22 20:43:13.490: INFO: Deleting DaemonSet.extensions daemon-set took: 7.858824ms
    Mar 22 20:43:13.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848626ms
    Mar 22 20:43:14.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 20:43:14.897: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 20:43:14.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24300"},"items":null}

    Mar 22 20:43:14.905: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24300"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:14.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6634" for this suite. 03/22/23 20:43:14.939
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:14.947
Mar 22 20:43:14.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:43:14.948
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:14.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:14.968
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:43:14.976
Mar 22 20:43:14.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb" in namespace "downward-api-1625" to be "Succeeded or Failed"
Mar 22 20:43:15.005: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.256021ms
Mar 22 20:43:17.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025161018s
Mar 22 20:43:19.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024761886s
STEP: Saw pod success 03/22/23 20:43:19.012
Mar 22 20:43:19.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb" satisfied condition "Succeeded or Failed"
Mar 22 20:43:19.016: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb container client-container: <nil>
STEP: delete the pod 03/22/23 20:43:19.027
Mar 22 20:43:19.042: INFO: Waiting for pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb to disappear
Mar 22 20:43:19.050: INFO: Pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1625" for this suite. 03/22/23 20:43:19.061
------------------------------
• [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:14.947
    Mar 22 20:43:14.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:43:14.948
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:14.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:14.968
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:43:14.976
    Mar 22 20:43:14.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb" in namespace "downward-api-1625" to be "Succeeded or Failed"
    Mar 22 20:43:15.005: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.256021ms
    Mar 22 20:43:17.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025161018s
    Mar 22 20:43:19.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024761886s
    STEP: Saw pod success 03/22/23 20:43:19.012
    Mar 22 20:43:19.012: INFO: Pod "downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb" satisfied condition "Succeeded or Failed"
    Mar 22 20:43:19.016: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb container client-container: <nil>
    STEP: delete the pod 03/22/23 20:43:19.027
    Mar 22 20:43:19.042: INFO: Waiting for pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb to disappear
    Mar 22 20:43:19.050: INFO: Pod downwardapi-volume-5634227c-e2e9-415c-a20f-5b3b3a7fcedb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1625" for this suite. 03/22/23 20:43:19.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:19.069
Mar 22 20:43:19.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 20:43:19.071
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:19.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:19.091
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/22/23 20:43:19.105
STEP: create the rc2 03/22/23 20:43:19.112
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/22/23 20:43:24.164
STEP: delete the rc simpletest-rc-to-be-deleted 03/22/23 20:43:27.65
STEP: wait for the rc to be deleted 03/22/23 20:43:27.724
Mar 22 20:43:32.749: INFO: 91 pods remaining
Mar 22 20:43:32.749: INFO: 72 pods has nil DeletionTimestamp
Mar 22 20:43:32.749: INFO: 
Mar 22 20:43:37.809: INFO: 77 pods remaining
Mar 22 20:43:37.809: INFO: 50 pods has nil DeletionTimestamp
Mar 22 20:43:37.809: INFO: 
STEP: Gathering metrics 03/22/23 20:43:42.739
W0322 20:43:42.751509      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 20:43:42.751: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 22 20:43:42.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5rl" in namespace "gc-706"
Mar 22 20:43:42.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jzfx" in namespace "gc-706"
Mar 22 20:43:42.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xwzr" in namespace "gc-706"
Mar 22 20:43:42.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-44jc9" in namespace "gc-706"
Mar 22 20:43:42.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-469bz" in namespace "gc-706"
Mar 22 20:43:42.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jd6v" in namespace "gc-706"
Mar 22 20:43:42.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-55wcd" in namespace "gc-706"
Mar 22 20:43:42.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-58k5q" in namespace "gc-706"
Mar 22 20:43:42.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cvk9" in namespace "gc-706"
Mar 22 20:43:42.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fdsx" in namespace "gc-706"
Mar 22 20:43:42.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hzg2" in namespace "gc-706"
Mar 22 20:43:42.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zk9f" in namespace "gc-706"
Mar 22 20:43:42.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b6xw" in namespace "gc-706"
Mar 22 20:43:42.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fwl5" in namespace "gc-706"
Mar 22 20:43:43.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-74fwr" in namespace "gc-706"
Mar 22 20:43:43.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmdp" in namespace "gc-706"
Mar 22 20:43:43.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kkfz" in namespace "gc-706"
Mar 22 20:43:43.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lwnp" in namespace "gc-706"
Mar 22 20:43:43.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-87vgt" in namespace "gc-706"
Mar 22 20:43:43.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-88cvf" in namespace "gc-706"
Mar 22 20:43:43.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-89sqf" in namespace "gc-706"
Mar 22 20:43:43.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-8brp9" in namespace "gc-706"
Mar 22 20:43:43.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pd8f" in namespace "gc-706"
Mar 22 20:43:43.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t5hr" in namespace "gc-706"
Mar 22 20:43:43.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-95tmb" in namespace "gc-706"
Mar 22 20:43:43.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-98t4d" in namespace "gc-706"
Mar 22 20:43:43.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dxj8" in namespace "gc-706"
Mar 22 20:43:43.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9mm" in namespace "gc-706"
Mar 22 20:43:43.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm4gr" in namespace "gc-706"
Mar 22 20:43:43.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq9vk" in namespace "gc-706"
Mar 22 20:43:43.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-dghj7" in namespace "gc-706"
Mar 22 20:43:43.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlfv8" in namespace "gc-706"
Mar 22 20:43:43.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2b2n" in namespace "gc-706"
Mar 22 20:43:43.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqt72" in namespace "gc-706"
Mar 22 20:43:43.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-frnvj" in namespace "gc-706"
Mar 22 20:43:43.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd4w8" in namespace "gc-706"
Mar 22 20:43:43.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw4jg" in namespace "gc-706"
Mar 22 20:43:43.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6jw2" in namespace "gc-706"
Mar 22 20:43:43.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgbrl" in namespace "gc-706"
Mar 22 20:43:43.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqx4w" in namespace "gc-706"
Mar 22 20:43:43.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-hswfr" in namespace "gc-706"
Mar 22 20:43:43.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvvwm" in namespace "gc-706"
Mar 22 20:43:43.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9b2h" in namespace "gc-706"
Mar 22 20:43:43.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhxwk" in namespace "gc-706"
Mar 22 20:43:43.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwb4l" in namespace "gc-706"
Mar 22 20:43:43.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7czz" in namespace "gc-706"
Mar 22 20:43:43.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7lxj" in namespace "gc-706"
Mar 22 20:43:43.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8hc7" in namespace "gc-706"
Mar 22 20:43:43.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkq8m" in namespace "gc-706"
Mar 22 20:43:43.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-krjfw" in namespace "gc-706"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:43.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-706" for this suite. 03/22/23 20:43:43.51
------------------------------
• [SLOW TEST] [24.447 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:19.069
    Mar 22 20:43:19.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 20:43:19.071
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:19.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:19.091
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/22/23 20:43:19.105
    STEP: create the rc2 03/22/23 20:43:19.112
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/22/23 20:43:24.164
    STEP: delete the rc simpletest-rc-to-be-deleted 03/22/23 20:43:27.65
    STEP: wait for the rc to be deleted 03/22/23 20:43:27.724
    Mar 22 20:43:32.749: INFO: 91 pods remaining
    Mar 22 20:43:32.749: INFO: 72 pods has nil DeletionTimestamp
    Mar 22 20:43:32.749: INFO: 
    Mar 22 20:43:37.809: INFO: 77 pods remaining
    Mar 22 20:43:37.809: INFO: 50 pods has nil DeletionTimestamp
    Mar 22 20:43:37.809: INFO: 
    STEP: Gathering metrics 03/22/23 20:43:42.739
    W0322 20:43:42.751509      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 20:43:42.751: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 22 20:43:42.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5rl" in namespace "gc-706"
    Mar 22 20:43:42.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jzfx" in namespace "gc-706"
    Mar 22 20:43:42.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xwzr" in namespace "gc-706"
    Mar 22 20:43:42.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-44jc9" in namespace "gc-706"
    Mar 22 20:43:42.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-469bz" in namespace "gc-706"
    Mar 22 20:43:42.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jd6v" in namespace "gc-706"
    Mar 22 20:43:42.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-55wcd" in namespace "gc-706"
    Mar 22 20:43:42.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-58k5q" in namespace "gc-706"
    Mar 22 20:43:42.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cvk9" in namespace "gc-706"
    Mar 22 20:43:42.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fdsx" in namespace "gc-706"
    Mar 22 20:43:42.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hzg2" in namespace "gc-706"
    Mar 22 20:43:42.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zk9f" in namespace "gc-706"
    Mar 22 20:43:42.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b6xw" in namespace "gc-706"
    Mar 22 20:43:42.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fwl5" in namespace "gc-706"
    Mar 22 20:43:43.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-74fwr" in namespace "gc-706"
    Mar 22 20:43:43.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jmdp" in namespace "gc-706"
    Mar 22 20:43:43.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kkfz" in namespace "gc-706"
    Mar 22 20:43:43.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lwnp" in namespace "gc-706"
    Mar 22 20:43:43.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-87vgt" in namespace "gc-706"
    Mar 22 20:43:43.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-88cvf" in namespace "gc-706"
    Mar 22 20:43:43.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-89sqf" in namespace "gc-706"
    Mar 22 20:43:43.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-8brp9" in namespace "gc-706"
    Mar 22 20:43:43.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pd8f" in namespace "gc-706"
    Mar 22 20:43:43.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t5hr" in namespace "gc-706"
    Mar 22 20:43:43.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-95tmb" in namespace "gc-706"
    Mar 22 20:43:43.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-98t4d" in namespace "gc-706"
    Mar 22 20:43:43.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dxj8" in namespace "gc-706"
    Mar 22 20:43:43.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9mm" in namespace "gc-706"
    Mar 22 20:43:43.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm4gr" in namespace "gc-706"
    Mar 22 20:43:43.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq9vk" in namespace "gc-706"
    Mar 22 20:43:43.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-dghj7" in namespace "gc-706"
    Mar 22 20:43:43.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlfv8" in namespace "gc-706"
    Mar 22 20:43:43.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2b2n" in namespace "gc-706"
    Mar 22 20:43:43.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqt72" in namespace "gc-706"
    Mar 22 20:43:43.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-frnvj" in namespace "gc-706"
    Mar 22 20:43:43.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd4w8" in namespace "gc-706"
    Mar 22 20:43:43.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw4jg" in namespace "gc-706"
    Mar 22 20:43:43.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6jw2" in namespace "gc-706"
    Mar 22 20:43:43.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgbrl" in namespace "gc-706"
    Mar 22 20:43:43.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqx4w" in namespace "gc-706"
    Mar 22 20:43:43.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-hswfr" in namespace "gc-706"
    Mar 22 20:43:43.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvvwm" in namespace "gc-706"
    Mar 22 20:43:43.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9b2h" in namespace "gc-706"
    Mar 22 20:43:43.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhxwk" in namespace "gc-706"
    Mar 22 20:43:43.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwb4l" in namespace "gc-706"
    Mar 22 20:43:43.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7czz" in namespace "gc-706"
    Mar 22 20:43:43.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7lxj" in namespace "gc-706"
    Mar 22 20:43:43.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8hc7" in namespace "gc-706"
    Mar 22 20:43:43.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkq8m" in namespace "gc-706"
    Mar 22 20:43:43.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-krjfw" in namespace "gc-706"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:43.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-706" for this suite. 03/22/23 20:43:43.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:43.518
Mar 22 20:43:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 20:43:43.519
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:43.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:43.538
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 22 20:43:43.543: INFO: Creating deployment "test-recreate-deployment"
Mar 22 20:43:43.549: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 22 20:43:43.563: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 22 20:43:45.604: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 22 20:43:45.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 20:43:47.614: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 22 20:43:47.640: INFO: Updating deployment test-recreate-deployment
Mar 22 20:43:47.640: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 20:43:47.712: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9353  3c48fc55-d653-4878-80d6-771b0e7743d1 25930 2 2023-03-22 20:43:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 20:43:47 +0000 UTC,LastTransitionTime:2023-03-22 20:43:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-22 20:43:47 +0000 UTC,LastTransitionTime:2023-03-22 20:43:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 22 20:43:47.717: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9353  3fefe3c1-3ba3-4669-9c84-8a32e9256457 25929 1 2023-03-22 20:43:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3c48fc55-d653-4878-80d6-771b0e7743d1 0xc005736ed0 0xc005736ed1}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c48fc55-d653-4878-80d6-771b0e7743d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:43:47.717: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 22 20:43:47.717: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9353  56dfaedb-3f43-434e-9e43-5b9deb5412f0 25918 2 2023-03-22 20:43:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3c48fc55-d653-4878-80d6-771b0e7743d1 0xc005736db7 0xc005736db8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c48fc55-d653-4878-80d6-771b0e7743d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:43:47.722: INFO: Pod "test-recreate-deployment-cff6dc657-jpngk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-jpngk test-recreate-deployment-cff6dc657- deployment-9353  64d1100f-b91f-4735-9c88-4d1849b45b6d 25926 0 2023-03-22 20:43:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 3fefe3c1-3ba3-4669-9c84-8a32e9256457 0xc000c94210 0xc000c94211}] [] [{kube-controller-manager Update v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fefe3c1-3ba3-4669-9c84-8a32e9256457\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc9hm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc9hm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:43:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:47.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9353" for this suite. 03/22/23 20:43:47.728
------------------------------
• [4.218 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:43.518
    Mar 22 20:43:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 20:43:43.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:43.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:43.538
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 22 20:43:43.543: INFO: Creating deployment "test-recreate-deployment"
    Mar 22 20:43:43.549: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 22 20:43:43.563: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 22 20:43:45.604: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 22 20:43:45.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 20, 43, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 20:43:47.614: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 22 20:43:47.640: INFO: Updating deployment test-recreate-deployment
    Mar 22 20:43:47.640: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 20:43:47.712: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9353  3c48fc55-d653-4878-80d6-771b0e7743d1 25930 2 2023-03-22 20:43:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 20:43:47 +0000 UTC,LastTransitionTime:2023-03-22 20:43:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-22 20:43:47 +0000 UTC,LastTransitionTime:2023-03-22 20:43:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 22 20:43:47.717: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9353  3fefe3c1-3ba3-4669-9c84-8a32e9256457 25929 1 2023-03-22 20:43:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3c48fc55-d653-4878-80d6-771b0e7743d1 0xc005736ed0 0xc005736ed1}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c48fc55-d653-4878-80d6-771b0e7743d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:43:47.717: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 22 20:43:47.717: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9353  56dfaedb-3f43-434e-9e43-5b9deb5412f0 25918 2 2023-03-22 20:43:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3c48fc55-d653-4878-80d6-771b0e7743d1 0xc005736db7 0xc005736db8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c48fc55-d653-4878-80d6-771b0e7743d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005736e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:43:47.722: INFO: Pod "test-recreate-deployment-cff6dc657-jpngk" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-jpngk test-recreate-deployment-cff6dc657- deployment-9353  64d1100f-b91f-4735-9c88-4d1849b45b6d 25926 0 2023-03-22 20:43:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 3fefe3c1-3ba3-4669-9c84-8a32e9256457 0xc000c94210 0xc000c94211}] [] [{kube-controller-manager Update v1 2023-03-22 20:43:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fefe3c1-3ba3-4669-9c84-8a32e9256457\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc9hm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc9hm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:43:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:47.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9353" for this suite. 03/22/23 20:43:47.728
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:47.737
Mar 22 20:43:47.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:43:47.738
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:47.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:47.758
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-f92f593f-34c8-4bf1-a698-55bf52337525 03/22/23 20:43:47.768
STEP: Creating a pod to test consume secrets 03/22/23 20:43:47.776
Mar 22 20:43:47.785: INFO: Waiting up to 5m0s for pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c" in namespace "secrets-6705" to be "Succeeded or Failed"
Mar 22 20:43:47.791: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529309ms
Mar 22 20:43:49.796: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0102228s
Mar 22 20:43:51.798: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012933943s
Mar 22 20:43:53.796: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010965921s
STEP: Saw pod success 03/22/23 20:43:53.796
Mar 22 20:43:53.797: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c" satisfied condition "Succeeded or Failed"
Mar 22 20:43:53.800: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c container secret-env-test: <nil>
STEP: delete the pod 03/22/23 20:43:53.814
Mar 22 20:43:53.828: INFO: Waiting for pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c to disappear
Mar 22 20:43:53.831: INFO: Pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:53.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6705" for this suite. 03/22/23 20:43:53.841
------------------------------
• [SLOW TEST] [6.112 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:47.737
    Mar 22 20:43:47.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:43:47.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:47.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:47.758
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-f92f593f-34c8-4bf1-a698-55bf52337525 03/22/23 20:43:47.768
    STEP: Creating a pod to test consume secrets 03/22/23 20:43:47.776
    Mar 22 20:43:47.785: INFO: Waiting up to 5m0s for pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c" in namespace "secrets-6705" to be "Succeeded or Failed"
    Mar 22 20:43:47.791: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529309ms
    Mar 22 20:43:49.796: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0102228s
    Mar 22 20:43:51.798: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012933943s
    Mar 22 20:43:53.796: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010965921s
    STEP: Saw pod success 03/22/23 20:43:53.796
    Mar 22 20:43:53.797: INFO: Pod "pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c" satisfied condition "Succeeded or Failed"
    Mar 22 20:43:53.800: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c container secret-env-test: <nil>
    STEP: delete the pod 03/22/23 20:43:53.814
    Mar 22 20:43:53.828: INFO: Waiting for pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c to disappear
    Mar 22 20:43:53.831: INFO: Pod pod-secrets-18a763b2-12fd-4df0-a6c3-344512faae2c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:53.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6705" for this suite. 03/22/23 20:43:53.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:53.854
Mar 22 20:43:53.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:43:53.855
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:53.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:53.873
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-b5b66765-af32-442b-9e75-0de59ba2817c 03/22/23 20:43:53.878
STEP: Creating a pod to test consume configMaps 03/22/23 20:43:53.885
Mar 22 20:43:53.894: INFO: Waiting up to 5m0s for pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790" in namespace "configmap-5715" to be "Succeeded or Failed"
Mar 22 20:43:53.902: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Pending", Reason="", readiness=false. Elapsed: 7.800955ms
Mar 22 20:43:55.908: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014166246s
Mar 22 20:43:57.907: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013453761s
STEP: Saw pod success 03/22/23 20:43:57.907
Mar 22 20:43:57.907: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790" satisfied condition "Succeeded or Failed"
Mar 22 20:43:57.912: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:43:57.922
Mar 22 20:43:57.933: INFO: Waiting for pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 to disappear
Mar 22 20:43:57.938: INFO: Pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:43:57.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5715" for this suite. 03/22/23 20:43:57.944
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:53.854
    Mar 22 20:43:53.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:43:53.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:53.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:53.873
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-b5b66765-af32-442b-9e75-0de59ba2817c 03/22/23 20:43:53.878
    STEP: Creating a pod to test consume configMaps 03/22/23 20:43:53.885
    Mar 22 20:43:53.894: INFO: Waiting up to 5m0s for pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790" in namespace "configmap-5715" to be "Succeeded or Failed"
    Mar 22 20:43:53.902: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Pending", Reason="", readiness=false. Elapsed: 7.800955ms
    Mar 22 20:43:55.908: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014166246s
    Mar 22 20:43:57.907: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013453761s
    STEP: Saw pod success 03/22/23 20:43:57.907
    Mar 22 20:43:57.907: INFO: Pod "pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790" satisfied condition "Succeeded or Failed"
    Mar 22 20:43:57.912: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:43:57.922
    Mar 22 20:43:57.933: INFO: Waiting for pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 to disappear
    Mar 22 20:43:57.938: INFO: Pod pod-configmaps-8caa678e-0a68-4709-8f76-32c9980ec790 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:43:57.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5715" for this suite. 03/22/23 20:43:57.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:43:57.959
Mar 22 20:43:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:43:57.961
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:57.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:57.982
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:43:57.992
Mar 22 20:43:58.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04" in namespace "downward-api-5123" to be "Succeeded or Failed"
Mar 22 20:43:58.014: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603164ms
Mar 22 20:44:00.021: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Running", Reason="", readiness=true. Elapsed: 2.018124872s
Mar 22 20:44:02.027: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024247355s
STEP: Saw pod success 03/22/23 20:44:02.028
Mar 22 20:44:02.028: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04" satisfied condition "Succeeded or Failed"
Mar 22 20:44:02.033: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 container client-container: <nil>
STEP: delete the pod 03/22/23 20:44:02.044
Mar 22 20:44:02.059: INFO: Waiting for pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 to disappear
Mar 22 20:44:02.063: INFO: Pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:02.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5123" for this suite. 03/22/23 20:44:02.069
------------------------------
• [4.123 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:43:57.959
    Mar 22 20:43:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:43:57.961
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:43:57.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:43:57.982
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:43:57.992
    Mar 22 20:43:58.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04" in namespace "downward-api-5123" to be "Succeeded or Failed"
    Mar 22 20:43:58.014: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603164ms
    Mar 22 20:44:00.021: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Running", Reason="", readiness=true. Elapsed: 2.018124872s
    Mar 22 20:44:02.027: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024247355s
    STEP: Saw pod success 03/22/23 20:44:02.028
    Mar 22 20:44:02.028: INFO: Pod "downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04" satisfied condition "Succeeded or Failed"
    Mar 22 20:44:02.033: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:44:02.044
    Mar 22 20:44:02.059: INFO: Waiting for pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 to disappear
    Mar 22 20:44:02.063: INFO: Pod downwardapi-volume-efc321e5-cb35-49c3-94b9-7ece266eaa04 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:02.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5123" for this suite. 03/22/23 20:44:02.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:02.09
Mar 22 20:44:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:44:02.095
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:02.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:02.123
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:44:02.129
Mar 22 20:44:02.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1" in namespace "projected-5413" to be "Succeeded or Failed"
Mar 22 20:44:02.148: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.294137ms
Mar 22 20:44:04.154: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016321272s
Mar 22 20:44:06.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Running", Reason="", readiness=false. Elapsed: 4.01553254s
Mar 22 20:44:08.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015204056s
STEP: Saw pod success 03/22/23 20:44:08.153
Mar 22 20:44:08.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1" satisfied condition "Succeeded or Failed"
Mar 22 20:44:08.157: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 container client-container: <nil>
STEP: delete the pod 03/22/23 20:44:08.171
Mar 22 20:44:08.187: INFO: Waiting for pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 to disappear
Mar 22 20:44:08.190: INFO: Pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:08.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5413" for this suite. 03/22/23 20:44:08.198
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:02.09
    Mar 22 20:44:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:44:02.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:02.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:02.123
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:44:02.129
    Mar 22 20:44:02.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1" in namespace "projected-5413" to be "Succeeded or Failed"
    Mar 22 20:44:02.148: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.294137ms
    Mar 22 20:44:04.154: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016321272s
    Mar 22 20:44:06.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Running", Reason="", readiness=false. Elapsed: 4.01553254s
    Mar 22 20:44:08.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015204056s
    STEP: Saw pod success 03/22/23 20:44:08.153
    Mar 22 20:44:08.153: INFO: Pod "downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1" satisfied condition "Succeeded or Failed"
    Mar 22 20:44:08.157: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:44:08.171
    Mar 22 20:44:08.187: INFO: Waiting for pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 to disappear
    Mar 22 20:44:08.190: INFO: Pod downwardapi-volume-5e1a1ca7-057d-4888-814e-702cb637a1d1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:08.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5413" for this suite. 03/22/23 20:44:08.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:08.217
Mar 22 20:44:08.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename limitrange 03/22/23 20:44:08.218
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:08.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:08.242
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-r9k4b" in namespace "limitrange-5762" 03/22/23 20:44:08.254
STEP: Creating another limitRange in another namespace 03/22/23 20:44:08.26
Mar 22 20:44:08.277: INFO: Namespace "e2e-limitrange-r9k4b-9650" created
Mar 22 20:44:08.277: INFO: Creating LimitRange "e2e-limitrange-r9k4b" in namespace "e2e-limitrange-r9k4b-9650"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-r9k4b" 03/22/23 20:44:08.282
Mar 22 20:44:08.286: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-r9k4b" in "limitrange-5762" namespace 03/22/23 20:44:08.286
Mar 22 20:44:08.295: INFO: LimitRange "e2e-limitrange-r9k4b" has been patched
STEP: Delete LimitRange "e2e-limitrange-r9k4b" by Collection with labelSelector: "e2e-limitrange-r9k4b=patched" 03/22/23 20:44:08.296
STEP: Confirm that the limitRange "e2e-limitrange-r9k4b" has been deleted 03/22/23 20:44:08.303
Mar 22 20:44:08.304: INFO: Requesting list of LimitRange to confirm quantity
Mar 22 20:44:08.307: INFO: Found 0 LimitRange with label "e2e-limitrange-r9k4b=patched"
Mar 22 20:44:08.308: INFO: LimitRange "e2e-limitrange-r9k4b" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-r9k4b" 03/22/23 20:44:08.308
Mar 22 20:44:08.312: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5762" for this suite. 03/22/23 20:44:08.318
STEP: Destroying namespace "e2e-limitrange-r9k4b-9650" for this suite. 03/22/23 20:44:08.325
------------------------------
• [0.116 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:08.217
    Mar 22 20:44:08.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename limitrange 03/22/23 20:44:08.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:08.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:08.242
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-r9k4b" in namespace "limitrange-5762" 03/22/23 20:44:08.254
    STEP: Creating another limitRange in another namespace 03/22/23 20:44:08.26
    Mar 22 20:44:08.277: INFO: Namespace "e2e-limitrange-r9k4b-9650" created
    Mar 22 20:44:08.277: INFO: Creating LimitRange "e2e-limitrange-r9k4b" in namespace "e2e-limitrange-r9k4b-9650"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-r9k4b" 03/22/23 20:44:08.282
    Mar 22 20:44:08.286: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-r9k4b" in "limitrange-5762" namespace 03/22/23 20:44:08.286
    Mar 22 20:44:08.295: INFO: LimitRange "e2e-limitrange-r9k4b" has been patched
    STEP: Delete LimitRange "e2e-limitrange-r9k4b" by Collection with labelSelector: "e2e-limitrange-r9k4b=patched" 03/22/23 20:44:08.296
    STEP: Confirm that the limitRange "e2e-limitrange-r9k4b" has been deleted 03/22/23 20:44:08.303
    Mar 22 20:44:08.304: INFO: Requesting list of LimitRange to confirm quantity
    Mar 22 20:44:08.307: INFO: Found 0 LimitRange with label "e2e-limitrange-r9k4b=patched"
    Mar 22 20:44:08.308: INFO: LimitRange "e2e-limitrange-r9k4b" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-r9k4b" 03/22/23 20:44:08.308
    Mar 22 20:44:08.312: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5762" for this suite. 03/22/23 20:44:08.318
    STEP: Destroying namespace "e2e-limitrange-r9k4b-9650" for this suite. 03/22/23 20:44:08.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:08.334
Mar 22 20:44:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:44:08.336
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:08.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:08.371
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 03/22/23 20:44:08.377
STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:44:08.383
STEP: Creating a ResourceQuota with not terminating scope 03/22/23 20:44:10.389
STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:44:10.396
STEP: Creating a long running pod 03/22/23 20:44:12.403
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/22/23 20:44:12.416
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/22/23 20:44:14.423
STEP: Deleting the pod 03/22/23 20:44:16.429
STEP: Ensuring resource quota status released the pod usage 03/22/23 20:44:16.447
STEP: Creating a terminating pod 03/22/23 20:44:18.453
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/22/23 20:44:18.466
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/22/23 20:44:20.472
STEP: Deleting the pod 03/22/23 20:44:22.479
STEP: Ensuring resource quota status released the pod usage 03/22/23 20:44:22.495
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:24.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9662" for this suite. 03/22/23 20:44:24.51
------------------------------
• [SLOW TEST] [16.188 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:08.334
    Mar 22 20:44:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:44:08.336
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:08.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:08.371
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 03/22/23 20:44:08.377
    STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:44:08.383
    STEP: Creating a ResourceQuota with not terminating scope 03/22/23 20:44:10.389
    STEP: Ensuring ResourceQuota status is calculated 03/22/23 20:44:10.396
    STEP: Creating a long running pod 03/22/23 20:44:12.403
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/22/23 20:44:12.416
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/22/23 20:44:14.423
    STEP: Deleting the pod 03/22/23 20:44:16.429
    STEP: Ensuring resource quota status released the pod usage 03/22/23 20:44:16.447
    STEP: Creating a terminating pod 03/22/23 20:44:18.453
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/22/23 20:44:18.466
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/22/23 20:44:20.472
    STEP: Deleting the pod 03/22/23 20:44:22.479
    STEP: Ensuring resource quota status released the pod usage 03/22/23 20:44:22.495
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:24.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9662" for this suite. 03/22/23 20:44:24.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:24.531
Mar 22 20:44:24.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:44:24.536
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:24.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:24.574
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-2966336a-eaa0-405c-81d1-8b8824aaf24d 03/22/23 20:44:24.584
STEP: Creating a pod to test consume secrets 03/22/23 20:44:24.603
Mar 22 20:44:24.612: INFO: Waiting up to 5m0s for pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de" in namespace "secrets-7438" to be "Succeeded or Failed"
Mar 22 20:44:24.619: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239066ms
Mar 22 20:44:26.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012220666s
Mar 22 20:44:28.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012229236s
STEP: Saw pod success 03/22/23 20:44:28.625
Mar 22 20:44:28.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de" satisfied condition "Succeeded or Failed"
Mar 22 20:44:28.634: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:44:28.646
Mar 22 20:44:28.660: INFO: Waiting for pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de to disappear
Mar 22 20:44:28.664: INFO: Pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7438" for this suite. 03/22/23 20:44:28.693
------------------------------
• [4.170 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:24.531
    Mar 22 20:44:24.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:44:24.536
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:24.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:24.574
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-2966336a-eaa0-405c-81d1-8b8824aaf24d 03/22/23 20:44:24.584
    STEP: Creating a pod to test consume secrets 03/22/23 20:44:24.603
    Mar 22 20:44:24.612: INFO: Waiting up to 5m0s for pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de" in namespace "secrets-7438" to be "Succeeded or Failed"
    Mar 22 20:44:24.619: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239066ms
    Mar 22 20:44:26.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012220666s
    Mar 22 20:44:28.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012229236s
    STEP: Saw pod success 03/22/23 20:44:28.625
    Mar 22 20:44:28.625: INFO: Pod "pod-secrets-639ec417-9f46-4737-a378-d906314d22de" satisfied condition "Succeeded or Failed"
    Mar 22 20:44:28.634: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:44:28.646
    Mar 22 20:44:28.660: INFO: Waiting for pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de to disappear
    Mar 22 20:44:28.664: INFO: Pod pod-secrets-639ec417-9f46-4737-a378-d906314d22de no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7438" for this suite. 03/22/23 20:44:28.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:28.701
Mar 22 20:44:28.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 20:44:28.703
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:28.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:28.745
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 03/22/23 20:44:28.759
Mar 22 20:44:28.768: INFO: Waiting up to 5m0s for pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4" in namespace "downward-api-8200" to be "running and ready"
Mar 22 20:44:28.775: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.653239ms
Mar 22 20:44:28.775: INFO: The phase of Pod annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:44:30.781: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01224459s
Mar 22 20:44:30.781: INFO: The phase of Pod annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4 is Running (Ready = true)
Mar 22 20:44:30.781: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4" satisfied condition "running and ready"
Mar 22 20:44:31.319: INFO: Successfully updated pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 20:44:33.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8200" for this suite. 03/22/23 20:44:33.349
------------------------------
• [4.657 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:28.701
    Mar 22 20:44:28.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 20:44:28.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:28.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:28.745
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 03/22/23 20:44:28.759
    Mar 22 20:44:28.768: INFO: Waiting up to 5m0s for pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4" in namespace "downward-api-8200" to be "running and ready"
    Mar 22 20:44:28.775: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.653239ms
    Mar 22 20:44:28.775: INFO: The phase of Pod annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:44:30.781: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01224459s
    Mar 22 20:44:30.781: INFO: The phase of Pod annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4 is Running (Ready = true)
    Mar 22 20:44:30.781: INFO: Pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4" satisfied condition "running and ready"
    Mar 22 20:44:31.319: INFO: Successfully updated pod "annotationupdatea3d91030-da09-44a3-8780-29b6a7e012c4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:44:33.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8200" for this suite. 03/22/23 20:44:33.349
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:44:33.362
Mar 22 20:44:33.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption 03/22/23 20:44:33.365
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:33.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:33.386
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 22 20:44:33.420: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 20:45:33.467: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:45:33.472
Mar 22 20:45:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption-path 03/22/23 20:45:33.474
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:33.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:33.493
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Mar 22 20:45:33.522: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 22 20:45:33.527: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Mar 22 20:45:33.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:45:33.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9157" for this suite. 03/22/23 20:45:33.658
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6591" for this suite. 03/22/23 20:45:33.667
------------------------------
• [SLOW TEST] [60.312 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:44:33.362
    Mar 22 20:44:33.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption 03/22/23 20:44:33.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:44:33.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:44:33.386
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 22 20:44:33.420: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 20:45:33.467: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:45:33.472
    Mar 22 20:45:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption-path 03/22/23 20:45:33.474
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:33.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:33.493
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Mar 22 20:45:33.522: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 22 20:45:33.527: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:45:33.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:45:33.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9157" for this suite. 03/22/23 20:45:33.658
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6591" for this suite. 03/22/23 20:45:33.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:45:33.682
Mar 22 20:45:33.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:45:33.684
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:33.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:33.703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 03/22/23 20:45:33.709
Mar 22 20:45:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: mark a version not serverd 03/22/23 20:45:37.715
STEP: check the unserved version gets removed 03/22/23 20:45:37.747
STEP: check the other version is not changed 03/22/23 20:45:39.3
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:45:42.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2138" for this suite. 03/22/23 20:45:42.467
------------------------------
• [SLOW TEST] [8.795 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:45:33.682
    Mar 22 20:45:33.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 20:45:33.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:33.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:33.703
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 03/22/23 20:45:33.709
    Mar 22 20:45:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: mark a version not serverd 03/22/23 20:45:37.715
    STEP: check the unserved version gets removed 03/22/23 20:45:37.747
    STEP: check the other version is not changed 03/22/23 20:45:39.3
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:45:42.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2138" for this suite. 03/22/23 20:45:42.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:45:42.478
Mar 22 20:45:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 20:45:42.48
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:42.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:42.5
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 03/22/23 20:45:42.506
Mar 22 20:45:42.516: INFO: Waiting up to 2m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811" to be "running"
Mar 22 20:45:42.521: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386467ms
Mar 22 20:45:44.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01041236s
Mar 22 20:45:46.532: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015845793s
Mar 22 20:45:48.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011790809s
Mar 22 20:45:50.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011278553s
Mar 22 20:45:52.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010534814s
Mar 22 20:45:54.537: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02102657s
Mar 22 20:45:56.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013398961s
Mar 22 20:45:58.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012639956s
Mar 22 20:46:00.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012311549s
Mar 22 20:46:02.541: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025159351s
Mar 22 20:46:04.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011531535s
Mar 22 20:46:06.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012216544s
Mar 22 20:46:08.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011378344s
Mar 22 20:46:10.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013200361s
Mar 22 20:46:12.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012042238s
Mar 22 20:46:14.536: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019692713s
Mar 22 20:46:16.530: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01383963s
Mar 22 20:46:18.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011860133s
Mar 22 20:46:20.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011405615s
Mar 22 20:46:22.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011487727s
Mar 22 20:46:24.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011102092s
Mar 22 20:46:26.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010959098s
Mar 22 20:46:28.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013088581s
Mar 22 20:46:30.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012710692s
Mar 22 20:46:32.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012704504s
Mar 22 20:46:34.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011559875s
Mar 22 20:46:36.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011817457s
Mar 22 20:46:38.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012101689s
Mar 22 20:46:40.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011302286s
Mar 22 20:46:42.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011947656s
Mar 22 20:46:44.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011997814s
Mar 22 20:46:46.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013519265s
Mar 22 20:46:48.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012470697s
Mar 22 20:46:50.540: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023980618s
Mar 22 20:46:52.532: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015696838s
Mar 22 20:46:54.530: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014124256s
Mar 22 20:46:56.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012173574s
Mar 22 20:46:58.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011557769s
Mar 22 20:47:00.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013023449s
Mar 22 20:47:02.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012425632s
Mar 22 20:47:04.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012839299s
Mar 22 20:47:06.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010189587s
Mar 22 20:47:08.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011383189s
Mar 22 20:47:10.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010956493s
Mar 22 20:47:12.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012063213s
Mar 22 20:47:14.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012789317s
Mar 22 20:47:16.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011105002s
Mar 22 20:47:18.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011688166s
Mar 22 20:47:20.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011571611s
Mar 22 20:47:22.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012408093s
Mar 22 20:47:24.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012695335s
Mar 22 20:47:26.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010551256s
Mar 22 20:47:28.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012748034s
Mar 22 20:47:30.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01315849s
Mar 22 20:47:32.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012669876s
Mar 22 20:47:34.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011193511s
Mar 22 20:47:36.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012917694s
Mar 22 20:47:38.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012185276s
Mar 22 20:47:40.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012916708s
Mar 22 20:47:42.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011982942s
Mar 22 20:47:42.534: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018050974s
STEP: updating the pod 03/22/23 20:47:42.534
Mar 22 20:47:43.054: INFO: Successfully updated pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef"
STEP: waiting for pod running 03/22/23 20:47:43.055
Mar 22 20:47:43.055: INFO: Waiting up to 2m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811" to be "running"
Mar 22 20:47:43.060: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.762655ms
Mar 22 20:47:45.066: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Running", Reason="", readiness=true. Elapsed: 2.01055345s
Mar 22 20:47:45.066: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" satisfied condition "running"
STEP: deleting the pod gracefully 03/22/23 20:47:45.066
Mar 22 20:47:45.066: INFO: Deleting pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811"
Mar 22 20:47:45.076: INFO: Wait up to 5m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 20:48:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1811" for this suite. 03/22/23 20:48:17.092
------------------------------
• [SLOW TEST] [154.624 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:45:42.478
    Mar 22 20:45:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 20:45:42.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:45:42.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:45:42.5
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 03/22/23 20:45:42.506
    Mar 22 20:45:42.516: INFO: Waiting up to 2m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811" to be "running"
    Mar 22 20:45:42.521: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386467ms
    Mar 22 20:45:44.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01041236s
    Mar 22 20:45:46.532: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015845793s
    Mar 22 20:45:48.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011790809s
    Mar 22 20:45:50.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011278553s
    Mar 22 20:45:52.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010534814s
    Mar 22 20:45:54.537: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02102657s
    Mar 22 20:45:56.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013398961s
    Mar 22 20:45:58.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012639956s
    Mar 22 20:46:00.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012311549s
    Mar 22 20:46:02.541: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025159351s
    Mar 22 20:46:04.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011531535s
    Mar 22 20:46:06.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012216544s
    Mar 22 20:46:08.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011378344s
    Mar 22 20:46:10.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013200361s
    Mar 22 20:46:12.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012042238s
    Mar 22 20:46:14.536: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019692713s
    Mar 22 20:46:16.530: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01383963s
    Mar 22 20:46:18.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011860133s
    Mar 22 20:46:20.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011405615s
    Mar 22 20:46:22.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011487727s
    Mar 22 20:46:24.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011102092s
    Mar 22 20:46:26.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010959098s
    Mar 22 20:46:28.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013088581s
    Mar 22 20:46:30.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012710692s
    Mar 22 20:46:32.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012704504s
    Mar 22 20:46:34.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011559875s
    Mar 22 20:46:36.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011817457s
    Mar 22 20:46:38.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012101689s
    Mar 22 20:46:40.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 58.011302286s
    Mar 22 20:46:42.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011947656s
    Mar 22 20:46:44.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011997814s
    Mar 22 20:46:46.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013519265s
    Mar 22 20:46:48.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012470697s
    Mar 22 20:46:50.540: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023980618s
    Mar 22 20:46:52.532: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015696838s
    Mar 22 20:46:54.530: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014124256s
    Mar 22 20:46:56.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012173574s
    Mar 22 20:46:58.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011557769s
    Mar 22 20:47:00.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013023449s
    Mar 22 20:47:02.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012425632s
    Mar 22 20:47:04.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012839299s
    Mar 22 20:47:06.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010189587s
    Mar 22 20:47:08.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011383189s
    Mar 22 20:47:10.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010956493s
    Mar 22 20:47:12.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012063213s
    Mar 22 20:47:14.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012789317s
    Mar 22 20:47:16.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011105002s
    Mar 22 20:47:18.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011688166s
    Mar 22 20:47:20.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011571611s
    Mar 22 20:47:22.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012408093s
    Mar 22 20:47:24.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012695335s
    Mar 22 20:47:26.526: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010551256s
    Mar 22 20:47:28.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012748034s
    Mar 22 20:47:30.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01315849s
    Mar 22 20:47:32.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012669876s
    Mar 22 20:47:34.527: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011193511s
    Mar 22 20:47:36.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012917694s
    Mar 22 20:47:38.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012185276s
    Mar 22 20:47:40.529: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012916708s
    Mar 22 20:47:42.528: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011982942s
    Mar 22 20:47:42.534: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018050974s
    STEP: updating the pod 03/22/23 20:47:42.534
    Mar 22 20:47:43.054: INFO: Successfully updated pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef"
    STEP: waiting for pod running 03/22/23 20:47:43.055
    Mar 22 20:47:43.055: INFO: Waiting up to 2m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811" to be "running"
    Mar 22 20:47:43.060: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.762655ms
    Mar 22 20:47:45.066: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef": Phase="Running", Reason="", readiness=true. Elapsed: 2.01055345s
    Mar 22 20:47:45.066: INFO: Pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" satisfied condition "running"
    STEP: deleting the pod gracefully 03/22/23 20:47:45.066
    Mar 22 20:47:45.066: INFO: Deleting pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" in namespace "var-expansion-1811"
    Mar 22 20:47:45.076: INFO: Wait up to 5m0s for pod "var-expansion-5cf8fd24-5bff-4b77-a64c-b691337f6cef" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:48:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1811" for this suite. 03/22/23 20:48:17.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:48:17.104
Mar 22 20:48:17.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 20:48:17.106
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:48:17.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:48:17.13
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 in namespace container-probe-9334 03/22/23 20:48:17.137
Mar 22 20:48:17.159: INFO: Waiting up to 5m0s for pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843" in namespace "container-probe-9334" to be "not pending"
Mar 22 20:48:17.163: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778888ms
Mar 22 20:48:19.317: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843": Phase="Running", Reason="", readiness=true. Elapsed: 2.158125226s
Mar 22 20:48:19.317: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843" satisfied condition "not pending"
Mar 22 20:48:19.317: INFO: Started pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 in namespace container-probe-9334
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:48:19.317
Mar 22 20:48:19.324: INFO: Initial restart count of pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 is 0
STEP: deleting the pod 03/22/23 20:52:20.119
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:20.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9334" for this suite. 03/22/23 20:52:20.144
------------------------------
• [SLOW TEST] [243.050 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:48:17.104
    Mar 22 20:48:17.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 20:48:17.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:48:17.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:48:17.13
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 in namespace container-probe-9334 03/22/23 20:48:17.137
    Mar 22 20:48:17.159: INFO: Waiting up to 5m0s for pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843" in namespace "container-probe-9334" to be "not pending"
    Mar 22 20:48:17.163: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778888ms
    Mar 22 20:48:19.317: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843": Phase="Running", Reason="", readiness=true. Elapsed: 2.158125226s
    Mar 22 20:48:19.317: INFO: Pod "liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843" satisfied condition "not pending"
    Mar 22 20:48:19.317: INFO: Started pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 in namespace container-probe-9334
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 20:48:19.317
    Mar 22 20:48:19.324: INFO: Initial restart count of pod liveness-441a39ff-c6f4-4bfc-a567-87a18ad9e843 is 0
    STEP: deleting the pod 03/22/23 20:52:20.119
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:20.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9334" for this suite. 03/22/23 20:52:20.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:20.163
Mar 22 20:52:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-runtime 03/22/23 20:52:20.165
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:20.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:20.188
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 03/22/23 20:52:20.196
STEP: wait for the container to reach Succeeded 03/22/23 20:52:20.207
STEP: get the container status 03/22/23 20:52:24.238
STEP: the container should be terminated 03/22/23 20:52:24.243
STEP: the termination message should be set 03/22/23 20:52:24.243
Mar 22 20:52:24.244: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/22/23 20:52:24.244
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:24.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1060" for this suite. 03/22/23 20:52:24.269
------------------------------
• [4.117 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:20.163
    Mar 22 20:52:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-runtime 03/22/23 20:52:20.165
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:20.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:20.188
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 03/22/23 20:52:20.196
    STEP: wait for the container to reach Succeeded 03/22/23 20:52:20.207
    STEP: get the container status 03/22/23 20:52:24.238
    STEP: the container should be terminated 03/22/23 20:52:24.243
    STEP: the termination message should be set 03/22/23 20:52:24.243
    Mar 22 20:52:24.244: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/22/23 20:52:24.244
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:24.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1060" for this suite. 03/22/23 20:52:24.269
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:24.281
Mar 22 20:52:24.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 20:52:24.284
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:24.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:24.312
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-4f35847a-3013-4df7-b580-79553a976236 03/22/23 20:52:24.32
STEP: Creating a pod to test consume secrets 03/22/23 20:52:24.327
Mar 22 20:52:24.336: INFO: Waiting up to 5m0s for pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695" in namespace "secrets-3555" to be "Succeeded or Failed"
Mar 22 20:52:24.342: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301538ms
Mar 22 20:52:26.350: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014385932s
Mar 22 20:52:28.349: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544468s
STEP: Saw pod success 03/22/23 20:52:28.349
Mar 22 20:52:28.350: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695" satisfied condition "Succeeded or Failed"
Mar 22 20:52:28.354: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:52:28.409
Mar 22 20:52:28.425: INFO: Waiting for pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 to disappear
Mar 22 20:52:28.429: INFO: Pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:28.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3555" for this suite. 03/22/23 20:52:28.436
------------------------------
• [4.163 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:24.281
    Mar 22 20:52:24.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 20:52:24.284
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:24.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:24.312
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-4f35847a-3013-4df7-b580-79553a976236 03/22/23 20:52:24.32
    STEP: Creating a pod to test consume secrets 03/22/23 20:52:24.327
    Mar 22 20:52:24.336: INFO: Waiting up to 5m0s for pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695" in namespace "secrets-3555" to be "Succeeded or Failed"
    Mar 22 20:52:24.342: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301538ms
    Mar 22 20:52:26.350: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014385932s
    Mar 22 20:52:28.349: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544468s
    STEP: Saw pod success 03/22/23 20:52:28.349
    Mar 22 20:52:28.350: INFO: Pod "pod-secrets-5f9f184c-a768-409e-8b20-98407501c695" satisfied condition "Succeeded or Failed"
    Mar 22 20:52:28.354: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:52:28.409
    Mar 22 20:52:28.425: INFO: Waiting for pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 to disappear
    Mar 22 20:52:28.429: INFO: Pod pod-secrets-5f9f184c-a768-409e-8b20-98407501c695 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:28.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3555" for this suite. 03/22/23 20:52:28.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:28.446
Mar 22 20:52:28.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:52:28.448
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:28.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:28.47
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 03/22/23 20:52:28.476
Mar 22 20:52:28.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0" in namespace "projected-3241" to be "Succeeded or Failed"
Mar 22 20:52:28.500: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115282ms
Mar 22 20:52:30.507: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127919s
Mar 22 20:52:32.506: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014070874s
STEP: Saw pod success 03/22/23 20:52:32.506
Mar 22 20:52:32.506: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0" satisfied condition "Succeeded or Failed"
Mar 22 20:52:32.511: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 container client-container: <nil>
STEP: delete the pod 03/22/23 20:52:32.523
Mar 22 20:52:32.539: INFO: Waiting for pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 to disappear
Mar 22 20:52:32.544: INFO: Pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:32.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3241" for this suite. 03/22/23 20:52:32.55
------------------------------
• [4.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:28.446
    Mar 22 20:52:28.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:52:28.448
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:28.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:28.47
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 03/22/23 20:52:28.476
    Mar 22 20:52:28.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0" in namespace "projected-3241" to be "Succeeded or Failed"
    Mar 22 20:52:28.500: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115282ms
    Mar 22 20:52:30.507: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127919s
    Mar 22 20:52:32.506: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014070874s
    STEP: Saw pod success 03/22/23 20:52:32.506
    Mar 22 20:52:32.506: INFO: Pod "downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0" satisfied condition "Succeeded or Failed"
    Mar 22 20:52:32.511: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 container client-container: <nil>
    STEP: delete the pod 03/22/23 20:52:32.523
    Mar 22 20:52:32.539: INFO: Waiting for pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 to disappear
    Mar 22 20:52:32.544: INFO: Pod downwardapi-volume-c7063b8b-c3c5-4938-b5bd-8bab923338c0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:32.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3241" for this suite. 03/22/23 20:52:32.55
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:32.563
Mar 22 20:52:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:52:32.565
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:32.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:32.588
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:52:32.61
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:52:33.451
STEP: Deploying the webhook pod 03/22/23 20:52:33.466
STEP: Wait for the deployment to be ready 03/22/23 20:52:33.487
Mar 22 20:52:33.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:52:35.523
STEP: Verifying the service has paired with the endpoint 03/22/23 20:52:35.539
Mar 22 20:52:36.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Mar 22 20:52:36.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6083-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 20:52:37.065
STEP: Creating a custom resource that should be mutated by the webhook 03/22/23 20:52:37.116
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:39.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5146" for this suite. 03/22/23 20:52:39.77
STEP: Destroying namespace "webhook-5146-markers" for this suite. 03/22/23 20:52:39.78
------------------------------
• [SLOW TEST] [7.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:32.563
    Mar 22 20:52:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:52:32.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:32.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:32.588
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:52:32.61
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:52:33.451
    STEP: Deploying the webhook pod 03/22/23 20:52:33.466
    STEP: Wait for the deployment to be ready 03/22/23 20:52:33.487
    Mar 22 20:52:33.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:52:35.523
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:52:35.539
    Mar 22 20:52:36.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Mar 22 20:52:36.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6083-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 20:52:37.065
    STEP: Creating a custom resource that should be mutated by the webhook 03/22/23 20:52:37.116
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:39.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5146" for this suite. 03/22/23 20:52:39.77
    STEP: Destroying namespace "webhook-5146-markers" for this suite. 03/22/23 20:52:39.78
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:39.796
Mar 22 20:52:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:52:39.797
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:39.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:39.828
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Mar 22 20:52:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: creating the pod 03/22/23 20:52:39.846
STEP: submitting the pod to kubernetes 03/22/23 20:52:39.848
Mar 22 20:52:39.857: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed" in namespace "pods-1519" to be "running and ready"
Mar 22 20:52:39.863: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.65251ms
Mar 22 20:52:39.864: INFO: The phase of Pod pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:52:41.871: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.013300195s
Mar 22 20:52:41.872: INFO: The phase of Pod pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed is Running (Ready = true)
Mar 22 20:52:41.872: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1519" for this suite. 03/22/23 20:52:41.907
------------------------------
• [2.120 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:39.796
    Mar 22 20:52:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:52:39.797
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:39.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:39.828
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Mar 22 20:52:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: creating the pod 03/22/23 20:52:39.846
    STEP: submitting the pod to kubernetes 03/22/23 20:52:39.848
    Mar 22 20:52:39.857: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed" in namespace "pods-1519" to be "running and ready"
    Mar 22 20:52:39.863: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.65251ms
    Mar 22 20:52:39.864: INFO: The phase of Pod pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:52:41.871: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.013300195s
    Mar 22 20:52:41.872: INFO: The phase of Pod pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed is Running (Ready = true)
    Mar 22 20:52:41.872: INFO: Pod "pod-logs-websocket-5c402d81-47c7-404b-a9f9-4300713245ed" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1519" for this suite. 03/22/23 20:52:41.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:41.917
Mar 22 20:52:41.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 20:52:41.918
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:41.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:41.939
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 22 20:52:41.945: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 22 20:52:41.956: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 22 20:52:46.961: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 20:52:46.961
Mar 22 20:52:46.962: INFO: Creating deployment "test-rolling-update-deployment"
Mar 22 20:52:46.969: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 22 20:52:46.978: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 22 20:52:48.990: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 22 20:52:48.994: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 20:52:49.021: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2049  3936efe0-da98-4217-83a5-7078fadfdbf0 29227 1 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1aeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 20:52:47 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-22 20:52:48 +0000 UTC,LastTransitionTime:2023-03-22 20:52:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 20:52:49.026: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2049  f73383ef-46e6-4c2d-9c74-e3a682f4b65d 29216 1 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3936efe0-da98-4217-83a5-7078fadfdbf0 0xc004f1b3c7 0xc004f1b3c8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3936efe0-da98-4217-83a5-7078fadfdbf0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:52:49.027: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 22 20:52:49.027: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2049  27e4a9f5-6648-4988-be59-b9c273af9f10 29225 2 2023-03-22 20:52:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3936efe0-da98-4217-83a5-7078fadfdbf0 0xc004f1b297 0xc004f1b298}] [] [{e2e.test Update apps/v1 2023-03-22 20:52:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3936efe0-da98-4217-83a5-7078fadfdbf0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004f1b358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 20:52:49.033: INFO: Pod "test-rolling-update-deployment-7549d9f46d-7w8xx" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-7w8xx test-rolling-update-deployment-7549d9f46d- deployment-2049  edf443f1-e3ae-4c57-b74e-1dc1dfbcb9bb 29214 0 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d f73383ef-46e6-4c2d-9c74-e3a682f4b65d 0xc004f1b8d7 0xc004f1b8d8}] [] [{kube-controller-manager Update v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f73383ef-46e6-4c2d-9c74-e3a682f4b65d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sck5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sck5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.233,StartTime:2023-03-22 20:52:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:52:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://502ef4f9ccc41bee44d1e15f72927ad4cc901086ee3b3c75dfa73cb6744b311c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2049" for this suite. 03/22/23 20:52:49.04
------------------------------
• [SLOW TEST] [7.131 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:41.917
    Mar 22 20:52:41.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 20:52:41.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:41.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:41.939
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 22 20:52:41.945: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 22 20:52:41.956: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 22 20:52:46.961: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 20:52:46.961
    Mar 22 20:52:46.962: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 22 20:52:46.969: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 22 20:52:46.978: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 22 20:52:48.990: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 22 20:52:48.994: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 20:52:49.021: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2049  3936efe0-da98-4217-83a5-7078fadfdbf0 29227 1 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1aeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 20:52:47 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-22 20:52:48 +0000 UTC,LastTransitionTime:2023-03-22 20:52:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 22 20:52:49.026: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2049  f73383ef-46e6-4c2d-9c74-e3a682f4b65d 29216 1 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3936efe0-da98-4217-83a5-7078fadfdbf0 0xc004f1b3c7 0xc004f1b3c8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3936efe0-da98-4217-83a5-7078fadfdbf0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f1b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:52:49.027: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 22 20:52:49.027: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2049  27e4a9f5-6648-4988-be59-b9c273af9f10 29225 2 2023-03-22 20:52:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3936efe0-da98-4217-83a5-7078fadfdbf0 0xc004f1b297 0xc004f1b298}] [] [{e2e.test Update apps/v1 2023-03-22 20:52:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3936efe0-da98-4217-83a5-7078fadfdbf0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004f1b358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 20:52:49.033: INFO: Pod "test-rolling-update-deployment-7549d9f46d-7w8xx" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-7w8xx test-rolling-update-deployment-7549d9f46d- deployment-2049  edf443f1-e3ae-4c57-b74e-1dc1dfbcb9bb 29214 0 2023-03-22 20:52:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d f73383ef-46e6-4c2d-9c74-e3a682f4b65d 0xc004f1b8d7 0xc004f1b8d8}] [] [{kube-controller-manager Update v1 2023-03-22 20:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f73383ef-46e6-4c2d-9c74-e3a682f4b65d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 20:52:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sck5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sck5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 20:52:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.233,StartTime:2023-03-22 20:52:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 20:52:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://502ef4f9ccc41bee44d1e15f72927ad4cc901086ee3b3c75dfa73cb6744b311c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2049" for this suite. 03/22/23 20:52:49.04
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:49.049
Mar 22 20:52:49.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:52:49.05
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:49.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:49.076
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 22 20:52:49.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:55.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5975" for this suite. 03/22/23 20:52:55.437
------------------------------
• [SLOW TEST] [6.396 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:49.049
    Mar 22 20:52:49.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:52:49.05
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:49.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:49.076
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 22 20:52:49.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:55.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5975" for this suite. 03/22/23 20:52:55.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:55.447
Mar 22 20:52:55.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:52:55.448
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:55.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:55.484
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 03/22/23 20:52:55.491
STEP: Getting a ResourceQuota 03/22/23 20:52:55.498
STEP: Listing all ResourceQuotas with LabelSelector 03/22/23 20:52:55.504
STEP: Patching the ResourceQuota 03/22/23 20:52:55.508
STEP: Deleting a Collection of ResourceQuotas 03/22/23 20:52:55.516
STEP: Verifying the deleted ResourceQuota 03/22/23 20:52:55.524
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:52:55.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-238" for this suite. 03/22/23 20:52:55.548
------------------------------
• [0.115 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:55.447
    Mar 22 20:52:55.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:52:55.448
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:55.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:55.484
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 03/22/23 20:52:55.491
    STEP: Getting a ResourceQuota 03/22/23 20:52:55.498
    STEP: Listing all ResourceQuotas with LabelSelector 03/22/23 20:52:55.504
    STEP: Patching the ResourceQuota 03/22/23 20:52:55.508
    STEP: Deleting a Collection of ResourceQuotas 03/22/23 20:52:55.516
    STEP: Verifying the deleted ResourceQuota 03/22/23 20:52:55.524
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:52:55.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-238" for this suite. 03/22/23 20:52:55.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:52:55.564
Mar 22 20:52:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 20:52:55.566
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:55.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:55.587
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-6435 03/22/23 20:52:55.593
STEP: creating service affinity-nodeport in namespace services-6435 03/22/23 20:52:55.593
STEP: creating replication controller affinity-nodeport in namespace services-6435 03/22/23 20:52:55.612
I0322 20:52:55.624735      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6435, replica count: 3
I0322 20:52:58.678700      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 20:52:58.698: INFO: Creating new exec pod
Mar 22 20:52:58.708: INFO: Waiting up to 5m0s for pod "execpod-affinitybkm54" in namespace "services-6435" to be "running"
Mar 22 20:52:58.714: INFO: Pod "execpod-affinitybkm54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.544747ms
Mar 22 20:53:00.732: INFO: Pod "execpod-affinitybkm54": Phase="Running", Reason="", readiness=true. Elapsed: 2.023428911s
Mar 22 20:53:00.732: INFO: Pod "execpod-affinitybkm54" satisfied condition "running"
Mar 22 20:53:01.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Mar 22 20:53:02.159: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 22 20:53:02.159: INFO: stdout: ""
Mar 22 20:53:02.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.245.181.38 80'
Mar 22 20:53:02.500: INFO: stderr: "+ nc -v -z -w 2 10.245.181.38 80\nConnection to 10.245.181.38 80 port [tcp/http] succeeded!\n"
Mar 22 20:53:02.500: INFO: stdout: ""
Mar 22 20:53:02.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 32693'
Mar 22 20:53:02.820: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 32693\nConnection to 10.124.0.2 32693 port [tcp/*] succeeded!\n"
Mar 22 20:53:02.820: INFO: stdout: ""
Mar 22 20:53:02.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 32693'
Mar 22 20:53:03.165: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 32693\nConnection to 10.124.0.4 32693 port [tcp/*] succeeded!\n"
Mar 22 20:53:03.165: INFO: stdout: ""
Mar 22 20:53:03.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32693/ ; done'
Mar 22 20:53:03.795: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n"
Mar 22 20:53:03.795: INFO: stdout: "\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd"
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
Mar 22 20:53:03.795: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6435, will wait for the garbage collector to delete the pods 03/22/23 20:53:03.808
Mar 22 20:53:03.872: INFO: Deleting ReplicationController affinity-nodeport took: 8.524393ms
Mar 22 20:53:03.973: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.88322ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:06.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6435" for this suite. 03/22/23 20:53:06.302
------------------------------
• [SLOW TEST] [10.747 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:52:55.564
    Mar 22 20:52:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 20:52:55.566
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:52:55.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:52:55.587
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-6435 03/22/23 20:52:55.593
    STEP: creating service affinity-nodeport in namespace services-6435 03/22/23 20:52:55.593
    STEP: creating replication controller affinity-nodeport in namespace services-6435 03/22/23 20:52:55.612
    I0322 20:52:55.624735      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6435, replica count: 3
    I0322 20:52:58.678700      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 20:52:58.698: INFO: Creating new exec pod
    Mar 22 20:52:58.708: INFO: Waiting up to 5m0s for pod "execpod-affinitybkm54" in namespace "services-6435" to be "running"
    Mar 22 20:52:58.714: INFO: Pod "execpod-affinitybkm54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.544747ms
    Mar 22 20:53:00.732: INFO: Pod "execpod-affinitybkm54": Phase="Running", Reason="", readiness=true. Elapsed: 2.023428911s
    Mar 22 20:53:00.732: INFO: Pod "execpod-affinitybkm54" satisfied condition "running"
    Mar 22 20:53:01.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Mar 22 20:53:02.159: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 22 20:53:02.159: INFO: stdout: ""
    Mar 22 20:53:02.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.245.181.38 80'
    Mar 22 20:53:02.500: INFO: stderr: "+ nc -v -z -w 2 10.245.181.38 80\nConnection to 10.245.181.38 80 port [tcp/http] succeeded!\n"
    Mar 22 20:53:02.500: INFO: stdout: ""
    Mar 22 20:53:02.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.2 32693'
    Mar 22 20:53:02.820: INFO: stderr: "+ nc -v -z -w 2 10.124.0.2 32693\nConnection to 10.124.0.2 32693 port [tcp/*] succeeded!\n"
    Mar 22 20:53:02.820: INFO: stdout: ""
    Mar 22 20:53:02.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c nc -v -z -w 2 10.124.0.4 32693'
    Mar 22 20:53:03.165: INFO: stderr: "+ nc -v -z -w 2 10.124.0.4 32693\nConnection to 10.124.0.4 32693 port [tcp/*] succeeded!\n"
    Mar 22 20:53:03.165: INFO: stdout: ""
    Mar 22 20:53:03.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6435 exec execpod-affinitybkm54 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.124.0.2:32693/ ; done'
    Mar 22 20:53:03.795: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.124.0.2:32693/\n"
    Mar 22 20:53:03.795: INFO: stdout: "\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd\naffinity-nodeport-cqpsd"
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Received response from host: affinity-nodeport-cqpsd
    Mar 22 20:53:03.795: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6435, will wait for the garbage collector to delete the pods 03/22/23 20:53:03.808
    Mar 22 20:53:03.872: INFO: Deleting ReplicationController affinity-nodeport took: 8.524393ms
    Mar 22 20:53:03.973: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.88322ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:06.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6435" for this suite. 03/22/23 20:53:06.302
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:06.313
Mar 22 20:53:06.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename endpointslice 03/22/23 20:53:06.316
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:06.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:06.341
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:06.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9581" for this suite. 03/22/23 20:53:06.423
------------------------------
• [0.117 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:06.313
    Mar 22 20:53:06.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename endpointslice 03/22/23 20:53:06.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:06.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:06.341
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:06.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9581" for this suite. 03/22/23 20:53:06.423
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:06.431
Mar 22 20:53:06.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:53:06.433
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:06.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:06.474
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 03/22/23 20:53:06.482
STEP: Creating a ResourceQuota 03/22/23 20:53:11.487
STEP: Ensuring resource quota status is calculated 03/22/23 20:53:11.496
STEP: Creating a Pod that fits quota 03/22/23 20:53:13.503
STEP: Ensuring ResourceQuota status captures the pod usage 03/22/23 20:53:13.52
STEP: Not allowing a pod to be created that exceeds remaining quota 03/22/23 20:53:15.527
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/22/23 20:53:15.532
STEP: Ensuring a pod cannot update its resource requirements 03/22/23 20:53:15.536
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/22/23 20:53:15.542
STEP: Deleting the pod 03/22/23 20:53:17.549
STEP: Ensuring resource quota status released the pod usage 03/22/23 20:53:17.565
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:19.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5332" for this suite. 03/22/23 20:53:19.579
------------------------------
• [SLOW TEST] [13.156 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:06.431
    Mar 22 20:53:06.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:53:06.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:06.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:06.474
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 03/22/23 20:53:06.482
    STEP: Creating a ResourceQuota 03/22/23 20:53:11.487
    STEP: Ensuring resource quota status is calculated 03/22/23 20:53:11.496
    STEP: Creating a Pod that fits quota 03/22/23 20:53:13.503
    STEP: Ensuring ResourceQuota status captures the pod usage 03/22/23 20:53:13.52
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/22/23 20:53:15.527
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/22/23 20:53:15.532
    STEP: Ensuring a pod cannot update its resource requirements 03/22/23 20:53:15.536
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/22/23 20:53:15.542
    STEP: Deleting the pod 03/22/23 20:53:17.549
    STEP: Ensuring resource quota status released the pod usage 03/22/23 20:53:17.565
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:19.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5332" for this suite. 03/22/23 20:53:19.579
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:19.59
Mar 22 20:53:19.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 20:53:19.591
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:19.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:19.612
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9965 03/22/23 20:53:19.62
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 03/22/23 20:53:19.629
STEP: Creating pod with conflicting port in namespace statefulset-9965 03/22/23 20:53:19.64
STEP: Waiting until pod test-pod will start running in namespace statefulset-9965 03/22/23 20:53:19.65
Mar 22 20:53:19.653: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9965" to be "running"
Mar 22 20:53:19.668: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.218734ms
Mar 22 20:53:21.674: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021325862s
Mar 22 20:53:23.675: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022080385s
Mar 22 20:53:23.675: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9965 03/22/23 20:53:23.676
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9965 03/22/23 20:53:23.688
Mar 22 20:53:23.700: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Pending. Waiting for statefulset controller to delete.
Mar 22 20:53:23.717: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Failed. Waiting for statefulset controller to delete.
Mar 22 20:53:23.729: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Failed. Waiting for statefulset controller to delete.
Mar 22 20:53:23.734: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9965
STEP: Removing pod with conflicting port in namespace statefulset-9965 03/22/23 20:53:23.734
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9965 and will be in running state 03/22/23 20:53:23.75
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 20:53:27.770: INFO: Deleting all statefulset in ns statefulset-9965
Mar 22 20:53:27.776: INFO: Scaling statefulset ss to 0
Mar 22 20:53:37.801: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:53:37.807: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:37.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9965" for this suite. 03/22/23 20:53:37.83
------------------------------
• [SLOW TEST] [18.249 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:19.59
    Mar 22 20:53:19.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 20:53:19.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:19.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:19.612
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9965 03/22/23 20:53:19.62
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 03/22/23 20:53:19.629
    STEP: Creating pod with conflicting port in namespace statefulset-9965 03/22/23 20:53:19.64
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9965 03/22/23 20:53:19.65
    Mar 22 20:53:19.653: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9965" to be "running"
    Mar 22 20:53:19.668: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.218734ms
    Mar 22 20:53:21.674: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021325862s
    Mar 22 20:53:23.675: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022080385s
    Mar 22 20:53:23.675: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9965 03/22/23 20:53:23.676
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9965 03/22/23 20:53:23.688
    Mar 22 20:53:23.700: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 22 20:53:23.717: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 22 20:53:23.729: INFO: Observed stateful pod in namespace: statefulset-9965, name: ss-0, uid: 896eac67-1525-4d3a-8208-020bd84dd843, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 22 20:53:23.734: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9965
    STEP: Removing pod with conflicting port in namespace statefulset-9965 03/22/23 20:53:23.734
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9965 and will be in running state 03/22/23 20:53:23.75
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 20:53:27.770: INFO: Deleting all statefulset in ns statefulset-9965
    Mar 22 20:53:27.776: INFO: Scaling statefulset ss to 0
    Mar 22 20:53:37.801: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:53:37.807: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:37.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9965" for this suite. 03/22/23 20:53:37.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:37.844
Mar 22 20:53:37.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pod-network-test 03/22/23 20:53:37.846
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:37.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:37.871
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1612 03/22/23 20:53:37.88
STEP: creating a selector 03/22/23 20:53:37.88
STEP: Creating the service pods in kubernetes 03/22/23 20:53:37.88
Mar 22 20:53:37.880: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 22 20:53:37.922: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1612" to be "running and ready"
Mar 22 20:53:37.932: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.379644ms
Mar 22 20:53:37.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 20:53:39.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016992165s
Mar 22 20:53:39.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:53:41.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017005853s
Mar 22 20:53:41.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:53:43.938: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016518186s
Mar 22 20:53:43.938: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:53:45.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019319953s
Mar 22 20:53:45.941: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:53:47.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016994837s
Mar 22 20:53:47.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 20:53:49.944: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.021898258s
Mar 22 20:53:49.944: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 22 20:53:49.944: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 22 20:53:49.949: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1612" to be "running and ready"
Mar 22 20:53:49.955: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.963672ms
Mar 22 20:53:49.955: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 22 20:53:49.955: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 22 20:53:49.959: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1612" to be "running and ready"
Mar 22 20:53:49.964: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.987298ms
Mar 22 20:53:49.965: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 22 20:53:49.965: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/22/23 20:53:49.97
Mar 22 20:53:49.980: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1612" to be "running"
Mar 22 20:53:49.987: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767565ms
Mar 22 20:53:51.993: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013144163s
Mar 22 20:53:51.993: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 22 20:53:51.998: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 22 20:53:51.998: INFO: Breadth first check of 10.244.1.17 on host 10.124.0.2...
Mar 22 20:53:52.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.1.17&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:53:52.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:53:52.002: INFO: ExecWithOptions: Clientset creation
Mar 22 20:53:52.002: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.17%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:53:52.185: INFO: Waiting for responses: map[]
Mar 22 20:53:52.185: INFO: reached 10.244.1.17 after 0/1 tries
Mar 22 20:53:52.185: INFO: Breadth first check of 10.244.0.16 on host 10.124.0.3...
Mar 22 20:53:52.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.0.16&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:53:52.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:53:52.192: INFO: ExecWithOptions: Clientset creation
Mar 22 20:53:52.192: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:53:52.364: INFO: Waiting for responses: map[]
Mar 22 20:53:52.364: INFO: reached 10.244.0.16 after 0/1 tries
Mar 22 20:53:52.364: INFO: Breadth first check of 10.244.0.150 on host 10.124.0.4...
Mar 22 20:53:52.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.0.150&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 20:53:52.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 20:53:52.369: INFO: ExecWithOptions: Clientset creation
Mar 22 20:53:52.369: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 22 20:53:52.524: INFO: Waiting for responses: map[]
Mar 22 20:53:52.524: INFO: reached 10.244.0.150 after 0/1 tries
Mar 22 20:53:52.524: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:52.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1612" for this suite. 03/22/23 20:53:52.533
------------------------------
• [SLOW TEST] [14.697 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:37.844
    Mar 22 20:53:37.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pod-network-test 03/22/23 20:53:37.846
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:37.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:37.871
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1612 03/22/23 20:53:37.88
    STEP: creating a selector 03/22/23 20:53:37.88
    STEP: Creating the service pods in kubernetes 03/22/23 20:53:37.88
    Mar 22 20:53:37.880: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 22 20:53:37.922: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1612" to be "running and ready"
    Mar 22 20:53:37.932: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.379644ms
    Mar 22 20:53:37.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 20:53:39.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016992165s
    Mar 22 20:53:39.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:53:41.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017005853s
    Mar 22 20:53:41.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:53:43.938: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016518186s
    Mar 22 20:53:43.938: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:53:45.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019319953s
    Mar 22 20:53:45.941: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:53:47.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016994837s
    Mar 22 20:53:47.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 20:53:49.944: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.021898258s
    Mar 22 20:53:49.944: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 22 20:53:49.944: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 22 20:53:49.949: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1612" to be "running and ready"
    Mar 22 20:53:49.955: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.963672ms
    Mar 22 20:53:49.955: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 22 20:53:49.955: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 22 20:53:49.959: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1612" to be "running and ready"
    Mar 22 20:53:49.964: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.987298ms
    Mar 22 20:53:49.965: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 22 20:53:49.965: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/22/23 20:53:49.97
    Mar 22 20:53:49.980: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1612" to be "running"
    Mar 22 20:53:49.987: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767565ms
    Mar 22 20:53:51.993: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013144163s
    Mar 22 20:53:51.993: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 22 20:53:51.998: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 22 20:53:51.998: INFO: Breadth first check of 10.244.1.17 on host 10.124.0.2...
    Mar 22 20:53:52.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.1.17&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:53:52.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:53:52.002: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:53:52.002: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.17%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:53:52.185: INFO: Waiting for responses: map[]
    Mar 22 20:53:52.185: INFO: reached 10.244.1.17 after 0/1 tries
    Mar 22 20:53:52.185: INFO: Breadth first check of 10.244.0.16 on host 10.124.0.3...
    Mar 22 20:53:52.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.0.16&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:53:52.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:53:52.192: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:53:52.192: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:53:52.364: INFO: Waiting for responses: map[]
    Mar 22 20:53:52.364: INFO: reached 10.244.0.16 after 0/1 tries
    Mar 22 20:53:52.364: INFO: Breadth first check of 10.244.0.150 on host 10.124.0.4...
    Mar 22 20:53:52.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.143:9080/dial?request=hostname&protocol=http&host=10.244.0.150&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 20:53:52.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 20:53:52.369: INFO: ExecWithOptions: Clientset creation
    Mar 22 20:53:52.369: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-1612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 22 20:53:52.524: INFO: Waiting for responses: map[]
    Mar 22 20:53:52.524: INFO: reached 10.244.0.150 after 0/1 tries
    Mar 22 20:53:52.524: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:52.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1612" for this suite. 03/22/23 20:53:52.533
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:52.549
Mar 22 20:53:52.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 20:53:52.551
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:52.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:52.574
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/22/23 20:53:52.586
STEP: submitting the pod to kubernetes 03/22/23 20:53:52.586
STEP: verifying QOS class is set on the pod 03/22/23 20:53:52.597
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:52.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3396" for this suite. 03/22/23 20:53:52.608
------------------------------
• [0.069 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:52.549
    Mar 22 20:53:52.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 20:53:52.551
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:52.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:52.574
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/22/23 20:53:52.586
    STEP: submitting the pod to kubernetes 03/22/23 20:53:52.586
    STEP: verifying QOS class is set on the pod 03/22/23 20:53:52.597
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:52.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3396" for this suite. 03/22/23 20:53:52.608
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:52.619
Mar 22 20:53:52.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename containers 03/22/23 20:53:52.62
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:52.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:52.644
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 03/22/23 20:53:52.651
Mar 22 20:53:52.659: INFO: Waiting up to 5m0s for pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd" in namespace "containers-4221" to be "Succeeded or Failed"
Mar 22 20:53:52.665: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509288ms
Mar 22 20:53:54.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254951s
Mar 22 20:53:56.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013084688s
STEP: Saw pod success 03/22/23 20:53:56.672
Mar 22 20:53:56.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd" satisfied condition "Succeeded or Failed"
Mar 22 20:53:56.677: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd container agnhost-container: <nil>
STEP: delete the pod 03/22/23 20:53:56.689
Mar 22 20:53:56.704: INFO: Waiting for pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd to disappear
Mar 22 20:53:56.708: INFO: Pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 22 20:53:56.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4221" for this suite. 03/22/23 20:53:56.715
------------------------------
• [4.104 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:52.619
    Mar 22 20:53:52.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename containers 03/22/23 20:53:52.62
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:52.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:52.644
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 03/22/23 20:53:52.651
    Mar 22 20:53:52.659: INFO: Waiting up to 5m0s for pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd" in namespace "containers-4221" to be "Succeeded or Failed"
    Mar 22 20:53:52.665: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509288ms
    Mar 22 20:53:54.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254951s
    Mar 22 20:53:56.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013084688s
    STEP: Saw pod success 03/22/23 20:53:56.672
    Mar 22 20:53:56.672: INFO: Pod "client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd" satisfied condition "Succeeded or Failed"
    Mar 22 20:53:56.677: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 20:53:56.689
    Mar 22 20:53:56.704: INFO: Waiting for pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd to disappear
    Mar 22 20:53:56.708: INFO: Pod client-containers-bf3649f6-9554-4a6b-af49-0e1613a3c0fd no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:53:56.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4221" for this suite. 03/22/23 20:53:56.715
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:53:56.727
Mar 22 20:53:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 20:53:56.73
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:56.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:56.762
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 03/22/23 20:53:56.769
STEP: Ensuring active pods == parallelism 03/22/23 20:53:56.777
STEP: delete a job 03/22/23 20:53:58.784
STEP: deleting Job.batch foo in namespace job-8370, will wait for the garbage collector to delete the pods 03/22/23 20:53:58.784
Mar 22 20:53:58.848: INFO: Deleting Job.batch foo took: 8.115522ms
Mar 22 20:53:58.949: INFO: Terminating Job.batch foo pods took: 100.489108ms
STEP: Ensuring job was deleted 03/22/23 20:54:31.55
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:31.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8370" for this suite. 03/22/23 20:54:31.563
------------------------------
• [SLOW TEST] [34.852 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:53:56.727
    Mar 22 20:53:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 20:53:56.73
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:53:56.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:53:56.762
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 03/22/23 20:53:56.769
    STEP: Ensuring active pods == parallelism 03/22/23 20:53:56.777
    STEP: delete a job 03/22/23 20:53:58.784
    STEP: deleting Job.batch foo in namespace job-8370, will wait for the garbage collector to delete the pods 03/22/23 20:53:58.784
    Mar 22 20:53:58.848: INFO: Deleting Job.batch foo took: 8.115522ms
    Mar 22 20:53:58.949: INFO: Terminating Job.batch foo pods took: 100.489108ms
    STEP: Ensuring job was deleted 03/22/23 20:54:31.55
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:31.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8370" for this suite. 03/22/23 20:54:31.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:31.586
Mar 22 20:54:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 20:54:31.588
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:31.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:31.611
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2633/configmap-test-2989a7f7-9040-459d-bd14-24c5e47122b9 03/22/23 20:54:31.618
STEP: Creating a pod to test consume configMaps 03/22/23 20:54:31.625
Mar 22 20:54:31.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2" in namespace "configmap-2633" to be "Succeeded or Failed"
Mar 22 20:54:31.641: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645579ms
Mar 22 20:54:33.647: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010542295s
Mar 22 20:54:35.649: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012969246s
STEP: Saw pod success 03/22/23 20:54:35.649
Mar 22 20:54:35.650: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2" satisfied condition "Succeeded or Failed"
Mar 22 20:54:35.654: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 container env-test: <nil>
STEP: delete the pod 03/22/23 20:54:35.673
Mar 22 20:54:35.688: INFO: Waiting for pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 to disappear
Mar 22 20:54:35.691: INFO: Pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:35.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2633" for this suite. 03/22/23 20:54:35.698
------------------------------
• [4.120 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:31.586
    Mar 22 20:54:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 20:54:31.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:31.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:31.611
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2633/configmap-test-2989a7f7-9040-459d-bd14-24c5e47122b9 03/22/23 20:54:31.618
    STEP: Creating a pod to test consume configMaps 03/22/23 20:54:31.625
    Mar 22 20:54:31.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2" in namespace "configmap-2633" to be "Succeeded or Failed"
    Mar 22 20:54:31.641: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645579ms
    Mar 22 20:54:33.647: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010542295s
    Mar 22 20:54:35.649: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012969246s
    STEP: Saw pod success 03/22/23 20:54:35.649
    Mar 22 20:54:35.650: INFO: Pod "pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2" satisfied condition "Succeeded or Failed"
    Mar 22 20:54:35.654: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 container env-test: <nil>
    STEP: delete the pod 03/22/23 20:54:35.673
    Mar 22 20:54:35.688: INFO: Waiting for pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 to disappear
    Mar 22 20:54:35.691: INFO: Pod pod-configmaps-a09aa1d9-8c3e-44d1-aae3-68feb72cd8a2 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:35.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2633" for this suite. 03/22/23 20:54:35.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:35.718
Mar 22 20:54:35.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 20:54:35.72
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:35.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:35.74
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/22/23 20:54:35.753
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_tcp@PTR;sleep 1; done
 03/22/23 20:54:35.775
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_tcp@PTR;sleep 1; done
 03/22/23 20:54:35.775
STEP: creating a pod to probe DNS 03/22/23 20:54:35.775
STEP: submitting the pod to kubernetes 03/22/23 20:54:35.775
Mar 22 20:54:35.789: INFO: Waiting up to 15m0s for pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc" in namespace "dns-5297" to be "running"
Mar 22 20:54:35.795: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944373ms
Mar 22 20:54:37.802: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012487823s
Mar 22 20:54:37.802: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc" satisfied condition "running"
STEP: retrieving the pod 03/22/23 20:54:37.802
STEP: looking for the results for each expected name from probers 03/22/23 20:54:37.807
Mar 22 20:54:37.851: INFO: Unable to read wheezy_udp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.869: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.877: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.922: INFO: Unable to read jessie_udp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.930: INFO: Unable to read jessie_tcp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.938: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.947: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:37.984: INFO: Lookups using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc failed for: [wheezy_udp@dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local jessie_udp@dns-test-service.dns-5297.svc.cluster.local jessie_tcp@dns-test-service.dns-5297.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local]

Mar 22 20:54:43.012: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:43.020: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
Mar 22 20:54:43.120: INFO: Lookups using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local]

Mar 22 20:54:48.123: INFO: DNS probes using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc succeeded

STEP: deleting the pod 03/22/23 20:54:48.123
STEP: deleting the test service 03/22/23 20:54:48.143
STEP: deleting the test headless service 03/22/23 20:54:48.17
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5297" for this suite. 03/22/23 20:54:48.189
------------------------------
• [SLOW TEST] [12.480 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:35.718
    Mar 22 20:54:35.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 20:54:35.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:35.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:35.74
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/22/23 20:54:35.753
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_tcp@PTR;sleep 1; done
     03/22/23 20:54:35.775
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5297.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5297.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5297.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_udp@PTR;check="$$(dig +tcp +noall +answer +search 150.72.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.72.150_tcp@PTR;sleep 1; done
     03/22/23 20:54:35.775
    STEP: creating a pod to probe DNS 03/22/23 20:54:35.775
    STEP: submitting the pod to kubernetes 03/22/23 20:54:35.775
    Mar 22 20:54:35.789: INFO: Waiting up to 15m0s for pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc" in namespace "dns-5297" to be "running"
    Mar 22 20:54:35.795: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944373ms
    Mar 22 20:54:37.802: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012487823s
    Mar 22 20:54:37.802: INFO: Pod "dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 20:54:37.802
    STEP: looking for the results for each expected name from probers 03/22/23 20:54:37.807
    Mar 22 20:54:37.851: INFO: Unable to read wheezy_udp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.869: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.877: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.922: INFO: Unable to read jessie_udp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.930: INFO: Unable to read jessie_tcp@dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.938: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.947: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:37.984: INFO: Lookups using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc failed for: [wheezy_udp@dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@dns-test-service.dns-5297.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local jessie_udp@dns-test-service.dns-5297.svc.cluster.local jessie_tcp@dns-test-service.dns-5297.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local]

    Mar 22 20:54:43.012: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:43.020: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local from pod dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc: the server could not find the requested resource (get pods dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc)
    Mar 22 20:54:43.120: INFO: Lookups using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5297.svc.cluster.local]

    Mar 22 20:54:48.123: INFO: DNS probes using dns-5297/dns-test-bb7b3576-0a51-45fc-bb24-6621d83e20fc succeeded

    STEP: deleting the pod 03/22/23 20:54:48.123
    STEP: deleting the test service 03/22/23 20:54:48.143
    STEP: deleting the test headless service 03/22/23 20:54:48.17
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5297" for this suite. 03/22/23 20:54:48.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:48.206
Mar 22 20:54:48.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 20:54:48.207
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:48.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:48.235
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 03/22/23 20:54:48.242
Mar 22 20:54:48.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6105 cluster-info'
Mar 22 20:54:48.376: INFO: stderr: ""
Mar 22 20:54:48.376: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.245.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:48.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6105" for this suite. 03/22/23 20:54:48.381
------------------------------
• [0.182 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:48.206
    Mar 22 20:54:48.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 20:54:48.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:48.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:48.235
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 03/22/23 20:54:48.242
    Mar 22 20:54:48.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6105 cluster-info'
    Mar 22 20:54:48.376: INFO: stderr: ""
    Mar 22 20:54:48.376: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.245.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:48.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6105" for this suite. 03/22/23 20:54:48.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:48.388
Mar 22 20:54:48.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 20:54:48.389
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:48.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:48.41
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/22/23 20:54:48.416
Mar 22 20:54:48.426: INFO: Waiting up to 5m0s for pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792" in namespace "emptydir-6158" to be "Succeeded or Failed"
Mar 22 20:54:48.431: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Pending", Reason="", readiness=false. Elapsed: 5.797632ms
Mar 22 20:54:50.438: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Running", Reason="", readiness=true. Elapsed: 2.012699369s
Mar 22 20:54:52.438: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01261497s
STEP: Saw pod success 03/22/23 20:54:52.438
Mar 22 20:54:52.439: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792" satisfied condition "Succeeded or Failed"
Mar 22 20:54:52.446: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 container test-container: <nil>
STEP: delete the pod 03/22/23 20:54:52.461
Mar 22 20:54:52.478: INFO: Waiting for pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 to disappear
Mar 22 20:54:52.481: INFO: Pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6158" for this suite. 03/22/23 20:54:52.487
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:48.388
    Mar 22 20:54:48.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 20:54:48.389
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:48.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:48.41
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/22/23 20:54:48.416
    Mar 22 20:54:48.426: INFO: Waiting up to 5m0s for pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792" in namespace "emptydir-6158" to be "Succeeded or Failed"
    Mar 22 20:54:48.431: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Pending", Reason="", readiness=false. Elapsed: 5.797632ms
    Mar 22 20:54:50.438: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Running", Reason="", readiness=true. Elapsed: 2.012699369s
    Mar 22 20:54:52.438: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01261497s
    STEP: Saw pod success 03/22/23 20:54:52.438
    Mar 22 20:54:52.439: INFO: Pod "pod-3338f896-9270-4fc8-90ca-b041d1ff5792" satisfied condition "Succeeded or Failed"
    Mar 22 20:54:52.446: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 container test-container: <nil>
    STEP: delete the pod 03/22/23 20:54:52.461
    Mar 22 20:54:52.478: INFO: Waiting for pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 to disappear
    Mar 22 20:54:52.481: INFO: Pod pod-3338f896-9270-4fc8-90ca-b041d1ff5792 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6158" for this suite. 03/22/23 20:54:52.487
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:52.494
Mar 22 20:54:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-runtime 03/22/23 20:54:52.496
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:52.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:52.514
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 03/22/23 20:54:52.522
STEP: wait for the container to reach Failed 03/22/23 20:54:52.531
STEP: get the container status 03/22/23 20:54:56.562
STEP: the container should be terminated 03/22/23 20:54:56.566
STEP: the termination message should be set 03/22/23 20:54:56.566
Mar 22 20:54:56.566: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/22/23 20:54:56.567
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 22 20:54:56.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6757" for this suite. 03/22/23 20:54:56.588
------------------------------
• [4.102 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:52.494
    Mar 22 20:54:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-runtime 03/22/23 20:54:52.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:52.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:52.514
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 03/22/23 20:54:52.522
    STEP: wait for the container to reach Failed 03/22/23 20:54:52.531
    STEP: get the container status 03/22/23 20:54:56.562
    STEP: the container should be terminated 03/22/23 20:54:56.566
    STEP: the termination message should be set 03/22/23 20:54:56.566
    Mar 22 20:54:56.566: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/22/23 20:54:56.567
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:54:56.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6757" for this suite. 03/22/23 20:54:56.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:54:56.596
Mar 22 20:54:56.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 20:54:56.598
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:56.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:56.62
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9181 03/22/23 20:54:56.626
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 03/22/23 20:54:56.633
Mar 22 20:54:56.652: INFO: Found 0 stateful pods, waiting for 3
Mar 22 20:55:06.659: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 20:55:06.660: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 20:55:06.660: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 20:55:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:55:07.074: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:55:07.074: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:55:07.074: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/22/23 20:55:17.094
Mar 22 20:55:17.123: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/22/23 20:55:17.123
STEP: Updating Pods in reverse ordinal order 03/22/23 20:55:27.143
Mar 22 20:55:27.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 20:55:27.396: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 20:55:27.396: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 20:55:27.396: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/22/23 20:55:47.425
Mar 22 20:55:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 20:55:47.697: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 20:55:47.697: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 20:55:47.697: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 20:55:57.741: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/22/23 20:56:07.767
Mar 22 20:56:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 20:56:08.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 20:56:08.108: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 20:56:08.108: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 20:56:18.136: INFO: Deleting all statefulset in ns statefulset-9181
Mar 22 20:56:18.141: INFO: Scaling statefulset ss2 to 0
Mar 22 20:56:28.175: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 20:56:28.180: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:56:28.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9181" for this suite. 03/22/23 20:56:28.204
------------------------------
• [SLOW TEST] [91.614 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:54:56.596
    Mar 22 20:54:56.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 20:54:56.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:54:56.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:54:56.62
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9181 03/22/23 20:54:56.626
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 03/22/23 20:54:56.633
    Mar 22 20:54:56.652: INFO: Found 0 stateful pods, waiting for 3
    Mar 22 20:55:06.659: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 20:55:06.660: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 20:55:06.660: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 20:55:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:55:07.074: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:55:07.074: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:55:07.074: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/22/23 20:55:17.094
    Mar 22 20:55:17.123: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/22/23 20:55:17.123
    STEP: Updating Pods in reverse ordinal order 03/22/23 20:55:27.143
    Mar 22 20:55:27.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 20:55:27.396: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 20:55:27.396: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 20:55:27.396: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/22/23 20:55:47.425
    Mar 22 20:55:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 20:55:47.697: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 20:55:47.697: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 20:55:47.697: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 20:55:57.741: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/22/23 20:56:07.767
    Mar 22 20:56:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-9181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 20:56:08.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 20:56:08.108: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 20:56:08.108: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 20:56:18.136: INFO: Deleting all statefulset in ns statefulset-9181
    Mar 22 20:56:18.141: INFO: Scaling statefulset ss2 to 0
    Mar 22 20:56:28.175: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 20:56:28.180: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:56:28.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9181" for this suite. 03/22/23 20:56:28.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:56:28.216
Mar 22 20:56:28.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:56:28.216
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:28.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:28.236
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/22/23 20:56:28.244
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/22/23 20:56:28.247
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/22/23 20:56:28.247
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/22/23 20:56:28.248
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/22/23 20:56:28.251
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/22/23 20:56:28.252
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/22/23 20:56:28.255
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:56:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-858" for this suite. 03/22/23 20:56:28.262
------------------------------
• [0.055 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:56:28.216
    Mar 22 20:56:28.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 20:56:28.216
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:28.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:28.236
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/22/23 20:56:28.244
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/22/23 20:56:28.247
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/22/23 20:56:28.247
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/22/23 20:56:28.248
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/22/23 20:56:28.251
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/22/23 20:56:28.252
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/22/23 20:56:28.255
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:56:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-858" for this suite. 03/22/23 20:56:28.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:56:28.272
Mar 22 20:56:28.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 20:56:28.273
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:28.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:28.293
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-be0ccf2f-bd91-43e3-869e-19101a0e1a8c 03/22/23 20:56:28.301
STEP: Creating a pod to test consume secrets 03/22/23 20:56:28.308
Mar 22 20:56:28.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb" in namespace "projected-3230" to be "Succeeded or Failed"
Mar 22 20:56:28.329: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759478ms
Mar 22 20:56:30.336: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011140816s
Mar 22 20:56:32.336: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01126366s
STEP: Saw pod success 03/22/23 20:56:32.336
Mar 22 20:56:32.337: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb" satisfied condition "Succeeded or Failed"
Mar 22 20:56:32.341: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb container projected-secret-volume-test: <nil>
STEP: delete the pod 03/22/23 20:56:32.382
Mar 22 20:56:32.396: INFO: Waiting for pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb to disappear
Mar 22 20:56:32.400: INFO: Pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 20:56:32.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3230" for this suite. 03/22/23 20:56:32.405
------------------------------
• [4.141 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:56:28.272
    Mar 22 20:56:28.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 20:56:28.273
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:28.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:28.293
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-be0ccf2f-bd91-43e3-869e-19101a0e1a8c 03/22/23 20:56:28.301
    STEP: Creating a pod to test consume secrets 03/22/23 20:56:28.308
    Mar 22 20:56:28.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb" in namespace "projected-3230" to be "Succeeded or Failed"
    Mar 22 20:56:28.329: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759478ms
    Mar 22 20:56:30.336: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011140816s
    Mar 22 20:56:32.336: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01126366s
    STEP: Saw pod success 03/22/23 20:56:32.336
    Mar 22 20:56:32.337: INFO: Pod "pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb" satisfied condition "Succeeded or Failed"
    Mar 22 20:56:32.341: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 20:56:32.382
    Mar 22 20:56:32.396: INFO: Waiting for pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb to disappear
    Mar 22 20:56:32.400: INFO: Pod pod-projected-secrets-58fd8644-5c93-453c-b2c5-fe507668d9cb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:56:32.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3230" for this suite. 03/22/23 20:56:32.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:56:32.422
Mar 22 20:56:32.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 20:56:32.423
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:32.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:32.45
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-9sf58" 03/22/23 20:56:32.461
Mar 22 20:56:32.474: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard cpu limit of 500m
Mar 22 20:56:32.475: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.475
STEP: Confirm /status for "e2e-rq-status-9sf58" resourceQuota via watch 03/22/23 20:56:32.485
Mar 22 20:56:32.489: INFO: observed resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList(nil)
Mar 22 20:56:32.489: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 22 20:56:32.489: INFO: ResourceQuota "e2e-rq-status-9sf58" /status was updated
STEP: Patching hard spec values for cpu & memory 03/22/23 20:56:32.494
Mar 22 20:56:32.501: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard cpu limit of 1
Mar 22 20:56:32.501: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.501
STEP: Confirm /status for "e2e-rq-status-9sf58" resourceQuota via watch 03/22/23 20:56:32.507
Mar 22 20:56:32.512: INFO: observed resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 22 20:56:32.512: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Mar 22 20:56:32.512: INFO: ResourceQuota "e2e-rq-status-9sf58" /status was patched
STEP: Get "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.512
Mar 22 20:56:32.527: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard cpu of 1
Mar 22 20:56:32.527: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-9sf58" /status before checking Spec is unchanged 03/22/23 20:56:32.531
Mar 22 20:56:32.540: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard cpu of 2
Mar 22 20:56:32.540: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard memory of 2Gi
Mar 22 20:56:32.544: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Mar 22 20:59:37.556: INFO: ResourceQuota "e2e-rq-status-9sf58" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:37.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8366" for this suite. 03/22/23 20:59:37.562
------------------------------
• [SLOW TEST] [185.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:56:32.422
    Mar 22 20:56:32.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 20:56:32.423
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:56:32.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:56:32.45
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-9sf58" 03/22/23 20:56:32.461
    Mar 22 20:56:32.474: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard cpu limit of 500m
    Mar 22 20:56:32.475: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.475
    STEP: Confirm /status for "e2e-rq-status-9sf58" resourceQuota via watch 03/22/23 20:56:32.485
    Mar 22 20:56:32.489: INFO: observed resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList(nil)
    Mar 22 20:56:32.489: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 22 20:56:32.489: INFO: ResourceQuota "e2e-rq-status-9sf58" /status was updated
    STEP: Patching hard spec values for cpu & memory 03/22/23 20:56:32.494
    Mar 22 20:56:32.501: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard cpu limit of 1
    Mar 22 20:56:32.501: INFO: Resource quota "e2e-rq-status-9sf58" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.501
    STEP: Confirm /status for "e2e-rq-status-9sf58" resourceQuota via watch 03/22/23 20:56:32.507
    Mar 22 20:56:32.512: INFO: observed resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 22 20:56:32.512: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Mar 22 20:56:32.512: INFO: ResourceQuota "e2e-rq-status-9sf58" /status was patched
    STEP: Get "e2e-rq-status-9sf58" /status 03/22/23 20:56:32.512
    Mar 22 20:56:32.527: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard cpu of 1
    Mar 22 20:56:32.527: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-9sf58" /status before checking Spec is unchanged 03/22/23 20:56:32.531
    Mar 22 20:56:32.540: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard cpu of 2
    Mar 22 20:56:32.540: INFO: Resourcequota "e2e-rq-status-9sf58" reports status: hard memory of 2Gi
    Mar 22 20:56:32.544: INFO: Found resourceQuota "e2e-rq-status-9sf58" in namespace "resourcequota-8366" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Mar 22 20:59:37.556: INFO: ResourceQuota "e2e-rq-status-9sf58" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:37.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8366" for this suite. 03/22/23 20:59:37.562
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:37.576
Mar 22 20:59:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 20:59:37.578
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.601
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-7695" 03/22/23 20:59:37.607
Mar 22 20:59:37.627: INFO: Namespace "namespaces-7695" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e69c15cc-2126-4a9e-901a-fc3a13da8e2f", "kubernetes.io/metadata.name":"namespaces-7695", "namespaces-7695":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:37.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7695" for this suite. 03/22/23 20:59:37.634
------------------------------
• [0.065 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:37.576
    Mar 22 20:59:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 20:59:37.578
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.601
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-7695" 03/22/23 20:59:37.607
    Mar 22 20:59:37.627: INFO: Namespace "namespaces-7695" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e69c15cc-2126-4a9e-901a-fc3a13da8e2f", "kubernetes.io/metadata.name":"namespaces-7695", "namespaces-7695":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:37.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7695" for this suite. 03/22/23 20:59:37.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:37.648
Mar 22 20:59:37.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 20:59:37.65
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.674
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 03/22/23 20:59:37.681
STEP: watching for the ServiceAccount to be added 03/22/23 20:59:37.692
STEP: patching the ServiceAccount 03/22/23 20:59:37.695
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/22/23 20:59:37.702
STEP: deleting the ServiceAccount 03/22/23 20:59:37.707
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5784" for this suite. 03/22/23 20:59:37.729
------------------------------
• [0.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:37.648
    Mar 22 20:59:37.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 20:59:37.65
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.674
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 03/22/23 20:59:37.681
    STEP: watching for the ServiceAccount to be added 03/22/23 20:59:37.692
    STEP: patching the ServiceAccount 03/22/23 20:59:37.695
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/22/23 20:59:37.702
    STEP: deleting the ServiceAccount 03/22/23 20:59:37.707
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5784" for this suite. 03/22/23 20:59:37.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:37.737
Mar 22 20:59:37.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:59:37.738
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.764
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/22/23 20:59:37.781
Mar 22 20:59:37.781: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0" in namespace "kubelet-test-4570" to be "completed"
Mar 22 20:59:37.786: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381894ms
Mar 22 20:59:39.793: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011600001s
Mar 22 20:59:41.792: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010833814s
Mar 22 20:59:41.793: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:41.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4570" for this suite. 03/22/23 20:59:41.84
------------------------------
• [4.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:37.737
    Mar 22 20:59:37.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubelet-test 03/22/23 20:59:37.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:37.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:37.764
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/22/23 20:59:37.781
    Mar 22 20:59:37.781: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0" in namespace "kubelet-test-4570" to be "completed"
    Mar 22 20:59:37.786: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381894ms
    Mar 22 20:59:39.793: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011600001s
    Mar 22 20:59:41.792: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010833814s
    Mar 22 20:59:41.793: INFO: Pod "agnhost-host-aliases27696440-8a7f-47f1-8868-7d40025793f0" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:41.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4570" for this suite. 03/22/23 20:59:41.84
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:41.851
Mar 22 20:59:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename init-container 03/22/23 20:59:41.853
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:41.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:41.875
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 03/22/23 20:59:41.881
Mar 22 20:59:41.881: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:46.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2158" for this suite. 03/22/23 20:59:46.608
------------------------------
• [4.764 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:41.851
    Mar 22 20:59:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename init-container 03/22/23 20:59:41.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:41.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:41.875
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 03/22/23 20:59:41.881
    Mar 22 20:59:41.881: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:46.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2158" for this suite. 03/22/23 20:59:46.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:46.616
Mar 22 20:59:46.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 20:59:46.616
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:46.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:46.635
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 20:59:46.654
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:59:47.857
STEP: Deploying the webhook pod 03/22/23 20:59:47.868
STEP: Wait for the deployment to be ready 03/22/23 20:59:47.887
Mar 22 20:59:47.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 20:59:49.914
STEP: Verifying the service has paired with the endpoint 03/22/23 20:59:49.928
Mar 22 20:59:50.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Mar 22 20:59:50.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/22/23 20:59:51.45
STEP: Creating a custom resource that should be denied by the webhook 03/22/23 20:59:51.503
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/22/23 20:59:53.571
STEP: Updating the custom resource with disallowed data should be denied 03/22/23 20:59:53.584
STEP: Deleting the custom resource should be denied 03/22/23 20:59:53.617
STEP: Remove the offending key and value from the custom resource data 03/22/23 20:59:53.64
STEP: Deleting the updated custom resource should be successful 03/22/23 20:59:53.667
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5529" for this suite. 03/22/23 20:59:54.26
STEP: Destroying namespace "webhook-5529-markers" for this suite. 03/22/23 20:59:54.269
------------------------------
• [SLOW TEST] [7.668 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:46.616
    Mar 22 20:59:46.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 20:59:46.616
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:46.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:46.635
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 20:59:46.654
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 20:59:47.857
    STEP: Deploying the webhook pod 03/22/23 20:59:47.868
    STEP: Wait for the deployment to be ready 03/22/23 20:59:47.887
    Mar 22 20:59:47.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 20:59:49.914
    STEP: Verifying the service has paired with the endpoint 03/22/23 20:59:49.928
    Mar 22 20:59:50.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Mar 22 20:59:50.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/22/23 20:59:51.45
    STEP: Creating a custom resource that should be denied by the webhook 03/22/23 20:59:51.503
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/22/23 20:59:53.571
    STEP: Updating the custom resource with disallowed data should be denied 03/22/23 20:59:53.584
    STEP: Deleting the custom resource should be denied 03/22/23 20:59:53.617
    STEP: Remove the offending key and value from the custom resource data 03/22/23 20:59:53.64
    STEP: Deleting the updated custom resource should be successful 03/22/23 20:59:53.667
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5529" for this suite. 03/22/23 20:59:54.26
    STEP: Destroying namespace "webhook-5529-markers" for this suite. 03/22/23 20:59:54.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:54.285
Mar 22 20:59:54.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-pred 03/22/23 20:59:54.286
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:54.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:54.31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 22 20:59:54.316: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 20:59:54.339: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 20:59:54.348: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
Mar 22 20:59:54.357: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:59:54.357: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container coredns ready: true, restart count 0
Mar 22 20:59:54.357: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:59:54.357: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:59:54.357: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:59:54.357: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:59:54.357: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:59:54.357: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:59:54.357: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 20:59:54.357: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:59:54.357: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 20:59:54.357: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
Mar 22 20:59:54.366: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.366: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:59:54.366: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.366: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 22 20:59:54.366: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.366: INFO: 	Container coredns ready: true, restart count 0
Mar 22 20:59:54.367: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:59:54.367: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:59:54.367: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:59:54.367: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:59:54.367: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:59:54.367: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:59:54.367: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container e2e ready: true, restart count 0
Mar 22 20:59:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:59:54.367: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:59:54.367: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 20:59:54.367: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
Mar 22 20:59:54.376: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.379: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 20:59:54.380: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 20:59:54.380: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 20:59:54.380: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 20:59:54.380: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 20:59:54.380: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 20:59:54.380: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 20:59:54.380: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 20:59:54.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 20:59:54.380: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 20:59:54.38
Mar 22 20:59:54.390: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7242" to be "running"
Mar 22 20:59:54.397: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.720517ms
Mar 22 20:59:56.404: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013853132s
Mar 22 20:59:56.404: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 20:59:56.408
STEP: Trying to apply a random label on the found node. 03/22/23 20:59:56.422
STEP: verifying the node has the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a 42 03/22/23 20:59:56.433
STEP: Trying to relaunch the pod, now with labels. 03/22/23 20:59:56.438
Mar 22 20:59:56.445: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7242" to be "not pending"
Mar 22 20:59:56.450: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389632ms
Mar 22 20:59:58.456: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010823051s
Mar 22 20:59:58.456: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a off the node pool-v7t41yxh0-q56kk 03/22/23 20:59:58.461
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a 03/22/23 20:59:58.477
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 20:59:58.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7242" for this suite. 03/22/23 20:59:58.489
------------------------------
• [4.216 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:54.285
    Mar 22 20:59:54.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-pred 03/22/23 20:59:54.286
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:54.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:54.31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 22 20:59:54.316: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 22 20:59:54.339: INFO: Waiting for terminating namespaces to be deleted...
    Mar 22 20:59:54.348: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
    Mar 22 20:59:54.357: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 20:59:54.357: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
    Mar 22 20:59:54.366: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.366: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:59:54.366: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.366: INFO: 	Container cilium-operator ready: true, restart count 0
    Mar 22 20:59:54.366: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.366: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container e2e ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 20:59:54.367: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
    Mar 22 20:59:54.376: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.379: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 20:59:54.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 20:59:54.380: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 20:59:54.38
    Mar 22 20:59:54.390: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7242" to be "running"
    Mar 22 20:59:54.397: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.720517ms
    Mar 22 20:59:56.404: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013853132s
    Mar 22 20:59:56.404: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 20:59:56.408
    STEP: Trying to apply a random label on the found node. 03/22/23 20:59:56.422
    STEP: verifying the node has the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a 42 03/22/23 20:59:56.433
    STEP: Trying to relaunch the pod, now with labels. 03/22/23 20:59:56.438
    Mar 22 20:59:56.445: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7242" to be "not pending"
    Mar 22 20:59:56.450: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389632ms
    Mar 22 20:59:58.456: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010823051s
    Mar 22 20:59:58.456: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a off the node pool-v7t41yxh0-q56kk 03/22/23 20:59:58.461
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f8e7aab2-9754-4a7d-aa41-596fb2a3561a 03/22/23 20:59:58.477
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 20:59:58.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7242" for this suite. 03/22/23 20:59:58.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 20:59:58.506
Mar 22 20:59:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context-test 03/22/23 20:59:58.508
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:58.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:58.528
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Mar 22 20:59:58.546: INFO: Waiting up to 5m0s for pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b" in namespace "security-context-test-9155" to be "Succeeded or Failed"
Mar 22 20:59:58.550: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.405668ms
Mar 22 21:00:00.556: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010350227s
Mar 22 21:00:02.558: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Running", Reason="", readiness=false. Elapsed: 4.012355041s
Mar 22 21:00:04.557: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011722859s
Mar 22 21:00:04.558: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 21:00:04.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9155" for this suite. 03/22/23 21:00:04.567
------------------------------
• [SLOW TEST] [6.071 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 20:59:58.506
    Mar 22 20:59:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context-test 03/22/23 20:59:58.508
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 20:59:58.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 20:59:58.528
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Mar 22 20:59:58.546: INFO: Waiting up to 5m0s for pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b" in namespace "security-context-test-9155" to be "Succeeded or Failed"
    Mar 22 20:59:58.550: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.405668ms
    Mar 22 21:00:00.556: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010350227s
    Mar 22 21:00:02.558: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Running", Reason="", readiness=false. Elapsed: 4.012355041s
    Mar 22 21:00:04.557: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011722859s
    Mar 22 21:00:04.558: INFO: Pod "busybox-user-65534-20e56cf7-1346-4be8-98d9-404c26a1a36b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:00:04.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9155" for this suite. 03/22/23 21:00:04.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:00:04.58
Mar 22 21:00:04.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption 03/22/23 21:00:04.582
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:00:04.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:00:04.607
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 22 21:00:04.646: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 21:01:04.733: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:04.738
Mar 22 21:01:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption-path 03/22/23 21:01:04.74
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:04.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:04.767
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 03/22/23 21:01:04.787
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 21:01:04.788
Mar 22 21:01:04.798: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7578" to be "running"
Mar 22 21:01:04.804: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.795591ms
Mar 22 21:01:06.811: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011466256s
Mar 22 21:01:06.811: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 21:01:06.819
Mar 22 21:01:06.833: INFO: found a healthy node: pool-v7t41yxh0-q56kk
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Mar 22 21:01:12.946: INFO: pods created so far: [1 1 1]
Mar 22 21:01:12.946: INFO: length of pods created so far: 3
Mar 22 21:01:14.964: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:21.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7578" for this suite. 03/22/23 21:01:22.077
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9376" for this suite. 03/22/23 21:01:22.086
------------------------------
• [SLOW TEST] [77.514 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:00:04.58
    Mar 22 21:00:04.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption 03/22/23 21:00:04.582
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:00:04.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:00:04.607
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 22 21:00:04.646: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 21:01:04.733: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:04.738
    Mar 22 21:01:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption-path 03/22/23 21:01:04.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:04.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:04.767
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 03/22/23 21:01:04.787
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 21:01:04.788
    Mar 22 21:01:04.798: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7578" to be "running"
    Mar 22 21:01:04.804: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.795591ms
    Mar 22 21:01:06.811: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011466256s
    Mar 22 21:01:06.811: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 21:01:06.819
    Mar 22 21:01:06.833: INFO: found a healthy node: pool-v7t41yxh0-q56kk
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Mar 22 21:01:12.946: INFO: pods created so far: [1 1 1]
    Mar 22 21:01:12.946: INFO: length of pods created so far: 3
    Mar 22 21:01:14.964: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:21.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7578" for this suite. 03/22/23 21:01:22.077
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9376" for this suite. 03/22/23 21:01:22.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:22.098
Mar 22 21:01:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:01:22.1
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:22.127
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 03/22/23 21:01:22.136
Mar 22 21:01:22.137: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-8307 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/22/23 21:01:22.231
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:22.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8307" for this suite. 03/22/23 21:01:22.254
------------------------------
• [0.165 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:22.098
    Mar 22 21:01:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:01:22.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:22.127
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 03/22/23 21:01:22.136
    Mar 22 21:01:22.137: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-8307 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/22/23 21:01:22.231
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:22.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8307" for this suite. 03/22/23 21:01:22.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:22.272
Mar 22 21:01:22.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 21:01:22.273
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:22.304
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 03/22/23 21:01:22.311
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.326
STEP: Creating a service in the namespace 03/22/23 21:01:22.333
STEP: Deleting the namespace 03/22/23 21:01:22.347
STEP: Waiting for the namespace to be removed. 03/22/23 21:01:22.358
STEP: Recreating the namespace 03/22/23 21:01:28.364
STEP: Verifying there is no service in the namespace 03/22/23 21:01:28.381
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:28.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7711" for this suite. 03/22/23 21:01:28.392
STEP: Destroying namespace "nsdeletetest-9260" for this suite. 03/22/23 21:01:28.399
Mar 22 21:01:28.402: INFO: Namespace nsdeletetest-9260 was already deleted
STEP: Destroying namespace "nsdeletetest-8816" for this suite. 03/22/23 21:01:28.402
------------------------------
• [SLOW TEST] [6.138 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:22.272
    Mar 22 21:01:22.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 21:01:22.273
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:22.304
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 03/22/23 21:01:22.311
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:22.326
    STEP: Creating a service in the namespace 03/22/23 21:01:22.333
    STEP: Deleting the namespace 03/22/23 21:01:22.347
    STEP: Waiting for the namespace to be removed. 03/22/23 21:01:22.358
    STEP: Recreating the namespace 03/22/23 21:01:28.364
    STEP: Verifying there is no service in the namespace 03/22/23 21:01:28.381
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:28.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7711" for this suite. 03/22/23 21:01:28.392
    STEP: Destroying namespace "nsdeletetest-9260" for this suite. 03/22/23 21:01:28.399
    Mar 22 21:01:28.402: INFO: Namespace nsdeletetest-9260 was already deleted
    STEP: Destroying namespace "nsdeletetest-8816" for this suite. 03/22/23 21:01:28.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:28.41
Mar 22 21:01:28.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:01:28.412
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:28.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:28.443
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/22/23 21:01:28.452
Mar 22 21:01:28.461: INFO: Waiting up to 5m0s for pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62" in namespace "emptydir-3385" to be "Succeeded or Failed"
Mar 22 21:01:28.466: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698923ms
Mar 22 21:01:30.472: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841296s
Mar 22 21:01:32.473: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011781103s
STEP: Saw pod success 03/22/23 21:01:32.473
Mar 22 21:01:32.474: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62" satisfied condition "Succeeded or Failed"
Mar 22 21:01:32.478: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 container test-container: <nil>
STEP: delete the pod 03/22/23 21:01:32.52
Mar 22 21:01:32.539: INFO: Waiting for pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 to disappear
Mar 22 21:01:32.544: INFO: Pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:32.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3385" for this suite. 03/22/23 21:01:32.553
------------------------------
• [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:28.41
    Mar 22 21:01:28.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:01:28.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:28.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:28.443
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/22/23 21:01:28.452
    Mar 22 21:01:28.461: INFO: Waiting up to 5m0s for pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62" in namespace "emptydir-3385" to be "Succeeded or Failed"
    Mar 22 21:01:28.466: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698923ms
    Mar 22 21:01:30.472: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010841296s
    Mar 22 21:01:32.473: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011781103s
    STEP: Saw pod success 03/22/23 21:01:32.473
    Mar 22 21:01:32.474: INFO: Pod "pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62" satisfied condition "Succeeded or Failed"
    Mar 22 21:01:32.478: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 container test-container: <nil>
    STEP: delete the pod 03/22/23 21:01:32.52
    Mar 22 21:01:32.539: INFO: Waiting for pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 to disappear
    Mar 22 21:01:32.544: INFO: Pod pod-bdb21c9e-08e2-4a57-9e97-c49b485cba62 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:32.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3385" for this suite. 03/22/23 21:01:32.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:32.564
Mar 22 21:01:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:01:32.565
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:32.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:32.6
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Mar 22 21:01:32.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/22/23 21:01:34.196
Mar 22 21:01:34.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
Mar 22 21:01:34.906: INFO: stderr: ""
Mar 22 21:01:34.906: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 22 21:01:34.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 delete e2e-test-crd-publish-openapi-5381-crds test-foo'
Mar 22 21:01:35.054: INFO: stderr: ""
Mar 22 21:01:35.054: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 22 21:01:35.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
Mar 22 21:01:35.390: INFO: stderr: ""
Mar 22 21:01:35.390: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 22 21:01:35.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 delete e2e-test-crd-publish-openapi-5381-crds test-foo'
Mar 22 21:01:35.517: INFO: stderr: ""
Mar 22 21:01:35.518: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/22/23 21:01:35.518
Mar 22 21:01:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
Mar 22 21:01:35.821: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/22/23 21:01:35.821
Mar 22 21:01:35.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
Mar 22 21:01:36.158: INFO: rc: 1
Mar 22 21:01:36.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
Mar 22 21:01:36.460: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/22/23 21:01:36.46
Mar 22 21:01:36.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
Mar 22 21:01:36.781: INFO: rc: 1
Mar 22 21:01:36.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
Mar 22 21:01:37.115: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/22/23 21:01:37.115
Mar 22 21:01:37.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds'
Mar 22 21:01:37.405: INFO: stderr: ""
Mar 22 21:01:37.405: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/22/23 21:01:37.406
Mar 22 21:01:37.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.metadata'
Mar 22 21:01:37.744: INFO: stderr: ""
Mar 22 21:01:37.744: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 22 21:01:37.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec'
Mar 22 21:01:38.110: INFO: stderr: ""
Mar 22 21:01:38.110: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 22 21:01:38.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec.bars'
Mar 22 21:01:38.479: INFO: stderr: ""
Mar 22 21:01:38.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/22/23 21:01:38.479
Mar 22 21:01:38.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec.bars2'
Mar 22 21:01:38.810: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:40.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5580" for this suite. 03/22/23 21:01:40.345
------------------------------
• [SLOW TEST] [7.790 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:32.564
    Mar 22 21:01:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:01:32.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:32.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:32.6
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Mar 22 21:01:32.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/22/23 21:01:34.196
    Mar 22 21:01:34.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
    Mar 22 21:01:34.906: INFO: stderr: ""
    Mar 22 21:01:34.906: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 22 21:01:34.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 delete e2e-test-crd-publish-openapi-5381-crds test-foo'
    Mar 22 21:01:35.054: INFO: stderr: ""
    Mar 22 21:01:35.054: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 22 21:01:35.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
    Mar 22 21:01:35.390: INFO: stderr: ""
    Mar 22 21:01:35.390: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 22 21:01:35.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 delete e2e-test-crd-publish-openapi-5381-crds test-foo'
    Mar 22 21:01:35.517: INFO: stderr: ""
    Mar 22 21:01:35.518: INFO: stdout: "e2e-test-crd-publish-openapi-5381-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/22/23 21:01:35.518
    Mar 22 21:01:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
    Mar 22 21:01:35.821: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/22/23 21:01:35.821
    Mar 22 21:01:35.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
    Mar 22 21:01:36.158: INFO: rc: 1
    Mar 22 21:01:36.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
    Mar 22 21:01:36.460: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/22/23 21:01:36.46
    Mar 22 21:01:36.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 create -f -'
    Mar 22 21:01:36.781: INFO: rc: 1
    Mar 22 21:01:36.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 --namespace=crd-publish-openapi-5580 apply -f -'
    Mar 22 21:01:37.115: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/22/23 21:01:37.115
    Mar 22 21:01:37.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds'
    Mar 22 21:01:37.405: INFO: stderr: ""
    Mar 22 21:01:37.405: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/22/23 21:01:37.406
    Mar 22 21:01:37.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.metadata'
    Mar 22 21:01:37.744: INFO: stderr: ""
    Mar 22 21:01:37.744: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 22 21:01:37.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec'
    Mar 22 21:01:38.110: INFO: stderr: ""
    Mar 22 21:01:38.110: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 22 21:01:38.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec.bars'
    Mar 22 21:01:38.479: INFO: stderr: ""
    Mar 22 21:01:38.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5381-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/22/23 21:01:38.479
    Mar 22 21:01:38.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-5580 explain e2e-test-crd-publish-openapi-5381-crds.spec.bars2'
    Mar 22 21:01:38.810: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:40.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5580" for this suite. 03/22/23 21:01:40.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:40.354
Mar 22 21:01:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:01:40.356
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:40.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:40.377
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:01:40.383
Mar 22 21:01:40.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa" in namespace "projected-9809" to be "Succeeded or Failed"
Mar 22 21:01:40.398: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.129269ms
Mar 22 21:01:42.404: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01127915s
Mar 22 21:01:44.404: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011252793s
STEP: Saw pod success 03/22/23 21:01:44.405
Mar 22 21:01:44.405: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa" satisfied condition "Succeeded or Failed"
Mar 22 21:01:44.409: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa container client-container: <nil>
STEP: delete the pod 03/22/23 21:01:44.422
Mar 22 21:01:44.444: INFO: Waiting for pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa to disappear
Mar 22 21:01:44.448: INFO: Pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:44.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9809" for this suite. 03/22/23 21:01:44.455
------------------------------
• [4.111 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:40.354
    Mar 22 21:01:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:01:40.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:40.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:40.377
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:01:40.383
    Mar 22 21:01:40.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa" in namespace "projected-9809" to be "Succeeded or Failed"
    Mar 22 21:01:40.398: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.129269ms
    Mar 22 21:01:42.404: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01127915s
    Mar 22 21:01:44.404: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011252793s
    STEP: Saw pod success 03/22/23 21:01:44.405
    Mar 22 21:01:44.405: INFO: Pod "downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa" satisfied condition "Succeeded or Failed"
    Mar 22 21:01:44.409: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa container client-container: <nil>
    STEP: delete the pod 03/22/23 21:01:44.422
    Mar 22 21:01:44.444: INFO: Waiting for pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa to disappear
    Mar 22 21:01:44.448: INFO: Pod downwardapi-volume-20a0dac5-9f01-4bfb-811b-43ca2e149faa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:44.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9809" for this suite. 03/22/23 21:01:44.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:44.468
Mar 22 21:01:44.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 21:01:44.471
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:44.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:44.491
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3373 03/22/23 21:01:44.497
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/22/23 21:01:44.511
STEP: creating service externalsvc in namespace services-3373 03/22/23 21:01:44.511
STEP: creating replication controller externalsvc in namespace services-3373 03/22/23 21:01:44.524
I0322 21:01:44.532516      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3373, replica count: 2
I0322 21:01:47.585156      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/22/23 21:01:47.589
Mar 22 21:01:47.610: INFO: Creating new exec pod
Mar 22 21:01:47.618: INFO: Waiting up to 5m0s for pod "execpod5wlsz" in namespace "services-3373" to be "running"
Mar 22 21:01:47.621: INFO: Pod "execpod5wlsz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494807ms
Mar 22 21:01:49.636: INFO: Pod "execpod5wlsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.018367192s
Mar 22 21:01:49.636: INFO: Pod "execpod5wlsz" satisfied condition "running"
Mar 22 21:01:49.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-3373 exec execpod5wlsz -- /bin/sh -x -c nslookup clusterip-service.services-3373.svc.cluster.local'
Mar 22 21:01:49.985: INFO: stderr: "+ nslookup clusterip-service.services-3373.svc.cluster.local\n"
Mar 22 21:01:49.985: INFO: stdout: "Server:\t\t10.245.0.10\nAddress:\t10.245.0.10#53\n\nclusterip-service.services-3373.svc.cluster.local\tcanonical name = externalsvc.services-3373.svc.cluster.local.\nName:\texternalsvc.services-3373.svc.cluster.local\nAddress: 10.245.5.138\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3373, will wait for the garbage collector to delete the pods 03/22/23 21:01:49.985
Mar 22 21:01:50.049: INFO: Deleting ReplicationController externalsvc took: 8.444341ms
Mar 22 21:01:50.149: INFO: Terminating ReplicationController externalsvc pods took: 100.202866ms
Mar 22 21:01:52.368: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 21:01:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3373" for this suite. 03/22/23 21:01:52.386
------------------------------
• [SLOW TEST] [7.925 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:44.468
    Mar 22 21:01:44.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 21:01:44.471
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:44.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:44.491
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3373 03/22/23 21:01:44.497
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/22/23 21:01:44.511
    STEP: creating service externalsvc in namespace services-3373 03/22/23 21:01:44.511
    STEP: creating replication controller externalsvc in namespace services-3373 03/22/23 21:01:44.524
    I0322 21:01:44.532516      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3373, replica count: 2
    I0322 21:01:47.585156      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/22/23 21:01:47.589
    Mar 22 21:01:47.610: INFO: Creating new exec pod
    Mar 22 21:01:47.618: INFO: Waiting up to 5m0s for pod "execpod5wlsz" in namespace "services-3373" to be "running"
    Mar 22 21:01:47.621: INFO: Pod "execpod5wlsz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494807ms
    Mar 22 21:01:49.636: INFO: Pod "execpod5wlsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.018367192s
    Mar 22 21:01:49.636: INFO: Pod "execpod5wlsz" satisfied condition "running"
    Mar 22 21:01:49.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-3373 exec execpod5wlsz -- /bin/sh -x -c nslookup clusterip-service.services-3373.svc.cluster.local'
    Mar 22 21:01:49.985: INFO: stderr: "+ nslookup clusterip-service.services-3373.svc.cluster.local\n"
    Mar 22 21:01:49.985: INFO: stdout: "Server:\t\t10.245.0.10\nAddress:\t10.245.0.10#53\n\nclusterip-service.services-3373.svc.cluster.local\tcanonical name = externalsvc.services-3373.svc.cluster.local.\nName:\texternalsvc.services-3373.svc.cluster.local\nAddress: 10.245.5.138\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3373, will wait for the garbage collector to delete the pods 03/22/23 21:01:49.985
    Mar 22 21:01:50.049: INFO: Deleting ReplicationController externalsvc took: 8.444341ms
    Mar 22 21:01:50.149: INFO: Terminating ReplicationController externalsvc pods took: 100.202866ms
    Mar 22 21:01:52.368: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:01:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3373" for this suite. 03/22/23 21:01:52.386
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:01:52.395
Mar 22 21:01:52.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 21:01:52.397
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:52.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:52.425
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd in namespace container-probe-3063 03/22/23 21:01:52.433
Mar 22 21:01:52.443: INFO: Waiting up to 5m0s for pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd" in namespace "container-probe-3063" to be "not pending"
Mar 22 21:01:52.450: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756856ms
Mar 22 21:01:54.457: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013565458s
Mar 22 21:01:54.457: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd" satisfied condition "not pending"
Mar 22 21:01:54.457: INFO: Started pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd in namespace container-probe-3063
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:01:54.457
Mar 22 21:01:54.461: INFO: Initial restart count of pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd is 0
STEP: deleting the pod 03/22/23 21:05:55.255
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 21:05:55.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3063" for this suite. 03/22/23 21:05:55.286
------------------------------
• [SLOW TEST] [242.898 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:01:52.395
    Mar 22 21:01:52.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 21:01:52.397
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:01:52.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:01:52.425
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd in namespace container-probe-3063 03/22/23 21:01:52.433
    Mar 22 21:01:52.443: INFO: Waiting up to 5m0s for pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd" in namespace "container-probe-3063" to be "not pending"
    Mar 22 21:01:52.450: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756856ms
    Mar 22 21:01:54.457: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013565458s
    Mar 22 21:01:54.457: INFO: Pod "busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd" satisfied condition "not pending"
    Mar 22 21:01:54.457: INFO: Started pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd in namespace container-probe-3063
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:01:54.457
    Mar 22 21:01:54.461: INFO: Initial restart count of pod busybox-97a0fec4-94ce-48b9-b22c-7fced69e3fbd is 0
    STEP: deleting the pod 03/22/23 21:05:55.255
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:05:55.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3063" for this suite. 03/22/23 21:05:55.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:05:55.294
Mar 22 21:05:55.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replicaset 03/22/23 21:05:55.296
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:05:55.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:05:55.317
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 22 21:05:55.324: INFO: Creating ReplicaSet my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8
Mar 22 21:05:55.335: INFO: Pod name my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Found 0 pods out of 1
Mar 22 21:06:00.341: INFO: Pod name my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Found 1 pods out of 1
Mar 22 21:06:00.341: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8" is running
Mar 22 21:06:00.341: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" in namespace "replicaset-6310" to be "running"
Mar 22 21:06:00.345: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld": Phase="Running", Reason="", readiness=true. Elapsed: 4.304154ms
Mar 22 21:06:00.346: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" satisfied condition "running"
Mar 22 21:06:00.346: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:55 +0000 UTC Reason: Message:}])
Mar 22 21:06:00.346: INFO: Trying to dial the pod
Mar 22 21:06:05.393: INFO: Controller my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Got expected result from replica 1 [my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld]: "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:06:05.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6310" for this suite. 03/22/23 21:06:05.399
------------------------------
• [SLOW TEST] [10.115 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:05:55.294
    Mar 22 21:05:55.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replicaset 03/22/23 21:05:55.296
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:05:55.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:05:55.317
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 22 21:05:55.324: INFO: Creating ReplicaSet my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8
    Mar 22 21:05:55.335: INFO: Pod name my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Found 0 pods out of 1
    Mar 22 21:06:00.341: INFO: Pod name my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Found 1 pods out of 1
    Mar 22 21:06:00.341: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8" is running
    Mar 22 21:06:00.341: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" in namespace "replicaset-6310" to be "running"
    Mar 22 21:06:00.345: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld": Phase="Running", Reason="", readiness=true. Elapsed: 4.304154ms
    Mar 22 21:06:00.346: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" satisfied condition "running"
    Mar 22 21:06:00.346: INFO: Pod "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-22 21:05:55 +0000 UTC Reason: Message:}])
    Mar 22 21:06:00.346: INFO: Trying to dial the pod
    Mar 22 21:06:05.393: INFO: Controller my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8: Got expected result from replica 1 [my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld]: "my-hostname-basic-4c9c01bd-8922-4875-b1ca-067c337eb0d8-n6gld", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:06:05.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6310" for this suite. 03/22/23 21:06:05.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:06:05.419
Mar 22 21:06:05.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-preemption 03/22/23 21:06:05.421
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:06:05.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:06:05.463
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 22 21:06:05.490: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 22 21:07:05.542: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 03/22/23 21:07:05.545
Mar 22 21:07:05.571: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 22 21:07:05.585: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 22 21:07:05.605: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 22 21:07:05.617: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 22 21:07:05.650: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 22 21:07:05.655: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/22/23 21:07:05.655
Mar 22 21:07:05.656: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:05.660: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86619ms
Mar 22 21:07:07.671: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015067787s
Mar 22 21:07:07.671: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 22 21:07:07.671: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:07.675: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.177611ms
Mar 22 21:07:07.675: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 21:07:07.676: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:07.681: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.724801ms
Mar 22 21:07:07.681: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 21:07:07.682: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:07.686: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.772607ms
Mar 22 21:07:07.687: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 21:07:07.687: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:07.691: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.008384ms
Mar 22 21:07:07.691: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 22 21:07:07.692: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
Mar 22 21:07:07.697: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.62377ms
Mar 22 21:07:07.697: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/22/23 21:07:07.697
Mar 22 21:07:07.707: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 22 21:07:07.711: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635779ms
Mar 22 21:07:09.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009741431s
Mar 22 21:07:11.716: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009175879s
Mar 22 21:07:13.716: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009067579s
Mar 22 21:07:13.716: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:13.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-675" for this suite. 03/22/23 21:07:13.841
------------------------------
• [SLOW TEST] [68.432 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:06:05.419
    Mar 22 21:06:05.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-preemption 03/22/23 21:06:05.421
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:06:05.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:06:05.463
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 22 21:06:05.490: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 22 21:07:05.542: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 03/22/23 21:07:05.545
    Mar 22 21:07:05.571: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 22 21:07:05.585: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 22 21:07:05.605: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 22 21:07:05.617: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 22 21:07:05.650: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 22 21:07:05.655: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/22/23 21:07:05.655
    Mar 22 21:07:05.656: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:05.660: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86619ms
    Mar 22 21:07:07.671: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015067787s
    Mar 22 21:07:07.671: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 22 21:07:07.671: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:07.675: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.177611ms
    Mar 22 21:07:07.675: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 21:07:07.676: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:07.681: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.724801ms
    Mar 22 21:07:07.681: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 21:07:07.682: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:07.686: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.772607ms
    Mar 22 21:07:07.687: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 21:07:07.687: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:07.691: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.008384ms
    Mar 22 21:07:07.691: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 22 21:07:07.692: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-675" to be "running"
    Mar 22 21:07:07.697: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.62377ms
    Mar 22 21:07:07.697: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/22/23 21:07:07.697
    Mar 22 21:07:07.707: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 22 21:07:07.711: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635779ms
    Mar 22 21:07:09.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009741431s
    Mar 22 21:07:11.716: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009175879s
    Mar 22 21:07:13.716: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009067579s
    Mar 22 21:07:13.716: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:13.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-675" for this suite. 03/22/23 21:07:13.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:13.853
Mar 22 21:07:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 21:07:13.854
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:13.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:13.877
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/22/23 21:07:13.885
STEP: delete the rc 03/22/23 21:07:18.902
STEP: wait for all pods to be garbage collected 03/22/23 21:07:18.908
STEP: Gathering metrics 03/22/23 21:07:23.919
W0322 21:07:23.941187      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 21:07:23.941: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:23.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7114" for this suite. 03/22/23 21:07:23.947
------------------------------
• [SLOW TEST] [10.103 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:13.853
    Mar 22 21:07:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 21:07:13.854
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:13.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:13.877
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/22/23 21:07:13.885
    STEP: delete the rc 03/22/23 21:07:18.902
    STEP: wait for all pods to be garbage collected 03/22/23 21:07:18.908
    STEP: Gathering metrics 03/22/23 21:07:23.919
    W0322 21:07:23.941187      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 21:07:23.941: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:23.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7114" for this suite. 03/22/23 21:07:23.947
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:23.957
Mar 22 21:07:23.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 21:07:23.96
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:23.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:23.987
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/22/23 21:07:24.004
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +notcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_tcp@PTR;sleep 1; done
 03/22/23 21:07:24.025
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +notcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_tcp@PTR;sleep 1; done
 03/22/23 21:07:24.025
STEP: creating a pod to probe DNS 03/22/23 21:07:24.025
STEP: submitting the pod to kubernetes 03/22/23 21:07:24.025
Mar 22 21:07:24.035: INFO: Waiting up to 15m0s for pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5" in namespace "dns-8935" to be "running"
Mar 22 21:07:24.043: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.442657ms
Mar 22 21:07:26.050: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705208s
Mar 22 21:07:26.050: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5" satisfied condition "running"
STEP: retrieving the pod 03/22/23 21:07:26.05
STEP: looking for the results for each expected name from probers 03/22/23 21:07:26.056
Mar 22 21:07:26.092: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.115: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.123: INFO: Unable to read wheezy_udp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.138: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.180: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.198: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.207: INFO: Unable to read jessie_udp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.214: INFO: Unable to read jessie_tcp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.221: INFO: Unable to read jessie_udp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.228: INFO: Unable to read jessie_tcp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.235: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.246: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
Mar 22 21:07:26.279: INFO: Lookups using dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8935 wheezy_tcp@dns-test-service.dns-8935 wheezy_udp@dns-test-service.dns-8935.svc wheezy_tcp@dns-test-service.dns-8935.svc wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8935 jessie_tcp@dns-test-service.dns-8935 jessie_udp@dns-test-service.dns-8935.svc jessie_tcp@dns-test-service.dns-8935.svc jessie_udp@_http._tcp.dns-test-service.dns-8935.svc jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc]

Mar 22 21:07:31.508: INFO: DNS probes using dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5 succeeded

STEP: deleting the pod 03/22/23 21:07:31.508
STEP: deleting the test service 03/22/23 21:07:31.524
STEP: deleting the test headless service 03/22/23 21:07:31.548
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:31.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8935" for this suite. 03/22/23 21:07:31.569
------------------------------
• [SLOW TEST] [7.620 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:23.957
    Mar 22 21:07:23.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 21:07:23.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:23.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:23.987
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/22/23 21:07:24.004
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +notcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_tcp@PTR;sleep 1; done
     03/22/23 21:07:24.025
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8935.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8935.svc;check="$$(dig +notcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.178.245.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.245.178.138_tcp@PTR;sleep 1; done
     03/22/23 21:07:24.025
    STEP: creating a pod to probe DNS 03/22/23 21:07:24.025
    STEP: submitting the pod to kubernetes 03/22/23 21:07:24.025
    Mar 22 21:07:24.035: INFO: Waiting up to 15m0s for pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5" in namespace "dns-8935" to be "running"
    Mar 22 21:07:24.043: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.442657ms
    Mar 22 21:07:26.050: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013705208s
    Mar 22 21:07:26.050: INFO: Pod "dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5" satisfied condition "running"
    STEP: retrieving the pod 03/22/23 21:07:26.05
    STEP: looking for the results for each expected name from probers 03/22/23 21:07:26.056
    Mar 22 21:07:26.092: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.115: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.123: INFO: Unable to read wheezy_udp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.138: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.180: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.198: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.207: INFO: Unable to read jessie_udp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.214: INFO: Unable to read jessie_tcp@dns-test-service.dns-8935 from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.221: INFO: Unable to read jessie_udp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.228: INFO: Unable to read jessie_tcp@dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.235: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.246: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc from pod dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5: the server could not find the requested resource (get pods dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5)
    Mar 22 21:07:26.279: INFO: Lookups using dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8935 wheezy_tcp@dns-test-service.dns-8935 wheezy_udp@dns-test-service.dns-8935.svc wheezy_tcp@dns-test-service.dns-8935.svc wheezy_udp@_http._tcp.dns-test-service.dns-8935.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8935 jessie_tcp@dns-test-service.dns-8935 jessie_udp@dns-test-service.dns-8935.svc jessie_tcp@dns-test-service.dns-8935.svc jessie_udp@_http._tcp.dns-test-service.dns-8935.svc jessie_tcp@_http._tcp.dns-test-service.dns-8935.svc]

    Mar 22 21:07:31.508: INFO: DNS probes using dns-8935/dns-test-59795bbc-7853-4e19-9ecb-9419a81a39a5 succeeded

    STEP: deleting the pod 03/22/23 21:07:31.508
    STEP: deleting the test service 03/22/23 21:07:31.524
    STEP: deleting the test headless service 03/22/23 21:07:31.548
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:31.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8935" for this suite. 03/22/23 21:07:31.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:31.58
Mar 22 21:07:31.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:07:31.581
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:31.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:31.606
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/22/23 21:07:31.614
Mar 22 21:07:31.645: INFO: Waiting up to 5m0s for pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d" in namespace "emptydir-9907" to be "Succeeded or Failed"
Mar 22 21:07:31.652: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678056ms
Mar 22 21:07:33.659: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01336645s
Mar 22 21:07:35.658: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012450941s
STEP: Saw pod success 03/22/23 21:07:35.658
Mar 22 21:07:35.658: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d" satisfied condition "Succeeded or Failed"
Mar 22 21:07:35.662: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d container test-container: <nil>
STEP: delete the pod 03/22/23 21:07:35.702
Mar 22 21:07:35.714: INFO: Waiting for pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d to disappear
Mar 22 21:07:35.721: INFO: Pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:35.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9907" for this suite. 03/22/23 21:07:35.733
------------------------------
• [4.161 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:31.58
    Mar 22 21:07:31.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:07:31.581
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:31.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:31.606
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/22/23 21:07:31.614
    Mar 22 21:07:31.645: INFO: Waiting up to 5m0s for pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d" in namespace "emptydir-9907" to be "Succeeded or Failed"
    Mar 22 21:07:31.652: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678056ms
    Mar 22 21:07:33.659: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01336645s
    Mar 22 21:07:35.658: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012450941s
    STEP: Saw pod success 03/22/23 21:07:35.658
    Mar 22 21:07:35.658: INFO: Pod "pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d" satisfied condition "Succeeded or Failed"
    Mar 22 21:07:35.662: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d container test-container: <nil>
    STEP: delete the pod 03/22/23 21:07:35.702
    Mar 22 21:07:35.714: INFO: Waiting for pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d to disappear
    Mar 22 21:07:35.721: INFO: Pod pod-0cf81d7e-c0f4-4cfe-92ab-ba1c709a179d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:35.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9907" for this suite. 03/22/23 21:07:35.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:35.743
Mar 22 21:07:35.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 21:07:35.744
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:35.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:35.784
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 03/22/23 21:07:35.792
STEP: Creating a ResourceQuota 03/22/23 21:07:40.801
STEP: Ensuring resource quota status is calculated 03/22/23 21:07:40.807
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:42.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8835" for this suite. 03/22/23 21:07:42.819
------------------------------
• [SLOW TEST] [7.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:35.743
    Mar 22 21:07:35.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 21:07:35.744
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:35.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:35.784
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 03/22/23 21:07:35.792
    STEP: Creating a ResourceQuota 03/22/23 21:07:40.801
    STEP: Ensuring resource quota status is calculated 03/22/23 21:07:40.807
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:42.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8835" for this suite. 03/22/23 21:07:42.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:42.827
Mar 22 21:07:42.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename discovery 03/22/23 21:07:42.829
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:42.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:42.852
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/22/23 21:07:42.863
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 22 21:07:43.599: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 22 21:07:43.604: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 22 21:07:43.605: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 22 21:07:43.605: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 22 21:07:43.605: INFO: Checking APIGroup: apps
Mar 22 21:07:43.608: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 22 21:07:43.608: INFO: Versions found [{apps/v1 v1}]
Mar 22 21:07:43.608: INFO: apps/v1 matches apps/v1
Mar 22 21:07:43.608: INFO: Checking APIGroup: events.k8s.io
Mar 22 21:07:43.611: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 22 21:07:43.611: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 22 21:07:43.611: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 22 21:07:43.611: INFO: Checking APIGroup: authentication.k8s.io
Mar 22 21:07:43.614: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 22 21:07:43.614: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 22 21:07:43.614: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 22 21:07:43.614: INFO: Checking APIGroup: authorization.k8s.io
Mar 22 21:07:43.618: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 22 21:07:43.623: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 22 21:07:43.623: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 22 21:07:43.624: INFO: Checking APIGroup: autoscaling
Mar 22 21:07:43.627: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 22 21:07:43.627: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Mar 22 21:07:43.627: INFO: autoscaling/v2 matches autoscaling/v2
Mar 22 21:07:43.627: INFO: Checking APIGroup: batch
Mar 22 21:07:43.634: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 22 21:07:43.634: INFO: Versions found [{batch/v1 v1}]
Mar 22 21:07:43.634: INFO: batch/v1 matches batch/v1
Mar 22 21:07:43.634: INFO: Checking APIGroup: certificates.k8s.io
Mar 22 21:07:43.637: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 22 21:07:43.637: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 22 21:07:43.637: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 22 21:07:43.637: INFO: Checking APIGroup: networking.k8s.io
Mar 22 21:07:43.640: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 22 21:07:43.640: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 22 21:07:43.640: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 22 21:07:43.640: INFO: Checking APIGroup: policy
Mar 22 21:07:43.649: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 22 21:07:43.649: INFO: Versions found [{policy/v1 v1}]
Mar 22 21:07:43.649: INFO: policy/v1 matches policy/v1
Mar 22 21:07:43.649: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 22 21:07:43.651: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 22 21:07:43.652: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 22 21:07:43.652: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 22 21:07:43.652: INFO: Checking APIGroup: storage.k8s.io
Mar 22 21:07:43.663: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 22 21:07:43.663: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 22 21:07:43.663: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 22 21:07:43.663: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 22 21:07:43.667: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 22 21:07:43.667: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 22 21:07:43.667: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 22 21:07:43.667: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 22 21:07:43.671: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 22 21:07:43.671: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 22 21:07:43.671: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 22 21:07:43.671: INFO: Checking APIGroup: scheduling.k8s.io
Mar 22 21:07:43.675: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 22 21:07:43.675: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 22 21:07:43.675: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 22 21:07:43.675: INFO: Checking APIGroup: coordination.k8s.io
Mar 22 21:07:43.681: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 22 21:07:43.681: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 22 21:07:43.682: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 22 21:07:43.682: INFO: Checking APIGroup: node.k8s.io
Mar 22 21:07:43.685: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 22 21:07:43.685: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 22 21:07:43.685: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 22 21:07:43.685: INFO: Checking APIGroup: discovery.k8s.io
Mar 22 21:07:43.691: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 22 21:07:43.691: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 22 21:07:43.691: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 22 21:07:43.691: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 22 21:07:43.694: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Mar 22 21:07:43.694: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Mar 22 21:07:43.694: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Mar 22 21:07:43.694: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 22 21:07:43.704: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar 22 21:07:43.705: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Mar 22 21:07:43.705: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar 22 21:07:43.705: INFO: Checking APIGroup: cilium.io
Mar 22 21:07:43.710: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Mar 22 21:07:43.710: INFO: Versions found [{cilium.io/v2 v2}]
Mar 22 21:07:43.710: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:43.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3681" for this suite. 03/22/23 21:07:43.721
------------------------------
• [0.902 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:42.827
    Mar 22 21:07:42.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename discovery 03/22/23 21:07:42.829
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:42.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:42.852
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/22/23 21:07:42.863
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 22 21:07:43.599: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 22 21:07:43.604: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 22 21:07:43.605: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 22 21:07:43.605: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 22 21:07:43.605: INFO: Checking APIGroup: apps
    Mar 22 21:07:43.608: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 22 21:07:43.608: INFO: Versions found [{apps/v1 v1}]
    Mar 22 21:07:43.608: INFO: apps/v1 matches apps/v1
    Mar 22 21:07:43.608: INFO: Checking APIGroup: events.k8s.io
    Mar 22 21:07:43.611: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 22 21:07:43.611: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 22 21:07:43.611: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 22 21:07:43.611: INFO: Checking APIGroup: authentication.k8s.io
    Mar 22 21:07:43.614: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 22 21:07:43.614: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 22 21:07:43.614: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 22 21:07:43.614: INFO: Checking APIGroup: authorization.k8s.io
    Mar 22 21:07:43.618: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 22 21:07:43.623: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 22 21:07:43.623: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 22 21:07:43.624: INFO: Checking APIGroup: autoscaling
    Mar 22 21:07:43.627: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 22 21:07:43.627: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Mar 22 21:07:43.627: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 22 21:07:43.627: INFO: Checking APIGroup: batch
    Mar 22 21:07:43.634: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 22 21:07:43.634: INFO: Versions found [{batch/v1 v1}]
    Mar 22 21:07:43.634: INFO: batch/v1 matches batch/v1
    Mar 22 21:07:43.634: INFO: Checking APIGroup: certificates.k8s.io
    Mar 22 21:07:43.637: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 22 21:07:43.637: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 22 21:07:43.637: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 22 21:07:43.637: INFO: Checking APIGroup: networking.k8s.io
    Mar 22 21:07:43.640: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 22 21:07:43.640: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 22 21:07:43.640: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 22 21:07:43.640: INFO: Checking APIGroup: policy
    Mar 22 21:07:43.649: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 22 21:07:43.649: INFO: Versions found [{policy/v1 v1}]
    Mar 22 21:07:43.649: INFO: policy/v1 matches policy/v1
    Mar 22 21:07:43.649: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 22 21:07:43.651: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 22 21:07:43.652: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 22 21:07:43.652: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 22 21:07:43.652: INFO: Checking APIGroup: storage.k8s.io
    Mar 22 21:07:43.663: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 22 21:07:43.663: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 22 21:07:43.663: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 22 21:07:43.663: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 22 21:07:43.667: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 22 21:07:43.667: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 22 21:07:43.667: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 22 21:07:43.667: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 22 21:07:43.671: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 22 21:07:43.671: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 22 21:07:43.671: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 22 21:07:43.671: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 22 21:07:43.675: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 22 21:07:43.675: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 22 21:07:43.675: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 22 21:07:43.675: INFO: Checking APIGroup: coordination.k8s.io
    Mar 22 21:07:43.681: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 22 21:07:43.681: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 22 21:07:43.682: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 22 21:07:43.682: INFO: Checking APIGroup: node.k8s.io
    Mar 22 21:07:43.685: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 22 21:07:43.685: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 22 21:07:43.685: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 22 21:07:43.685: INFO: Checking APIGroup: discovery.k8s.io
    Mar 22 21:07:43.691: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 22 21:07:43.691: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 22 21:07:43.691: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 22 21:07:43.691: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 22 21:07:43.694: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Mar 22 21:07:43.694: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Mar 22 21:07:43.694: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Mar 22 21:07:43.694: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar 22 21:07:43.704: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar 22 21:07:43.705: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Mar 22 21:07:43.705: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar 22 21:07:43.705: INFO: Checking APIGroup: cilium.io
    Mar 22 21:07:43.710: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Mar 22 21:07:43.710: INFO: Versions found [{cilium.io/v2 v2}]
    Mar 22 21:07:43.710: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:43.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3681" for this suite. 03/22/23 21:07:43.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:43.739
Mar 22 21:07:43.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename runtimeclass 03/22/23 21:07:43.741
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:43.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:43.761
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 22 21:07:43.784: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5851 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:43.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5851" for this suite. 03/22/23 21:07:43.802
------------------------------
• [0.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:43.739
    Mar 22 21:07:43.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename runtimeclass 03/22/23 21:07:43.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:43.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:43.761
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 22 21:07:43.784: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5851 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:43.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5851" for this suite. 03/22/23 21:07:43.802
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:43.809
Mar 22 21:07:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:07:43.81
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:43.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:43.835
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-bc4c634b-3f0d-4f74-86fa-42c6570a3e75 03/22/23 21:07:43.847
STEP: Creating configMap with name cm-test-opt-upd-cd50d733-7f30-4edc-9dd2-f4f76a1770c1 03/22/23 21:07:43.855
STEP: Creating the pod 03/22/23 21:07:43.861
Mar 22 21:07:43.871: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6" in namespace "projected-8850" to be "running and ready"
Mar 22 21:07:43.883: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.719216ms
Mar 22 21:07:43.883: INFO: The phase of Pod pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:07:45.889: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017462452s
Mar 22 21:07:45.889: INFO: The phase of Pod pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6 is Running (Ready = true)
Mar 22 21:07:45.889: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-bc4c634b-3f0d-4f74-86fa-42c6570a3e75 03/22/23 21:07:45.927
STEP: Updating configmap cm-test-opt-upd-cd50d733-7f30-4edc-9dd2-f4f76a1770c1 03/22/23 21:07:45.934
STEP: Creating configMap with name cm-test-opt-create-ea1c0767-ccc4-4d48-9ff1-4bb34691b743 03/22/23 21:07:45.94
STEP: waiting to observe update in volume 03/22/23 21:07:45.948
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:48.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8850" for this suite. 03/22/23 21:07:48.017
------------------------------
• [4.216 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:43.809
    Mar 22 21:07:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:07:43.81
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:43.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:43.835
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-bc4c634b-3f0d-4f74-86fa-42c6570a3e75 03/22/23 21:07:43.847
    STEP: Creating configMap with name cm-test-opt-upd-cd50d733-7f30-4edc-9dd2-f4f76a1770c1 03/22/23 21:07:43.855
    STEP: Creating the pod 03/22/23 21:07:43.861
    Mar 22 21:07:43.871: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6" in namespace "projected-8850" to be "running and ready"
    Mar 22 21:07:43.883: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.719216ms
    Mar 22 21:07:43.883: INFO: The phase of Pod pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:07:45.889: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017462452s
    Mar 22 21:07:45.889: INFO: The phase of Pod pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6 is Running (Ready = true)
    Mar 22 21:07:45.889: INFO: Pod "pod-projected-configmaps-de27fccb-0c8a-44d7-a62c-2b43600082a6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-bc4c634b-3f0d-4f74-86fa-42c6570a3e75 03/22/23 21:07:45.927
    STEP: Updating configmap cm-test-opt-upd-cd50d733-7f30-4edc-9dd2-f4f76a1770c1 03/22/23 21:07:45.934
    STEP: Creating configMap with name cm-test-opt-create-ea1c0767-ccc4-4d48-9ff1-4bb34691b743 03/22/23 21:07:45.94
    STEP: waiting to observe update in volume 03/22/23 21:07:45.948
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:48.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8850" for this suite. 03/22/23 21:07:48.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:48.031
Mar 22 21:07:48.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 21:07:48.033
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:48.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:48.056
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 03/22/23 21:07:48.075
Mar 22 21:07:48.086: INFO: Waiting up to 5m0s for pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c" in namespace "pods-8560" to be "running and ready"
Mar 22 21:07:48.090: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678705ms
Mar 22 21:07:48.090: INFO: The phase of Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:07:50.097: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010338894s
Mar 22 21:07:50.097: INFO: The phase of Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c is Running (Ready = true)
Mar 22 21:07:50.097: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c" satisfied condition "running and ready"
Mar 22 21:07:50.114: INFO: Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c has hostIP: 10.124.0.2
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:50.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8560" for this suite. 03/22/23 21:07:50.124
------------------------------
• [2.109 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:48.031
    Mar 22 21:07:48.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 21:07:48.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:48.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:48.056
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 03/22/23 21:07:48.075
    Mar 22 21:07:48.086: INFO: Waiting up to 5m0s for pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c" in namespace "pods-8560" to be "running and ready"
    Mar 22 21:07:48.090: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678705ms
    Mar 22 21:07:48.090: INFO: The phase of Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:07:50.097: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010338894s
    Mar 22 21:07:50.097: INFO: The phase of Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c is Running (Ready = true)
    Mar 22 21:07:50.097: INFO: Pod "pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c" satisfied condition "running and ready"
    Mar 22 21:07:50.114: INFO: Pod pod-hostip-042baea5-a3f4-4d55-ac11-7069282fc66c has hostIP: 10.124.0.2
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:50.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8560" for this suite. 03/22/23 21:07:50.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:50.148
Mar 22 21:07:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:07:50.15
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:50.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:50.175
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-8fd2ea7b-1ef2-4884-903d-f2760df5ebdc 03/22/23 21:07:50.182
STEP: Creating a pod to test consume configMaps 03/22/23 21:07:50.19
Mar 22 21:07:50.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349" in namespace "projected-3634" to be "Succeeded or Failed"
Mar 22 21:07:50.211: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Pending", Reason="", readiness=false. Elapsed: 9.853937ms
Mar 22 21:07:52.220: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019653178s
Mar 22 21:07:54.217: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016490071s
STEP: Saw pod success 03/22/23 21:07:54.217
Mar 22 21:07:54.218: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349" satisfied condition "Succeeded or Failed"
Mar 22 21:07:54.222: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:07:54.265
Mar 22 21:07:54.288: INFO: Waiting for pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 to disappear
Mar 22 21:07:54.293: INFO: Pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3634" for this suite. 03/22/23 21:07:54.302
------------------------------
• [4.162 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:50.148
    Mar 22 21:07:50.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:07:50.15
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:50.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:50.175
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-8fd2ea7b-1ef2-4884-903d-f2760df5ebdc 03/22/23 21:07:50.182
    STEP: Creating a pod to test consume configMaps 03/22/23 21:07:50.19
    Mar 22 21:07:50.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349" in namespace "projected-3634" to be "Succeeded or Failed"
    Mar 22 21:07:50.211: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Pending", Reason="", readiness=false. Elapsed: 9.853937ms
    Mar 22 21:07:52.220: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019653178s
    Mar 22 21:07:54.217: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016490071s
    STEP: Saw pod success 03/22/23 21:07:54.217
    Mar 22 21:07:54.218: INFO: Pod "pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349" satisfied condition "Succeeded or Failed"
    Mar 22 21:07:54.222: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:07:54.265
    Mar 22 21:07:54.288: INFO: Waiting for pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 to disappear
    Mar 22 21:07:54.293: INFO: Pod pod-projected-configmaps-3f159c87-d414-47e6-b37c-7bdae1b26349 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3634" for this suite. 03/22/23 21:07:54.302
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:54.31
Mar 22 21:07:54.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:07:54.311
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:54.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:54.339
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 21:07:54.347
Mar 22 21:07:54.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Mar 22 21:07:54.478: INFO: stderr: ""
Mar 22 21:07:54.478: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/22/23 21:07:54.478
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Mar 22 21:07:54.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7150 delete pods e2e-test-httpd-pod'
Mar 22 21:07:56.632: INFO: stderr: ""
Mar 22 21:07:56.632: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:07:56.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7150" for this suite. 03/22/23 21:07:56.638
------------------------------
• [2.341 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:54.31
    Mar 22 21:07:54.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:07:54.311
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:54.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:54.339
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/22/23 21:07:54.347
    Mar 22 21:07:54.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Mar 22 21:07:54.478: INFO: stderr: ""
    Mar 22 21:07:54.478: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/22/23 21:07:54.478
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Mar 22 21:07:54.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7150 delete pods e2e-test-httpd-pod'
    Mar 22 21:07:56.632: INFO: stderr: ""
    Mar 22 21:07:56.632: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:07:56.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7150" for this suite. 03/22/23 21:07:56.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:07:56.653
Mar 22 21:07:56.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:07:56.654
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:56.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:56.675
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-a1437016-3837-47cd-9891-795d3a2c4674 03/22/23 21:07:56.682
STEP: Creating a pod to test consume configMaps 03/22/23 21:07:56.688
Mar 22 21:07:56.699: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5" in namespace "configmap-9921" to be "Succeeded or Failed"
Mar 22 21:07:56.703: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307421ms
Mar 22 21:07:58.718: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019522534s
Mar 22 21:08:00.717: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018529872s
STEP: Saw pod success 03/22/23 21:08:00.717
Mar 22 21:08:00.717: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5" satisfied condition "Succeeded or Failed"
Mar 22 21:08:00.729: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:08:00.742
Mar 22 21:08:00.757: INFO: Waiting for pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 to disappear
Mar 22 21:08:00.772: INFO: Pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:00.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9921" for this suite. 03/22/23 21:08:00.784
------------------------------
• [4.140 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:07:56.653
    Mar 22 21:07:56.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:07:56.654
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:07:56.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:07:56.675
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-a1437016-3837-47cd-9891-795d3a2c4674 03/22/23 21:07:56.682
    STEP: Creating a pod to test consume configMaps 03/22/23 21:07:56.688
    Mar 22 21:07:56.699: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5" in namespace "configmap-9921" to be "Succeeded or Failed"
    Mar 22 21:07:56.703: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307421ms
    Mar 22 21:07:58.718: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019522534s
    Mar 22 21:08:00.717: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018529872s
    STEP: Saw pod success 03/22/23 21:08:00.717
    Mar 22 21:08:00.717: INFO: Pod "pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5" satisfied condition "Succeeded or Failed"
    Mar 22 21:08:00.729: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:08:00.742
    Mar 22 21:08:00.757: INFO: Waiting for pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 to disappear
    Mar 22 21:08:00.772: INFO: Pod pod-configmaps-f4e2d49b-11ba-414a-b5e8-e954d49814f5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:00.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9921" for this suite. 03/22/23 21:08:00.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:00.802
Mar 22 21:08:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename runtimeclass 03/22/23 21:08:00.803
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:00.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:00.839
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 22 21:08:00.892: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1967 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:00.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1967" for this suite. 03/22/23 21:08:00.912
------------------------------
• [0.122 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:00.802
    Mar 22 21:08:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename runtimeclass 03/22/23 21:08:00.803
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:00.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:00.839
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 22 21:08:00.892: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1967 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:00.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1967" for this suite. 03/22/23 21:08:00.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:00.93
Mar 22 21:08:00.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:08:00.933
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:00.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:00.978
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-2d4eecad-e5fb-4870-b96f-82f40f5a6268 03/22/23 21:08:00.987
STEP: Creating a pod to test consume configMaps 03/22/23 21:08:00.998
Mar 22 21:08:01.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab" in namespace "projected-2893" to be "Succeeded or Failed"
Mar 22 21:08:01.014: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7755ms
Mar 22 21:08:03.022: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013363942s
Mar 22 21:08:05.023: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014273419s
STEP: Saw pod success 03/22/23 21:08:05.023
Mar 22 21:08:05.023: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab" satisfied condition "Succeeded or Failed"
Mar 22 21:08:05.028: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:08:05.041
Mar 22 21:08:05.056: INFO: Waiting for pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab to disappear
Mar 22 21:08:05.059: INFO: Pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:05.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2893" for this suite. 03/22/23 21:08:05.072
------------------------------
• [4.151 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:00.93
    Mar 22 21:08:00.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:08:00.933
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:00.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:00.978
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-2d4eecad-e5fb-4870-b96f-82f40f5a6268 03/22/23 21:08:00.987
    STEP: Creating a pod to test consume configMaps 03/22/23 21:08:00.998
    Mar 22 21:08:01.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab" in namespace "projected-2893" to be "Succeeded or Failed"
    Mar 22 21:08:01.014: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7755ms
    Mar 22 21:08:03.022: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013363942s
    Mar 22 21:08:05.023: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014273419s
    STEP: Saw pod success 03/22/23 21:08:05.023
    Mar 22 21:08:05.023: INFO: Pod "pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab" satisfied condition "Succeeded or Failed"
    Mar 22 21:08:05.028: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:08:05.041
    Mar 22 21:08:05.056: INFO: Waiting for pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab to disappear
    Mar 22 21:08:05.059: INFO: Pod pod-projected-configmaps-8e0b7de6-251f-4435-9e99-3974c55e78ab no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:05.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2893" for this suite. 03/22/23 21:08:05.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:05.085
Mar 22 21:08:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption 03/22/23 21:08:05.087
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:05.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:05.146
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 03/22/23 21:08:05.173
STEP: Updating PodDisruptionBudget status 03/22/23 21:08:05.182
STEP: Waiting for all pods to be running 03/22/23 21:08:05.196
Mar 22 21:08:05.205: INFO: running pods: 0 < 1
STEP: locating a running pod 03/22/23 21:08:07.215
STEP: Waiting for the pdb to be processed 03/22/23 21:08:07.25
STEP: Patching PodDisruptionBudget status 03/22/23 21:08:07.262
STEP: Waiting for the pdb to be processed 03/22/23 21:08:07.288
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5804" for this suite. 03/22/23 21:08:07.321
------------------------------
• [2.260 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:05.085
    Mar 22 21:08:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption 03/22/23 21:08:05.087
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:05.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:05.146
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 03/22/23 21:08:05.173
    STEP: Updating PodDisruptionBudget status 03/22/23 21:08:05.182
    STEP: Waiting for all pods to be running 03/22/23 21:08:05.196
    Mar 22 21:08:05.205: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/22/23 21:08:07.215
    STEP: Waiting for the pdb to be processed 03/22/23 21:08:07.25
    STEP: Patching PodDisruptionBudget status 03/22/23 21:08:07.262
    STEP: Waiting for the pdb to be processed 03/22/23 21:08:07.288
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5804" for this suite. 03/22/23 21:08:07.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:07.351
Mar 22 21:08:07.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:08:07.359
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:07.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:07.388
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-e0c29fe5-cd6c-4751-8d81-04aafddbdfd6 03/22/23 21:08:07.4
STEP: Creating a pod to test consume configMaps 03/22/23 21:08:07.41
Mar 22 21:08:07.421: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa" in namespace "projected-8464" to be "Succeeded or Failed"
Mar 22 21:08:07.428: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216155ms
Mar 22 21:08:09.436: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507566s
Mar 22 21:08:11.460: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03969823s
STEP: Saw pod success 03/22/23 21:08:11.46
Mar 22 21:08:11.461: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa" satisfied condition "Succeeded or Failed"
Mar 22 21:08:11.465: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:08:11.503
Mar 22 21:08:11.514: INFO: Waiting for pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa to disappear
Mar 22 21:08:11.519: INFO: Pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:11.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8464" for this suite. 03/22/23 21:08:11.529
------------------------------
• [4.195 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:07.351
    Mar 22 21:08:07.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:08:07.359
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:07.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:07.388
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-e0c29fe5-cd6c-4751-8d81-04aafddbdfd6 03/22/23 21:08:07.4
    STEP: Creating a pod to test consume configMaps 03/22/23 21:08:07.41
    Mar 22 21:08:07.421: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa" in namespace "projected-8464" to be "Succeeded or Failed"
    Mar 22 21:08:07.428: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216155ms
    Mar 22 21:08:09.436: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507566s
    Mar 22 21:08:11.460: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03969823s
    STEP: Saw pod success 03/22/23 21:08:11.46
    Mar 22 21:08:11.461: INFO: Pod "pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa" satisfied condition "Succeeded or Failed"
    Mar 22 21:08:11.465: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:08:11.503
    Mar 22 21:08:11.514: INFO: Waiting for pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa to disappear
    Mar 22 21:08:11.519: INFO: Pod pod-projected-configmaps-899866cc-c5ac-4bb3-9813-2d81d4c8d8aa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:11.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8464" for this suite. 03/22/23 21:08:11.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:11.563
Mar 22 21:08:11.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 21:08:11.577
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:11.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:11.622
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 22 21:08:11.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6541" for this suite. 03/22/23 21:08:12.698
------------------------------
• [1.144 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:11.563
    Mar 22 21:08:11.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 21:08:11.577
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:11.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:11.622
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 22 21:08:11.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6541" for this suite. 03/22/23 21:08:12.698
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:12.71
Mar 22 21:08:12.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename events 03/22/23 21:08:12.713
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:12.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:12.754
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/22/23 21:08:12.764
STEP: listing events in all namespaces 03/22/23 21:08:12.773
STEP: listing events in test namespace 03/22/23 21:08:12.778
STEP: listing events with field selection filtering on source 03/22/23 21:08:12.782
STEP: listing events with field selection filtering on reportingController 03/22/23 21:08:12.793
STEP: getting the test event 03/22/23 21:08:12.801
STEP: patching the test event 03/22/23 21:08:12.808
STEP: getting the test event 03/22/23 21:08:12.817
STEP: updating the test event 03/22/23 21:08:12.821
STEP: getting the test event 03/22/23 21:08:12.83
STEP: deleting the test event 03/22/23 21:08:12.836
STEP: listing events in all namespaces 03/22/23 21:08:12.844
STEP: listing events in test namespace 03/22/23 21:08:12.85
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5030" for this suite. 03/22/23 21:08:12.877
------------------------------
• [0.176 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:12.71
    Mar 22 21:08:12.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename events 03/22/23 21:08:12.713
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:12.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:12.754
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/22/23 21:08:12.764
    STEP: listing events in all namespaces 03/22/23 21:08:12.773
    STEP: listing events in test namespace 03/22/23 21:08:12.778
    STEP: listing events with field selection filtering on source 03/22/23 21:08:12.782
    STEP: listing events with field selection filtering on reportingController 03/22/23 21:08:12.793
    STEP: getting the test event 03/22/23 21:08:12.801
    STEP: patching the test event 03/22/23 21:08:12.808
    STEP: getting the test event 03/22/23 21:08:12.817
    STEP: updating the test event 03/22/23 21:08:12.821
    STEP: getting the test event 03/22/23 21:08:12.83
    STEP: deleting the test event 03/22/23 21:08:12.836
    STEP: listing events in all namespaces 03/22/23 21:08:12.844
    STEP: listing events in test namespace 03/22/23 21:08:12.85
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5030" for this suite. 03/22/23 21:08:12.877
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:12.886
Mar 22 21:08:12.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename containers 03/22/23 21:08:12.892
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:12.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:12.929
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Mar 22 21:08:12.954: INFO: Waiting up to 5m0s for pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690" in namespace "containers-2282" to be "running"
Mar 22 21:08:12.962: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690": Phase="Pending", Reason="", readiness=false. Elapsed: 7.539561ms
Mar 22 21:08:14.967: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690": Phase="Running", Reason="", readiness=true. Elapsed: 2.013158167s
Mar 22 21:08:14.967: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:14.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2282" for this suite. 03/22/23 21:08:15.001
------------------------------
• [2.127 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:12.886
    Mar 22 21:08:12.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename containers 03/22/23 21:08:12.892
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:12.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:12.929
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Mar 22 21:08:12.954: INFO: Waiting up to 5m0s for pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690" in namespace "containers-2282" to be "running"
    Mar 22 21:08:12.962: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690": Phase="Pending", Reason="", readiness=false. Elapsed: 7.539561ms
    Mar 22 21:08:14.967: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690": Phase="Running", Reason="", readiness=true. Elapsed: 2.013158167s
    Mar 22 21:08:14.967: INFO: Pod "client-containers-c7770686-c2c4-41b9-b947-9f4a3637f690" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:14.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2282" for this suite. 03/22/23 21:08:15.001
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:15.014
Mar 22 21:08:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 21:08:15.019
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:15.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:15.06
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-6791 03/22/23 21:08:15.08
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[] 03/22/23 21:08:15.095
Mar 22 21:08:15.152: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6791 03/22/23 21:08:15.153
Mar 22 21:08:15.166: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6791" to be "running and ready"
Mar 22 21:08:15.180: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.944867ms
Mar 22 21:08:15.181: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:08:17.189: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022825808s
Mar 22 21:08:17.189: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 22 21:08:17.189: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod1:[80]] 03/22/23 21:08:17.196
Mar 22 21:08:17.214: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/22/23 21:08:17.214
Mar 22 21:08:17.214: INFO: Creating new exec pod
Mar 22 21:08:17.224: INFO: Waiting up to 5m0s for pod "execpod8jhgf" in namespace "services-6791" to be "running"
Mar 22 21:08:17.233: INFO: Pod "execpod8jhgf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363298ms
Mar 22 21:08:19.255: INFO: Pod "execpod8jhgf": Phase="Running", Reason="", readiness=true. Elapsed: 2.03073588s
Mar 22 21:08:19.255: INFO: Pod "execpod8jhgf" satisfied condition "running"
Mar 22 21:08:20.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 22 21:08:20.706: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:20.708: INFO: stdout: ""
Mar 22 21:08:20.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
Mar 22 21:08:21.089: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:21.089: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-6791 03/22/23 21:08:21.089
Mar 22 21:08:21.097: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6791" to be "running and ready"
Mar 22 21:08:21.101: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556949ms
Mar 22 21:08:21.101: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:08:23.131: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.034489038s
Mar 22 21:08:23.132: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 22 21:08:23.132: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod1:[80] pod2:[80]] 03/22/23 21:08:23.148
Mar 22 21:08:23.196: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/22/23 21:08:23.197
Mar 22 21:08:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 22 21:08:24.684: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:24.684: INFO: stdout: ""
Mar 22 21:08:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
Mar 22 21:08:25.160: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:25.160: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-6791 03/22/23 21:08:25.16
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod2:[80]] 03/22/23 21:08:25.172
Mar 22 21:08:26.290: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/22/23 21:08:26.29
Mar 22 21:08:27.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 22 21:08:27.829: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:27.829: INFO: stdout: ""
Mar 22 21:08:27.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
Mar 22 21:08:28.352: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
Mar 22 21:08:28.353: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-6791 03/22/23 21:08:28.353
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[] 03/22/23 21:08:28.387
Mar 22 21:08:28.408: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:28.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6791" for this suite. 03/22/23 21:08:28.46
------------------------------
• [SLOW TEST] [13.456 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:15.014
    Mar 22 21:08:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 21:08:15.019
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:15.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:15.06
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-6791 03/22/23 21:08:15.08
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[] 03/22/23 21:08:15.095
    Mar 22 21:08:15.152: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6791 03/22/23 21:08:15.153
    Mar 22 21:08:15.166: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6791" to be "running and ready"
    Mar 22 21:08:15.180: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.944867ms
    Mar 22 21:08:15.181: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:08:17.189: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022825808s
    Mar 22 21:08:17.189: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 22 21:08:17.189: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod1:[80]] 03/22/23 21:08:17.196
    Mar 22 21:08:17.214: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/22/23 21:08:17.214
    Mar 22 21:08:17.214: INFO: Creating new exec pod
    Mar 22 21:08:17.224: INFO: Waiting up to 5m0s for pod "execpod8jhgf" in namespace "services-6791" to be "running"
    Mar 22 21:08:17.233: INFO: Pod "execpod8jhgf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363298ms
    Mar 22 21:08:19.255: INFO: Pod "execpod8jhgf": Phase="Running", Reason="", readiness=true. Elapsed: 2.03073588s
    Mar 22 21:08:19.255: INFO: Pod "execpod8jhgf" satisfied condition "running"
    Mar 22 21:08:20.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 22 21:08:20.706: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:20.708: INFO: stdout: ""
    Mar 22 21:08:20.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
    Mar 22 21:08:21.089: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:21.089: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-6791 03/22/23 21:08:21.089
    Mar 22 21:08:21.097: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6791" to be "running and ready"
    Mar 22 21:08:21.101: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556949ms
    Mar 22 21:08:21.101: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:08:23.131: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.034489038s
    Mar 22 21:08:23.132: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 22 21:08:23.132: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod1:[80] pod2:[80]] 03/22/23 21:08:23.148
    Mar 22 21:08:23.196: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/22/23 21:08:23.197
    Mar 22 21:08:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 22 21:08:24.684: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:24.684: INFO: stdout: ""
    Mar 22 21:08:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
    Mar 22 21:08:25.160: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:25.160: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-6791 03/22/23 21:08:25.16
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[pod2:[80]] 03/22/23 21:08:25.172
    Mar 22 21:08:26.290: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/22/23 21:08:26.29
    Mar 22 21:08:27.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 22 21:08:27.829: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:27.829: INFO: stdout: ""
    Mar 22 21:08:27.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-6791 exec execpod8jhgf -- /bin/sh -x -c nc -v -z -w 2 10.245.52.19 80'
    Mar 22 21:08:28.352: INFO: stderr: "+ nc -v -z -w 2 10.245.52.19 80\nConnection to 10.245.52.19 80 port [tcp/http] succeeded!\n"
    Mar 22 21:08:28.353: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-6791 03/22/23 21:08:28.353
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6791 to expose endpoints map[] 03/22/23 21:08:28.387
    Mar 22 21:08:28.408: INFO: successfully validated that service endpoint-test2 in namespace services-6791 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:28.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6791" for this suite. 03/22/23 21:08:28.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:28.475
Mar 22 21:08:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 21:08:28.476
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:28.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:28.539
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:28.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7422" for this suite. 03/22/23 21:08:28.668
------------------------------
• [0.200 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:28.475
    Mar 22 21:08:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 21:08:28.476
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:28.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:28.539
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:28.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7422" for this suite. 03/22/23 21:08:28.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:28.694
Mar 22 21:08:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pod-network-test 03/22/23 21:08:28.698
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:28.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:28.792
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3763 03/22/23 21:08:28.82
STEP: creating a selector 03/22/23 21:08:28.821
STEP: Creating the service pods in kubernetes 03/22/23 21:08:28.821
Mar 22 21:08:28.821: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 22 21:08:28.872: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3763" to be "running and ready"
Mar 22 21:08:28.877: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189274ms
Mar 22 21:08:28.878: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:08:30.889: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016688275s
Mar 22 21:08:30.889: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:08:32.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0116019s
Mar 22 21:08:32.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:08:34.890: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017760663s
Mar 22 21:08:34.890: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:08:36.930: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.058026416s
Mar 22 21:08:36.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:08:38.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013841646s
Mar 22 21:08:38.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:08:40.887: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014626194s
Mar 22 21:08:40.887: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 22 21:08:40.887: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 22 21:08:40.897: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3763" to be "running and ready"
Mar 22 21:08:40.911: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 7.460345ms
Mar 22 21:08:40.911: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 21:08:42.922: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.018223321s
Mar 22 21:08:42.924: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 21:08:44.918: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.014259243s
Mar 22 21:08:44.918: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 21:08:46.924: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.020563388s
Mar 22 21:08:46.924: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 21:08:48.918: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.014013888s
Mar 22 21:08:48.918: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Mar 22 21:08:50.917: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.013171372s
Mar 22 21:08:50.917: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 22 21:08:50.917: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 22 21:08:50.922: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3763" to be "running and ready"
Mar 22 21:08:50.934: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.914488ms
Mar 22 21:08:50.935: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 22 21:08:50.935: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/22/23 21:08:50.939
Mar 22 21:08:50.956: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3763" to be "running"
Mar 22 21:08:50.969: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.67118ms
Mar 22 21:08:52.986: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028801447s
Mar 22 21:08:52.986: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 22 21:08:52.990: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3763" to be "running"
Mar 22 21:08:52.994: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.202554ms
Mar 22 21:08:52.994: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 22 21:08:53.002: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 22 21:08:53.007: INFO: Going to poll 10.244.1.4 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:08:53.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:08:53.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:08:53.015: INFO: ExecWithOptions: Clientset creation
Mar 22 21:08:53.015: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:08:53.201: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 22 21:08:53.201: INFO: Going to poll 10.244.0.76 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:08:53.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:08:53.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:08:53.207: INFO: ExecWithOptions: Clientset creation
Mar 22 21:08:53.207: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:08:53.383: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 22 21:08:53.384: INFO: Going to poll 10.244.0.153 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:08:53.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:08:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:08:53.391: INFO: ExecWithOptions: Clientset creation
Mar 22 21:08:53.391: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:08:53.567: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:53.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3763" for this suite. 03/22/23 21:08:53.574
------------------------------
• [SLOW TEST] [24.890 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:28.694
    Mar 22 21:08:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pod-network-test 03/22/23 21:08:28.698
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:28.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:28.792
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3763 03/22/23 21:08:28.82
    STEP: creating a selector 03/22/23 21:08:28.821
    STEP: Creating the service pods in kubernetes 03/22/23 21:08:28.821
    Mar 22 21:08:28.821: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 22 21:08:28.872: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3763" to be "running and ready"
    Mar 22 21:08:28.877: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189274ms
    Mar 22 21:08:28.878: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:08:30.889: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016688275s
    Mar 22 21:08:30.889: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:08:32.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0116019s
    Mar 22 21:08:32.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:08:34.890: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017760663s
    Mar 22 21:08:34.890: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:08:36.930: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.058026416s
    Mar 22 21:08:36.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:08:38.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013841646s
    Mar 22 21:08:38.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:08:40.887: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014626194s
    Mar 22 21:08:40.887: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 22 21:08:40.887: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 22 21:08:40.897: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3763" to be "running and ready"
    Mar 22 21:08:40.911: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 7.460345ms
    Mar 22 21:08:40.911: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 21:08:42.922: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.018223321s
    Mar 22 21:08:42.924: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 21:08:44.918: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.014259243s
    Mar 22 21:08:44.918: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 21:08:46.924: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.020563388s
    Mar 22 21:08:46.924: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 21:08:48.918: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.014013888s
    Mar 22 21:08:48.918: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Mar 22 21:08:50.917: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.013171372s
    Mar 22 21:08:50.917: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 22 21:08:50.917: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 22 21:08:50.922: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3763" to be "running and ready"
    Mar 22 21:08:50.934: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.914488ms
    Mar 22 21:08:50.935: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 22 21:08:50.935: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/22/23 21:08:50.939
    Mar 22 21:08:50.956: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3763" to be "running"
    Mar 22 21:08:50.969: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.67118ms
    Mar 22 21:08:52.986: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028801447s
    Mar 22 21:08:52.986: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 22 21:08:52.990: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3763" to be "running"
    Mar 22 21:08:52.994: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.202554ms
    Mar 22 21:08:52.994: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 22 21:08:53.002: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 22 21:08:53.007: INFO: Going to poll 10.244.1.4 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:08:53.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:08:53.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:08:53.015: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:08:53.015: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:08:53.201: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 22 21:08:53.201: INFO: Going to poll 10.244.0.76 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:08:53.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:08:53.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:08:53.207: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:08:53.207: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:08:53.383: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 22 21:08:53.384: INFO: Going to poll 10.244.0.153 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:08:53.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3763 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:08:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:08:53.391: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:08:53.391: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-3763/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:08:53.567: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:53.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3763" for this suite. 03/22/23 21:08:53.574
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:53.585
Mar 22 21:08:53.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:08:53.587
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:53.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:53.612
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-afa4b5db-8a1c-49ac-8285-b1e8da607704 03/22/23 21:08:53.619
STEP: Creating a pod to test consume configMaps 03/22/23 21:08:53.633
Mar 22 21:08:53.645: INFO: Waiting up to 5m0s for pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5" in namespace "configmap-5009" to be "Succeeded or Failed"
Mar 22 21:08:53.650: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420418ms
Mar 22 21:08:55.656: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663589s
Mar 22 21:08:57.658: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012122805s
STEP: Saw pod success 03/22/23 21:08:57.658
Mar 22 21:08:57.658: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5" satisfied condition "Succeeded or Failed"
Mar 22 21:08:57.663: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:08:57.685
Mar 22 21:08:57.695: INFO: Waiting for pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 to disappear
Mar 22 21:08:57.698: INFO: Pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:08:57.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5009" for this suite. 03/22/23 21:08:57.704
------------------------------
• [4.128 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:53.585
    Mar 22 21:08:53.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:08:53.587
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:53.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:53.612
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-afa4b5db-8a1c-49ac-8285-b1e8da607704 03/22/23 21:08:53.619
    STEP: Creating a pod to test consume configMaps 03/22/23 21:08:53.633
    Mar 22 21:08:53.645: INFO: Waiting up to 5m0s for pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5" in namespace "configmap-5009" to be "Succeeded or Failed"
    Mar 22 21:08:53.650: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420418ms
    Mar 22 21:08:55.656: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663589s
    Mar 22 21:08:57.658: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012122805s
    STEP: Saw pod success 03/22/23 21:08:57.658
    Mar 22 21:08:57.658: INFO: Pod "pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5" satisfied condition "Succeeded or Failed"
    Mar 22 21:08:57.663: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:08:57.685
    Mar 22 21:08:57.695: INFO: Waiting for pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 to disappear
    Mar 22 21:08:57.698: INFO: Pod pod-configmaps-259caf9a-536e-4d8a-90f5-3712dd9f64c5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:08:57.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5009" for this suite. 03/22/23 21:08:57.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:08:57.718
Mar 22 21:08:57.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:08:57.72
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:57.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:57.741
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:08:57.761
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:08:58.523
STEP: Deploying the webhook pod 03/22/23 21:08:58.542
STEP: Wait for the deployment to be ready 03/22/23 21:08:58.558
Mar 22 21:08:58.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 21:09:00.593
STEP: Verifying the service has paired with the endpoint 03/22/23 21:09:00.605
Mar 22 21:09:01.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/22/23 21:09:01.611
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/22/23 21:09:01.659
STEP: Creating a dummy validating-webhook-configuration object 03/22/23 21:09:01.718
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/22/23 21:09:01.778
STEP: Creating a dummy mutating-webhook-configuration object 03/22/23 21:09:01.788
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/22/23 21:09:01.804
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:09:01.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1182" for this suite. 03/22/23 21:09:01.891
STEP: Destroying namespace "webhook-1182-markers" for this suite. 03/22/23 21:09:01.906
------------------------------
• [4.205 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:08:57.718
    Mar 22 21:08:57.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:08:57.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:08:57.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:08:57.741
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:08:57.761
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:08:58.523
    STEP: Deploying the webhook pod 03/22/23 21:08:58.542
    STEP: Wait for the deployment to be ready 03/22/23 21:08:58.558
    Mar 22 21:08:58.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 21:09:00.593
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:09:00.605
    Mar 22 21:09:01.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/22/23 21:09:01.611
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/22/23 21:09:01.659
    STEP: Creating a dummy validating-webhook-configuration object 03/22/23 21:09:01.718
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/22/23 21:09:01.778
    STEP: Creating a dummy mutating-webhook-configuration object 03/22/23 21:09:01.788
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/22/23 21:09:01.804
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:09:01.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1182" for this suite. 03/22/23 21:09:01.891
    STEP: Destroying namespace "webhook-1182-markers" for this suite. 03/22/23 21:09:01.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:09:01.923
Mar 22 21:09:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:09:01.925
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:01.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:01.993
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:09:02.012
Mar 22 21:09:02.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2" in namespace "projected-1405" to be "Succeeded or Failed"
Mar 22 21:09:02.039: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.89524ms
Mar 22 21:09:04.046: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014286231s
Mar 22 21:09:06.045: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013635739s
STEP: Saw pod success 03/22/23 21:09:06.045
Mar 22 21:09:06.045: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2" satisfied condition "Succeeded or Failed"
Mar 22 21:09:06.050: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 container client-container: <nil>
STEP: delete the pod 03/22/23 21:09:06.062
Mar 22 21:09:06.078: INFO: Waiting for pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 to disappear
Mar 22 21:09:06.095: INFO: Pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 21:09:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1405" for this suite. 03/22/23 21:09:06.115
------------------------------
• [4.207 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:09:01.923
    Mar 22 21:09:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:09:01.925
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:01.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:01.993
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:09:02.012
    Mar 22 21:09:02.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2" in namespace "projected-1405" to be "Succeeded or Failed"
    Mar 22 21:09:02.039: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.89524ms
    Mar 22 21:09:04.046: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014286231s
    Mar 22 21:09:06.045: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013635739s
    STEP: Saw pod success 03/22/23 21:09:06.045
    Mar 22 21:09:06.045: INFO: Pod "downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2" satisfied condition "Succeeded or Failed"
    Mar 22 21:09:06.050: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:09:06.062
    Mar 22 21:09:06.078: INFO: Waiting for pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 to disappear
    Mar 22 21:09:06.095: INFO: Pod downwardapi-volume-928752f9-0017-4c21-adc6-eb5935dbb6e2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:09:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1405" for this suite. 03/22/23 21:09:06.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:09:06.134
Mar 22 21:09:06.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:09:06.138
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:06.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:06.174
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 03/22/23 21:09:06.196
Mar 22 21:09:06.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 create -f -'
Mar 22 21:09:07.032: INFO: stderr: ""
Mar 22 21:09:07.032: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/22/23 21:09:07.032
Mar 22 21:09:07.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 diff -f -'
Mar 22 21:09:07.516: INFO: rc: 1
Mar 22 21:09:07.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 delete -f -'
Mar 22 21:09:07.708: INFO: stderr: ""
Mar 22 21:09:07.708: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:09:07.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4527" for this suite. 03/22/23 21:09:07.714
------------------------------
• [1.599 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:09:06.134
    Mar 22 21:09:06.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:09:06.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:06.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:06.174
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 03/22/23 21:09:06.196
    Mar 22 21:09:06.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 create -f -'
    Mar 22 21:09:07.032: INFO: stderr: ""
    Mar 22 21:09:07.032: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/22/23 21:09:07.032
    Mar 22 21:09:07.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 diff -f -'
    Mar 22 21:09:07.516: INFO: rc: 1
    Mar 22 21:09:07.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-4527 delete -f -'
    Mar 22 21:09:07.708: INFO: stderr: ""
    Mar 22 21:09:07.708: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:09:07.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4527" for this suite. 03/22/23 21:09:07.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:09:07.74
Mar 22 21:09:07.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:09:07.742
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:07.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:07.804
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Mar 22 21:09:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 21:09:09.412
Mar 22 21:09:09.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 create -f -'
Mar 22 21:09:10.189: INFO: stderr: ""
Mar 22 21:09:10.189: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 22 21:09:10.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
Mar 22 21:09:10.327: INFO: stderr: ""
Mar 22 21:09:10.327: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 22 21:09:10.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 apply -f -'
Mar 22 21:09:10.672: INFO: stderr: ""
Mar 22 21:09:10.672: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 22 21:09:10.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
Mar 22 21:09:10.815: INFO: stderr: ""
Mar 22 21:09:10.815: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/22/23 21:09:10.815
Mar 22 21:09:10.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 explain e2e-test-crd-publish-openapi-3667-crds'
Mar 22 21:09:11.144: INFO: stderr: ""
Mar 22 21:09:11.144: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3667-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:09:12.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-401" for this suite. 03/22/23 21:09:12.73
------------------------------
• [5.000 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:09:07.74
    Mar 22 21:09:07.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:09:07.742
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:07.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:07.804
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Mar 22 21:09:07.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 21:09:09.412
    Mar 22 21:09:09.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 create -f -'
    Mar 22 21:09:10.189: INFO: stderr: ""
    Mar 22 21:09:10.189: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 22 21:09:10.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
    Mar 22 21:09:10.327: INFO: stderr: ""
    Mar 22 21:09:10.327: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 22 21:09:10.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 apply -f -'
    Mar 22 21:09:10.672: INFO: stderr: ""
    Mar 22 21:09:10.672: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 22 21:09:10.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 --namespace=crd-publish-openapi-401 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
    Mar 22 21:09:10.815: INFO: stderr: ""
    Mar 22 21:09:10.815: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/22/23 21:09:10.815
    Mar 22 21:09:10.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-401 explain e2e-test-crd-publish-openapi-3667-crds'
    Mar 22 21:09:11.144: INFO: stderr: ""
    Mar 22 21:09:11.144: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3667-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:09:12.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-401" for this suite. 03/22/23 21:09:12.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:09:12.746
Mar 22 21:09:12.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 21:09:12.747
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:12.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:12.776
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8672 03/22/23 21:09:12.785
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 03/22/23 21:09:12.792
Mar 22 21:09:12.813: INFO: Found 0 stateful pods, waiting for 3
Mar 22 21:09:22.820: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:09:22.820: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:09:22.820: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/22/23 21:09:22.851
Mar 22 21:09:22.874: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/22/23 21:09:22.874
STEP: Not applying an update when the partition is greater than the number of replicas 03/22/23 21:09:32.895
STEP: Performing a canary update 03/22/23 21:09:32.896
Mar 22 21:09:32.925: INFO: Updating stateful set ss2
Mar 22 21:09:32.958: INFO: Waiting for Pod statefulset-8672/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 03/22/23 21:09:42.971
Mar 22 21:09:43.015: INFO: Found 2 stateful pods, waiting for 3
Mar 22 21:09:53.022: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:09:53.022: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:09:53.022: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/22/23 21:09:53.032
Mar 22 21:09:53.056: INFO: Updating stateful set ss2
Mar 22 21:09:53.079: INFO: Waiting for Pod statefulset-8672/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Mar 22 21:10:03.135: INFO: Updating stateful set ss2
Mar 22 21:10:03.148: INFO: Waiting for StatefulSet statefulset-8672/ss2 to complete update
Mar 22 21:10:03.148: INFO: Waiting for Pod statefulset-8672/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 21:10:13.165: INFO: Deleting all statefulset in ns statefulset-8672
Mar 22 21:10:13.169: INFO: Scaling statefulset ss2 to 0
Mar 22 21:10:23.213: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 21:10:23.218: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:10:23.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8672" for this suite. 03/22/23 21:10:23.24
------------------------------
• [SLOW TEST] [70.503 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:09:12.746
    Mar 22 21:09:12.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 21:09:12.747
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:09:12.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:09:12.776
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8672 03/22/23 21:09:12.785
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 03/22/23 21:09:12.792
    Mar 22 21:09:12.813: INFO: Found 0 stateful pods, waiting for 3
    Mar 22 21:09:22.820: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:09:22.820: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:09:22.820: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/22/23 21:09:22.851
    Mar 22 21:09:22.874: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/22/23 21:09:22.874
    STEP: Not applying an update when the partition is greater than the number of replicas 03/22/23 21:09:32.895
    STEP: Performing a canary update 03/22/23 21:09:32.896
    Mar 22 21:09:32.925: INFO: Updating stateful set ss2
    Mar 22 21:09:32.958: INFO: Waiting for Pod statefulset-8672/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 03/22/23 21:09:42.971
    Mar 22 21:09:43.015: INFO: Found 2 stateful pods, waiting for 3
    Mar 22 21:09:53.022: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:09:53.022: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:09:53.022: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/22/23 21:09:53.032
    Mar 22 21:09:53.056: INFO: Updating stateful set ss2
    Mar 22 21:09:53.079: INFO: Waiting for Pod statefulset-8672/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Mar 22 21:10:03.135: INFO: Updating stateful set ss2
    Mar 22 21:10:03.148: INFO: Waiting for StatefulSet statefulset-8672/ss2 to complete update
    Mar 22 21:10:03.148: INFO: Waiting for Pod statefulset-8672/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 21:10:13.165: INFO: Deleting all statefulset in ns statefulset-8672
    Mar 22 21:10:13.169: INFO: Scaling statefulset ss2 to 0
    Mar 22 21:10:23.213: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 21:10:23.218: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:10:23.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8672" for this suite. 03/22/23 21:10:23.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:10:23.256
Mar 22 21:10:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:10:23.258
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:23.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:23.279
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-500ea727-c58e-48c1-abb6-607ae163a2a3 03/22/23 21:10:23.286
STEP: Creating a pod to test consume configMaps 03/22/23 21:10:23.293
Mar 22 21:10:23.301: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09" in namespace "projected-7439" to be "Succeeded or Failed"
Mar 22 21:10:23.308: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603724ms
Mar 22 21:10:25.330: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029381024s
Mar 22 21:10:27.314: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012840721s
Mar 22 21:10:29.314: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01366978s
STEP: Saw pod success 03/22/23 21:10:29.315
Mar 22 21:10:29.315: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09" satisfied condition "Succeeded or Failed"
Mar 22 21:10:29.319: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/22/23 21:10:29.353
Mar 22 21:10:29.374: INFO: Waiting for pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 to disappear
Mar 22 21:10:29.379: INFO: Pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:10:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7439" for this suite. 03/22/23 21:10:29.392
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:10:23.256
    Mar 22 21:10:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:10:23.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:23.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:23.279
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-500ea727-c58e-48c1-abb6-607ae163a2a3 03/22/23 21:10:23.286
    STEP: Creating a pod to test consume configMaps 03/22/23 21:10:23.293
    Mar 22 21:10:23.301: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09" in namespace "projected-7439" to be "Succeeded or Failed"
    Mar 22 21:10:23.308: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.603724ms
    Mar 22 21:10:25.330: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029381024s
    Mar 22 21:10:27.314: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012840721s
    Mar 22 21:10:29.314: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01366978s
    STEP: Saw pod success 03/22/23 21:10:29.315
    Mar 22 21:10:29.315: INFO: Pod "pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09" satisfied condition "Succeeded or Failed"
    Mar 22 21:10:29.319: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:10:29.353
    Mar 22 21:10:29.374: INFO: Waiting for pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 to disappear
    Mar 22 21:10:29.379: INFO: Pod pod-projected-configmaps-57d0107a-e2f7-41b5-8d0a-bd173f9e6f09 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:10:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7439" for this suite. 03/22/23 21:10:29.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:10:29.401
Mar 22 21:10:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:10:29.405
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.432
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-gzlrt"  03/22/23 21:10:29.441
Mar 22 21:10:29.452: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-gzlrt"  03/22/23 21:10:29.452
Mar 22 21:10:29.467: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 21:10:29.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3671" for this suite. 03/22/23 21:10:29.475
------------------------------
• [0.081 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:10:29.401
    Mar 22 21:10:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:10:29.405
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.432
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-gzlrt"  03/22/23 21:10:29.441
    Mar 22 21:10:29.452: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-gzlrt"  03/22/23 21:10:29.452
    Mar 22 21:10:29.467: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:10:29.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3671" for this suite. 03/22/23 21:10:29.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:10:29.484
Mar 22 21:10:29.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename events 03/22/23 21:10:29.485
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.513
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/22/23 21:10:29.521
STEP: get a list of Events with a label in the current namespace 03/22/23 21:10:29.541
STEP: delete a list of events 03/22/23 21:10:29.549
Mar 22 21:10:29.549: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/22/23 21:10:29.563
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:10:29.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1362" for this suite. 03/22/23 21:10:29.581
------------------------------
• [0.104 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:10:29.484
    Mar 22 21:10:29.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename events 03/22/23 21:10:29.485
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.513
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/22/23 21:10:29.521
    STEP: get a list of Events with a label in the current namespace 03/22/23 21:10:29.541
    STEP: delete a list of events 03/22/23 21:10:29.549
    Mar 22 21:10:29.549: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/22/23 21:10:29.563
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:10:29.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1362" for this suite. 03/22/23 21:10:29.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:10:29.595
Mar 22 21:10:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 21:10:29.598
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.631
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7602 03/22/23 21:10:29.639
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 03/22/23 21:10:29.647
STEP: Creating stateful set ss in namespace statefulset-7602 03/22/23 21:10:29.654
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7602 03/22/23 21:10:29.667
Mar 22 21:10:29.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 22 21:10:39.682: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/22/23 21:10:39.682
Mar 22 21:10:39.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 21:10:40.046: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 21:10:40.046: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 21:10:40.046: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 21:10:40.052: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 22 21:10:50.058: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 21:10:50.058: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 21:10:50.081: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999625s
Mar 22 21:10:51.088: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99468003s
Mar 22 21:10:52.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988208981s
Mar 22 21:10:53.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981630449s
Mar 22 21:10:54.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.968175707s
Mar 22 21:10:55.121: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961423187s
Mar 22 21:10:56.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.946688155s
Mar 22 21:10:57.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.939914607s
Mar 22 21:10:58.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934492925s
Mar 22 21:10:59.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.090003ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7602 03/22/23 21:11:00.154
Mar 22 21:11:00.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:00.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 21:11:00.538: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 21:11:00.538: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 21:11:00.544: INFO: Found 1 stateful pods, waiting for 3
Mar 22 21:11:10.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:11:10.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:11:10.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/22/23 21:11:10.551
STEP: Scale down will halt with unhealthy stateful pod 03/22/23 21:11:10.551
Mar 22 21:11:10.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 21:11:10.931: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 21:11:10.931: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 21:11:10.931: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 21:11:10.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 21:11:11.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 21:11:11.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 21:11:11.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 21:11:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 22 21:11:11.524: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 22 21:11:11.524: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 22 21:11:11.524: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 22 21:11:11.524: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 21:11:11.529: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 22 21:11:21.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 21:11:21.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 21:11:21.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 22 21:11:21.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999572s
Mar 22 21:11:22.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994715271s
Mar 22 21:11:23.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987202616s
Mar 22 21:11:24.590: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97911102s
Mar 22 21:11:25.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966668864s
Mar 22 21:11:26.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960117093s
Mar 22 21:11:27.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951901723s
Mar 22 21:11:28.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939480624s
Mar 22 21:11:29.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929356054s
Mar 22 21:11:30.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.418276ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7602 03/22/23 21:11:31.642
Mar 22 21:11:31.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:32.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 21:11:32.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 21:11:32.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 21:11:32.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:32.341: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 22 21:11:32.341: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 22 21:11:32.341: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 22 21:11:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:32.597: INFO: rc: 1
Mar 22 21:11:32.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: failed to load task: no running task found: task 44d64cade71d3ab43a03e1cd0f2c8169faa93bf2c4c0cd98a4ee088d6a2aa4e8 not found: not found

error:
exit status 1
Mar 22 21:11:42.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:42.714: INFO: rc: 1
Mar 22 21:11:42.714: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:11:52.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:11:52.849: INFO: rc: 1
Mar 22 21:11:52.849: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:02.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:03.009: INFO: rc: 1
Mar 22 21:12:03.009: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:13.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:13.167: INFO: rc: 1
Mar 22 21:12:13.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:23.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:23.322: INFO: rc: 1
Mar 22 21:12:23.322: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:33.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:33.507: INFO: rc: 1
Mar 22 21:12:33.507: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:43.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:43.723: INFO: rc: 1
Mar 22 21:12:43.723: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:12:53.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:12:53.886: INFO: rc: 1
Mar 22 21:12:53.886: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:03.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:04.088: INFO: rc: 1
Mar 22 21:13:04.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:14.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:14.220: INFO: rc: 1
Mar 22 21:13:14.220: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:24.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:24.352: INFO: rc: 1
Mar 22 21:13:24.352: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:34.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:34.489: INFO: rc: 1
Mar 22 21:13:34.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:44.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:44.639: INFO: rc: 1
Mar 22 21:13:44.639: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:13:54.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:13:54.771: INFO: rc: 1
Mar 22 21:13:54.771: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:04.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:05.016: INFO: rc: 1
Mar 22 21:14:05.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:15.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:15.158: INFO: rc: 1
Mar 22 21:14:15.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:25.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:25.303: INFO: rc: 1
Mar 22 21:14:25.303: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:35.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:35.451: INFO: rc: 1
Mar 22 21:14:35.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:45.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:45.592: INFO: rc: 1
Mar 22 21:14:45.593: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:14:55.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:14:55.741: INFO: rc: 1
Mar 22 21:14:55.741: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:05.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:05.887: INFO: rc: 1
Mar 22 21:15:05.887: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:15.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:16.095: INFO: rc: 1
Mar 22 21:15:16.096: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:26.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:26.272: INFO: rc: 1
Mar 22 21:15:26.272: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:36.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:36.383: INFO: rc: 1
Mar 22 21:15:36.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:46.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:46.525: INFO: rc: 1
Mar 22 21:15:46.525: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:15:56.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:15:56.687: INFO: rc: 1
Mar 22 21:15:56.687: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:16:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:16:06.852: INFO: rc: 1
Mar 22 21:16:06.852: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:16:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:16:17.006: INFO: rc: 1
Mar 22 21:16:17.006: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:16:27.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:16:27.161: INFO: rc: 1
Mar 22 21:16:27.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 22 21:16:37.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 22 21:16:37.278: INFO: rc: 1
Mar 22 21:16:37.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar 22 21:16:37.278: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/22/23 21:16:37.292
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 21:16:37.292: INFO: Deleting all statefulset in ns statefulset-7602
Mar 22 21:16:37.297: INFO: Scaling statefulset ss to 0
Mar 22 21:16:37.311: INFO: Waiting for statefulset status.replicas updated to 0
Mar 22 21:16:37.315: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:16:37.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7602" for this suite. 03/22/23 21:16:37.338
------------------------------
• [SLOW TEST] [367.750 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:10:29.595
    Mar 22 21:10:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 21:10:29.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:10:29.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:10:29.631
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7602 03/22/23 21:10:29.639
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/22/23 21:10:29.647
    STEP: Creating stateful set ss in namespace statefulset-7602 03/22/23 21:10:29.654
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7602 03/22/23 21:10:29.667
    Mar 22 21:10:29.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 22 21:10:39.682: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/22/23 21:10:39.682
    Mar 22 21:10:39.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 21:10:40.046: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 21:10:40.046: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 21:10:40.046: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 21:10:40.052: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 22 21:10:50.058: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 21:10:50.058: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 21:10:50.081: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999625s
    Mar 22 21:10:51.088: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99468003s
    Mar 22 21:10:52.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988208981s
    Mar 22 21:10:53.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981630449s
    Mar 22 21:10:54.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.968175707s
    Mar 22 21:10:55.121: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961423187s
    Mar 22 21:10:56.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.946688155s
    Mar 22 21:10:57.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.939914607s
    Mar 22 21:10:58.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934492925s
    Mar 22 21:10:59.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.090003ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7602 03/22/23 21:11:00.154
    Mar 22 21:11:00.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:00.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 21:11:00.538: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 21:11:00.538: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 21:11:00.544: INFO: Found 1 stateful pods, waiting for 3
    Mar 22 21:11:10.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:11:10.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:11:10.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/22/23 21:11:10.551
    STEP: Scale down will halt with unhealthy stateful pod 03/22/23 21:11:10.551
    Mar 22 21:11:10.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 21:11:10.931: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 21:11:10.931: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 21:11:10.931: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 21:11:10.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 21:11:11.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 21:11:11.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 21:11:11.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 21:11:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 22 21:11:11.524: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 22 21:11:11.524: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 22 21:11:11.524: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 22 21:11:11.524: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 21:11:11.529: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Mar 22 21:11:21.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 21:11:21.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 21:11:21.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 22 21:11:21.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999572s
    Mar 22 21:11:22.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994715271s
    Mar 22 21:11:23.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987202616s
    Mar 22 21:11:24.590: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97911102s
    Mar 22 21:11:25.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966668864s
    Mar 22 21:11:26.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960117093s
    Mar 22 21:11:27.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951901723s
    Mar 22 21:11:28.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939480624s
    Mar 22 21:11:29.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929356054s
    Mar 22 21:11:30.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.418276ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7602 03/22/23 21:11:31.642
    Mar 22 21:11:31.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:32.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 21:11:32.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 21:11:32.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 21:11:32.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:32.341: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 22 21:11:32.341: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 22 21:11:32.341: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 22 21:11:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:32.597: INFO: rc: 1
    Mar 22 21:11:32.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: failed to exec in container: failed to load task: no running task found: task 44d64cade71d3ab43a03e1cd0f2c8169faa93bf2c4c0cd98a4ee088d6a2aa4e8 not found: not found

    error:
    exit status 1
    Mar 22 21:11:42.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:42.714: INFO: rc: 1
    Mar 22 21:11:42.714: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:11:52.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:11:52.849: INFO: rc: 1
    Mar 22 21:11:52.849: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:02.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:03.009: INFO: rc: 1
    Mar 22 21:12:03.009: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:13.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:13.167: INFO: rc: 1
    Mar 22 21:12:13.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:23.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:23.322: INFO: rc: 1
    Mar 22 21:12:23.322: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:33.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:33.507: INFO: rc: 1
    Mar 22 21:12:33.507: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:43.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:43.723: INFO: rc: 1
    Mar 22 21:12:43.723: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:12:53.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:12:53.886: INFO: rc: 1
    Mar 22 21:12:53.886: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:03.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:04.088: INFO: rc: 1
    Mar 22 21:13:04.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:14.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:14.220: INFO: rc: 1
    Mar 22 21:13:14.220: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:24.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:24.352: INFO: rc: 1
    Mar 22 21:13:24.352: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:34.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:34.489: INFO: rc: 1
    Mar 22 21:13:34.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:44.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:44.639: INFO: rc: 1
    Mar 22 21:13:44.639: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:13:54.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:13:54.771: INFO: rc: 1
    Mar 22 21:13:54.771: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:04.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:05.016: INFO: rc: 1
    Mar 22 21:14:05.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:15.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:15.158: INFO: rc: 1
    Mar 22 21:14:15.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:25.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:25.303: INFO: rc: 1
    Mar 22 21:14:25.303: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:35.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:35.451: INFO: rc: 1
    Mar 22 21:14:35.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:45.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:45.592: INFO: rc: 1
    Mar 22 21:14:45.593: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:14:55.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:14:55.741: INFO: rc: 1
    Mar 22 21:14:55.741: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:05.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:05.887: INFO: rc: 1
    Mar 22 21:15:05.887: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:15.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:16.095: INFO: rc: 1
    Mar 22 21:15:16.096: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:26.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:26.272: INFO: rc: 1
    Mar 22 21:15:26.272: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:36.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:36.383: INFO: rc: 1
    Mar 22 21:15:36.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:46.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:46.525: INFO: rc: 1
    Mar 22 21:15:46.525: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:15:56.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:15:56.687: INFO: rc: 1
    Mar 22 21:15:56.687: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:16:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:16:06.852: INFO: rc: 1
    Mar 22 21:16:06.852: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:16:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:16:17.006: INFO: rc: 1
    Mar 22 21:16:17.006: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:16:27.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:16:27.161: INFO: rc: 1
    Mar 22 21:16:27.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Mar 22 21:16:37.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=statefulset-7602 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 22 21:16:37.278: INFO: rc: 1
    Mar 22 21:16:37.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Mar 22 21:16:37.278: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/22/23 21:16:37.292
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 21:16:37.292: INFO: Deleting all statefulset in ns statefulset-7602
    Mar 22 21:16:37.297: INFO: Scaling statefulset ss to 0
    Mar 22 21:16:37.311: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 22 21:16:37.315: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:16:37.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7602" for this suite. 03/22/23 21:16:37.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:16:37.346
Mar 22 21:16:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 21:16:37.348
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:37.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:37.371
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/22/23 21:16:37.383
Mar 22 21:16:37.383: INFO: Creating simple deployment test-deployment-lzjnb
Mar 22 21:16:37.402: INFO: deployment "test-deployment-lzjnb" doesn't have the required revision set
STEP: Getting /status 03/22/23 21:16:39.421
Mar 22 21:16:39.426: INFO: Deployment test-deployment-lzjnb has Conditions: [{Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 03/22/23 21:16:39.426
Mar 22 21:16:39.443: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 16, 37, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lzjnb-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/22/23 21:16:39.443
Mar 22 21:16:39.446: INFO: Observed &Deployment event: ADDED
Mar 22 21:16:39.446: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
Mar 22 21:16:39.447: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 22 21:16:39.447: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lzjnb-54bc444df" is progressing.}
Mar 22 21:16:39.448: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.448: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 22 21:16:39.448: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
Mar 22 21:16:39.448: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.449: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 22 21:16:39.449: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
Mar 22 21:16:39.449: INFO: Found Deployment test-deployment-lzjnb in namespace deployment-2956 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 22 21:16:39.449: INFO: Deployment test-deployment-lzjnb has an updated status
STEP: patching the Statefulset Status 03/22/23 21:16:39.449
Mar 22 21:16:39.449: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 22 21:16:39.456: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/22/23 21:16:39.456
Mar 22 21:16:39.460: INFO: Observed &Deployment event: ADDED
Mar 22 21:16:39.460: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
Mar 22 21:16:39.461: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.461: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
Mar 22 21:16:39.461: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 22 21:16:39.462: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.462: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 22 21:16:39.462: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lzjnb-54bc444df" is progressing.}
Mar 22 21:16:39.462: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.463: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 22 21:16:39.463: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
Mar 22 21:16:39.463: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 22 21:16:39.464: INFO: Observed &Deployment event: MODIFIED
Mar 22 21:16:39.464: INFO: Found deployment test-deployment-lzjnb in namespace deployment-2956 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 22 21:16:39.465: INFO: Deployment test-deployment-lzjnb has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 21:16:39.469: INFO: Deployment "test-deployment-lzjnb":
&Deployment{ObjectMeta:{test-deployment-lzjnb  deployment-2956  326d8b3e-d888-416c-90da-ec0ad2454d79 38638 1 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-22 21:16:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-22 21:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0016b3108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lzjnb-54bc444df",LastUpdateTime:2023-03-22 21:16:39 +0000 UTC,LastTransitionTime:2023-03-22 21:16:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 21:16:39.473: INFO: New ReplicaSet "test-deployment-lzjnb-54bc444df" of Deployment "test-deployment-lzjnb":
&ReplicaSet{ObjectMeta:{test-deployment-lzjnb-54bc444df  deployment-2956  007a9fb4-6138-4a9c-8449-d741b02352a2 38631 1 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lzjnb 326d8b3e-d888-416c-90da-ec0ad2454d79 0xc003f028d0 0xc003f028d1}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"326d8b3e-d888-416c-90da-ec0ad2454d79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:16:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f02978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 22 21:16:39.478: INFO: Pod "test-deployment-lzjnb-54bc444df-g5grk" is available:
&Pod{ObjectMeta:{test-deployment-lzjnb-54bc444df-g5grk test-deployment-lzjnb-54bc444df- deployment-2956  08098d47-178f-4c90-89fc-cd65abfe5381 38630 0 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-lzjnb-54bc444df 007a9fb4-6138-4a9c-8449-d741b02352a2 0xc003f02d20 0xc003f02d21}] [] [{kube-controller-manager Update v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"007a9fb4-6138-4a9c-8449-d741b02352a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:16:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kldzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kldzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.136,StartTime:2023-03-22 21:16:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:16:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bc123a5be1d76ec789a67f2359b6e855050c2e6e7a3e3eb675d5ae26c46c7280,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 21:16:39.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2956" for this suite. 03/22/23 21:16:39.484
------------------------------
• [2.145 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:16:37.346
    Mar 22 21:16:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 21:16:37.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:37.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:37.371
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/22/23 21:16:37.383
    Mar 22 21:16:37.383: INFO: Creating simple deployment test-deployment-lzjnb
    Mar 22 21:16:37.402: INFO: deployment "test-deployment-lzjnb" doesn't have the required revision set
    STEP: Getting /status 03/22/23 21:16:39.421
    Mar 22 21:16:39.426: INFO: Deployment test-deployment-lzjnb has Conditions: [{Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 03/22/23 21:16:39.426
    Mar 22 21:16:39.443: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 16, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 16, 37, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lzjnb-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/22/23 21:16:39.443
    Mar 22 21:16:39.446: INFO: Observed &Deployment event: ADDED
    Mar 22 21:16:39.446: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
    Mar 22 21:16:39.447: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
    Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 22 21:16:39.447: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 22 21:16:39.447: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lzjnb-54bc444df" is progressing.}
    Mar 22 21:16:39.448: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.448: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 22 21:16:39.448: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
    Mar 22 21:16:39.448: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.449: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 22 21:16:39.449: INFO: Observed Deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
    Mar 22 21:16:39.449: INFO: Found Deployment test-deployment-lzjnb in namespace deployment-2956 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 22 21:16:39.449: INFO: Deployment test-deployment-lzjnb has an updated status
    STEP: patching the Statefulset Status 03/22/23 21:16:39.449
    Mar 22 21:16:39.449: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 22 21:16:39.456: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/22/23 21:16:39.456
    Mar 22 21:16:39.460: INFO: Observed &Deployment event: ADDED
    Mar 22 21:16:39.460: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
    Mar 22 21:16:39.461: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.461: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lzjnb-54bc444df"}
    Mar 22 21:16:39.461: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 22 21:16:39.462: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.462: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 22 21:16:39.462: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:37 +0000 UTC 2023-03-22 21:16:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lzjnb-54bc444df" is progressing.}
    Mar 22 21:16:39.462: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.463: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 22 21:16:39.463: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
    Mar 22 21:16:39.463: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-22 21:16:38 +0000 UTC 2023-03-22 21:16:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lzjnb-54bc444df" has successfully progressed.}
    Mar 22 21:16:39.464: INFO: Observed deployment test-deployment-lzjnb in namespace deployment-2956 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 22 21:16:39.464: INFO: Observed &Deployment event: MODIFIED
    Mar 22 21:16:39.464: INFO: Found deployment test-deployment-lzjnb in namespace deployment-2956 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 22 21:16:39.465: INFO: Deployment test-deployment-lzjnb has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 21:16:39.469: INFO: Deployment "test-deployment-lzjnb":
    &Deployment{ObjectMeta:{test-deployment-lzjnb  deployment-2956  326d8b3e-d888-416c-90da-ec0ad2454d79 38638 1 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-22 21:16:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-22 21:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0016b3108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lzjnb-54bc444df",LastUpdateTime:2023-03-22 21:16:39 +0000 UTC,LastTransitionTime:2023-03-22 21:16:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 22 21:16:39.473: INFO: New ReplicaSet "test-deployment-lzjnb-54bc444df" of Deployment "test-deployment-lzjnb":
    &ReplicaSet{ObjectMeta:{test-deployment-lzjnb-54bc444df  deployment-2956  007a9fb4-6138-4a9c-8449-d741b02352a2 38631 1 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lzjnb 326d8b3e-d888-416c-90da-ec0ad2454d79 0xc003f028d0 0xc003f028d1}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"326d8b3e-d888-416c-90da-ec0ad2454d79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:16:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f02978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 21:16:39.478: INFO: Pod "test-deployment-lzjnb-54bc444df-g5grk" is available:
    &Pod{ObjectMeta:{test-deployment-lzjnb-54bc444df-g5grk test-deployment-lzjnb-54bc444df- deployment-2956  08098d47-178f-4c90-89fc-cd65abfe5381 38630 0 2023-03-22 21:16:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-lzjnb-54bc444df 007a9fb4-6138-4a9c-8449-d741b02352a2 0xc003f02d20 0xc003f02d21}] [] [{kube-controller-manager Update v1 2023-03-22 21:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"007a9fb4-6138-4a9c-8449-d741b02352a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:16:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kldzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kldzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:16:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.136,StartTime:2023-03-22 21:16:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:16:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bc123a5be1d76ec789a67f2359b6e855050c2e6e7a3e3eb675d5ae26c46c7280,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:16:39.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2956" for this suite. 03/22/23 21:16:39.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:16:39.495
Mar 22 21:16:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:16:39.496
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:39.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:39.516
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 03/22/23 21:16:39.525
Mar 22 21:16:39.535: INFO: Waiting up to 5m0s for pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67" in namespace "projected-4072" to be "running and ready"
Mar 22 21:16:39.540: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.34501ms
Mar 22 21:16:39.541: INFO: The phase of Pod labelsupdatefa9fb293-0487-4f99-8924-850c21594d67 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:16:41.548: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67": Phase="Running", Reason="", readiness=true. Elapsed: 2.012779065s
Mar 22 21:16:41.548: INFO: The phase of Pod labelsupdatefa9fb293-0487-4f99-8924-850c21594d67 is Running (Ready = true)
Mar 22 21:16:41.548: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67" satisfied condition "running and ready"
Mar 22 21:16:42.105: INFO: Successfully updated pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 21:16:46.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4072" for this suite. 03/22/23 21:16:46.158
------------------------------
• [SLOW TEST] [6.671 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:16:39.495
    Mar 22 21:16:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:16:39.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:39.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:39.516
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 03/22/23 21:16:39.525
    Mar 22 21:16:39.535: INFO: Waiting up to 5m0s for pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67" in namespace "projected-4072" to be "running and ready"
    Mar 22 21:16:39.540: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.34501ms
    Mar 22 21:16:39.541: INFO: The phase of Pod labelsupdatefa9fb293-0487-4f99-8924-850c21594d67 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:16:41.548: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67": Phase="Running", Reason="", readiness=true. Elapsed: 2.012779065s
    Mar 22 21:16:41.548: INFO: The phase of Pod labelsupdatefa9fb293-0487-4f99-8924-850c21594d67 is Running (Ready = true)
    Mar 22 21:16:41.548: INFO: Pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67" satisfied condition "running and ready"
    Mar 22 21:16:42.105: INFO: Successfully updated pod "labelsupdatefa9fb293-0487-4f99-8924-850c21594d67"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:16:46.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4072" for this suite. 03/22/23 21:16:46.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:16:46.168
Mar 22 21:16:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:16:46.17
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:46.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:46.191
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-6e68884d-58cd-4364-a8d0-285dd696395f 03/22/23 21:16:46.198
STEP: Creating a pod to test consume configMaps 03/22/23 21:16:46.204
Mar 22 21:16:46.215: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7" in namespace "configmap-7292" to be "Succeeded or Failed"
Mar 22 21:16:46.220: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373406ms
Mar 22 21:16:48.225: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009901946s
Mar 22 21:16:50.226: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010230689s
STEP: Saw pod success 03/22/23 21:16:50.226
Mar 22 21:16:50.226: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7" satisfied condition "Succeeded or Failed"
Mar 22 21:16:50.231: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 container configmap-volume-test: <nil>
STEP: delete the pod 03/22/23 21:16:50.242
Mar 22 21:16:50.256: INFO: Waiting for pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 to disappear
Mar 22 21:16:50.261: INFO: Pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:16:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7292" for this suite. 03/22/23 21:16:50.267
------------------------------
• [4.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:16:46.168
    Mar 22 21:16:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:16:46.17
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:46.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:46.191
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-6e68884d-58cd-4364-a8d0-285dd696395f 03/22/23 21:16:46.198
    STEP: Creating a pod to test consume configMaps 03/22/23 21:16:46.204
    Mar 22 21:16:46.215: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7" in namespace "configmap-7292" to be "Succeeded or Failed"
    Mar 22 21:16:46.220: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373406ms
    Mar 22 21:16:48.225: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009901946s
    Mar 22 21:16:50.226: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010230689s
    STEP: Saw pod success 03/22/23 21:16:50.226
    Mar 22 21:16:50.226: INFO: Pod "pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7" satisfied condition "Succeeded or Failed"
    Mar 22 21:16:50.231: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 container configmap-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:16:50.242
    Mar 22 21:16:50.256: INFO: Waiting for pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 to disappear
    Mar 22 21:16:50.261: INFO: Pod pod-configmaps-a3013de4-fb76-4df4-aee6-95e50f4fe2d7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:16:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7292" for this suite. 03/22/23 21:16:50.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:16:50.288
Mar 22 21:16:50.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:16:50.289
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:50.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:50.315
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:16:50.322
Mar 22 21:16:50.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d" in namespace "downward-api-8272" to be "Succeeded or Failed"
Mar 22 21:16:50.337: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.923602ms
Mar 22 21:16:52.343: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010194135s
Mar 22 21:16:54.347: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014057719s
STEP: Saw pod success 03/22/23 21:16:54.347
Mar 22 21:16:54.347: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d" satisfied condition "Succeeded or Failed"
Mar 22 21:16:54.352: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d container client-container: <nil>
STEP: delete the pod 03/22/23 21:16:54.362
Mar 22 21:16:54.377: INFO: Waiting for pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d to disappear
Mar 22 21:16:54.381: INFO: Pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 21:16:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8272" for this suite. 03/22/23 21:16:54.39
------------------------------
• [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:16:50.288
    Mar 22 21:16:50.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:16:50.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:50.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:50.315
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:16:50.322
    Mar 22 21:16:50.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d" in namespace "downward-api-8272" to be "Succeeded or Failed"
    Mar 22 21:16:50.337: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.923602ms
    Mar 22 21:16:52.343: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010194135s
    Mar 22 21:16:54.347: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014057719s
    STEP: Saw pod success 03/22/23 21:16:54.347
    Mar 22 21:16:54.347: INFO: Pod "downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d" satisfied condition "Succeeded or Failed"
    Mar 22 21:16:54.352: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d container client-container: <nil>
    STEP: delete the pod 03/22/23 21:16:54.362
    Mar 22 21:16:54.377: INFO: Waiting for pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d to disappear
    Mar 22 21:16:54.381: INFO: Pod downwardapi-volume-0b407dff-b3ed-493c-be37-b9b86906464d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:16:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8272" for this suite. 03/22/23 21:16:54.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:16:54.401
Mar 22 21:16:54.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 21:16:54.403
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:54.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:54.434
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 03/22/23 21:16:54.445
STEP: waiting for pod running 03/22/23 21:16:54.456
Mar 22 21:16:54.457: INFO: Waiting up to 2m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910" to be "running"
Mar 22 21:16:54.463: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662924ms
Mar 22 21:16:56.478: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Running", Reason="", readiness=true. Elapsed: 2.021280326s
Mar 22 21:16:56.478: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" satisfied condition "running"
STEP: creating a file in subpath 03/22/23 21:16:56.478
Mar 22 21:16:56.482: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3910 PodName:var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:16:56.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:16:56.482: INFO: ExecWithOptions: Clientset creation
Mar 22 21:16:56.483: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/var-expansion-3910/pods/var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/22/23 21:16:56.631
Mar 22 21:16:56.637: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3910 PodName:var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:16:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:16:56.638: INFO: ExecWithOptions: Clientset creation
Mar 22 21:16:56.638: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/var-expansion-3910/pods/var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/22/23 21:16:56.805
Mar 22 21:16:57.336: INFO: Successfully updated pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672"
STEP: waiting for annotated pod running 03/22/23 21:16:57.336
Mar 22 21:16:57.337: INFO: Waiting up to 2m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910" to be "running"
Mar 22 21:16:57.341: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Running", Reason="", readiness=true. Elapsed: 4.642291ms
Mar 22 21:16:57.341: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" satisfied condition "running"
STEP: deleting the pod gracefully 03/22/23 21:16:57.341
Mar 22 21:16:57.342: INFO: Deleting pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910"
Mar 22 21:16:57.351: INFO: Wait up to 5m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 21:17:31.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3910" for this suite. 03/22/23 21:17:31.379
------------------------------
• [SLOW TEST] [36.985 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:16:54.401
    Mar 22 21:16:54.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 21:16:54.403
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:16:54.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:16:54.434
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 03/22/23 21:16:54.445
    STEP: waiting for pod running 03/22/23 21:16:54.456
    Mar 22 21:16:54.457: INFO: Waiting up to 2m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910" to be "running"
    Mar 22 21:16:54.463: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662924ms
    Mar 22 21:16:56.478: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Running", Reason="", readiness=true. Elapsed: 2.021280326s
    Mar 22 21:16:56.478: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" satisfied condition "running"
    STEP: creating a file in subpath 03/22/23 21:16:56.478
    Mar 22 21:16:56.482: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3910 PodName:var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:16:56.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:16:56.482: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:16:56.483: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/var-expansion-3910/pods/var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/22/23 21:16:56.631
    Mar 22 21:16:56.637: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3910 PodName:var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:16:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:16:56.638: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:16:56.638: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/var-expansion-3910/pods/var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/22/23 21:16:56.805
    Mar 22 21:16:57.336: INFO: Successfully updated pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672"
    STEP: waiting for annotated pod running 03/22/23 21:16:57.336
    Mar 22 21:16:57.337: INFO: Waiting up to 2m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910" to be "running"
    Mar 22 21:16:57.341: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672": Phase="Running", Reason="", readiness=true. Elapsed: 4.642291ms
    Mar 22 21:16:57.341: INFO: Pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" satisfied condition "running"
    STEP: deleting the pod gracefully 03/22/23 21:16:57.341
    Mar 22 21:16:57.342: INFO: Deleting pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" in namespace "var-expansion-3910"
    Mar 22 21:16:57.351: INFO: Wait up to 5m0s for pod "var-expansion-d477033b-543f-43c2-8b1f-58fbd63d6672" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:17:31.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3910" for this suite. 03/22/23 21:17:31.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:17:31.388
Mar 22 21:17:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:17:31.389
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:31.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:31.411
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Mar 22 21:17:31.436: INFO: Waiting up to 5m0s for pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424" in namespace "svcaccounts-7726" to be "running"
Mar 22 21:17:31.442: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424": Phase="Pending", Reason="", readiness=false. Elapsed: 6.317107ms
Mar 22 21:17:33.450: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424": Phase="Running", Reason="", readiness=true. Elapsed: 2.013842683s
Mar 22 21:17:33.450: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424" satisfied condition "running"
STEP: reading a file in the container 03/22/23 21:17:33.45
Mar 22 21:17:33.450: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/22/23 21:17:33.753
Mar 22 21:17:33.753: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/22/23 21:17:34.002
Mar 22 21:17:34.003: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 22 21:17:34.264: INFO: Got root ca configmap in namespace "svcaccounts-7726"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 21:17:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7726" for this suite. 03/22/23 21:17:34.274
------------------------------
• [2.898 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:17:31.388
    Mar 22 21:17:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:17:31.389
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:31.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:31.411
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Mar 22 21:17:31.436: INFO: Waiting up to 5m0s for pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424" in namespace "svcaccounts-7726" to be "running"
    Mar 22 21:17:31.442: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424": Phase="Pending", Reason="", readiness=false. Elapsed: 6.317107ms
    Mar 22 21:17:33.450: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424": Phase="Running", Reason="", readiness=true. Elapsed: 2.013842683s
    Mar 22 21:17:33.450: INFO: Pod "pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424" satisfied condition "running"
    STEP: reading a file in the container 03/22/23 21:17:33.45
    Mar 22 21:17:33.450: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/22/23 21:17:33.753
    Mar 22 21:17:33.753: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/22/23 21:17:34.002
    Mar 22 21:17:34.003: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7726 pod-service-account-a5aa8fda-f570-4968-9c91-bfb3d1450424 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 22 21:17:34.264: INFO: Got root ca configmap in namespace "svcaccounts-7726"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:17:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7726" for this suite. 03/22/23 21:17:34.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:17:34.29
Mar 22 21:17:34.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename dns 03/22/23 21:17:34.291
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:34.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:34.311
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/22/23 21:17:34.317
Mar 22 21:17:34.328: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4653  e1581b3f-cd10-475b-98ca-3b7c946dcefa 39074 0 2023-03-22 21:17:34 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-22 21:17:34 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vl8cd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vl8cd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:17:34.329: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4653" to be "running and ready"
Mar 22 21:17:34.333: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.421346ms
Mar 22 21:17:34.333: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:17:36.341: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012683215s
Mar 22 21:17:36.341: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 22 21:17:36.341: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/22/23 21:17:36.342
Mar 22 21:17:36.342: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4653 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:17:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:17:36.343: INFO: ExecWithOptions: Clientset creation
Mar 22 21:17:36.343: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/dns-4653/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/22/23 21:17:36.521
Mar 22 21:17:36.521: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4653 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:17:36.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:17:36.522: INFO: ExecWithOptions: Clientset creation
Mar 22 21:17:36.522: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/dns-4653/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:17:36.703: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 22 21:17:36.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4653" for this suite. 03/22/23 21:17:36.722
------------------------------
• [2.439 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:17:34.29
    Mar 22 21:17:34.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename dns 03/22/23 21:17:34.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:34.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:34.311
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/22/23 21:17:34.317
    Mar 22 21:17:34.328: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4653  e1581b3f-cd10-475b-98ca-3b7c946dcefa 39074 0 2023-03-22 21:17:34 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-22 21:17:34 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vl8cd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vl8cd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:17:34.329: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4653" to be "running and ready"
    Mar 22 21:17:34.333: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.421346ms
    Mar 22 21:17:34.333: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:17:36.341: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012683215s
    Mar 22 21:17:36.341: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 22 21:17:36.341: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/22/23 21:17:36.342
    Mar 22 21:17:36.342: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4653 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:17:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:17:36.343: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:17:36.343: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/dns-4653/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/22/23 21:17:36.521
    Mar 22 21:17:36.521: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4653 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:17:36.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:17:36.522: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:17:36.522: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/dns-4653/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:17:36.703: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:17:36.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4653" for this suite. 03/22/23 21:17:36.722
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:17:36.729
Mar 22 21:17:36.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:17:36.73
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:36.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:36.749
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/22/23 21:17:36.757
Mar 22 21:17:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/22/23 21:17:43.139
Mar 22 21:17:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:17:44.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:17:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8045" for this suite. 03/22/23 21:17:51.65
------------------------------
• [SLOW TEST] [14.931 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:17:36.729
    Mar 22 21:17:36.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:17:36.73
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:36.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:36.749
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/22/23 21:17:36.757
    Mar 22 21:17:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/22/23 21:17:43.139
    Mar 22 21:17:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:17:44.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:17:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8045" for this suite. 03/22/23 21:17:51.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:17:51.662
Mar 22 21:17:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:17:51.665
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:51.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:51.687
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:17:51.692
Mar 22 21:17:51.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3" in namespace "downward-api-2861" to be "Succeeded or Failed"
Mar 22 21:17:51.706: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.94506ms
Mar 22 21:17:53.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012075646s
Mar 22 21:17:55.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011136862s
STEP: Saw pod success 03/22/23 21:17:55.712
Mar 22 21:17:55.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3" satisfied condition "Succeeded or Failed"
Mar 22 21:17:55.716: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 container client-container: <nil>
STEP: delete the pod 03/22/23 21:17:55.733
Mar 22 21:17:55.746: INFO: Waiting for pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 to disappear
Mar 22 21:17:55.750: INFO: Pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 21:17:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2861" for this suite. 03/22/23 21:17:55.757
------------------------------
• [4.102 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:17:51.662
    Mar 22 21:17:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:17:51.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:51.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:51.687
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:17:51.692
    Mar 22 21:17:51.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3" in namespace "downward-api-2861" to be "Succeeded or Failed"
    Mar 22 21:17:51.706: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.94506ms
    Mar 22 21:17:53.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012075646s
    Mar 22 21:17:55.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011136862s
    STEP: Saw pod success 03/22/23 21:17:55.712
    Mar 22 21:17:55.712: INFO: Pod "downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3" satisfied condition "Succeeded or Failed"
    Mar 22 21:17:55.716: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:17:55.733
    Mar 22 21:17:55.746: INFO: Waiting for pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 to disappear
    Mar 22 21:17:55.750: INFO: Pod downwardapi-volume-17be5aa5-9f00-4f18-a275-3f0caa203bd3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:17:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2861" for this suite. 03/22/23 21:17:55.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:17:55.778
Mar 22 21:17:55.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:17:55.779
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:55.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:55.805
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/22/23 21:17:55.812
Mar 22 21:17:55.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:17:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:03.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2018" for this suite. 03/22/23 21:18:03.77
------------------------------
• [SLOW TEST] [8.004 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:17:55.778
    Mar 22 21:17:55.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:17:55.779
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:17:55.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:17:55.805
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/22/23 21:17:55.812
    Mar 22 21:17:55.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:17:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:03.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2018" for this suite. 03/22/23 21:18:03.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:03.793
Mar 22 21:18:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubelet-test 03/22/23 21:18:03.796
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:03.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:03.819
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:03.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2273" for this suite. 03/22/23 21:18:03.851
------------------------------
• [0.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:03.793
    Mar 22 21:18:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubelet-test 03/22/23 21:18:03.796
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:03.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:03.819
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:03.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2273" for this suite. 03/22/23 21:18:03.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:03.859
Mar 22 21:18:03.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:18:03.861
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:03.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:03.883
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 03/22/23 21:18:03.889
Mar 22 21:18:03.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 create -f -'
Mar 22 21:18:04.574: INFO: stderr: ""
Mar 22 21:18:04.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:18:04.574
Mar 22 21:18:04.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:18:04.679: INFO: stderr: ""
Mar 22 21:18:04.679: INFO: stdout: "update-demo-nautilus-2lcp5 update-demo-nautilus-4dn9s "
Mar 22 21:18:04.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:18:04.785: INFO: stderr: ""
Mar 22 21:18:04.785: INFO: stdout: ""
Mar 22 21:18:04.785: INFO: update-demo-nautilus-2lcp5 is created but not running
Mar 22 21:18:09.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:18:09.900: INFO: stderr: ""
Mar 22 21:18:09.900: INFO: stdout: "update-demo-nautilus-2lcp5 update-demo-nautilus-4dn9s "
Mar 22 21:18:09.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:18:10.062: INFO: stderr: ""
Mar 22 21:18:10.062: INFO: stdout: "true"
Mar 22 21:18:10.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:18:10.177: INFO: stderr: ""
Mar 22 21:18:10.177: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:18:10.177: INFO: validating pod update-demo-nautilus-2lcp5
Mar 22 21:18:10.227: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:18:10.227: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:18:10.227: INFO: update-demo-nautilus-2lcp5 is verified up and running
Mar 22 21:18:10.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-4dn9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:18:10.336: INFO: stderr: ""
Mar 22 21:18:10.336: INFO: stdout: "true"
Mar 22 21:18:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-4dn9s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:18:10.459: INFO: stderr: ""
Mar 22 21:18:10.459: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:18:10.459: INFO: validating pod update-demo-nautilus-4dn9s
Mar 22 21:18:10.498: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:18:10.498: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:18:10.498: INFO: update-demo-nautilus-4dn9s is verified up and running
STEP: using delete to clean up resources 03/22/23 21:18:10.498
Mar 22 21:18:10.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 delete --grace-period=0 --force -f -'
Mar 22 21:18:10.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:18:10.651: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 22 21:18:10.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get rc,svc -l name=update-demo --no-headers'
Mar 22 21:18:10.768: INFO: stderr: "No resources found in kubectl-6466 namespace.\n"
Mar 22 21:18:10.768: INFO: stdout: ""
Mar 22 21:18:10.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 21:18:10.930: INFO: stderr: ""
Mar 22 21:18:10.930: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:10.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6466" for this suite. 03/22/23 21:18:10.937
------------------------------
• [SLOW TEST] [7.087 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:03.859
    Mar 22 21:18:03.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:18:03.861
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:03.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:03.883
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 03/22/23 21:18:03.889
    Mar 22 21:18:03.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 create -f -'
    Mar 22 21:18:04.574: INFO: stderr: ""
    Mar 22 21:18:04.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:18:04.574
    Mar 22 21:18:04.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:18:04.679: INFO: stderr: ""
    Mar 22 21:18:04.679: INFO: stdout: "update-demo-nautilus-2lcp5 update-demo-nautilus-4dn9s "
    Mar 22 21:18:04.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:18:04.785: INFO: stderr: ""
    Mar 22 21:18:04.785: INFO: stdout: ""
    Mar 22 21:18:04.785: INFO: update-demo-nautilus-2lcp5 is created but not running
    Mar 22 21:18:09.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:18:09.900: INFO: stderr: ""
    Mar 22 21:18:09.900: INFO: stdout: "update-demo-nautilus-2lcp5 update-demo-nautilus-4dn9s "
    Mar 22 21:18:09.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:18:10.062: INFO: stderr: ""
    Mar 22 21:18:10.062: INFO: stdout: "true"
    Mar 22 21:18:10.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-2lcp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:18:10.177: INFO: stderr: ""
    Mar 22 21:18:10.177: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:18:10.177: INFO: validating pod update-demo-nautilus-2lcp5
    Mar 22 21:18:10.227: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:18:10.227: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:18:10.227: INFO: update-demo-nautilus-2lcp5 is verified up and running
    Mar 22 21:18:10.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-4dn9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:18:10.336: INFO: stderr: ""
    Mar 22 21:18:10.336: INFO: stdout: "true"
    Mar 22 21:18:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods update-demo-nautilus-4dn9s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:18:10.459: INFO: stderr: ""
    Mar 22 21:18:10.459: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:18:10.459: INFO: validating pod update-demo-nautilus-4dn9s
    Mar 22 21:18:10.498: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:18:10.498: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:18:10.498: INFO: update-demo-nautilus-4dn9s is verified up and running
    STEP: using delete to clean up resources 03/22/23 21:18:10.498
    Mar 22 21:18:10.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 delete --grace-period=0 --force -f -'
    Mar 22 21:18:10.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:18:10.651: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 22 21:18:10.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get rc,svc -l name=update-demo --no-headers'
    Mar 22 21:18:10.768: INFO: stderr: "No resources found in kubectl-6466 namespace.\n"
    Mar 22 21:18:10.768: INFO: stdout: ""
    Mar 22 21:18:10.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-6466 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 22 21:18:10.930: INFO: stderr: ""
    Mar 22 21:18:10.930: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:10.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6466" for this suite. 03/22/23 21:18:10.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:10.949
Mar 22 21:18:10.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-pred 03/22/23 21:18:10.95
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:10.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:10.973
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 22 21:18:10.979: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 21:18:10.991: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 21:18:10.995: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
Mar 22 21:18:11.004: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.004: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:18:11.005: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.005: INFO: 	Container coredns ready: true, restart count 0
Mar 22 21:18:11.005: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.005: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:18:11.005: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.005: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:18:11.005: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:18:11.006: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.006: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:18:11.006: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.006: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:18:11.006: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.006: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:18:11.006: INFO: update-demo-nautilus-2lcp5 from kubectl-6466 started at 2023-03-22 21:18:04 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.006: INFO: 	Container update-demo ready: true, restart count 0
Mar 22 21:18:11.006: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.006: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 21:18:11.007: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.007: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:18:11.007: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 21:18:11.007: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
Mar 22 21:18:11.015: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:18:11.015: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 22 21:18:11.015: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container coredns ready: true, restart count 0
Mar 22 21:18:11.015: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:18:11.015: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:18:11.015: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:18:11.015: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:18:11.015: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:18:11.015: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:18:11.015: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container e2e ready: true, restart count 0
Mar 22 21:18:11.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:18:11.015: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:18:11.015: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 21:18:11.015: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
Mar 22 21:18:11.026: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:18:11.026: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:18:11.026: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:18:11.026: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:18:11.026: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:18:11.026: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:18:11.026: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:18:11.026: INFO: update-demo-nautilus-4dn9s from kubectl-6466 started at 2023-03-22 21:18:04 +0000 UTC (1 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container update-demo ready: true, restart count 0
Mar 22 21:18:11.026: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:18:11.026: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:18:11.026: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/22/23 21:18:11.027
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174eda71443a2659], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 03/22/23 21:18:11.076
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:12.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-544" for this suite. 03/22/23 21:18:12.078
------------------------------
• [1.141 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:10.949
    Mar 22 21:18:10.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-pred 03/22/23 21:18:10.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:10.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:10.973
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 22 21:18:10.979: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 22 21:18:10.991: INFO: Waiting for terminating namespaces to be deleted...
    Mar 22 21:18:10.995: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
    Mar 22 21:18:11.004: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.004: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:18:11.005: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.005: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 21:18:11.005: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.005: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:18:11.005: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.005: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:18:11.005: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:18:11.006: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.006: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:18:11.006: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.006: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:18:11.006: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.006: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:18:11.006: INFO: update-demo-nautilus-2lcp5 from kubectl-6466 started at 2023-03-22 21:18:04 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.006: INFO: 	Container update-demo ready: true, restart count 0
    Mar 22 21:18:11.006: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.006: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 22 21:18:11.007: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.007: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:18:11.007: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 21:18:11.007: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
    Mar 22 21:18:11.015: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container cilium-operator ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container e2e ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 21:18:11.015: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
    Mar 22 21:18:11.026: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: update-demo-nautilus-4dn9s from kubectl-6466 started at 2023-03-22 21:18:04 +0000 UTC (1 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container update-demo ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:18:11.026: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:18:11.026: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/22/23 21:18:11.027
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.174eda71443a2659], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 03/22/23 21:18:11.076
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:12.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-544" for this suite. 03/22/23 21:18:12.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:12.092
Mar 22 21:18:12.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 21:18:12.094
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:12.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:12.124
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/22/23 21:18:12.144
STEP: waiting for Deployment to be created 03/22/23 21:18:12.151
STEP: waiting for all Replicas to be Ready 03/22/23 21:18:12.154
Mar 22 21:18:12.157: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.158: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.170: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.218: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:12.218: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 22 21:18:13.789: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 22 21:18:13.789: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 22 21:18:14.124: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/22/23 21:18:14.125
W0322 21:18:14.133528      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 22 21:18:14.140: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/22/23 21:18:14.141
Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.155: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.155: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.156: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.156: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.166: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.166: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:14.188: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:14.188: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:15.133: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:15.133: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:15.150: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
STEP: listing Deployments 03/22/23 21:18:15.15
Mar 22 21:18:15.154: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/22/23 21:18:15.155
Mar 22 21:18:15.174: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/22/23 21:18:15.174
Mar 22 21:18:15.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:15.193: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:15.200: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:15.213: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:15.217: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:16.846: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:17.191: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:17.212: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:17.220: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 22 21:18:18.839: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/22/23 21:18:18.855
STEP: fetching the DeploymentStatus 03/22/23 21:18:18.865
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3
Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3
STEP: deleting the Deployment 03/22/23 21:18:18.874
Mar 22 21:18:18.888: INFO: observed event type MODIFIED
Mar 22 21:18:18.888: INFO: observed event type MODIFIED
Mar 22 21:18:18.889: INFO: observed event type MODIFIED
Mar 22 21:18:18.890: INFO: observed event type MODIFIED
Mar 22 21:18:18.890: INFO: observed event type MODIFIED
Mar 22 21:18:18.890: INFO: observed event type MODIFIED
Mar 22 21:18:18.891: INFO: observed event type MODIFIED
Mar 22 21:18:18.891: INFO: observed event type MODIFIED
Mar 22 21:18:18.891: INFO: observed event type MODIFIED
Mar 22 21:18:18.892: INFO: observed event type MODIFIED
Mar 22 21:18:18.892: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 21:18:18.895: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:18.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7027" for this suite. 03/22/23 21:18:18.91
------------------------------
• [SLOW TEST] [6.825 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:12.092
    Mar 22 21:18:12.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 21:18:12.094
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:12.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:12.124
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/22/23 21:18:12.144
    STEP: waiting for Deployment to be created 03/22/23 21:18:12.151
    STEP: waiting for all Replicas to be Ready 03/22/23 21:18:12.154
    Mar 22 21:18:12.157: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.158: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.170: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.181: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.218: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:12.218: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 22 21:18:13.789: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 22 21:18:13.789: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 22 21:18:14.124: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/22/23 21:18:14.125
    W0322 21:18:14.133528      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 22 21:18:14.140: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/22/23 21:18:14.141
    Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.153: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 0
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.154: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.155: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.155: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.156: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.156: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.166: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.166: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:14.188: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:14.188: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:15.133: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:15.133: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:15.150: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    STEP: listing Deployments 03/22/23 21:18:15.15
    Mar 22 21:18:15.154: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/22/23 21:18:15.155
    Mar 22 21:18:15.174: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/22/23 21:18:15.174
    Mar 22 21:18:15.183: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:15.193: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:15.200: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:15.213: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:15.217: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:16.846: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:17.191: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:17.212: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:17.220: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 22 21:18:18.839: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/22/23 21:18:18.855
    STEP: fetching the DeploymentStatus 03/22/23 21:18:18.865
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 1
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:18.873: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3
    Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 2
    Mar 22 21:18:18.874: INFO: observed Deployment test-deployment in namespace deployment-7027 with ReadyReplicas 3
    STEP: deleting the Deployment 03/22/23 21:18:18.874
    Mar 22 21:18:18.888: INFO: observed event type MODIFIED
    Mar 22 21:18:18.888: INFO: observed event type MODIFIED
    Mar 22 21:18:18.889: INFO: observed event type MODIFIED
    Mar 22 21:18:18.890: INFO: observed event type MODIFIED
    Mar 22 21:18:18.890: INFO: observed event type MODIFIED
    Mar 22 21:18:18.890: INFO: observed event type MODIFIED
    Mar 22 21:18:18.891: INFO: observed event type MODIFIED
    Mar 22 21:18:18.891: INFO: observed event type MODIFIED
    Mar 22 21:18:18.891: INFO: observed event type MODIFIED
    Mar 22 21:18:18.892: INFO: observed event type MODIFIED
    Mar 22 21:18:18.892: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 21:18:18.895: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:18.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7027" for this suite. 03/22/23 21:18:18.91
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:18.928
Mar 22 21:18:18.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename subpath 03/22/23 21:18:18.93
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:18.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:18.957
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/22/23 21:18:18.965
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-pnnd 03/22/23 21:18:18.979
STEP: Creating a pod to test atomic-volume-subpath 03/22/23 21:18:18.979
Mar 22 21:18:18.993: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pnnd" in namespace "subpath-8568" to be "Succeeded or Failed"
Mar 22 21:18:18.997: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078547ms
Mar 22 21:18:21.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010958105s
Mar 22 21:18:23.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.010228791s
Mar 22 21:18:25.010: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.017149901s
Mar 22 21:18:27.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.010212619s
Mar 22 21:18:29.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.010281692s
Mar 22 21:18:31.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.01053209s
Mar 22 21:18:33.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.01083674s
Mar 22 21:18:35.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.01133505s
Mar 22 21:18:37.002: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.009590956s
Mar 22 21:18:39.005: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.012212149s
Mar 22 21:18:41.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=false. Elapsed: 22.011494653s
Mar 22 21:18:43.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009928736s
STEP: Saw pod success 03/22/23 21:18:43.003
Mar 22 21:18:43.003: INFO: Pod "pod-subpath-test-configmap-pnnd" satisfied condition "Succeeded or Failed"
Mar 22 21:18:43.008: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-configmap-pnnd container test-container-subpath-configmap-pnnd: <nil>
STEP: delete the pod 03/22/23 21:18:43.019
Mar 22 21:18:43.034: INFO: Waiting for pod pod-subpath-test-configmap-pnnd to disappear
Mar 22 21:18:43.038: INFO: Pod pod-subpath-test-configmap-pnnd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pnnd 03/22/23 21:18:43.038
Mar 22 21:18:43.038: INFO: Deleting pod "pod-subpath-test-configmap-pnnd" in namespace "subpath-8568"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:43.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8568" for this suite. 03/22/23 21:18:43.048
------------------------------
• [SLOW TEST] [24.132 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:18.928
    Mar 22 21:18:18.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename subpath 03/22/23 21:18:18.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:18.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:18.957
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/22/23 21:18:18.965
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-pnnd 03/22/23 21:18:18.979
    STEP: Creating a pod to test atomic-volume-subpath 03/22/23 21:18:18.979
    Mar 22 21:18:18.993: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pnnd" in namespace "subpath-8568" to be "Succeeded or Failed"
    Mar 22 21:18:18.997: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078547ms
    Mar 22 21:18:21.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010958105s
    Mar 22 21:18:23.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.010228791s
    Mar 22 21:18:25.010: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.017149901s
    Mar 22 21:18:27.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.010212619s
    Mar 22 21:18:29.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.010281692s
    Mar 22 21:18:31.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.01053209s
    Mar 22 21:18:33.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.01083674s
    Mar 22 21:18:35.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.01133505s
    Mar 22 21:18:37.002: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.009590956s
    Mar 22 21:18:39.005: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.012212149s
    Mar 22 21:18:41.004: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Running", Reason="", readiness=false. Elapsed: 22.011494653s
    Mar 22 21:18:43.003: INFO: Pod "pod-subpath-test-configmap-pnnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009928736s
    STEP: Saw pod success 03/22/23 21:18:43.003
    Mar 22 21:18:43.003: INFO: Pod "pod-subpath-test-configmap-pnnd" satisfied condition "Succeeded or Failed"
    Mar 22 21:18:43.008: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-subpath-test-configmap-pnnd container test-container-subpath-configmap-pnnd: <nil>
    STEP: delete the pod 03/22/23 21:18:43.019
    Mar 22 21:18:43.034: INFO: Waiting for pod pod-subpath-test-configmap-pnnd to disappear
    Mar 22 21:18:43.038: INFO: Pod pod-subpath-test-configmap-pnnd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pnnd 03/22/23 21:18:43.038
    Mar 22 21:18:43.038: INFO: Deleting pod "pod-subpath-test-configmap-pnnd" in namespace "subpath-8568"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:43.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8568" for this suite. 03/22/23 21:18:43.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:43.063
Mar 22 21:18:43.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context 03/22/23 21:18:43.065
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:43.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:43.099
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/22/23 21:18:43.106
Mar 22 21:18:43.116: INFO: Waiting up to 5m0s for pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b" in namespace "security-context-9859" to be "Succeeded or Failed"
Mar 22 21:18:43.121: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332986ms
Mar 22 21:18:45.128: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012386142s
Mar 22 21:18:47.127: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Running", Reason="", readiness=false. Elapsed: 4.011133852s
Mar 22 21:18:49.131: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014641673s
STEP: Saw pod success 03/22/23 21:18:49.131
Mar 22 21:18:49.131: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b" satisfied condition "Succeeded or Failed"
Mar 22 21:18:49.135: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b container test-container: <nil>
STEP: delete the pod 03/22/23 21:18:49.147
Mar 22 21:18:49.161: INFO: Waiting for pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b to disappear
Mar 22 21:18:49.165: INFO: Pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:49.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9859" for this suite. 03/22/23 21:18:49.171
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:43.063
    Mar 22 21:18:43.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context 03/22/23 21:18:43.065
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:43.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:43.099
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/22/23 21:18:43.106
    Mar 22 21:18:43.116: INFO: Waiting up to 5m0s for pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b" in namespace "security-context-9859" to be "Succeeded or Failed"
    Mar 22 21:18:43.121: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332986ms
    Mar 22 21:18:45.128: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012386142s
    Mar 22 21:18:47.127: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Running", Reason="", readiness=false. Elapsed: 4.011133852s
    Mar 22 21:18:49.131: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014641673s
    STEP: Saw pod success 03/22/23 21:18:49.131
    Mar 22 21:18:49.131: INFO: Pod "security-context-e5051e24-3413-49af-9c91-91b4f15aed3b" satisfied condition "Succeeded or Failed"
    Mar 22 21:18:49.135: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b container test-container: <nil>
    STEP: delete the pod 03/22/23 21:18:49.147
    Mar 22 21:18:49.161: INFO: Waiting for pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b to disappear
    Mar 22 21:18:49.165: INFO: Pod security-context-e5051e24-3413-49af-9c91-91b4f15aed3b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:49.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9859" for this suite. 03/22/23 21:18:49.171
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:49.188
Mar 22 21:18:49.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 21:18:49.19
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:49.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:49.214
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-9720d2d2-c25d-4152-b44b-0b6c726db9bb 03/22/23 21:18:49.221
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 21:18:49.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9677" for this suite. 03/22/23 21:18:49.235
------------------------------
• [0.062 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:49.188
    Mar 22 21:18:49.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 21:18:49.19
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:49.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:49.214
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-9720d2d2-c25d-4152-b44b-0b6c726db9bb 03/22/23 21:18:49.221
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:18:49.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9677" for this suite. 03/22/23 21:18:49.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:18:49.256
Mar 22 21:18:49.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename cronjob 03/22/23 21:18:49.258
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:49.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:49.288
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/22/23 21:18:49.297
STEP: Ensuring no jobs are scheduled 03/22/23 21:18:49.307
STEP: Ensuring no job exists by listing jobs explicitly 03/22/23 21:23:49.318
STEP: Removing cronjob 03/22/23 21:23:49.333
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 22 21:23:49.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2089" for this suite. 03/22/23 21:23:49.348
------------------------------
• [SLOW TEST] [300.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:18:49.256
    Mar 22 21:18:49.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename cronjob 03/22/23 21:18:49.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:18:49.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:18:49.288
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/22/23 21:18:49.297
    STEP: Ensuring no jobs are scheduled 03/22/23 21:18:49.307
    STEP: Ensuring no job exists by listing jobs explicitly 03/22/23 21:23:49.318
    STEP: Removing cronjob 03/22/23 21:23:49.333
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:23:49.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2089" for this suite. 03/22/23 21:23:49.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:23:49.366
Mar 22 21:23:49.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:23:49.368
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:49.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:49.389
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5543/configmap-test-52fa38e4-f2f1-4ab5-bd86-e9604fd2dfb9 03/22/23 21:23:49.397
STEP: Creating a pod to test consume configMaps 03/22/23 21:23:49.404
Mar 22 21:23:49.414: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97" in namespace "configmap-5543" to be "Succeeded or Failed"
Mar 22 21:23:49.419: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4097ms
Mar 22 21:23:51.426: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219352s
Mar 22 21:23:53.425: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010621465s
STEP: Saw pod success 03/22/23 21:23:53.425
Mar 22 21:23:53.425: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97" satisfied condition "Succeeded or Failed"
Mar 22 21:23:53.429: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 container env-test: <nil>
STEP: delete the pod 03/22/23 21:23:53.47
Mar 22 21:23:53.484: INFO: Waiting for pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 to disappear
Mar 22 21:23:53.488: INFO: Pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:23:53.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5543" for this suite. 03/22/23 21:23:53.494
------------------------------
• [4.135 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:23:49.366
    Mar 22 21:23:49.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:23:49.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:49.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:49.389
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5543/configmap-test-52fa38e4-f2f1-4ab5-bd86-e9604fd2dfb9 03/22/23 21:23:49.397
    STEP: Creating a pod to test consume configMaps 03/22/23 21:23:49.404
    Mar 22 21:23:49.414: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97" in namespace "configmap-5543" to be "Succeeded or Failed"
    Mar 22 21:23:49.419: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4097ms
    Mar 22 21:23:51.426: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219352s
    Mar 22 21:23:53.425: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010621465s
    STEP: Saw pod success 03/22/23 21:23:53.425
    Mar 22 21:23:53.425: INFO: Pod "pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97" satisfied condition "Succeeded or Failed"
    Mar 22 21:23:53.429: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 container env-test: <nil>
    STEP: delete the pod 03/22/23 21:23:53.47
    Mar 22 21:23:53.484: INFO: Waiting for pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 to disappear
    Mar 22 21:23:53.488: INFO: Pod pod-configmaps-dfab05a5-0a92-4e08-8b06-6a13e2393e97 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:23:53.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5543" for this suite. 03/22/23 21:23:53.494
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:23:53.504
Mar 22 21:23:53.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename ingressclass 03/22/23 21:23:53.506
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:53.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:53.524
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/22/23 21:23:53.532
STEP: getting /apis/networking.k8s.io 03/22/23 21:23:53.538
STEP: getting /apis/networking.k8s.iov1 03/22/23 21:23:53.541
STEP: creating 03/22/23 21:23:53.545
STEP: getting 03/22/23 21:23:53.567
STEP: listing 03/22/23 21:23:53.572
STEP: watching 03/22/23 21:23:53.576
Mar 22 21:23:53.576: INFO: starting watch
STEP: patching 03/22/23 21:23:53.579
STEP: updating 03/22/23 21:23:53.588
Mar 22 21:23:53.596: INFO: waiting for watch events with expected annotations
Mar 22 21:23:53.597: INFO: saw patched and updated annotations
STEP: deleting 03/22/23 21:23:53.598
STEP: deleting a collection 03/22/23 21:23:53.614
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:23:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4050" for this suite. 03/22/23 21:23:53.644
------------------------------
• [0.148 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:23:53.504
    Mar 22 21:23:53.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename ingressclass 03/22/23 21:23:53.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:53.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:53.524
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/22/23 21:23:53.532
    STEP: getting /apis/networking.k8s.io 03/22/23 21:23:53.538
    STEP: getting /apis/networking.k8s.iov1 03/22/23 21:23:53.541
    STEP: creating 03/22/23 21:23:53.545
    STEP: getting 03/22/23 21:23:53.567
    STEP: listing 03/22/23 21:23:53.572
    STEP: watching 03/22/23 21:23:53.576
    Mar 22 21:23:53.576: INFO: starting watch
    STEP: patching 03/22/23 21:23:53.579
    STEP: updating 03/22/23 21:23:53.588
    Mar 22 21:23:53.596: INFO: waiting for watch events with expected annotations
    Mar 22 21:23:53.597: INFO: saw patched and updated annotations
    STEP: deleting 03/22/23 21:23:53.598
    STEP: deleting a collection 03/22/23 21:23:53.614
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:23:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4050" for this suite. 03/22/23 21:23:53.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:23:53.655
Mar 22 21:23:53.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 21:23:53.656
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:53.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:53.676
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 03/22/23 21:23:53.683
Mar 22 21:23:53.692: INFO: Waiting up to 5m0s for pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3" in namespace "var-expansion-469" to be "Succeeded or Failed"
Mar 22 21:23:53.697: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991478ms
Mar 22 21:23:55.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011658249s
Mar 22 21:23:57.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011418889s
STEP: Saw pod success 03/22/23 21:23:57.704
Mar 22 21:23:57.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3" satisfied condition "Succeeded or Failed"
Mar 22 21:23:57.709: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 container dapi-container: <nil>
STEP: delete the pod 03/22/23 21:23:57.724
Mar 22 21:23:57.740: INFO: Waiting for pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 to disappear
Mar 22 21:23:57.744: INFO: Pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 21:23:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-469" for this suite. 03/22/23 21:23:57.753
------------------------------
• [4.109 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:23:53.655
    Mar 22 21:23:53.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 21:23:53.656
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:53.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:53.676
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 03/22/23 21:23:53.683
    Mar 22 21:23:53.692: INFO: Waiting up to 5m0s for pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3" in namespace "var-expansion-469" to be "Succeeded or Failed"
    Mar 22 21:23:53.697: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991478ms
    Mar 22 21:23:55.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011658249s
    Mar 22 21:23:57.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011418889s
    STEP: Saw pod success 03/22/23 21:23:57.704
    Mar 22 21:23:57.704: INFO: Pod "var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3" satisfied condition "Succeeded or Failed"
    Mar 22 21:23:57.709: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 21:23:57.724
    Mar 22 21:23:57.740: INFO: Waiting for pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 to disappear
    Mar 22 21:23:57.744: INFO: Pod var-expansion-f5413f3b-5d63-4fb4-91c8-e9ccfe7e8ce3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:23:57.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-469" for this suite. 03/22/23 21:23:57.753
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:23:57.764
Mar 22 21:23:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 21:23:57.766
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:57.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:57.787
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 03/22/23 21:23:57.799
STEP: patching the Namespace 03/22/23 21:23:57.825
STEP: get the Namespace and ensuring it has the label 03/22/23 21:23:57.833
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:23:57.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3081" for this suite. 03/22/23 21:23:57.844
STEP: Destroying namespace "nspatchtest-b8b9d1ff-5124-4dca-b3a8-e71b403eed72-9754" for this suite. 03/22/23 21:23:57.854
------------------------------
• [0.100 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:23:57.764
    Mar 22 21:23:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 21:23:57.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:57.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:57.787
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 03/22/23 21:23:57.799
    STEP: patching the Namespace 03/22/23 21:23:57.825
    STEP: get the Namespace and ensuring it has the label 03/22/23 21:23:57.833
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:23:57.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3081" for this suite. 03/22/23 21:23:57.844
    STEP: Destroying namespace "nspatchtest-b8b9d1ff-5124-4dca-b3a8-e71b403eed72-9754" for this suite. 03/22/23 21:23:57.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:23:57.875
Mar 22 21:23:57.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:23:57.878
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:57.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:57.911
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:23:57.944
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:23:58.7
STEP: Deploying the webhook pod 03/22/23 21:23:58.721
STEP: Wait for the deployment to be ready 03/22/23 21:23:58.738
Mar 22 21:23:58.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 21:24:00.775
STEP: Verifying the service has paired with the endpoint 03/22/23 21:24:00.789
Mar 22 21:24:01.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/22/23 21:24:01.797
STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:01.797
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/22/23 21:24:01.846
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/22/23 21:24:02.858
STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:02.858
STEP: Having no error when timeout is longer than webhook latency 03/22/23 21:24:03.902
STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:03.902
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/22/23 21:24:08.982
STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:08.982
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:24:14.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3012" for this suite. 03/22/23 21:24:14.1
STEP: Destroying namespace "webhook-3012-markers" for this suite. 03/22/23 21:24:14.109
------------------------------
• [SLOW TEST] [16.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:23:57.875
    Mar 22 21:23:57.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:23:57.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:23:57.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:23:57.911
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:23:57.944
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:23:58.7
    STEP: Deploying the webhook pod 03/22/23 21:23:58.721
    STEP: Wait for the deployment to be ready 03/22/23 21:23:58.738
    Mar 22 21:23:58.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 21:24:00.775
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:24:00.789
    Mar 22 21:24:01.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/22/23 21:24:01.797
    STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:01.797
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/22/23 21:24:01.846
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/22/23 21:24:02.858
    STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:02.858
    STEP: Having no error when timeout is longer than webhook latency 03/22/23 21:24:03.902
    STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:03.902
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/22/23 21:24:08.982
    STEP: Registering slow webhook via the AdmissionRegistration API 03/22/23 21:24:08.982
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:24:14.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3012" for this suite. 03/22/23 21:24:14.1
    STEP: Destroying namespace "webhook-3012-markers" for this suite. 03/22/23 21:24:14.109
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:24:14.133
Mar 22 21:24:14.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename disruption 03/22/23 21:24:14.138
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:24:14.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:24:14.173
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 03/22/23 21:24:14.215
STEP: Waiting for all pods to be running 03/22/23 21:24:16.265
Mar 22 21:24:16.271: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 22 21:24:18.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3855" for this suite. 03/22/23 21:24:18.293
------------------------------
• [4.167 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:24:14.133
    Mar 22 21:24:14.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename disruption 03/22/23 21:24:14.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:24:14.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:24:14.173
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 03/22/23 21:24:14.215
    STEP: Waiting for all pods to be running 03/22/23 21:24:16.265
    Mar 22 21:24:16.271: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:24:18.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3855" for this suite. 03/22/23 21:24:18.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:24:18.307
Mar 22 21:24:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 21:24:18.308
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:24:18.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:24:18.333
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/22/23 21:24:18.356
STEP: delete the rc 03/22/23 21:24:23.743
STEP: wait for the rc to be deleted 03/22/23 21:24:23.761
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/22/23 21:24:28.767
STEP: Gathering metrics 03/22/23 21:24:58.783
W0322 21:24:58.795935      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 21:24:58.796: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 22 21:24:58.796: INFO: Deleting pod "simpletest.rc-26762" in namespace "gc-56"
Mar 22 21:24:58.811: INFO: Deleting pod "simpletest.rc-28dxs" in namespace "gc-56"
Mar 22 21:24:58.828: INFO: Deleting pod "simpletest.rc-2gclc" in namespace "gc-56"
Mar 22 21:24:58.839: INFO: Deleting pod "simpletest.rc-2kk7q" in namespace "gc-56"
Mar 22 21:24:58.852: INFO: Deleting pod "simpletest.rc-2t6rb" in namespace "gc-56"
Mar 22 21:24:58.869: INFO: Deleting pod "simpletest.rc-47crj" in namespace "gc-56"
Mar 22 21:24:58.886: INFO: Deleting pod "simpletest.rc-48rsc" in namespace "gc-56"
Mar 22 21:24:58.897: INFO: Deleting pod "simpletest.rc-4c425" in namespace "gc-56"
Mar 22 21:24:58.910: INFO: Deleting pod "simpletest.rc-4ks65" in namespace "gc-56"
Mar 22 21:24:58.923: INFO: Deleting pod "simpletest.rc-4pdsz" in namespace "gc-56"
Mar 22 21:24:58.937: INFO: Deleting pod "simpletest.rc-4vkdj" in namespace "gc-56"
Mar 22 21:24:58.950: INFO: Deleting pod "simpletest.rc-5dmg5" in namespace "gc-56"
Mar 22 21:24:58.960: INFO: Deleting pod "simpletest.rc-5mdlr" in namespace "gc-56"
Mar 22 21:24:58.985: INFO: Deleting pod "simpletest.rc-5n8wk" in namespace "gc-56"
Mar 22 21:24:58.997: INFO: Deleting pod "simpletest.rc-5qmz8" in namespace "gc-56"
Mar 22 21:24:59.010: INFO: Deleting pod "simpletest.rc-5r4bx" in namespace "gc-56"
Mar 22 21:24:59.022: INFO: Deleting pod "simpletest.rc-65mp5" in namespace "gc-56"
Mar 22 21:24:59.037: INFO: Deleting pod "simpletest.rc-6rnbv" in namespace "gc-56"
Mar 22 21:24:59.051: INFO: Deleting pod "simpletest.rc-6zhpc" in namespace "gc-56"
Mar 22 21:24:59.067: INFO: Deleting pod "simpletest.rc-859xp" in namespace "gc-56"
Mar 22 21:24:59.088: INFO: Deleting pod "simpletest.rc-87g8h" in namespace "gc-56"
Mar 22 21:24:59.101: INFO: Deleting pod "simpletest.rc-8mxwj" in namespace "gc-56"
Mar 22 21:24:59.111: INFO: Deleting pod "simpletest.rc-956wj" in namespace "gc-56"
Mar 22 21:24:59.123: INFO: Deleting pod "simpletest.rc-965km" in namespace "gc-56"
Mar 22 21:24:59.142: INFO: Deleting pod "simpletest.rc-98fw6" in namespace "gc-56"
Mar 22 21:24:59.157: INFO: Deleting pod "simpletest.rc-9tgvd" in namespace "gc-56"
Mar 22 21:24:59.171: INFO: Deleting pod "simpletest.rc-b2qz6" in namespace "gc-56"
Mar 22 21:24:59.185: INFO: Deleting pod "simpletest.rc-b587h" in namespace "gc-56"
Mar 22 21:24:59.196: INFO: Deleting pod "simpletest.rc-bbbnl" in namespace "gc-56"
Mar 22 21:24:59.205: INFO: Deleting pod "simpletest.rc-bhmxw" in namespace "gc-56"
Mar 22 21:24:59.228: INFO: Deleting pod "simpletest.rc-bjmlh" in namespace "gc-56"
Mar 22 21:24:59.240: INFO: Deleting pod "simpletest.rc-bw9bz" in namespace "gc-56"
Mar 22 21:24:59.255: INFO: Deleting pod "simpletest.rc-bwb7p" in namespace "gc-56"
Mar 22 21:24:59.277: INFO: Deleting pod "simpletest.rc-cw7wm" in namespace "gc-56"
Mar 22 21:24:59.298: INFO: Deleting pod "simpletest.rc-cxp9v" in namespace "gc-56"
Mar 22 21:24:59.306: INFO: Deleting pod "simpletest.rc-dd6ht" in namespace "gc-56"
Mar 22 21:24:59.314: INFO: Deleting pod "simpletest.rc-dr78g" in namespace "gc-56"
Mar 22 21:24:59.326: INFO: Deleting pod "simpletest.rc-dt6jr" in namespace "gc-56"
Mar 22 21:24:59.337: INFO: Deleting pod "simpletest.rc-f7nmh" in namespace "gc-56"
Mar 22 21:24:59.347: INFO: Deleting pod "simpletest.rc-fbmql" in namespace "gc-56"
Mar 22 21:24:59.359: INFO: Deleting pod "simpletest.rc-g4rsf" in namespace "gc-56"
Mar 22 21:24:59.371: INFO: Deleting pod "simpletest.rc-g7lvh" in namespace "gc-56"
Mar 22 21:24:59.387: INFO: Deleting pod "simpletest.rc-g8mjj" in namespace "gc-56"
Mar 22 21:24:59.402: INFO: Deleting pod "simpletest.rc-g99dm" in namespace "gc-56"
Mar 22 21:24:59.416: INFO: Deleting pod "simpletest.rc-g9czl" in namespace "gc-56"
Mar 22 21:24:59.427: INFO: Deleting pod "simpletest.rc-gc5cj" in namespace "gc-56"
Mar 22 21:24:59.443: INFO: Deleting pod "simpletest.rc-gdtbb" in namespace "gc-56"
Mar 22 21:24:59.453: INFO: Deleting pod "simpletest.rc-gjnzp" in namespace "gc-56"
Mar 22 21:24:59.463: INFO: Deleting pod "simpletest.rc-gpzrn" in namespace "gc-56"
Mar 22 21:24:59.478: INFO: Deleting pod "simpletest.rc-gqrxq" in namespace "gc-56"
Mar 22 21:24:59.495: INFO: Deleting pod "simpletest.rc-gz7hs" in namespace "gc-56"
Mar 22 21:24:59.511: INFO: Deleting pod "simpletest.rc-h5tk8" in namespace "gc-56"
Mar 22 21:24:59.524: INFO: Deleting pod "simpletest.rc-hbtch" in namespace "gc-56"
Mar 22 21:24:59.535: INFO: Deleting pod "simpletest.rc-hbxg6" in namespace "gc-56"
Mar 22 21:24:59.545: INFO: Deleting pod "simpletest.rc-hg56q" in namespace "gc-56"
Mar 22 21:24:59.555: INFO: Deleting pod "simpletest.rc-hhzwj" in namespace "gc-56"
Mar 22 21:24:59.563: INFO: Deleting pod "simpletest.rc-hsgzn" in namespace "gc-56"
Mar 22 21:24:59.573: INFO: Deleting pod "simpletest.rc-htlk5" in namespace "gc-56"
Mar 22 21:24:59.584: INFO: Deleting pod "simpletest.rc-hx4fc" in namespace "gc-56"
Mar 22 21:24:59.593: INFO: Deleting pod "simpletest.rc-kgvjz" in namespace "gc-56"
Mar 22 21:24:59.610: INFO: Deleting pod "simpletest.rc-klqsm" in namespace "gc-56"
Mar 22 21:24:59.619: INFO: Deleting pod "simpletest.rc-kqjbj" in namespace "gc-56"
Mar 22 21:24:59.631: INFO: Deleting pod "simpletest.rc-kxbtq" in namespace "gc-56"
Mar 22 21:24:59.639: INFO: Deleting pod "simpletest.rc-l85fm" in namespace "gc-56"
Mar 22 21:24:59.652: INFO: Deleting pod "simpletest.rc-lgblh" in namespace "gc-56"
Mar 22 21:24:59.661: INFO: Deleting pod "simpletest.rc-lkt7q" in namespace "gc-56"
Mar 22 21:24:59.688: INFO: Deleting pod "simpletest.rc-ltbts" in namespace "gc-56"
Mar 22 21:24:59.736: INFO: Deleting pod "simpletest.rc-lv52j" in namespace "gc-56"
Mar 22 21:24:59.790: INFO: Deleting pod "simpletest.rc-lxwbm" in namespace "gc-56"
Mar 22 21:24:59.832: INFO: Deleting pod "simpletest.rc-n2vs9" in namespace "gc-56"
Mar 22 21:24:59.888: INFO: Deleting pod "simpletest.rc-n7n4p" in namespace "gc-56"
Mar 22 21:24:59.931: INFO: Deleting pod "simpletest.rc-njjtc" in namespace "gc-56"
Mar 22 21:24:59.981: INFO: Deleting pod "simpletest.rc-nkk6s" in namespace "gc-56"
Mar 22 21:25:00.034: INFO: Deleting pod "simpletest.rc-nspp7" in namespace "gc-56"
Mar 22 21:25:00.078: INFO: Deleting pod "simpletest.rc-nv7m7" in namespace "gc-56"
Mar 22 21:25:00.136: INFO: Deleting pod "simpletest.rc-p5rr7" in namespace "gc-56"
Mar 22 21:25:00.183: INFO: Deleting pod "simpletest.rc-pfj5z" in namespace "gc-56"
Mar 22 21:25:00.230: INFO: Deleting pod "simpletest.rc-phfgj" in namespace "gc-56"
Mar 22 21:25:00.285: INFO: Deleting pod "simpletest.rc-pmxjc" in namespace "gc-56"
Mar 22 21:25:00.331: INFO: Deleting pod "simpletest.rc-pqxmr" in namespace "gc-56"
Mar 22 21:25:00.382: INFO: Deleting pod "simpletest.rc-qjlvv" in namespace "gc-56"
Mar 22 21:25:00.433: INFO: Deleting pod "simpletest.rc-rldvp" in namespace "gc-56"
Mar 22 21:25:00.483: INFO: Deleting pod "simpletest.rc-shq5d" in namespace "gc-56"
Mar 22 21:25:00.533: INFO: Deleting pod "simpletest.rc-slkqs" in namespace "gc-56"
Mar 22 21:25:00.588: INFO: Deleting pod "simpletest.rc-sv6nr" in namespace "gc-56"
Mar 22 21:25:00.631: INFO: Deleting pod "simpletest.rc-ts5gr" in namespace "gc-56"
Mar 22 21:25:00.685: INFO: Deleting pod "simpletest.rc-tvnzv" in namespace "gc-56"
Mar 22 21:25:00.728: INFO: Deleting pod "simpletest.rc-vbv6g" in namespace "gc-56"
Mar 22 21:25:00.780: INFO: Deleting pod "simpletest.rc-vgf8d" in namespace "gc-56"
Mar 22 21:25:00.829: INFO: Deleting pod "simpletest.rc-vs87b" in namespace "gc-56"
Mar 22 21:25:00.889: INFO: Deleting pod "simpletest.rc-wh5kg" in namespace "gc-56"
Mar 22 21:25:00.936: INFO: Deleting pod "simpletest.rc-wl797" in namespace "gc-56"
Mar 22 21:25:00.988: INFO: Deleting pod "simpletest.rc-wznwv" in namespace "gc-56"
Mar 22 21:25:01.034: INFO: Deleting pod "simpletest.rc-x7jcm" in namespace "gc-56"
Mar 22 21:25:01.090: INFO: Deleting pod "simpletest.rc-x7kjt" in namespace "gc-56"
Mar 22 21:25:01.135: INFO: Deleting pod "simpletest.rc-xx4q9" in namespace "gc-56"
Mar 22 21:25:01.183: INFO: Deleting pod "simpletest.rc-zfgd7" in namespace "gc-56"
Mar 22 21:25:01.252: INFO: Deleting pod "simpletest.rc-zh75g" in namespace "gc-56"
Mar 22 21:25:01.301: INFO: Deleting pod "simpletest.rc-zq66j" in namespace "gc-56"
Mar 22 21:25:01.333: INFO: Deleting pod "simpletest.rc-zvwtd" in namespace "gc-56"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 21:25:01.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-56" for this suite. 03/22/23 21:25:01.425
------------------------------
• [SLOW TEST] [43.170 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:24:18.307
    Mar 22 21:24:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 21:24:18.308
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:24:18.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:24:18.333
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/22/23 21:24:18.356
    STEP: delete the rc 03/22/23 21:24:23.743
    STEP: wait for the rc to be deleted 03/22/23 21:24:23.761
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/22/23 21:24:28.767
    STEP: Gathering metrics 03/22/23 21:24:58.783
    W0322 21:24:58.795935      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 21:24:58.796: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 22 21:24:58.796: INFO: Deleting pod "simpletest.rc-26762" in namespace "gc-56"
    Mar 22 21:24:58.811: INFO: Deleting pod "simpletest.rc-28dxs" in namespace "gc-56"
    Mar 22 21:24:58.828: INFO: Deleting pod "simpletest.rc-2gclc" in namespace "gc-56"
    Mar 22 21:24:58.839: INFO: Deleting pod "simpletest.rc-2kk7q" in namespace "gc-56"
    Mar 22 21:24:58.852: INFO: Deleting pod "simpletest.rc-2t6rb" in namespace "gc-56"
    Mar 22 21:24:58.869: INFO: Deleting pod "simpletest.rc-47crj" in namespace "gc-56"
    Mar 22 21:24:58.886: INFO: Deleting pod "simpletest.rc-48rsc" in namespace "gc-56"
    Mar 22 21:24:58.897: INFO: Deleting pod "simpletest.rc-4c425" in namespace "gc-56"
    Mar 22 21:24:58.910: INFO: Deleting pod "simpletest.rc-4ks65" in namespace "gc-56"
    Mar 22 21:24:58.923: INFO: Deleting pod "simpletest.rc-4pdsz" in namespace "gc-56"
    Mar 22 21:24:58.937: INFO: Deleting pod "simpletest.rc-4vkdj" in namespace "gc-56"
    Mar 22 21:24:58.950: INFO: Deleting pod "simpletest.rc-5dmg5" in namespace "gc-56"
    Mar 22 21:24:58.960: INFO: Deleting pod "simpletest.rc-5mdlr" in namespace "gc-56"
    Mar 22 21:24:58.985: INFO: Deleting pod "simpletest.rc-5n8wk" in namespace "gc-56"
    Mar 22 21:24:58.997: INFO: Deleting pod "simpletest.rc-5qmz8" in namespace "gc-56"
    Mar 22 21:24:59.010: INFO: Deleting pod "simpletest.rc-5r4bx" in namespace "gc-56"
    Mar 22 21:24:59.022: INFO: Deleting pod "simpletest.rc-65mp5" in namespace "gc-56"
    Mar 22 21:24:59.037: INFO: Deleting pod "simpletest.rc-6rnbv" in namespace "gc-56"
    Mar 22 21:24:59.051: INFO: Deleting pod "simpletest.rc-6zhpc" in namespace "gc-56"
    Mar 22 21:24:59.067: INFO: Deleting pod "simpletest.rc-859xp" in namespace "gc-56"
    Mar 22 21:24:59.088: INFO: Deleting pod "simpletest.rc-87g8h" in namespace "gc-56"
    Mar 22 21:24:59.101: INFO: Deleting pod "simpletest.rc-8mxwj" in namespace "gc-56"
    Mar 22 21:24:59.111: INFO: Deleting pod "simpletest.rc-956wj" in namespace "gc-56"
    Mar 22 21:24:59.123: INFO: Deleting pod "simpletest.rc-965km" in namespace "gc-56"
    Mar 22 21:24:59.142: INFO: Deleting pod "simpletest.rc-98fw6" in namespace "gc-56"
    Mar 22 21:24:59.157: INFO: Deleting pod "simpletest.rc-9tgvd" in namespace "gc-56"
    Mar 22 21:24:59.171: INFO: Deleting pod "simpletest.rc-b2qz6" in namespace "gc-56"
    Mar 22 21:24:59.185: INFO: Deleting pod "simpletest.rc-b587h" in namespace "gc-56"
    Mar 22 21:24:59.196: INFO: Deleting pod "simpletest.rc-bbbnl" in namespace "gc-56"
    Mar 22 21:24:59.205: INFO: Deleting pod "simpletest.rc-bhmxw" in namespace "gc-56"
    Mar 22 21:24:59.228: INFO: Deleting pod "simpletest.rc-bjmlh" in namespace "gc-56"
    Mar 22 21:24:59.240: INFO: Deleting pod "simpletest.rc-bw9bz" in namespace "gc-56"
    Mar 22 21:24:59.255: INFO: Deleting pod "simpletest.rc-bwb7p" in namespace "gc-56"
    Mar 22 21:24:59.277: INFO: Deleting pod "simpletest.rc-cw7wm" in namespace "gc-56"
    Mar 22 21:24:59.298: INFO: Deleting pod "simpletest.rc-cxp9v" in namespace "gc-56"
    Mar 22 21:24:59.306: INFO: Deleting pod "simpletest.rc-dd6ht" in namespace "gc-56"
    Mar 22 21:24:59.314: INFO: Deleting pod "simpletest.rc-dr78g" in namespace "gc-56"
    Mar 22 21:24:59.326: INFO: Deleting pod "simpletest.rc-dt6jr" in namespace "gc-56"
    Mar 22 21:24:59.337: INFO: Deleting pod "simpletest.rc-f7nmh" in namespace "gc-56"
    Mar 22 21:24:59.347: INFO: Deleting pod "simpletest.rc-fbmql" in namespace "gc-56"
    Mar 22 21:24:59.359: INFO: Deleting pod "simpletest.rc-g4rsf" in namespace "gc-56"
    Mar 22 21:24:59.371: INFO: Deleting pod "simpletest.rc-g7lvh" in namespace "gc-56"
    Mar 22 21:24:59.387: INFO: Deleting pod "simpletest.rc-g8mjj" in namespace "gc-56"
    Mar 22 21:24:59.402: INFO: Deleting pod "simpletest.rc-g99dm" in namespace "gc-56"
    Mar 22 21:24:59.416: INFO: Deleting pod "simpletest.rc-g9czl" in namespace "gc-56"
    Mar 22 21:24:59.427: INFO: Deleting pod "simpletest.rc-gc5cj" in namespace "gc-56"
    Mar 22 21:24:59.443: INFO: Deleting pod "simpletest.rc-gdtbb" in namespace "gc-56"
    Mar 22 21:24:59.453: INFO: Deleting pod "simpletest.rc-gjnzp" in namespace "gc-56"
    Mar 22 21:24:59.463: INFO: Deleting pod "simpletest.rc-gpzrn" in namespace "gc-56"
    Mar 22 21:24:59.478: INFO: Deleting pod "simpletest.rc-gqrxq" in namespace "gc-56"
    Mar 22 21:24:59.495: INFO: Deleting pod "simpletest.rc-gz7hs" in namespace "gc-56"
    Mar 22 21:24:59.511: INFO: Deleting pod "simpletest.rc-h5tk8" in namespace "gc-56"
    Mar 22 21:24:59.524: INFO: Deleting pod "simpletest.rc-hbtch" in namespace "gc-56"
    Mar 22 21:24:59.535: INFO: Deleting pod "simpletest.rc-hbxg6" in namespace "gc-56"
    Mar 22 21:24:59.545: INFO: Deleting pod "simpletest.rc-hg56q" in namespace "gc-56"
    Mar 22 21:24:59.555: INFO: Deleting pod "simpletest.rc-hhzwj" in namespace "gc-56"
    Mar 22 21:24:59.563: INFO: Deleting pod "simpletest.rc-hsgzn" in namespace "gc-56"
    Mar 22 21:24:59.573: INFO: Deleting pod "simpletest.rc-htlk5" in namespace "gc-56"
    Mar 22 21:24:59.584: INFO: Deleting pod "simpletest.rc-hx4fc" in namespace "gc-56"
    Mar 22 21:24:59.593: INFO: Deleting pod "simpletest.rc-kgvjz" in namespace "gc-56"
    Mar 22 21:24:59.610: INFO: Deleting pod "simpletest.rc-klqsm" in namespace "gc-56"
    Mar 22 21:24:59.619: INFO: Deleting pod "simpletest.rc-kqjbj" in namespace "gc-56"
    Mar 22 21:24:59.631: INFO: Deleting pod "simpletest.rc-kxbtq" in namespace "gc-56"
    Mar 22 21:24:59.639: INFO: Deleting pod "simpletest.rc-l85fm" in namespace "gc-56"
    Mar 22 21:24:59.652: INFO: Deleting pod "simpletest.rc-lgblh" in namespace "gc-56"
    Mar 22 21:24:59.661: INFO: Deleting pod "simpletest.rc-lkt7q" in namespace "gc-56"
    Mar 22 21:24:59.688: INFO: Deleting pod "simpletest.rc-ltbts" in namespace "gc-56"
    Mar 22 21:24:59.736: INFO: Deleting pod "simpletest.rc-lv52j" in namespace "gc-56"
    Mar 22 21:24:59.790: INFO: Deleting pod "simpletest.rc-lxwbm" in namespace "gc-56"
    Mar 22 21:24:59.832: INFO: Deleting pod "simpletest.rc-n2vs9" in namespace "gc-56"
    Mar 22 21:24:59.888: INFO: Deleting pod "simpletest.rc-n7n4p" in namespace "gc-56"
    Mar 22 21:24:59.931: INFO: Deleting pod "simpletest.rc-njjtc" in namespace "gc-56"
    Mar 22 21:24:59.981: INFO: Deleting pod "simpletest.rc-nkk6s" in namespace "gc-56"
    Mar 22 21:25:00.034: INFO: Deleting pod "simpletest.rc-nspp7" in namespace "gc-56"
    Mar 22 21:25:00.078: INFO: Deleting pod "simpletest.rc-nv7m7" in namespace "gc-56"
    Mar 22 21:25:00.136: INFO: Deleting pod "simpletest.rc-p5rr7" in namespace "gc-56"
    Mar 22 21:25:00.183: INFO: Deleting pod "simpletest.rc-pfj5z" in namespace "gc-56"
    Mar 22 21:25:00.230: INFO: Deleting pod "simpletest.rc-phfgj" in namespace "gc-56"
    Mar 22 21:25:00.285: INFO: Deleting pod "simpletest.rc-pmxjc" in namespace "gc-56"
    Mar 22 21:25:00.331: INFO: Deleting pod "simpletest.rc-pqxmr" in namespace "gc-56"
    Mar 22 21:25:00.382: INFO: Deleting pod "simpletest.rc-qjlvv" in namespace "gc-56"
    Mar 22 21:25:00.433: INFO: Deleting pod "simpletest.rc-rldvp" in namespace "gc-56"
    Mar 22 21:25:00.483: INFO: Deleting pod "simpletest.rc-shq5d" in namespace "gc-56"
    Mar 22 21:25:00.533: INFO: Deleting pod "simpletest.rc-slkqs" in namespace "gc-56"
    Mar 22 21:25:00.588: INFO: Deleting pod "simpletest.rc-sv6nr" in namespace "gc-56"
    Mar 22 21:25:00.631: INFO: Deleting pod "simpletest.rc-ts5gr" in namespace "gc-56"
    Mar 22 21:25:00.685: INFO: Deleting pod "simpletest.rc-tvnzv" in namespace "gc-56"
    Mar 22 21:25:00.728: INFO: Deleting pod "simpletest.rc-vbv6g" in namespace "gc-56"
    Mar 22 21:25:00.780: INFO: Deleting pod "simpletest.rc-vgf8d" in namespace "gc-56"
    Mar 22 21:25:00.829: INFO: Deleting pod "simpletest.rc-vs87b" in namespace "gc-56"
    Mar 22 21:25:00.889: INFO: Deleting pod "simpletest.rc-wh5kg" in namespace "gc-56"
    Mar 22 21:25:00.936: INFO: Deleting pod "simpletest.rc-wl797" in namespace "gc-56"
    Mar 22 21:25:00.988: INFO: Deleting pod "simpletest.rc-wznwv" in namespace "gc-56"
    Mar 22 21:25:01.034: INFO: Deleting pod "simpletest.rc-x7jcm" in namespace "gc-56"
    Mar 22 21:25:01.090: INFO: Deleting pod "simpletest.rc-x7kjt" in namespace "gc-56"
    Mar 22 21:25:01.135: INFO: Deleting pod "simpletest.rc-xx4q9" in namespace "gc-56"
    Mar 22 21:25:01.183: INFO: Deleting pod "simpletest.rc-zfgd7" in namespace "gc-56"
    Mar 22 21:25:01.252: INFO: Deleting pod "simpletest.rc-zh75g" in namespace "gc-56"
    Mar 22 21:25:01.301: INFO: Deleting pod "simpletest.rc-zq66j" in namespace "gc-56"
    Mar 22 21:25:01.333: INFO: Deleting pod "simpletest.rc-zvwtd" in namespace "gc-56"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:25:01.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-56" for this suite. 03/22/23 21:25:01.425
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:25:01.477
Mar 22 21:25:01.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 21:25:01.478
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:25:01.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:25:01.508
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:01.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2682" for this suite. 03/22/23 21:26:01.545
------------------------------
• [SLOW TEST] [60.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:25:01.477
    Mar 22 21:25:01.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 21:25:01.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:25:01.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:25:01.508
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:01.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2682" for this suite. 03/22/23 21:26:01.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:01.561
Mar 22 21:26:01.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:26:01.562
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:01.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:01.607
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:26:01.625
Mar 22 21:26:01.636: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32" in namespace "projected-9195" to be "Succeeded or Failed"
Mar 22 21:26:01.642: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16553ms
Mar 22 21:26:03.650: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013278881s
Mar 22 21:26:05.650: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013992135s
STEP: Saw pod success 03/22/23 21:26:05.65
Mar 22 21:26:05.651: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32" satisfied condition "Succeeded or Failed"
Mar 22 21:26:05.655: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 container client-container: <nil>
STEP: delete the pod 03/22/23 21:26:05.704
Mar 22 21:26:05.720: INFO: Waiting for pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 to disappear
Mar 22 21:26:05.724: INFO: Pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:05.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9195" for this suite. 03/22/23 21:26:05.731
------------------------------
• [4.178 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:01.561
    Mar 22 21:26:01.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:26:01.562
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:01.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:01.607
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:26:01.625
    Mar 22 21:26:01.636: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32" in namespace "projected-9195" to be "Succeeded or Failed"
    Mar 22 21:26:01.642: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16553ms
    Mar 22 21:26:03.650: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013278881s
    Mar 22 21:26:05.650: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013992135s
    STEP: Saw pod success 03/22/23 21:26:05.65
    Mar 22 21:26:05.651: INFO: Pod "downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32" satisfied condition "Succeeded or Failed"
    Mar 22 21:26:05.655: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:26:05.704
    Mar 22 21:26:05.720: INFO: Waiting for pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 to disappear
    Mar 22 21:26:05.724: INFO: Pod downwardapi-volume-f30dcda8-579d-4531-a93d-97c40b714b32 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:05.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9195" for this suite. 03/22/23 21:26:05.731
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:05.74
Mar 22 21:26:05.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sysctl 03/22/23 21:26:05.742
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:05.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:05.763
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/22/23 21:26:05.775
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:05.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3709" for this suite. 03/22/23 21:26:05.794
------------------------------
• [0.069 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:05.74
    Mar 22 21:26:05.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sysctl 03/22/23 21:26:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:05.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:05.763
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/22/23 21:26:05.775
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:05.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3709" for this suite. 03/22/23 21:26:05.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:05.814
Mar 22 21:26:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 21:26:05.816
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:05.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:05.839
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 03/22/23 21:26:05.846
STEP: Creating a ResourceQuota 03/22/23 21:26:10.851
STEP: Ensuring resource quota status is calculated 03/22/23 21:26:10.858
STEP: Creating a ReplicationController 03/22/23 21:26:12.869
STEP: Ensuring resource quota status captures replication controller creation 03/22/23 21:26:12.883
STEP: Deleting a ReplicationController 03/22/23 21:26:14.891
STEP: Ensuring resource quota status released usage 03/22/23 21:26:14.899
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:16.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-138" for this suite. 03/22/23 21:26:16.911
------------------------------
• [SLOW TEST] [11.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:05.814
    Mar 22 21:26:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 21:26:05.816
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:05.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:05.839
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 03/22/23 21:26:05.846
    STEP: Creating a ResourceQuota 03/22/23 21:26:10.851
    STEP: Ensuring resource quota status is calculated 03/22/23 21:26:10.858
    STEP: Creating a ReplicationController 03/22/23 21:26:12.869
    STEP: Ensuring resource quota status captures replication controller creation 03/22/23 21:26:12.883
    STEP: Deleting a ReplicationController 03/22/23 21:26:14.891
    STEP: Ensuring resource quota status released usage 03/22/23 21:26:14.899
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:16.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-138" for this suite. 03/22/23 21:26:16.911
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:16.922
Mar 22 21:26:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubelet-test 03/22/23 21:26:16.924
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:16.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:16.951
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 22 21:26:16.966: INFO: Waiting up to 5m0s for pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509" in namespace "kubelet-test-99" to be "running and ready"
Mar 22 21:26:16.971: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509": Phase="Pending", Reason="", readiness=false. Elapsed: 5.330669ms
Mar 22 21:26:16.972: INFO: The phase of Pod busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:26:18.977: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509": Phase="Running", Reason="", readiness=true. Elapsed: 2.011191816s
Mar 22 21:26:18.977: INFO: The phase of Pod busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509 is Running (Ready = true)
Mar 22 21:26:18.978: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:19.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-99" for this suite. 03/22/23 21:26:19.026
------------------------------
• [2.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:16.922
    Mar 22 21:26:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubelet-test 03/22/23 21:26:16.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:16.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:16.951
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 22 21:26:16.966: INFO: Waiting up to 5m0s for pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509" in namespace "kubelet-test-99" to be "running and ready"
    Mar 22 21:26:16.971: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509": Phase="Pending", Reason="", readiness=false. Elapsed: 5.330669ms
    Mar 22 21:26:16.972: INFO: The phase of Pod busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:26:18.977: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509": Phase="Running", Reason="", readiness=true. Elapsed: 2.011191816s
    Mar 22 21:26:18.977: INFO: The phase of Pod busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509 is Running (Ready = true)
    Mar 22 21:26:18.978: INFO: Pod "busybox-scheduling-6efbaa41-f71c-461e-8934-d7890bb87509" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:19.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-99" for this suite. 03/22/23 21:26:19.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:19.035
Mar 22 21:26:19.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:26:19.037
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:19.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:19.065
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:26:19.09
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:26:20.198
STEP: Deploying the webhook pod 03/22/23 21:26:20.218
STEP: Wait for the deployment to be ready 03/22/23 21:26:20.235
Mar 22 21:26:20.243: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 21:26:22.258
STEP: Verifying the service has paired with the endpoint 03/22/23 21:26:22.273
Mar 22 21:26:23.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 03/22/23 21:26:23.288
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/22/23 21:26:23.302
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/22/23 21:26:23.302
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/22/23 21:26:23.302
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/22/23 21:26:23.316
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/22/23 21:26:23.317
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/22/23 21:26:23.32
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:23.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4127" for this suite. 03/22/23 21:26:23.416
STEP: Destroying namespace "webhook-4127-markers" for this suite. 03/22/23 21:26:23.429
------------------------------
• [4.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:19.035
    Mar 22 21:26:19.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:26:19.037
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:19.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:19.065
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:26:19.09
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:26:20.198
    STEP: Deploying the webhook pod 03/22/23 21:26:20.218
    STEP: Wait for the deployment to be ready 03/22/23 21:26:20.235
    Mar 22 21:26:20.243: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 21:26:22.258
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:26:22.273
    Mar 22 21:26:23.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 03/22/23 21:26:23.288
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/22/23 21:26:23.302
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/22/23 21:26:23.302
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/22/23 21:26:23.302
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/22/23 21:26:23.316
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/22/23 21:26:23.317
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/22/23 21:26:23.32
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:23.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4127" for this suite. 03/22/23 21:26:23.416
    STEP: Destroying namespace "webhook-4127-markers" for this suite. 03/22/23 21:26:23.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:23.461
Mar 22 21:26:23.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 21:26:23.463
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:23.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:23.488
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 in namespace container-probe-7222 03/22/23 21:26:23.495
Mar 22 21:26:23.506: INFO: Waiting up to 5m0s for pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5" in namespace "container-probe-7222" to be "not pending"
Mar 22 21:26:23.511: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.058013ms
Mar 22 21:26:25.517: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011201236s
Mar 22 21:26:25.517: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5" satisfied condition "not pending"
Mar 22 21:26:25.517: INFO: Started pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 in namespace container-probe-7222
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:26:25.517
Mar 22 21:26:25.525: INFO: Initial restart count of pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 is 0
Mar 22 21:26:45.617: INFO: Restart count of pod container-probe-7222/liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 is now 1 (20.091985575s elapsed)
STEP: deleting the pod 03/22/23 21:26:45.617
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7222" for this suite. 03/22/23 21:26:45.641
------------------------------
• [SLOW TEST] [22.188 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:23.461
    Mar 22 21:26:23.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 21:26:23.463
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:23.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:23.488
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 in namespace container-probe-7222 03/22/23 21:26:23.495
    Mar 22 21:26:23.506: INFO: Waiting up to 5m0s for pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5" in namespace "container-probe-7222" to be "not pending"
    Mar 22 21:26:23.511: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.058013ms
    Mar 22 21:26:25.517: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011201236s
    Mar 22 21:26:25.517: INFO: Pod "liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5" satisfied condition "not pending"
    Mar 22 21:26:25.517: INFO: Started pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 in namespace container-probe-7222
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:26:25.517
    Mar 22 21:26:25.525: INFO: Initial restart count of pod liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 is 0
    Mar 22 21:26:45.617: INFO: Restart count of pod container-probe-7222/liveness-7c6e5283-1939-4601-8a1f-8cf1190cc1d5 is now 1 (20.091985575s elapsed)
    STEP: deleting the pod 03/22/23 21:26:45.617
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7222" for this suite. 03/22/23 21:26:45.641
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:45.65
Mar 22 21:26:45.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:26:45.654
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:45.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:45.68
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 03/22/23 21:26:45.686
Mar 22 21:26:45.698: INFO: Waiting up to 5m0s for pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248" in namespace "downward-api-4457" to be "Succeeded or Failed"
Mar 22 21:26:45.702: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211882ms
Mar 22 21:26:47.708: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010578299s
Mar 22 21:26:49.708: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010750883s
STEP: Saw pod success 03/22/23 21:26:49.709
Mar 22 21:26:49.709: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248" satisfied condition "Succeeded or Failed"
Mar 22 21:26:49.714: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 container dapi-container: <nil>
STEP: delete the pod 03/22/23 21:26:49.725
Mar 22 21:26:49.740: INFO: Waiting for pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 to disappear
Mar 22 21:26:49.745: INFO: Pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:49.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4457" for this suite. 03/22/23 21:26:49.752
------------------------------
• [4.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:45.65
    Mar 22 21:26:45.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:26:45.654
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:45.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:45.68
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 03/22/23 21:26:45.686
    Mar 22 21:26:45.698: INFO: Waiting up to 5m0s for pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248" in namespace "downward-api-4457" to be "Succeeded or Failed"
    Mar 22 21:26:45.702: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211882ms
    Mar 22 21:26:47.708: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010578299s
    Mar 22 21:26:49.708: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010750883s
    STEP: Saw pod success 03/22/23 21:26:49.709
    Mar 22 21:26:49.709: INFO: Pod "downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248" satisfied condition "Succeeded or Failed"
    Mar 22 21:26:49.714: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 container dapi-container: <nil>
    STEP: delete the pod 03/22/23 21:26:49.725
    Mar 22 21:26:49.740: INFO: Waiting for pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 to disappear
    Mar 22 21:26:49.745: INFO: Pod downward-api-392c3b90-c03f-43d9-9a34-538ea14cc248 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:49.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4457" for this suite. 03/22/23 21:26:49.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:49.768
Mar 22 21:26:49.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:26:49.769
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:49.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:49.789
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:26:49.817
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:26:51.159
STEP: Deploying the webhook pod 03/22/23 21:26:51.169
STEP: Wait for the deployment to be ready 03/22/23 21:26:51.185
Mar 22 21:26:51.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 21:26:53.231
STEP: Verifying the service has paired with the endpoint 03/22/23 21:26:53.247
Mar 22 21:26:54.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 03/22/23 21:26:54.261
STEP: create a pod 03/22/23 21:26:54.306
Mar 22 21:26:54.316: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8362" to be "running"
Mar 22 21:26:54.321: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882119ms
Mar 22 21:26:56.328: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011880348s
Mar 22 21:26:56.328: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/22/23 21:26:56.328
Mar 22 21:26:56.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=webhook-8362 attach --namespace=webhook-8362 to-be-attached-pod -i -c=container1'
Mar 22 21:26:56.528: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:26:56.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8362" for this suite. 03/22/23 21:26:56.581
STEP: Destroying namespace "webhook-8362-markers" for this suite. 03/22/23 21:26:56.59
------------------------------
• [SLOW TEST] [6.836 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:49.768
    Mar 22 21:26:49.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:26:49.769
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:49.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:49.789
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:26:49.817
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:26:51.159
    STEP: Deploying the webhook pod 03/22/23 21:26:51.169
    STEP: Wait for the deployment to be ready 03/22/23 21:26:51.185
    Mar 22 21:26:51.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 21:26:53.231
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:26:53.247
    Mar 22 21:26:54.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 03/22/23 21:26:54.261
    STEP: create a pod 03/22/23 21:26:54.306
    Mar 22 21:26:54.316: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8362" to be "running"
    Mar 22 21:26:54.321: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882119ms
    Mar 22 21:26:56.328: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011880348s
    Mar 22 21:26:56.328: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/22/23 21:26:56.328
    Mar 22 21:26:56.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=webhook-8362 attach --namespace=webhook-8362 to-be-attached-pod -i -c=container1'
    Mar 22 21:26:56.528: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:26:56.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8362" for this suite. 03/22/23 21:26:56.581
    STEP: Destroying namespace "webhook-8362-markers" for this suite. 03/22/23 21:26:56.59
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:26:56.606
Mar 22 21:26:56.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:26:56.61
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:56.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:56.668
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 03/22/23 21:26:56.678
Mar 22 21:26:56.695: INFO: Waiting up to 5m0s for pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db" in namespace "emptydir-5830" to be "Succeeded or Failed"
Mar 22 21:26:56.715: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Pending", Reason="", readiness=false. Elapsed: 19.894305ms
Mar 22 21:26:58.724: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028802312s
Mar 22 21:27:00.722: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02671477s
STEP: Saw pod success 03/22/23 21:27:00.722
Mar 22 21:27:00.722: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db" satisfied condition "Succeeded or Failed"
Mar 22 21:27:00.728: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-a5919999-e29c-452f-acfc-3a1861efd2db container test-container: <nil>
STEP: delete the pod 03/22/23 21:27:00.739
Mar 22 21:27:00.752: INFO: Waiting for pod pod-a5919999-e29c-452f-acfc-3a1861efd2db to disappear
Mar 22 21:27:00.757: INFO: Pod pod-a5919999-e29c-452f-acfc-3a1861efd2db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:00.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5830" for this suite. 03/22/23 21:27:00.771
------------------------------
• [4.175 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:26:56.606
    Mar 22 21:26:56.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:26:56.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:26:56.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:26:56.668
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/22/23 21:26:56.678
    Mar 22 21:26:56.695: INFO: Waiting up to 5m0s for pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db" in namespace "emptydir-5830" to be "Succeeded or Failed"
    Mar 22 21:26:56.715: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Pending", Reason="", readiness=false. Elapsed: 19.894305ms
    Mar 22 21:26:58.724: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028802312s
    Mar 22 21:27:00.722: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02671477s
    STEP: Saw pod success 03/22/23 21:27:00.722
    Mar 22 21:27:00.722: INFO: Pod "pod-a5919999-e29c-452f-acfc-3a1861efd2db" satisfied condition "Succeeded or Failed"
    Mar 22 21:27:00.728: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-a5919999-e29c-452f-acfc-3a1861efd2db container test-container: <nil>
    STEP: delete the pod 03/22/23 21:27:00.739
    Mar 22 21:27:00.752: INFO: Waiting for pod pod-a5919999-e29c-452f-acfc-3a1861efd2db to disappear
    Mar 22 21:27:00.757: INFO: Pod pod-a5919999-e29c-452f-acfc-3a1861efd2db no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:00.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5830" for this suite. 03/22/23 21:27:00.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:00.783
Mar 22 21:27:00.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:27:00.786
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:00.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:00.812
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-2deb6eed-3a11-4d1b-ad73-60c8f6f6e014 03/22/23 21:27:00.836
STEP: Creating a pod to test consume configMaps 03/22/23 21:27:00.843
Mar 22 21:27:00.866: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762" in namespace "projected-6891" to be "Succeeded or Failed"
Mar 22 21:27:00.873: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335237ms
Mar 22 21:27:02.879: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012089355s
Mar 22 21:27:04.880: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012971652s
STEP: Saw pod success 03/22/23 21:27:04.881
Mar 22 21:27:04.881: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762" satisfied condition "Succeeded or Failed"
Mar 22 21:27:04.897: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:27:04.912
Mar 22 21:27:04.927: INFO: Waiting for pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 to disappear
Mar 22 21:27:04.931: INFO: Pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:04.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6891" for this suite. 03/22/23 21:27:04.939
------------------------------
• [4.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:00.783
    Mar 22 21:27:00.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:27:00.786
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:00.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:00.812
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-2deb6eed-3a11-4d1b-ad73-60c8f6f6e014 03/22/23 21:27:00.836
    STEP: Creating a pod to test consume configMaps 03/22/23 21:27:00.843
    Mar 22 21:27:00.866: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762" in namespace "projected-6891" to be "Succeeded or Failed"
    Mar 22 21:27:00.873: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335237ms
    Mar 22 21:27:02.879: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012089355s
    Mar 22 21:27:04.880: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012971652s
    STEP: Saw pod success 03/22/23 21:27:04.881
    Mar 22 21:27:04.881: INFO: Pod "pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762" satisfied condition "Succeeded or Failed"
    Mar 22 21:27:04.897: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:27:04.912
    Mar 22 21:27:04.927: INFO: Waiting for pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 to disappear
    Mar 22 21:27:04.931: INFO: Pod pod-projected-configmaps-6afa2cc7-1e4f-476f-8858-69b526944762 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:04.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6891" for this suite. 03/22/23 21:27:04.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:04.974
Mar 22 21:27:04.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:27:04.976
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:05.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:05.02
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 03/22/23 21:27:05.03
Mar 22 21:27:05.041: INFO: Waiting up to 5m0s for pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2" in namespace "emptydir-3595" to be "Succeeded or Failed"
Mar 22 21:27:05.047: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30045ms
Mar 22 21:27:07.052: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011421508s
Mar 22 21:27:09.062: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021082255s
STEP: Saw pod success 03/22/23 21:27:09.062
Mar 22 21:27:09.064: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2" satisfied condition "Succeeded or Failed"
Mar 22 21:27:09.080: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 container test-container: <nil>
STEP: delete the pod 03/22/23 21:27:09.11
Mar 22 21:27:09.128: INFO: Waiting for pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 to disappear
Mar 22 21:27:09.132: INFO: Pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:09.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3595" for this suite. 03/22/23 21:27:09.141
------------------------------
• [4.183 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:04.974
    Mar 22 21:27:04.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:27:04.976
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:05.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:05.02
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/22/23 21:27:05.03
    Mar 22 21:27:05.041: INFO: Waiting up to 5m0s for pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2" in namespace "emptydir-3595" to be "Succeeded or Failed"
    Mar 22 21:27:05.047: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30045ms
    Mar 22 21:27:07.052: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011421508s
    Mar 22 21:27:09.062: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021082255s
    STEP: Saw pod success 03/22/23 21:27:09.062
    Mar 22 21:27:09.064: INFO: Pod "pod-e561d319-2e09-4842-811d-d110bb3d0ee2" satisfied condition "Succeeded or Failed"
    Mar 22 21:27:09.080: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 container test-container: <nil>
    STEP: delete the pod 03/22/23 21:27:09.11
    Mar 22 21:27:09.128: INFO: Waiting for pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 to disappear
    Mar 22 21:27:09.132: INFO: Pod pod-e561d319-2e09-4842-811d-d110bb3d0ee2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:09.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3595" for this suite. 03/22/23 21:27:09.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:09.161
Mar 22 21:27:09.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:27:09.163
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:09.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:09.184
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 03/22/23 21:27:09.19
Mar 22 21:27:09.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: rename a version 03/22/23 21:27:13.335
STEP: check the new version name is served 03/22/23 21:27:13.355
STEP: check the old version name is removed 03/22/23 21:27:14.383
STEP: check the other version is not changed 03/22/23 21:27:15.289
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1824" for this suite. 03/22/23 21:27:18.611
------------------------------
• [SLOW TEST] [9.458 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:09.161
    Mar 22 21:27:09.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:27:09.163
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:09.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:09.184
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 03/22/23 21:27:09.19
    Mar 22 21:27:09.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: rename a version 03/22/23 21:27:13.335
    STEP: check the new version name is served 03/22/23 21:27:13.355
    STEP: check the old version name is removed 03/22/23 21:27:14.383
    STEP: check the other version is not changed 03/22/23 21:27:15.289
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1824" for this suite. 03/22/23 21:27:18.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:18.629
Mar 22 21:27:18.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 21:27:18.631
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:18.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:18.651
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 22 21:27:18.669: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 22 21:27:23.674: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/22/23 21:27:23.674
Mar 22 21:27:23.674: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/22/23 21:27:23.688
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 21:27:25.715: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2133  3ada8a57-a74b-4b37-b8ac-dd6276c92ff0 44630 1 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c5d358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 21:27:23 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-22 21:27:24 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 22 21:27:25.718: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-2133  1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6 44619 1 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3ada8a57-a74b-4b37-b8ac-dd6276c92ff0 0xc0022cdbf7 0xc0022cdbf8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ada8a57-a74b-4b37-b8ac-dd6276c92ff0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022cdca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 22 21:27:25.723: INFO: Pod "test-cleanup-deployment-7698ff6f6b-4kvk6" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-4kvk6 test-cleanup-deployment-7698ff6f6b- deployment-2133  182f5f42-d93c-4578-bf22-1fb4992148c0 44618 0 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6 0xc005736147 0xc005736148}] [] [{kube-controller-manager Update v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8tl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8tl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.185,StartTime:2023-03-22 21:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:27:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0a93b937bf6a912df81f9bba9fd08e69677dbe88ce9fb8f2ae94c28c2ee87206,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:25.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2133" for this suite. 03/22/23 21:27:25.729
------------------------------
• [SLOW TEST] [7.117 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:18.629
    Mar 22 21:27:18.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 21:27:18.631
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:18.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:18.651
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 22 21:27:18.669: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 22 21:27:23.674: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/22/23 21:27:23.674
    Mar 22 21:27:23.674: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/22/23 21:27:23.688
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 21:27:25.715: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2133  3ada8a57-a74b-4b37-b8ac-dd6276c92ff0 44630 1 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c5d358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-22 21:27:23 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-22 21:27:24 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 22 21:27:25.718: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-2133  1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6 44619 1 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3ada8a57-a74b-4b37-b8ac-dd6276c92ff0 0xc0022cdbf7 0xc0022cdbf8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ada8a57-a74b-4b37-b8ac-dd6276c92ff0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022cdca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 21:27:25.723: INFO: Pod "test-cleanup-deployment-7698ff6f6b-4kvk6" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-4kvk6 test-cleanup-deployment-7698ff6f6b- deployment-2133  182f5f42-d93c-4578-bf22-1fb4992148c0 44618 0 2023-03-22 21:27:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6 0xc005736147 0xc005736148}] [] [{kube-controller-manager Update v1 2023-03-22 21:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b5fc535-ce78-460d-a1a9-d1ba7c40b4f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:27:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8tl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8tl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.185,StartTime:2023-03-22 21:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:27:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0a93b937bf6a912df81f9bba9fd08e69677dbe88ce9fb8f2ae94c28c2ee87206,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:25.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2133" for this suite. 03/22/23 21:27:25.729
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:25.746
Mar 22 21:27:25.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pod-network-test 03/22/23 21:27:25.747
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:25.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:25.77
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6110 03/22/23 21:27:25.776
STEP: creating a selector 03/22/23 21:27:25.777
STEP: Creating the service pods in kubernetes 03/22/23 21:27:25.777
Mar 22 21:27:25.777: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 22 21:27:25.813: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6110" to be "running and ready"
Mar 22 21:27:25.825: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06432ms
Mar 22 21:27:25.825: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:27:27.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010722824s
Mar 22 21:27:27.830: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:27:29.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011511498s
Mar 22 21:27:29.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:27:31.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011350635s
Mar 22 21:27:31.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:27:33.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011709199s
Mar 22 21:27:33.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:27:35.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011494449s
Mar 22 21:27:35.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 22 21:27:37.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011374601s
Mar 22 21:27:37.831: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 22 21:27:37.831: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 22 21:27:37.835: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6110" to be "running and ready"
Mar 22 21:27:37.840: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.024235ms
Mar 22 21:27:37.841: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 22 21:27:37.841: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 22 21:27:37.844: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6110" to be "running and ready"
Mar 22 21:27:37.849: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.504163ms
Mar 22 21:27:37.849: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 22 21:27:37.849: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/22/23 21:27:37.856
Mar 22 21:27:37.872: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6110" to be "running"
Mar 22 21:27:37.881: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.460391ms
Mar 22 21:27:39.887: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015384582s
Mar 22 21:27:39.888: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 22 21:27:39.902: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6110" to be "running"
Mar 22 21:27:39.907: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.89381ms
Mar 22 21:27:39.907: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 22 21:27:39.911: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 22 21:27:39.911: INFO: Going to poll 10.244.1.122 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:27:39.914: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.122 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:27:39.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:27:39.916: INFO: ExecWithOptions: Clientset creation
Mar 22 21:27:39.916: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.122+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:27:41.117: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 22 21:27:41.117: INFO: Going to poll 10.244.0.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:27:41.123: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:27:41.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:27:41.124: INFO: ExecWithOptions: Clientset creation
Mar 22 21:27:41.125: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:27:42.281: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 22 21:27:42.281: INFO: Going to poll 10.244.0.241 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 22 21:27:42.286: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.241 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:27:42.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:27:42.287: INFO: ExecWithOptions: Clientset creation
Mar 22 21:27:42.287: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.241+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 22 21:27:43.452: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:43.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6110" for this suite. 03/22/23 21:27:43.46
------------------------------
• [SLOW TEST] [17.728 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:25.746
    Mar 22 21:27:25.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pod-network-test 03/22/23 21:27:25.747
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:25.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:25.77
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6110 03/22/23 21:27:25.776
    STEP: creating a selector 03/22/23 21:27:25.777
    STEP: Creating the service pods in kubernetes 03/22/23 21:27:25.777
    Mar 22 21:27:25.777: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 22 21:27:25.813: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6110" to be "running and ready"
    Mar 22 21:27:25.825: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06432ms
    Mar 22 21:27:25.825: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:27:27.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010722824s
    Mar 22 21:27:27.830: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:27:29.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011511498s
    Mar 22 21:27:29.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:27:31.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011350635s
    Mar 22 21:27:31.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:27:33.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011709199s
    Mar 22 21:27:33.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:27:35.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011494449s
    Mar 22 21:27:35.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 22 21:27:37.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011374601s
    Mar 22 21:27:37.831: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 22 21:27:37.831: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 22 21:27:37.835: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6110" to be "running and ready"
    Mar 22 21:27:37.840: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.024235ms
    Mar 22 21:27:37.841: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 22 21:27:37.841: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 22 21:27:37.844: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6110" to be "running and ready"
    Mar 22 21:27:37.849: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.504163ms
    Mar 22 21:27:37.849: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 22 21:27:37.849: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/22/23 21:27:37.856
    Mar 22 21:27:37.872: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6110" to be "running"
    Mar 22 21:27:37.881: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.460391ms
    Mar 22 21:27:39.887: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015384582s
    Mar 22 21:27:39.888: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 22 21:27:39.902: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6110" to be "running"
    Mar 22 21:27:39.907: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.89381ms
    Mar 22 21:27:39.907: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 22 21:27:39.911: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 22 21:27:39.911: INFO: Going to poll 10.244.1.122 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:27:39.914: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.122 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:27:39.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:27:39.916: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:27:39.916: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.122+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:27:41.117: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 22 21:27:41.117: INFO: Going to poll 10.244.0.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:27:41.123: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:27:41.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:27:41.124: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:27:41.125: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:27:42.281: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 22 21:27:42.281: INFO: Going to poll 10.244.0.241 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 22 21:27:42.286: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.241 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6110 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:27:42.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:27:42.287: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:27:42.287: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/pod-network-test-6110/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.241+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 22 21:27:43.452: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:43.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6110" for this suite. 03/22/23 21:27:43.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:43.485
Mar 22 21:27:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:27:43.487
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:43.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:43.509
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:27:43.515
Mar 22 21:27:43.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4" in namespace "downward-api-9466" to be "Succeeded or Failed"
Mar 22 21:27:43.532: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999722ms
Mar 22 21:27:45.538: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009571893s
Mar 22 21:27:47.542: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013404788s
Mar 22 21:27:49.544: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015768315s
STEP: Saw pod success 03/22/23 21:27:49.546
Mar 22 21:27:49.546: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4" satisfied condition "Succeeded or Failed"
Mar 22 21:27:49.552: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 container client-container: <nil>
STEP: delete the pod 03/22/23 21:27:49.599
Mar 22 21:27:49.613: INFO: Waiting for pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 to disappear
Mar 22 21:27:49.617: INFO: Pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:49.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9466" for this suite. 03/22/23 21:27:49.637
------------------------------
• [SLOW TEST] [6.162 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:43.485
    Mar 22 21:27:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:27:43.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:43.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:43.509
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:27:43.515
    Mar 22 21:27:43.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4" in namespace "downward-api-9466" to be "Succeeded or Failed"
    Mar 22 21:27:43.532: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999722ms
    Mar 22 21:27:45.538: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009571893s
    Mar 22 21:27:47.542: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013404788s
    Mar 22 21:27:49.544: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015768315s
    STEP: Saw pod success 03/22/23 21:27:49.546
    Mar 22 21:27:49.546: INFO: Pod "downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4" satisfied condition "Succeeded or Failed"
    Mar 22 21:27:49.552: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:27:49.599
    Mar 22 21:27:49.613: INFO: Waiting for pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 to disappear
    Mar 22 21:27:49.617: INFO: Pod downwardapi-volume-469a1e4b-c4e4-40dc-977f-fff4cf1938d4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:49.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9466" for this suite. 03/22/23 21:27:49.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:49.649
Mar 22 21:27:49.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:27:49.65
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:49.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:49.674
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:27:49.681
Mar 22 21:27:49.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079" in namespace "projected-5381" to be "Succeeded or Failed"
Mar 22 21:27:49.696: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 5.030948ms
Mar 22 21:27:51.702: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011462354s
Mar 22 21:27:53.702: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011270082s
Mar 22 21:27:55.703: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012371562s
STEP: Saw pod success 03/22/23 21:27:55.704
Mar 22 21:27:55.704: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079" satisfied condition "Succeeded or Failed"
Mar 22 21:27:55.711: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 container client-container: <nil>
STEP: delete the pod 03/22/23 21:27:55.731
Mar 22 21:27:55.743: INFO: Waiting for pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 to disappear
Mar 22 21:27:55.752: INFO: Pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 22 21:27:55.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5381" for this suite. 03/22/23 21:27:55.759
------------------------------
• [SLOW TEST] [6.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:49.649
    Mar 22 21:27:49.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:27:49.65
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:49.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:49.674
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:27:49.681
    Mar 22 21:27:49.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079" in namespace "projected-5381" to be "Succeeded or Failed"
    Mar 22 21:27:49.696: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 5.030948ms
    Mar 22 21:27:51.702: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011462354s
    Mar 22 21:27:53.702: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011270082s
    Mar 22 21:27:55.703: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012371562s
    STEP: Saw pod success 03/22/23 21:27:55.704
    Mar 22 21:27:55.704: INFO: Pod "downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079" satisfied condition "Succeeded or Failed"
    Mar 22 21:27:55.711: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:27:55.731
    Mar 22 21:27:55.743: INFO: Waiting for pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 to disappear
    Mar 22 21:27:55.752: INFO: Pod downwardapi-volume-7968d724-85ce-4a27-a2bc-a3a8fa9b4079 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:27:55.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5381" for this suite. 03/22/23 21:27:55.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:27:55.784
Mar 22 21:27:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename gc 03/22/23 21:27:55.793
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:55.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:55.816
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/22/23 21:27:55.828
STEP: delete the rc 03/22/23 21:28:01.044
STEP: wait for the rc to be deleted 03/22/23 21:28:01.131
Mar 22 21:28:03.475: INFO: 91 pods remaining
Mar 22 21:28:03.488: INFO: 80 pods has nil DeletionTimestamp
Mar 22 21:28:03.488: INFO: 
Mar 22 21:28:04.775: INFO: 82 pods remaining
Mar 22 21:28:04.775: INFO: 60 pods has nil DeletionTimestamp
Mar 22 21:28:04.775: INFO: 
Mar 22 21:28:05.162: INFO: 72 pods remaining
Mar 22 21:28:05.201: INFO: 40 pods has nil DeletionTimestamp
Mar 22 21:28:05.201: INFO: 
Mar 22 21:28:06.182: INFO: 71 pods remaining
Mar 22 21:28:06.182: INFO: 29 pods has nil DeletionTimestamp
Mar 22 21:28:06.182: INFO: 
Mar 22 21:28:07.146: INFO: 68 pods remaining
Mar 22 21:28:07.146: INFO: 20 pods has nil DeletionTimestamp
Mar 22 21:28:07.146: INFO: 
Mar 22 21:28:08.211: INFO: 66 pods remaining
Mar 22 21:28:08.211: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:08.211: INFO: 
Mar 22 21:28:09.161: INFO: 61 pods remaining
Mar 22 21:28:09.172: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:09.172: INFO: 
Mar 22 21:28:10.145: INFO: 54 pods remaining
Mar 22 21:28:10.145: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:10.145: INFO: 
Mar 22 21:28:11.272: INFO: 45 pods remaining
Mar 22 21:28:11.280: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:11.280: INFO: 
Mar 22 21:28:12.153: INFO: 37 pods remaining
Mar 22 21:28:12.153: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:12.153: INFO: 
Mar 22 21:28:13.150: INFO: 32 pods remaining
Mar 22 21:28:13.150: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:13.150: INFO: 
Mar 22 21:28:14.150: INFO: 23 pods remaining
Mar 22 21:28:14.150: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:14.150: INFO: 
Mar 22 21:28:15.142: INFO: 14 pods remaining
Mar 22 21:28:15.142: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:15.142: INFO: 
Mar 22 21:28:16.182: INFO: 6 pods remaining
Mar 22 21:28:16.182: INFO: 0 pods has nil DeletionTimestamp
Mar 22 21:28:16.182: INFO: 
STEP: Gathering metrics 03/22/23 21:28:17.139
W0322 21:28:17.152984      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 22 21:28:17.153: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:17.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9652" for this suite. 03/22/23 21:28:17.162
------------------------------
• [SLOW TEST] [21.407 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:27:55.784
    Mar 22 21:27:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename gc 03/22/23 21:27:55.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:27:55.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:27:55.816
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/22/23 21:27:55.828
    STEP: delete the rc 03/22/23 21:28:01.044
    STEP: wait for the rc to be deleted 03/22/23 21:28:01.131
    Mar 22 21:28:03.475: INFO: 91 pods remaining
    Mar 22 21:28:03.488: INFO: 80 pods has nil DeletionTimestamp
    Mar 22 21:28:03.488: INFO: 
    Mar 22 21:28:04.775: INFO: 82 pods remaining
    Mar 22 21:28:04.775: INFO: 60 pods has nil DeletionTimestamp
    Mar 22 21:28:04.775: INFO: 
    Mar 22 21:28:05.162: INFO: 72 pods remaining
    Mar 22 21:28:05.201: INFO: 40 pods has nil DeletionTimestamp
    Mar 22 21:28:05.201: INFO: 
    Mar 22 21:28:06.182: INFO: 71 pods remaining
    Mar 22 21:28:06.182: INFO: 29 pods has nil DeletionTimestamp
    Mar 22 21:28:06.182: INFO: 
    Mar 22 21:28:07.146: INFO: 68 pods remaining
    Mar 22 21:28:07.146: INFO: 20 pods has nil DeletionTimestamp
    Mar 22 21:28:07.146: INFO: 
    Mar 22 21:28:08.211: INFO: 66 pods remaining
    Mar 22 21:28:08.211: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:08.211: INFO: 
    Mar 22 21:28:09.161: INFO: 61 pods remaining
    Mar 22 21:28:09.172: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:09.172: INFO: 
    Mar 22 21:28:10.145: INFO: 54 pods remaining
    Mar 22 21:28:10.145: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:10.145: INFO: 
    Mar 22 21:28:11.272: INFO: 45 pods remaining
    Mar 22 21:28:11.280: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:11.280: INFO: 
    Mar 22 21:28:12.153: INFO: 37 pods remaining
    Mar 22 21:28:12.153: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:12.153: INFO: 
    Mar 22 21:28:13.150: INFO: 32 pods remaining
    Mar 22 21:28:13.150: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:13.150: INFO: 
    Mar 22 21:28:14.150: INFO: 23 pods remaining
    Mar 22 21:28:14.150: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:14.150: INFO: 
    Mar 22 21:28:15.142: INFO: 14 pods remaining
    Mar 22 21:28:15.142: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:15.142: INFO: 
    Mar 22 21:28:16.182: INFO: 6 pods remaining
    Mar 22 21:28:16.182: INFO: 0 pods has nil DeletionTimestamp
    Mar 22 21:28:16.182: INFO: 
    STEP: Gathering metrics 03/22/23 21:28:17.139
    W0322 21:28:17.152984      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 22 21:28:17.153: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:17.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9652" for this suite. 03/22/23 21:28:17.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:17.194
Mar 22 21:28:17.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:28:17.197
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:17.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:17.226
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  03/22/23 21:28:17.237
Mar 22 21:28:17.263: INFO: Waiting up to 5m0s for pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22" in namespace "svcaccounts-3416" to be "Succeeded or Failed"
Mar 22 21:28:17.269: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203167ms
Mar 22 21:28:19.368: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104471528s
Mar 22 21:28:21.274: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010972364s
Mar 22 21:28:23.277: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01377982s
STEP: Saw pod success 03/22/23 21:28:23.277
Mar 22 21:28:23.277: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22" satisfied condition "Succeeded or Failed"
Mar 22 21:28:23.282: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:28:23.296
Mar 22 21:28:23.311: INFO: Waiting for pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 to disappear
Mar 22 21:28:23.315: INFO: Pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3416" for this suite. 03/22/23 21:28:23.32
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:17.194
    Mar 22 21:28:17.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:28:17.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:17.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:17.226
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  03/22/23 21:28:17.237
    Mar 22 21:28:17.263: INFO: Waiting up to 5m0s for pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22" in namespace "svcaccounts-3416" to be "Succeeded or Failed"
    Mar 22 21:28:17.269: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203167ms
    Mar 22 21:28:19.368: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104471528s
    Mar 22 21:28:21.274: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010972364s
    Mar 22 21:28:23.277: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01377982s
    STEP: Saw pod success 03/22/23 21:28:23.277
    Mar 22 21:28:23.277: INFO: Pod "test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22" satisfied condition "Succeeded or Failed"
    Mar 22 21:28:23.282: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:28:23.296
    Mar 22 21:28:23.311: INFO: Waiting for pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 to disappear
    Mar 22 21:28:23.315: INFO: Pod test-pod-ebd7df32-a8eb-41ee-a9f0-08a3eaaf2d22 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3416" for this suite. 03/22/23 21:28:23.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:23.348
Mar 22 21:28:23.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:28:23.349
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:23.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:23.371
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 03/22/23 21:28:23.397
Mar 22 21:28:23.398: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 22 21:28:23.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:24.132: INFO: stderr: ""
Mar 22 21:28:24.132: INFO: stdout: "service/agnhost-replica created\n"
Mar 22 21:28:24.132: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 22 21:28:24.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:24.590: INFO: stderr: ""
Mar 22 21:28:24.590: INFO: stdout: "service/agnhost-primary created\n"
Mar 22 21:28:24.590: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 22 21:28:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:24.965: INFO: stderr: ""
Mar 22 21:28:24.965: INFO: stdout: "service/frontend created\n"
Mar 22 21:28:24.965: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 22 21:28:24.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:25.418: INFO: stderr: ""
Mar 22 21:28:25.418: INFO: stdout: "deployment.apps/frontend created\n"
Mar 22 21:28:25.418: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 22 21:28:25.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:25.880: INFO: stderr: ""
Mar 22 21:28:25.880: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 22 21:28:25.880: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 22 21:28:25.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
Mar 22 21:28:26.475: INFO: stderr: ""
Mar 22 21:28:26.475: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/22/23 21:28:26.475
Mar 22 21:28:26.476: INFO: Waiting for all frontend pods to be Running.
Mar 22 21:28:31.527: INFO: Waiting for frontend to serve content.
Mar 22 21:28:31.591: INFO: Trying to add a new entry to the guestbook.
Mar 22 21:28:31.649: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/22/23 21:28:31.691
Mar 22 21:28:31.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:31.851: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:31.851: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/22/23 21:28:31.851
Mar 22 21:28:31.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:32.181: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:32.181: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/22/23 21:28:32.181
Mar 22 21:28:32.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:32.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:32.408: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/22/23 21:28:32.408
Mar 22 21:28:32.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:32.604: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:32.604: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/22/23 21:28:32.604
Mar 22 21:28:32.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:32.848: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:32.848: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/22/23 21:28:32.848
Mar 22 21:28:32.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
Mar 22 21:28:33.084: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:28:33.084: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:33.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3165" for this suite. 03/22/23 21:28:33.094
------------------------------
• [SLOW TEST] [9.757 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:23.348
    Mar 22 21:28:23.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:28:23.349
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:23.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:23.371
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 03/22/23 21:28:23.397
    Mar 22 21:28:23.398: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 22 21:28:23.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:24.132: INFO: stderr: ""
    Mar 22 21:28:24.132: INFO: stdout: "service/agnhost-replica created\n"
    Mar 22 21:28:24.132: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 22 21:28:24.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:24.590: INFO: stderr: ""
    Mar 22 21:28:24.590: INFO: stdout: "service/agnhost-primary created\n"
    Mar 22 21:28:24.590: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 22 21:28:24.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:24.965: INFO: stderr: ""
    Mar 22 21:28:24.965: INFO: stdout: "service/frontend created\n"
    Mar 22 21:28:24.965: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 22 21:28:24.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:25.418: INFO: stderr: ""
    Mar 22 21:28:25.418: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 22 21:28:25.418: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 22 21:28:25.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:25.880: INFO: stderr: ""
    Mar 22 21:28:25.880: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 22 21:28:25.880: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 22 21:28:25.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 create -f -'
    Mar 22 21:28:26.475: INFO: stderr: ""
    Mar 22 21:28:26.475: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/22/23 21:28:26.475
    Mar 22 21:28:26.476: INFO: Waiting for all frontend pods to be Running.
    Mar 22 21:28:31.527: INFO: Waiting for frontend to serve content.
    Mar 22 21:28:31.591: INFO: Trying to add a new entry to the guestbook.
    Mar 22 21:28:31.649: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/22/23 21:28:31.691
    Mar 22 21:28:31.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:31.851: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:31.851: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/22/23 21:28:31.851
    Mar 22 21:28:31.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:32.181: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:32.181: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/22/23 21:28:32.181
    Mar 22 21:28:32.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:32.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:32.408: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/22/23 21:28:32.408
    Mar 22 21:28:32.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:32.604: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:32.604: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/22/23 21:28:32.604
    Mar 22 21:28:32.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:32.848: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:32.848: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/22/23 21:28:32.848
    Mar 22 21:28:32.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-3165 delete --grace-period=0 --force -f -'
    Mar 22 21:28:33.084: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:28:33.084: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:33.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3165" for this suite. 03/22/23 21:28:33.094
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:33.105
Mar 22 21:28:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:28:33.114
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:33.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:33.138
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Mar 22 21:28:33.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 21:28:34.817
Mar 22 21:28:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 create -f -'
Mar 22 21:28:35.671: INFO: stderr: ""
Mar 22 21:28:35.671: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 22 21:28:35.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
Mar 22 21:28:35.859: INFO: stderr: ""
Mar 22 21:28:35.859: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 22 21:28:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 apply -f -'
Mar 22 21:28:36.334: INFO: stderr: ""
Mar 22 21:28:36.334: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 22 21:28:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
Mar 22 21:28:36.475: INFO: stderr: ""
Mar 22 21:28:36.475: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/22/23 21:28:36.475
Mar 22 21:28:36.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 explain e2e-test-crd-publish-openapi-1147-crds'
Mar 22 21:28:36.887: INFO: stderr: ""
Mar 22 21:28:36.887: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1147-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:38.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9066" for this suite. 03/22/23 21:28:38.45
------------------------------
• [SLOW TEST] [5.353 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:33.105
    Mar 22 21:28:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-publish-openapi 03/22/23 21:28:33.114
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:33.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:33.138
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Mar 22 21:28:33.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/22/23 21:28:34.817
    Mar 22 21:28:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 create -f -'
    Mar 22 21:28:35.671: INFO: stderr: ""
    Mar 22 21:28:35.671: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 22 21:28:35.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
    Mar 22 21:28:35.859: INFO: stderr: ""
    Mar 22 21:28:35.859: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 22 21:28:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 apply -f -'
    Mar 22 21:28:36.334: INFO: stderr: ""
    Mar 22 21:28:36.334: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 22 21:28:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 --namespace=crd-publish-openapi-9066 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
    Mar 22 21:28:36.475: INFO: stderr: ""
    Mar 22 21:28:36.475: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/22/23 21:28:36.475
    Mar 22 21:28:36.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=crd-publish-openapi-9066 explain e2e-test-crd-publish-openapi-1147-crds'
    Mar 22 21:28:36.887: INFO: stderr: ""
    Mar 22 21:28:36.887: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1147-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:38.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9066" for this suite. 03/22/23 21:28:38.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:38.464
Mar 22 21:28:38.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename conformance-tests 03/22/23 21:28:38.465
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:38.488
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/22/23 21:28:38.494
Mar 22 21:28:38.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:38.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-5119" for this suite. 03/22/23 21:28:38.513
------------------------------
• [0.056 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:38.464
    Mar 22 21:28:38.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename conformance-tests 03/22/23 21:28:38.465
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:38.488
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/22/23 21:28:38.494
    Mar 22 21:28:38.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:38.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-5119" for this suite. 03/22/23 21:28:38.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:38.55
Mar 22 21:28:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename namespaces 03/22/23 21:28:38.552
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:38.594
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 03/22/23 21:28:38.609
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.629
STEP: Creating a pod in the namespace 03/22/23 21:28:38.635
STEP: Waiting for the pod to have running status 03/22/23 21:28:38.654
Mar 22 21:28:38.654: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6591" to be "running"
Mar 22 21:28:38.659: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890113ms
Mar 22 21:28:40.673: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018335798s
Mar 22 21:28:40.673: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/22/23 21:28:40.673
STEP: Waiting for the namespace to be removed. 03/22/23 21:28:40.681
STEP: Recreating the namespace 03/22/23 21:28:51.689
STEP: Verifying there are no pods in the namespace 03/22/23 21:28:51.705
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3032" for this suite. 03/22/23 21:28:51.72
STEP: Destroying namespace "nsdeletetest-6591" for this suite. 03/22/23 21:28:51.732
Mar 22 21:28:51.737: INFO: Namespace nsdeletetest-6591 was already deleted
STEP: Destroying namespace "nsdeletetest-7598" for this suite. 03/22/23 21:28:51.737
------------------------------
• [SLOW TEST] [13.196 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:38.55
    Mar 22 21:28:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename namespaces 03/22/23 21:28:38.552
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:38.594
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 03/22/23 21:28:38.609
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:38.629
    STEP: Creating a pod in the namespace 03/22/23 21:28:38.635
    STEP: Waiting for the pod to have running status 03/22/23 21:28:38.654
    Mar 22 21:28:38.654: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6591" to be "running"
    Mar 22 21:28:38.659: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890113ms
    Mar 22 21:28:40.673: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018335798s
    Mar 22 21:28:40.673: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/22/23 21:28:40.673
    STEP: Waiting for the namespace to be removed. 03/22/23 21:28:40.681
    STEP: Recreating the namespace 03/22/23 21:28:51.689
    STEP: Verifying there are no pods in the namespace 03/22/23 21:28:51.705
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3032" for this suite. 03/22/23 21:28:51.72
    STEP: Destroying namespace "nsdeletetest-6591" for this suite. 03/22/23 21:28:51.732
    Mar 22 21:28:51.737: INFO: Namespace nsdeletetest-6591 was already deleted
    STEP: Destroying namespace "nsdeletetest-7598" for this suite. 03/22/23 21:28:51.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:51.755
Mar 22 21:28:51.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:28:51.757
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:51.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:51.779
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Mar 22 21:28:51.813: INFO: created pod pod-service-account-defaultsa
Mar 22 21:28:51.813: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 22 21:28:51.821: INFO: created pod pod-service-account-mountsa
Mar 22 21:28:51.821: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 22 21:28:51.827: INFO: created pod pod-service-account-nomountsa
Mar 22 21:28:51.827: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 22 21:28:51.837: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 22 21:28:51.837: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 22 21:28:51.850: INFO: created pod pod-service-account-mountsa-mountspec
Mar 22 21:28:51.850: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 22 21:28:51.857: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 22 21:28:51.857: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 22 21:28:51.863: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 22 21:28:51.863: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 22 21:28:51.877: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 22 21:28:51.877: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 22 21:28:51.888: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 22 21:28:51.888: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 21:28:51.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3461" for this suite. 03/22/23 21:28:51.893
------------------------------
• [0.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:51.755
    Mar 22 21:28:51.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:28:51.757
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:51.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:51.779
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Mar 22 21:28:51.813: INFO: created pod pod-service-account-defaultsa
    Mar 22 21:28:51.813: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 22 21:28:51.821: INFO: created pod pod-service-account-mountsa
    Mar 22 21:28:51.821: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 22 21:28:51.827: INFO: created pod pod-service-account-nomountsa
    Mar 22 21:28:51.827: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 22 21:28:51.837: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 22 21:28:51.837: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 22 21:28:51.850: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 22 21:28:51.850: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 22 21:28:51.857: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 22 21:28:51.857: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 22 21:28:51.863: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 22 21:28:51.863: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 22 21:28:51.877: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 22 21:28:51.877: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 22 21:28:51.888: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 22 21:28:51.888: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:28:51.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3461" for this suite. 03/22/23 21:28:51.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:28:51.927
Mar 22 21:28:51.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename sched-pred 03/22/23 21:28:51.929
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:51.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:51.971
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 22 21:28:52.004: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 22 21:28:52.016: INFO: Waiting for terminating namespaces to be deleted...
Mar 22 21:28:52.020: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
Mar 22 21:28:52.031: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:28:52.033: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container coredns ready: true, restart count 0
Mar 22 21:28:52.033: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:28:52.033: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:28:52.033: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:28:52.033: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:28:52.033: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:28:52.033: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:28:52.033: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 22 21:28:52.033: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:28:52.033: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 21:28:52.033: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.033: INFO: pod-service-account-mountsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.033: INFO: pod-service-account-nomountsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.033: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
Mar 22 21:28:52.073: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:28:52.073: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 22 21:28:52.073: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container coredns ready: true, restart count 0
Mar 22 21:28:52.073: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:28:52.073: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:28:52.073: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:28:52.073: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:28:52.073: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:28:52.073: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:28:52.073: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container e2e ready: true, restart count 0
Mar 22 21:28:52.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:28:52.073: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:28:52.073: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 21:28:52.073: INFO: pod-service-account-defaultsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.073: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.073: INFO: 
Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
Mar 22 21:28:52.085: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 22 21:28:52.086: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
Mar 22 21:28:52.086: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container csi-do-plugin ready: true, restart count 0
Mar 22 21:28:52.086: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar 22 21:28:52.086: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container do-node-agent ready: true, restart count 0
Mar 22 21:28:52.086: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 22 21:28:52.086: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 22 21:28:52.086: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 22 21:28:52.086: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 22 21:28:52.086: INFO: pod-service-account-defaultsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.086: INFO: pod-service-account-mountsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.086: INFO: pod-service-account-mountsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.086: INFO: pod-service-account-nomountsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
Mar 22 21:28:52.087: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
Mar 22 21:28:52.087: INFO: 	Container token-test ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 21:28:52.087
Mar 22 21:28:52.095: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1941" to be "running"
Mar 22 21:28:52.113: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.833313ms
Mar 22 21:28:54.121: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.025923283s
Mar 22 21:28:54.121: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 21:28:54.125
STEP: Trying to apply a random label on the found node. 03/22/23 21:28:54.147
STEP: verifying the node has the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 95 03/22/23 21:28:54.161
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/22/23 21:28:54.168
Mar 22 21:28:54.177: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1941" to be "not pending"
Mar 22 21:28:54.187: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.987728ms
Mar 22 21:28:56.195: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017975381s
Mar 22 21:28:56.195: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.124.0.3 on the node which pod4 resides and expect not scheduled 03/22/23 21:28:56.195
Mar 22 21:28:56.206: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1941" to be "not pending"
Mar 22 21:28:56.215: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468363ms
Mar 22 21:28:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01476213s
Mar 22 21:29:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016230548s
Mar 22 21:29:02.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014694091s
Mar 22 21:29:04.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027754069s
Mar 22 21:29:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015668802s
Mar 22 21:29:08.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015222453s
Mar 22 21:29:10.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016296348s
Mar 22 21:29:12.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023025222s
Mar 22 21:29:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01611559s
Mar 22 21:29:16.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016374371s
Mar 22 21:29:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.015575556s
Mar 22 21:29:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01532184s
Mar 22 21:29:22.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023112804s
Mar 22 21:29:24.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022634018s
Mar 22 21:29:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015249035s
Mar 22 21:29:28.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.016201122s
Mar 22 21:29:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015625618s
Mar 22 21:29:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015461914s
Mar 22 21:29:34.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.036774409s
Mar 22 21:29:36.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016029777s
Mar 22 21:29:38.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019229692s
Mar 22 21:29:40.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.02311633s
Mar 22 21:29:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.015093696s
Mar 22 21:29:44.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015536682s
Mar 22 21:29:46.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016912191s
Mar 22 21:29:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015500139s
Mar 22 21:29:50.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015805413s
Mar 22 21:29:52.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015497825s
Mar 22 21:29:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015253439s
Mar 22 21:29:56.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.02032559s
Mar 22 21:29:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014844084s
Mar 22 21:30:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016224576s
Mar 22 21:30:02.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022920657s
Mar 22 21:30:04.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016050138s
Mar 22 21:30:06.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027944864s
Mar 22 21:30:08.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015434513s
Mar 22 21:30:10.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016847719s
Mar 22 21:30:12.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015248424s
Mar 22 21:30:14.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015376842s
Mar 22 21:30:16.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016399832s
Mar 22 21:30:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015358833s
Mar 22 21:30:20.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018221632s
Mar 22 21:30:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014861169s
Mar 22 21:30:24.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015285908s
Mar 22 21:30:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.015242465s
Mar 22 21:30:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014727985s
Mar 22 21:30:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015340901s
Mar 22 21:30:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015195034s
Mar 22 21:30:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015202648s
Mar 22 21:30:36.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.014493349s
Mar 22 21:30:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014760512s
Mar 22 21:30:40.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017363745s
Mar 22 21:30:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015386034s
Mar 22 21:30:44.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016552563s
Mar 22 21:30:46.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.016406394s
Mar 22 21:30:48.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01612088s
Mar 22 21:30:50.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01889966s
Mar 22 21:30:52.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016019679s
Mar 22 21:30:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015284991s
Mar 22 21:30:56.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01614843s
Mar 22 21:30:58.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.015825985s
Mar 22 21:31:00.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017853796s
Mar 22 21:31:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.015480297s
Mar 22 21:31:04.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.015719237s
Mar 22 21:31:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.015202985s
Mar 22 21:31:08.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.015877984s
Mar 22 21:31:10.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.017048925s
Mar 22 21:31:12.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.015991703s
Mar 22 21:31:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016342676s
Mar 22 21:31:16.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.022806978s
Mar 22 21:31:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.015507104s
Mar 22 21:31:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01490963s
Mar 22 21:31:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.014866684s
Mar 22 21:31:24.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016074208s
Mar 22 21:31:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.015286107s
Mar 22 21:31:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.015071227s
Mar 22 21:31:30.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015797646s
Mar 22 21:31:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.0156517s
Mar 22 21:31:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015204343s
Mar 22 21:31:36.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.014982569s
Mar 22 21:31:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015383246s
Mar 22 21:31:40.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015507804s
Mar 22 21:31:42.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.018177377s
Mar 22 21:31:44.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016755841s
Mar 22 21:31:46.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015408969s
Mar 22 21:31:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.015356502s
Mar 22 21:31:50.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.015508158s
Mar 22 21:31:52.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.020624222s
Mar 22 21:31:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.015263655s
Mar 22 21:31:56.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.019270494s
Mar 22 21:31:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015435006s
Mar 22 21:32:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.016433319s
Mar 22 21:32:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015634097s
Mar 22 21:32:04.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.015295189s
Mar 22 21:32:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.015065066s
Mar 22 21:32:08.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.015892557s
Mar 22 21:32:10.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016427648s
Mar 22 21:32:12.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014829716s
Mar 22 21:32:14.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.015585306s
Mar 22 21:32:16.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017345033s
Mar 22 21:32:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.015482097s
Mar 22 21:32:20.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017064044s
Mar 22 21:32:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015693783s
Mar 22 21:32:24.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.017704964s
Mar 22 21:32:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015517745s
Mar 22 21:32:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015154092s
Mar 22 21:32:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.015458158s
Mar 22 21:32:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015348788s
Mar 22 21:32:34.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.014680868s
Mar 22 21:32:36.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.014650886s
Mar 22 21:32:38.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.020299026s
Mar 22 21:32:40.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016548667s
Mar 22 21:32:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015388499s
Mar 22 21:32:44.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.014669199s
Mar 22 21:32:46.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014879156s
Mar 22 21:32:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.01501313s
Mar 22 21:32:50.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016802905s
Mar 22 21:32:52.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.0154209s
Mar 22 21:32:54.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.015777838s
Mar 22 21:32:56.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017329488s
Mar 22 21:32:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015285381s
Mar 22 21:33:00.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.015484308s
Mar 22 21:33:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.015253326s
Mar 22 21:33:04.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.015557081s
Mar 22 21:33:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015338064s
Mar 22 21:33:08.227: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.021327559s
Mar 22 21:33:10.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.014943139s
Mar 22 21:33:12.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.01782179s
Mar 22 21:33:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016641125s
Mar 22 21:33:16.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.015163238s
Mar 22 21:33:18.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.017052638s
Mar 22 21:33:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015217443s
Mar 22 21:33:22.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.015706088s
Mar 22 21:33:24.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.020112549s
Mar 22 21:33:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.015296895s
Mar 22 21:33:28.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.019770219s
Mar 22 21:33:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014701518s
Mar 22 21:33:32.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015764643s
Mar 22 21:33:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.014868043s
Mar 22 21:33:36.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015362319s
Mar 22 21:33:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014741501s
Mar 22 21:33:40.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.017448502s
Mar 22 21:33:42.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.014447449s
Mar 22 21:33:44.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.016560179s
Mar 22 21:33:46.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016552736s
Mar 22 21:33:48.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015792739s
Mar 22 21:33:50.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016635576s
Mar 22 21:33:52.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.01613024s
Mar 22 21:33:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.01541429s
Mar 22 21:33:56.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015701831s
Mar 22 21:33:56.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.019629982s
STEP: removing the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 off the node pool-v7t41yxh0-q56kh 03/22/23 21:33:56.226
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 03/22/23 21:33:56.245
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:33:56.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1941" for this suite. 03/22/23 21:33:56.268
------------------------------
• [SLOW TEST] [304.350 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:28:51.927
    Mar 22 21:28:51.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename sched-pred 03/22/23 21:28:51.929
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:28:51.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:28:51.971
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 22 21:28:52.004: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 22 21:28:52.016: INFO: Waiting for terminating namespaces to be deleted...
    Mar 22 21:28:52.020: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56k5 before test
    Mar 22 21:28:52.031: INFO: cilium-nkg6k from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: coredns-9765d8f5f-k57jv from kube-system started at 2023-03-22 19:42:05 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: cpc-bridge-proxy-mvx4m from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: csi-do-node-q2wlc from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: do-node-agent-4mgkz from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: konnectivity-agent-s74g2 from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: kube-proxy-p8crx from kube-system started at 2023-03-22 19:41:26 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: sonobuoy from sonobuoy started at 2023-03-22 20:04:10 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-6czrq from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 21:28:52.033: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.033: INFO: pod-service-account-mountsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.033: INFO: pod-service-account-nomountsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.033: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.033: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kh before test
    Mar 22 21:28:52.073: INFO: cilium-8ks5z from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: cilium-operator-55c59769f8-kp64k from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container cilium-operator ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: coredns-9765d8f5f-9hz8r from kube-system started at 2023-03-22 20:33:48 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container coredns ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: cpc-bridge-proxy-bkpg5 from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: csi-do-node-mlbms from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: do-node-agent-g7j2s from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: konnectivity-agent-7nwnk from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: kube-proxy-tr8ql from kube-system started at 2023-03-22 19:41:18 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: sonobuoy-e2e-job-0f9deb38bbcd4941 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container e2e ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-r66b5 from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 21:28:52.073: INFO: pod-service-account-defaultsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.073: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.073: INFO: 
    Logging pods the apiserver thinks is on node pool-v7t41yxh0-q56kk before test
    Mar 22 21:28:52.085: INFO: cilium-bzq4m from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container cilium-agent ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: cpc-bridge-proxy-4xt8w from kube-system started at 2023-03-22 19:41:54 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container cpc-bridge-proxy ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: csi-do-node-9svlx from kube-system started at 2023-03-22 19:42:11 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container csi-do-plugin ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: do-node-agent-wt4p9 from kube-system started at 2023-03-22 19:42:14 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container do-node-agent ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: konnectivity-agent-nrnds from kube-system started at 2023-03-22 19:41:33 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: kube-proxy-cgktr from kube-system started at 2023-03-22 19:41:22 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: sonobuoy-systemd-logs-daemon-set-50eb8262e4ba42e4-lxkwv from sonobuoy started at 2023-03-22 20:04:13 +0000 UTC (2 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 22 21:28:52.086: INFO: pod-service-account-defaultsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.086: INFO: pod-service-account-mountsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.086: INFO: pod-service-account-mountsa-nomountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.086: INFO: pod-service-account-nomountsa from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.086: INFO: 	Container token-test ready: false, restart count 0
    Mar 22 21:28:52.087: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-3461 started at 2023-03-22 21:28:51 +0000 UTC (1 container statuses recorded)
    Mar 22 21:28:52.087: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/22/23 21:28:52.087
    Mar 22 21:28:52.095: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1941" to be "running"
    Mar 22 21:28:52.113: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.833313ms
    Mar 22 21:28:54.121: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.025923283s
    Mar 22 21:28:54.121: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/22/23 21:28:54.125
    STEP: Trying to apply a random label on the found node. 03/22/23 21:28:54.147
    STEP: verifying the node has the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 95 03/22/23 21:28:54.161
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/22/23 21:28:54.168
    Mar 22 21:28:54.177: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1941" to be "not pending"
    Mar 22 21:28:54.187: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.987728ms
    Mar 22 21:28:56.195: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017975381s
    Mar 22 21:28:56.195: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.124.0.3 on the node which pod4 resides and expect not scheduled 03/22/23 21:28:56.195
    Mar 22 21:28:56.206: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1941" to be "not pending"
    Mar 22 21:28:56.215: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468363ms
    Mar 22 21:28:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01476213s
    Mar 22 21:29:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016230548s
    Mar 22 21:29:02.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014694091s
    Mar 22 21:29:04.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027754069s
    Mar 22 21:29:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015668802s
    Mar 22 21:29:08.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015222453s
    Mar 22 21:29:10.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016296348s
    Mar 22 21:29:12.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023025222s
    Mar 22 21:29:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01611559s
    Mar 22 21:29:16.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016374371s
    Mar 22 21:29:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.015575556s
    Mar 22 21:29:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01532184s
    Mar 22 21:29:22.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023112804s
    Mar 22 21:29:24.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022634018s
    Mar 22 21:29:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015249035s
    Mar 22 21:29:28.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.016201122s
    Mar 22 21:29:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015625618s
    Mar 22 21:29:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015461914s
    Mar 22 21:29:34.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.036774409s
    Mar 22 21:29:36.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016029777s
    Mar 22 21:29:38.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019229692s
    Mar 22 21:29:40.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.02311633s
    Mar 22 21:29:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.015093696s
    Mar 22 21:29:44.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015536682s
    Mar 22 21:29:46.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016912191s
    Mar 22 21:29:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015500139s
    Mar 22 21:29:50.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015805413s
    Mar 22 21:29:52.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015497825s
    Mar 22 21:29:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015253439s
    Mar 22 21:29:56.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.02032559s
    Mar 22 21:29:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014844084s
    Mar 22 21:30:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016224576s
    Mar 22 21:30:02.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022920657s
    Mar 22 21:30:04.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016050138s
    Mar 22 21:30:06.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027944864s
    Mar 22 21:30:08.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015434513s
    Mar 22 21:30:10.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016847719s
    Mar 22 21:30:12.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015248424s
    Mar 22 21:30:14.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015376842s
    Mar 22 21:30:16.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016399832s
    Mar 22 21:30:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015358833s
    Mar 22 21:30:20.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018221632s
    Mar 22 21:30:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014861169s
    Mar 22 21:30:24.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015285908s
    Mar 22 21:30:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.015242465s
    Mar 22 21:30:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014727985s
    Mar 22 21:30:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015340901s
    Mar 22 21:30:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015195034s
    Mar 22 21:30:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015202648s
    Mar 22 21:30:36.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.014493349s
    Mar 22 21:30:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014760512s
    Mar 22 21:30:40.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017363745s
    Mar 22 21:30:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015386034s
    Mar 22 21:30:44.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016552563s
    Mar 22 21:30:46.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.016406394s
    Mar 22 21:30:48.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01612088s
    Mar 22 21:30:50.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01889966s
    Mar 22 21:30:52.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016019679s
    Mar 22 21:30:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015284991s
    Mar 22 21:30:56.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01614843s
    Mar 22 21:30:58.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.015825985s
    Mar 22 21:31:00.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017853796s
    Mar 22 21:31:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.015480297s
    Mar 22 21:31:04.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.015719237s
    Mar 22 21:31:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.015202985s
    Mar 22 21:31:08.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.015877984s
    Mar 22 21:31:10.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.017048925s
    Mar 22 21:31:12.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.015991703s
    Mar 22 21:31:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016342676s
    Mar 22 21:31:16.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.022806978s
    Mar 22 21:31:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.015507104s
    Mar 22 21:31:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01490963s
    Mar 22 21:31:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.014866684s
    Mar 22 21:31:24.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016074208s
    Mar 22 21:31:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.015286107s
    Mar 22 21:31:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.015071227s
    Mar 22 21:31:30.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015797646s
    Mar 22 21:31:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.0156517s
    Mar 22 21:31:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015204343s
    Mar 22 21:31:36.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.014982569s
    Mar 22 21:31:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015383246s
    Mar 22 21:31:40.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015507804s
    Mar 22 21:31:42.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.018177377s
    Mar 22 21:31:44.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016755841s
    Mar 22 21:31:46.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015408969s
    Mar 22 21:31:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.015356502s
    Mar 22 21:31:50.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.015508158s
    Mar 22 21:31:52.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.020624222s
    Mar 22 21:31:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.015263655s
    Mar 22 21:31:56.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.019270494s
    Mar 22 21:31:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015435006s
    Mar 22 21:32:00.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.016433319s
    Mar 22 21:32:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015634097s
    Mar 22 21:32:04.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.015295189s
    Mar 22 21:32:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.015065066s
    Mar 22 21:32:08.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.015892557s
    Mar 22 21:32:10.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016427648s
    Mar 22 21:32:12.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014829716s
    Mar 22 21:32:14.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.015585306s
    Mar 22 21:32:16.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017345033s
    Mar 22 21:32:18.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.015482097s
    Mar 22 21:32:20.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017064044s
    Mar 22 21:32:22.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015693783s
    Mar 22 21:32:24.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.017704964s
    Mar 22 21:32:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015517745s
    Mar 22 21:32:28.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015154092s
    Mar 22 21:32:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.015458158s
    Mar 22 21:32:32.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015348788s
    Mar 22 21:32:34.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.014680868s
    Mar 22 21:32:36.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.014650886s
    Mar 22 21:32:38.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.020299026s
    Mar 22 21:32:40.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016548667s
    Mar 22 21:32:42.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015388499s
    Mar 22 21:32:44.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.014669199s
    Mar 22 21:32:46.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014879156s
    Mar 22 21:32:48.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.01501313s
    Mar 22 21:32:50.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016802905s
    Mar 22 21:32:52.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.0154209s
    Mar 22 21:32:54.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.015777838s
    Mar 22 21:32:56.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017329488s
    Mar 22 21:32:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015285381s
    Mar 22 21:33:00.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.015484308s
    Mar 22 21:33:02.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.015253326s
    Mar 22 21:33:04.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.015557081s
    Mar 22 21:33:06.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015338064s
    Mar 22 21:33:08.227: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.021327559s
    Mar 22 21:33:10.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.014943139s
    Mar 22 21:33:12.224: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.01782179s
    Mar 22 21:33:14.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016641125s
    Mar 22 21:33:16.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.015163238s
    Mar 22 21:33:18.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.017052638s
    Mar 22 21:33:20.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015217443s
    Mar 22 21:33:22.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.015706088s
    Mar 22 21:33:24.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.020112549s
    Mar 22 21:33:26.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.015296895s
    Mar 22 21:33:28.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.019770219s
    Mar 22 21:33:30.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014701518s
    Mar 22 21:33:32.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015764643s
    Mar 22 21:33:34.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.014868043s
    Mar 22 21:33:36.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015362319s
    Mar 22 21:33:38.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014741501s
    Mar 22 21:33:40.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.017448502s
    Mar 22 21:33:42.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.014447449s
    Mar 22 21:33:44.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.016560179s
    Mar 22 21:33:46.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016552736s
    Mar 22 21:33:48.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015792739s
    Mar 22 21:33:50.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016635576s
    Mar 22 21:33:52.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.01613024s
    Mar 22 21:33:54.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.01541429s
    Mar 22 21:33:56.222: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015701831s
    Mar 22 21:33:56.225: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.019629982s
    STEP: removing the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 off the node pool-v7t41yxh0-q56kh 03/22/23 21:33:56.226
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b925c1e0-d7da-46e4-acc8-f88dfa92ef99 03/22/23 21:33:56.245
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:33:56.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1941" for this suite. 03/22/23 21:33:56.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:33:56.28
Mar 22 21:33:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 21:33:56.282
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:33:56.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:33:56.305
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 03/22/23 21:33:56.312
STEP: Counting existing ResourceQuota 03/22/23 21:34:01.316
STEP: Creating a ResourceQuota 03/22/23 21:34:06.323
STEP: Ensuring resource quota status is calculated 03/22/23 21:34:06.331
STEP: Creating a Secret 03/22/23 21:34:08.342
STEP: Ensuring resource quota status captures secret creation 03/22/23 21:34:08.353
STEP: Deleting a secret 03/22/23 21:34:10.359
STEP: Ensuring resource quota status released usage 03/22/23 21:34:10.368
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 21:34:12.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-387" for this suite. 03/22/23 21:34:12.391
------------------------------
• [SLOW TEST] [16.131 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:33:56.28
    Mar 22 21:33:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 21:33:56.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:33:56.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:33:56.305
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 03/22/23 21:33:56.312
    STEP: Counting existing ResourceQuota 03/22/23 21:34:01.316
    STEP: Creating a ResourceQuota 03/22/23 21:34:06.323
    STEP: Ensuring resource quota status is calculated 03/22/23 21:34:06.331
    STEP: Creating a Secret 03/22/23 21:34:08.342
    STEP: Ensuring resource quota status captures secret creation 03/22/23 21:34:08.353
    STEP: Deleting a secret 03/22/23 21:34:10.359
    STEP: Ensuring resource quota status released usage 03/22/23 21:34:10.368
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:34:12.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-387" for this suite. 03/22/23 21:34:12.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:34:12.415
Mar 22 21:34:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 21:34:12.417
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:12.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:12.438
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 03/22/23 21:34:12.445
STEP: setting up watch 03/22/23 21:34:12.445
STEP: submitting the pod to kubernetes 03/22/23 21:34:12.55
STEP: verifying the pod is in kubernetes 03/22/23 21:34:12.562
STEP: verifying pod creation was observed 03/22/23 21:34:12.569
Mar 22 21:34:12.570: INFO: Waiting up to 5m0s for pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7" in namespace "pods-7277" to be "running"
Mar 22 21:34:12.586: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.340519ms
Mar 22 21:34:14.592: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02177693s
Mar 22 21:34:14.592: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7" satisfied condition "running"
STEP: deleting the pod gracefully 03/22/23 21:34:14.596
STEP: verifying pod deletion was observed 03/22/23 21:34:14.605
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 21:34:17.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7277" for this suite. 03/22/23 21:34:17.44
------------------------------
• [SLOW TEST] [5.034 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:34:12.415
    Mar 22 21:34:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 21:34:12.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:12.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:12.438
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 03/22/23 21:34:12.445
    STEP: setting up watch 03/22/23 21:34:12.445
    STEP: submitting the pod to kubernetes 03/22/23 21:34:12.55
    STEP: verifying the pod is in kubernetes 03/22/23 21:34:12.562
    STEP: verifying pod creation was observed 03/22/23 21:34:12.569
    Mar 22 21:34:12.570: INFO: Waiting up to 5m0s for pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7" in namespace "pods-7277" to be "running"
    Mar 22 21:34:12.586: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.340519ms
    Mar 22 21:34:14.592: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02177693s
    Mar 22 21:34:14.592: INFO: Pod "pod-submit-remove-11e4636b-bfc1-42fe-b51c-53d191f2bfa7" satisfied condition "running"
    STEP: deleting the pod gracefully 03/22/23 21:34:14.596
    STEP: verifying pod deletion was observed 03/22/23 21:34:14.605
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:34:17.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7277" for this suite. 03/22/23 21:34:17.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:34:17.451
Mar 22 21:34:17.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename statefulset 03/22/23 21:34:17.453
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:17.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:17.47
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1090 03/22/23 21:34:17.476
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Mar 22 21:34:17.497: INFO: Found 0 stateful pods, waiting for 1
Mar 22 21:34:27.503: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/22/23 21:34:27.518
W0322 21:34:27.527488      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 22 21:34:27.548: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:34:27.548: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Mar 22 21:34:37.554: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 22 21:34:37.554: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/22/23 21:34:37.562
STEP: Delete all of the StatefulSets 03/22/23 21:34:37.566
STEP: Verify that StatefulSets have been deleted 03/22/23 21:34:37.574
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 22 21:34:37.577: INFO: Deleting all statefulset in ns statefulset-1090
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 22 21:34:37.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1090" for this suite. 03/22/23 21:34:37.595
------------------------------
• [SLOW TEST] [20.151 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:34:17.451
    Mar 22 21:34:17.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename statefulset 03/22/23 21:34:17.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:17.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:17.47
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1090 03/22/23 21:34:17.476
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Mar 22 21:34:17.497: INFO: Found 0 stateful pods, waiting for 1
    Mar 22 21:34:27.503: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/22/23 21:34:27.518
    W0322 21:34:27.527488      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 22 21:34:27.548: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:34:27.548: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 22 21:34:37.554: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 22 21:34:37.554: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/22/23 21:34:37.562
    STEP: Delete all of the StatefulSets 03/22/23 21:34:37.566
    STEP: Verify that StatefulSets have been deleted 03/22/23 21:34:37.574
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 22 21:34:37.577: INFO: Deleting all statefulset in ns statefulset-1090
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:34:37.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1090" for this suite. 03/22/23 21:34:37.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:34:37.607
Mar 22 21:34:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-probe 03/22/23 21:34:37.609
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:37.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:37.632
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 in namespace container-probe-2236 03/22/23 21:34:37.639
Mar 22 21:34:37.657: INFO: Waiting up to 5m0s for pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474" in namespace "container-probe-2236" to be "not pending"
Mar 22 21:34:37.663: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110137ms
Mar 22 21:34:39.669: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474": Phase="Running", Reason="", readiness=true. Elapsed: 2.012102281s
Mar 22 21:34:39.669: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474" satisfied condition "not pending"
Mar 22 21:34:39.669: INFO: Started pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 in namespace container-probe-2236
STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:34:39.669
Mar 22 21:34:39.674: INFO: Initial restart count of pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is 0
Mar 22 21:34:59.741: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 1 (20.066351737s elapsed)
Mar 22 21:35:19.808: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 2 (40.134158014s elapsed)
Mar 22 21:35:39.863: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 3 (1m0.188482819s elapsed)
Mar 22 21:35:59.931: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 4 (1m20.256818674s elapsed)
Mar 22 21:37:00.131: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 5 (2m20.457249756s elapsed)
STEP: deleting the pod 03/22/23 21:37:00.132
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:00.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2236" for this suite. 03/22/23 21:37:00.157
------------------------------
• [SLOW TEST] [142.560 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:34:37.607
    Mar 22 21:34:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-probe 03/22/23 21:34:37.609
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:34:37.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:34:37.632
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 in namespace container-probe-2236 03/22/23 21:34:37.639
    Mar 22 21:34:37.657: INFO: Waiting up to 5m0s for pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474" in namespace "container-probe-2236" to be "not pending"
    Mar 22 21:34:37.663: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110137ms
    Mar 22 21:34:39.669: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474": Phase="Running", Reason="", readiness=true. Elapsed: 2.012102281s
    Mar 22 21:34:39.669: INFO: Pod "liveness-afc7e265-6d32-4970-8bfb-b05f78bef474" satisfied condition "not pending"
    Mar 22 21:34:39.669: INFO: Started pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 in namespace container-probe-2236
    STEP: checking the pod's current state and verifying that restartCount is present 03/22/23 21:34:39.669
    Mar 22 21:34:39.674: INFO: Initial restart count of pod liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is 0
    Mar 22 21:34:59.741: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 1 (20.066351737s elapsed)
    Mar 22 21:35:19.808: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 2 (40.134158014s elapsed)
    Mar 22 21:35:39.863: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 3 (1m0.188482819s elapsed)
    Mar 22 21:35:59.931: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 4 (1m20.256818674s elapsed)
    Mar 22 21:37:00.131: INFO: Restart count of pod container-probe-2236/liveness-afc7e265-6d32-4970-8bfb-b05f78bef474 is now 5 (2m20.457249756s elapsed)
    STEP: deleting the pod 03/22/23 21:37:00.132
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:00.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2236" for this suite. 03/22/23 21:37:00.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:00.172
Mar 22 21:37:00.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:37:00.174
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:00.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:00.2
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:37:00.209
Mar 22 21:37:00.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56" in namespace "downward-api-7842" to be "Succeeded or Failed"
Mar 22 21:37:00.228: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356165ms
Mar 22 21:37:02.235: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01119329s
Mar 22 21:37:04.238: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014568367s
STEP: Saw pod success 03/22/23 21:37:04.238
Mar 22 21:37:04.238: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56" satisfied condition "Succeeded or Failed"
Mar 22 21:37:04.242: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 container client-container: <nil>
STEP: delete the pod 03/22/23 21:37:04.292
Mar 22 21:37:04.307: INFO: Waiting for pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 to disappear
Mar 22 21:37:04.314: INFO: Pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:04.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7842" for this suite. 03/22/23 21:37:04.323
------------------------------
• [4.159 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:00.172
    Mar 22 21:37:00.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:37:00.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:00.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:00.2
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:37:00.209
    Mar 22 21:37:00.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56" in namespace "downward-api-7842" to be "Succeeded or Failed"
    Mar 22 21:37:00.228: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356165ms
    Mar 22 21:37:02.235: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01119329s
    Mar 22 21:37:04.238: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014568367s
    STEP: Saw pod success 03/22/23 21:37:04.238
    Mar 22 21:37:04.238: INFO: Pod "downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56" satisfied condition "Succeeded or Failed"
    Mar 22 21:37:04.242: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 container client-container: <nil>
    STEP: delete the pod 03/22/23 21:37:04.292
    Mar 22 21:37:04.307: INFO: Waiting for pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 to disappear
    Mar 22 21:37:04.314: INFO: Pod downwardapi-volume-462d5898-6627-478c-8e49-1dbd77551c56 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:04.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7842" for this suite. 03/22/23 21:37:04.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:04.334
Mar 22 21:37:04.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 21:37:04.335
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:04.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:04.364
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Mar 22 21:37:04.388: INFO: Waiting up to 5m0s for pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb" in namespace "pods-6263" to be "running and ready"
Mar 22 21:37:04.397: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.05733ms
Mar 22 21:37:04.397: INFO: The phase of Pod server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:37:06.403: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014864756s
Mar 22 21:37:06.403: INFO: The phase of Pod server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb is Running (Ready = true)
Mar 22 21:37:06.403: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb" satisfied condition "running and ready"
Mar 22 21:37:06.428: INFO: Waiting up to 5m0s for pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179" in namespace "pods-6263" to be "Succeeded or Failed"
Mar 22 21:37:06.436: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900722ms
Mar 22 21:37:08.442: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01340453s
Mar 22 21:37:10.441: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012637789s
STEP: Saw pod success 03/22/23 21:37:10.441
Mar 22 21:37:10.442: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179" satisfied condition "Succeeded or Failed"
Mar 22 21:37:10.446: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 container env3cont: <nil>
STEP: delete the pod 03/22/23 21:37:10.472
Mar 22 21:37:10.489: INFO: Waiting for pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 to disappear
Mar 22 21:37:10.493: INFO: Pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:10.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6263" for this suite. 03/22/23 21:37:10.5
------------------------------
• [SLOW TEST] [6.179 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:04.334
    Mar 22 21:37:04.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 21:37:04.335
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:04.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:04.364
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Mar 22 21:37:04.388: INFO: Waiting up to 5m0s for pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb" in namespace "pods-6263" to be "running and ready"
    Mar 22 21:37:04.397: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.05733ms
    Mar 22 21:37:04.397: INFO: The phase of Pod server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:37:06.403: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014864756s
    Mar 22 21:37:06.403: INFO: The phase of Pod server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb is Running (Ready = true)
    Mar 22 21:37:06.403: INFO: Pod "server-envvars-758d8311-3f9e-4fbe-bfd8-85ae68ecf1fb" satisfied condition "running and ready"
    Mar 22 21:37:06.428: INFO: Waiting up to 5m0s for pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179" in namespace "pods-6263" to be "Succeeded or Failed"
    Mar 22 21:37:06.436: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900722ms
    Mar 22 21:37:08.442: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01340453s
    Mar 22 21:37:10.441: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012637789s
    STEP: Saw pod success 03/22/23 21:37:10.441
    Mar 22 21:37:10.442: INFO: Pod "client-envvars-145dae74-286a-442d-8f9b-c29c786ea179" satisfied condition "Succeeded or Failed"
    Mar 22 21:37:10.446: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 container env3cont: <nil>
    STEP: delete the pod 03/22/23 21:37:10.472
    Mar 22 21:37:10.489: INFO: Waiting for pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 to disappear
    Mar 22 21:37:10.493: INFO: Pod client-envvars-145dae74-286a-442d-8f9b-c29c786ea179 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:10.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6263" for this suite. 03/22/23 21:37:10.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:10.521
Mar 22 21:37:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename daemonsets 03/22/23 21:37:10.523
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:10.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:10.557
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 03/22/23 21:37:10.599
STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 21:37:10.609
Mar 22 21:37:10.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 21:37:10.621: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 21:37:11.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 21:37:11.645: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
Mar 22 21:37:12.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 21:37:12.637: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/22/23 21:37:12.641
Mar 22 21:37:12.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 21:37:12.675: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 21:37:13.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 22 21:37:13.688: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
Mar 22 21:37:14.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 22 21:37:14.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/22/23 21:37:14.687
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/22/23 21:37:14.695
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3510, will wait for the garbage collector to delete the pods 03/22/23 21:37:14.696
Mar 22 21:37:14.761: INFO: Deleting DaemonSet.extensions daemon-set took: 8.179953ms
Mar 22 21:37:14.862: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.727086ms
Mar 22 21:37:17.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 22 21:37:17.267: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 22 21:37:17.272: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49671"},"items":null}

Mar 22 21:37:17.276: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49671"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3510" for this suite. 03/22/23 21:37:17.308
------------------------------
• [SLOW TEST] [6.796 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:10.521
    Mar 22 21:37:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename daemonsets 03/22/23 21:37:10.523
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:10.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:10.557
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 03/22/23 21:37:10.599
    STEP: Check that daemon pods launch on every node of the cluster. 03/22/23 21:37:10.609
    Mar 22 21:37:10.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 21:37:10.621: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 21:37:11.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 21:37:11.645: INFO: Node pool-v7t41yxh0-q56k5 is running 0 daemon pod, expected 1
    Mar 22 21:37:12.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 21:37:12.637: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/22/23 21:37:12.641
    Mar 22 21:37:12.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 21:37:12.675: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 21:37:13.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 22 21:37:13.688: INFO: Node pool-v7t41yxh0-q56kh is running 0 daemon pod, expected 1
    Mar 22 21:37:14.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 22 21:37:14.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/22/23 21:37:14.687
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/22/23 21:37:14.695
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3510, will wait for the garbage collector to delete the pods 03/22/23 21:37:14.696
    Mar 22 21:37:14.761: INFO: Deleting DaemonSet.extensions daemon-set took: 8.179953ms
    Mar 22 21:37:14.862: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.727086ms
    Mar 22 21:37:17.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 22 21:37:17.267: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 22 21:37:17.272: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49671"},"items":null}

    Mar 22 21:37:17.276: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49671"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3510" for this suite. 03/22/23 21:37:17.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:17.338
Mar 22 21:37:17.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename endpointslicemirroring 03/22/23 21:37:17.34
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:17.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:17.384
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/22/23 21:37:17.414
Mar 22 21:37:17.427: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/22/23 21:37:19.433
STEP: mirroring deletion of a custom Endpoint 03/22/23 21:37:19.446
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:19.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3122" for this suite. 03/22/23 21:37:19.471
------------------------------
• [2.142 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:17.338
    Mar 22 21:37:17.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename endpointslicemirroring 03/22/23 21:37:17.34
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:17.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:17.384
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/22/23 21:37:17.414
    Mar 22 21:37:17.427: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/22/23 21:37:19.433
    STEP: mirroring deletion of a custom Endpoint 03/22/23 21:37:19.446
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:19.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3122" for this suite. 03/22/23 21:37:19.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:19.503
Mar 22 21:37:19.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename deployment 03/22/23 21:37:19.512
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:19.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:19.554
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 22 21:37:19.562: INFO: Creating deployment "webserver-deployment"
Mar 22 21:37:19.570: INFO: Waiting for observed generation 1
Mar 22 21:37:21.581: INFO: Waiting for all required pods to come up
Mar 22 21:37:21.596: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/22/23 21:37:21.597
Mar 22 21:37:21.597: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-495pd" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.598: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vt9j7" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k4klt" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hhd4l" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-n9dh6" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s7kz8" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k89w2" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m5rpc" in namespace "deployment-9243" to be "running"
Mar 22 21:37:21.602: INFO: Pod "webserver-deployment-7f5969cbc7-495pd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576265ms
Mar 22 21:37:21.602: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407745ms
Mar 22 21:37:21.603: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622839ms
Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023938ms
Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182778ms
Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.214096ms
Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430497ms
Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452522ms
Mar 22 21:37:23.607: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt": Phase="Running", Reason="", readiness=true. Elapsed: 2.00823549s
Mar 22 21:37:23.608: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt" satisfied condition "running"
Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-495pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011521681s
Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-495pd" satisfied condition "running"
Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.009654122s
Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l" satisfied condition "running"
Mar 22 21:37:23.610: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010429599s
Mar 22 21:37:23.610: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8" satisfied condition "running"
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010807355s
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc" satisfied condition "running"
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7": Phase="Running", Reason="", readiness=true. Elapsed: 2.011997831s
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7" satisfied condition "running"
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011032887s
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2" satisfied condition "running"
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011552894s
Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6" satisfied condition "running"
Mar 22 21:37:23.612: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 22 21:37:23.623: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 22 21:37:23.640: INFO: Updating deployment webserver-deployment
Mar 22 21:37:23.641: INFO: Waiting for observed generation 2
Mar 22 21:37:25.663: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 22 21:37:25.668: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 22 21:37:25.672: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 22 21:37:25.688: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 22 21:37:25.689: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 22 21:37:25.694: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 22 21:37:25.702: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 22 21:37:25.702: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 22 21:37:25.714: INFO: Updating deployment webserver-deployment
Mar 22 21:37:25.714: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 22 21:37:25.729: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 22 21:37:27.772: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 22 21:37:27.792: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9243  89c9e246-717b-4bba-bc70-796ac9eca975 50046 3 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 21:37:25 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-22 21:37:25 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 22 21:37:27.799: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9243  2b4baec4-4e0e-4956-9b81-d1c101efaf8d 50040 3 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 89c9e246-717b-4bba-bc70-796ac9eca975 0xc00433c6c7 0xc00433c6c8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c9e246-717b-4bba-bc70-796ac9eca975\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 22 21:37:27.799: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 22 21:37:27.799: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9243  2d50ebf2-b169-406e-80bb-aaa4669ff070 50044 3 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 89c9e246-717b-4bba-bc70-796ac9eca975 0xc00433c5d7 0xc00433c5d8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c9e246-717b-4bba-bc70-796ac9eca975\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-6kj9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6kj9w webserver-deployment-7f5969cbc7- deployment-9243  6c9a792d-7f16-42eb-ac0a-5edad52d2bd9 50043 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433cc67 0xc00433cc68}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlkcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlkcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-758r9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-758r9 webserver-deployment-7f5969cbc7- deployment-9243  c96b3b53-8a6a-4fcf-b689-0ef957cc70e4 49805 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433ce20 0xc00433ce21}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z98xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z98xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.6,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e76cc0b3bbdf629bf2de55befdd16488ed783dd9581d7016f417dd1c7163f236,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-8vdj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8vdj9 webserver-deployment-7f5969cbc7- deployment-9243  564d6662-871b-429b-b84b-413b9a496a2d 50072 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d000 0xc00433d001}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbrn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbrn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-92r87" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92r87 webserver-deployment-7f5969cbc7- deployment-9243  87dac4dc-efdb-4ecf-a39f-9c48a0279fe0 50064 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d1c0 0xc00433d1c1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9l2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9l2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-b98m5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b98m5 webserver-deployment-7f5969cbc7- deployment-9243  c5685035-3181-413c-a0dc-b2800e84b170 50049 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d370 0xc00433d371}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l45q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l45q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-brkb9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-brkb9 webserver-deployment-7f5969cbc7- deployment-9243  8394be07-9eff-4902-a567-6e19f7b8c303 50048 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d520 0xc00433d521}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crzmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crzmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-cq447" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cq447 webserver-deployment-7f5969cbc7- deployment-9243  b31601b0-8b85-41df-989f-f307195c2eb3 49802 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d6d0 0xc00433d6d1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhvwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhvwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.87,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://358efe8a9b0486e5087f77d3dc9b03aafae23ceeab553273d168809cc1b057b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k4klt webserver-deployment-7f5969cbc7- deployment-9243  fbbfaa44-3088-4cc7-b07e-3cb99f2af784 49811 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d8a0 0xc00433d8a1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqr56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqr56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.77,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9e23a20bf48d4964d46c6970a015b9bb9ea36706995afee0906271eeedfb1b07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k89w2 webserver-deployment-7f5969cbc7- deployment-9243  686e970c-92ae-4539-9a12-acd1b18bff04 49829 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433da80 0xc00433da81}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9t9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9t9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.250,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d187ae6ecf9c47790ac1d9952c743a14fa1c0ece9899b65c5aca0752b7d14fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.851: INFO: Pod "webserver-deployment-7f5969cbc7-k99f8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k99f8 webserver-deployment-7f5969cbc7- deployment-9243  8624f6e3-2252-4334-aa15-f6ef548ee05b 50047 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433dc60 0xc00433dc61}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rz2h5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rz2h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.852: INFO: Pod "webserver-deployment-7f5969cbc7-lp4sr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lp4sr webserver-deployment-7f5969cbc7- deployment-9243  fe8f2e3b-c94c-40b8-8a6e-b983a73aaf3a 50025 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433de10 0xc00433de11}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xrlkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xrlkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.853: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m5rpc webserver-deployment-7f5969cbc7- deployment-9243  538d28a9-6627-4fee-9a24-ff3a48d58938 49821 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433df60 0xc00433df61}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmxhh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmxhh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.140,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fe4f0f6bc84af809ed274c7f2c462497373ef6088dde6036edf944515cc71192,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.853: INFO: Pod "webserver-deployment-7f5969cbc7-mtzt7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtzt7 webserver-deployment-7f5969cbc7- deployment-9243  43318eac-4f25-4afb-8f4b-93d4a98b196c 50063 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b025d0 0xc003b025d1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54vjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54vjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.860: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n9dh6 webserver-deployment-7f5969cbc7- deployment-9243  e6194c50-4e59-42a7-a606-c70d5dd671c8 49835 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02850 0xc003b02851}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bfnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bfnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.20,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://451a47535b97f8ee6fb55478e57c933be13611ab0ca69092c3f2893fc265dd60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.876: INFO: Pod "webserver-deployment-7f5969cbc7-nmdvt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmdvt webserver-deployment-7f5969cbc7- deployment-9243  c18ea2d8-0092-4123-bc96-2cad6e13e8a2 50007 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02a90 0xc003b02a91}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn7tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn7tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-q2vz6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q2vz6 webserver-deployment-7f5969cbc7- deployment-9243  07e54c6d-3a61-4796-b3fa-10c64f7f7f56 50059 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02c50 0xc003b02c51}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxs7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxs7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s7kz8 webserver-deployment-7f5969cbc7- deployment-9243  33df3369-0182-4cc0-834d-538cb2c1363b 49816 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02e00 0xc003b02e01}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbx2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbx2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.61,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c148dfbafc50bf74606949bd35411feb27d8236d03ed461535a34910c8fce2a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-sfdqs" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sfdqs webserver-deployment-7f5969cbc7- deployment-9243  65b41474-3299-489c-b9cd-0cdbd109c730 50022 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02ff0 0xc003b02ff1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ddmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ddmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.932: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vt9j7 webserver-deployment-7f5969cbc7- deployment-9243  ef9f5e1f-5020-466a-84f2-4e3d72cabe6c 49814 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b03140 0xc003b03141}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d24tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d24tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.27,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://94d055a38ebac49472b298f6e49faf770a79ff8bbc690a437781cb3b70c33f7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.965: INFO: Pod "webserver-deployment-7f5969cbc7-wfqsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wfqsp webserver-deployment-7f5969cbc7- deployment-9243  2e87014f-1b5e-41a7-a10f-1439fe9223ce 50033 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b03310 0xc003b03311}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn8rd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn8rd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.965: INFO: Pod "webserver-deployment-d9f79cb5-2ggf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2ggf5 webserver-deployment-d9f79cb5- deployment-9243  1dfbaf78-8e6d-4aae-83c0-72d2ed745621 50073 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003b034af 0xc003b034c0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62lgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62lgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.17,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.966: INFO: Pod "webserver-deployment-d9f79cb5-4vvrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4vvrp webserver-deployment-d9f79cb5- deployment-9243  dac9b594-0842-447d-8d6f-7a96f5bccfcb 49959 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003b03fff 0xc003c3e010}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qp88g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qp88g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.121,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.966: INFO: Pod "webserver-deployment-d9f79cb5-88cln" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-88cln webserver-deployment-d9f79cb5- deployment-9243  7c481002-b62f-4c96-be5c-ab055597a51a 50118 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e1ff 0xc003c3e210}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcj7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcj7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.120,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-8x7t9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8x7t9 webserver-deployment-d9f79cb5- deployment-9243  7284497a-7d7d-40dc-899a-f5c18b651e8d 50042 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e3ff 0xc003c3e410}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq9qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq9qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-9lmsj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9lmsj webserver-deployment-d9f79cb5- deployment-9243  cb633436-d5e2-4640-ab6d-4dac8e65c1fa 50052 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e5cf 0xc003c3e5e0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqxff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqxff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-bnrzg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bnrzg webserver-deployment-d9f79cb5- deployment-9243  bc9cc764-80e2-4001-8cdf-bd8b9f015a78 50053 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e79f 0xc003c3e7b0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bhlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bhlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.968: INFO: Pod "webserver-deployment-d9f79cb5-c576t" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c576t webserver-deployment-d9f79cb5- deployment-9243  75cac1a2-b1da-4ec3-af86-cbfd5c5062c0 50074 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e96f 0xc003c3e980}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv4cq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv4cq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:27.968: INFO: Pod "webserver-deployment-d9f79cb5-frvqd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-frvqd webserver-deployment-d9f79cb5- deployment-9243  e1e176b3-ccfb-4c06-9dd5-5c9f8fb5dc1a 50062 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3eb3f 0xc003c3eb50}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmb7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmb7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:28.041: INFO: Pod "webserver-deployment-d9f79cb5-kbj72" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kbj72 webserver-deployment-d9f79cb5- deployment-9243  5044eaa5-fc48-472d-9c4d-ec0297912ab3 50061 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3ed0f 0xc003c3ed20}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv58w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv58w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:28.041: INFO: Pod "webserver-deployment-d9f79cb5-lkc7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lkc7b webserver-deployment-d9f79cb5- deployment-9243  9524f733-30ef-4e7c-9313-c9c95e5ace01 50032 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3eedf 0xc003c3eef0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqx8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqx8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-lrhd4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrhd4 webserver-deployment-d9f79cb5- deployment-9243  9a3843bf-9547-4f01-9a7e-43b8e550e65f 50057 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f05f 0xc003c3f070}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8r87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8r87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-md8bz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-md8bz webserver-deployment-d9f79cb5- deployment-9243  2ad074f4-3d11-48e4-a8a4-13fcd51dd796 49897 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f22f 0xc003c3f240}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lqbrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lqbrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-s2nwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s2nwx webserver-deployment-d9f79cb5- deployment-9243  4f8f2e3b-61d4-4b7f-b129-3eb4f739e602 49922 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f40f 0xc003c3f420}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk8t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk8t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9243" for this suite. 03/22/23 21:37:28.134
------------------------------
• [SLOW TEST] [8.693 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:19.503
    Mar 22 21:37:19.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename deployment 03/22/23 21:37:19.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:19.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:19.554
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 22 21:37:19.562: INFO: Creating deployment "webserver-deployment"
    Mar 22 21:37:19.570: INFO: Waiting for observed generation 1
    Mar 22 21:37:21.581: INFO: Waiting for all required pods to come up
    Mar 22 21:37:21.596: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/22/23 21:37:21.597
    Mar 22 21:37:21.597: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-495pd" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.598: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vt9j7" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k4klt" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hhd4l" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-n9dh6" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s7kz8" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k89w2" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.599: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m5rpc" in namespace "deployment-9243" to be "running"
    Mar 22 21:37:21.602: INFO: Pod "webserver-deployment-7f5969cbc7-495pd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576265ms
    Mar 22 21:37:21.602: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407745ms
    Mar 22 21:37:21.603: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622839ms
    Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023938ms
    Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182778ms
    Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.214096ms
    Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430497ms
    Mar 22 21:37:21.605: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452522ms
    Mar 22 21:37:23.607: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt": Phase="Running", Reason="", readiness=true. Elapsed: 2.00823549s
    Mar 22 21:37:23.608: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt" satisfied condition "running"
    Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-495pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011521681s
    Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-495pd" satisfied condition "running"
    Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l": Phase="Running", Reason="", readiness=true. Elapsed: 2.009654122s
    Mar 22 21:37:23.609: INFO: Pod "webserver-deployment-7f5969cbc7-hhd4l" satisfied condition "running"
    Mar 22 21:37:23.610: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010429599s
    Mar 22 21:37:23.610: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8" satisfied condition "running"
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010807355s
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc" satisfied condition "running"
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7": Phase="Running", Reason="", readiness=true. Elapsed: 2.011997831s
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7" satisfied condition "running"
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011032887s
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2" satisfied condition "running"
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011552894s
    Mar 22 21:37:23.611: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6" satisfied condition "running"
    Mar 22 21:37:23.612: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 22 21:37:23.623: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 22 21:37:23.640: INFO: Updating deployment webserver-deployment
    Mar 22 21:37:23.641: INFO: Waiting for observed generation 2
    Mar 22 21:37:25.663: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 22 21:37:25.668: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 22 21:37:25.672: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 22 21:37:25.688: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 22 21:37:25.689: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 22 21:37:25.694: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 22 21:37:25.702: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 22 21:37:25.702: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 22 21:37:25.714: INFO: Updating deployment webserver-deployment
    Mar 22 21:37:25.714: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 22 21:37:25.729: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 22 21:37:27.772: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 22 21:37:27.792: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9243  89c9e246-717b-4bba-bc70-796ac9eca975 50046 3 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-22 21:37:25 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-22 21:37:25 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 22 21:37:27.799: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9243  2b4baec4-4e0e-4956-9b81-d1c101efaf8d 50040 3 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 89c9e246-717b-4bba-bc70-796ac9eca975 0xc00433c6c7 0xc00433c6c8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c9e246-717b-4bba-bc70-796ac9eca975\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 21:37:27.799: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 22 21:37:27.799: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9243  2d50ebf2-b169-406e-80bb-aaa4669ff070 50044 3 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 89c9e246-717b-4bba-bc70-796ac9eca975 0xc00433c5d7 0xc00433c5d8}] [] [{kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c9e246-717b-4bba-bc70-796ac9eca975\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433c668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-6kj9w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6kj9w webserver-deployment-7f5969cbc7- deployment-9243  6c9a792d-7f16-42eb-ac0a-5edad52d2bd9 50043 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433cc67 0xc00433cc68}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlkcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlkcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-758r9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-758r9 webserver-deployment-7f5969cbc7- deployment-9243  c96b3b53-8a6a-4fcf-b689-0ef957cc70e4 49805 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433ce20 0xc00433ce21}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z98xv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z98xv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.6,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e76cc0b3bbdf629bf2de55befdd16488ed783dd9581d7016f417dd1c7163f236,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-8vdj9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8vdj9 webserver-deployment-7f5969cbc7- deployment-9243  564d6662-871b-429b-b84b-413b9a496a2d 50072 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d000 0xc00433d001}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbrn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbrn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-92r87" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92r87 webserver-deployment-7f5969cbc7- deployment-9243  87dac4dc-efdb-4ecf-a39f-9c48a0279fe0 50064 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d1c0 0xc00433d1c1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9l2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9l2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.830: INFO: Pod "webserver-deployment-7f5969cbc7-b98m5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b98m5 webserver-deployment-7f5969cbc7- deployment-9243  c5685035-3181-413c-a0dc-b2800e84b170 50049 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d370 0xc00433d371}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l45q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l45q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-brkb9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-brkb9 webserver-deployment-7f5969cbc7- deployment-9243  8394be07-9eff-4902-a567-6e19f7b8c303 50048 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d520 0xc00433d521}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crzmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crzmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-cq447" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cq447 webserver-deployment-7f5969cbc7- deployment-9243  b31601b0-8b85-41df-989f-f307195c2eb3 49802 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d6d0 0xc00433d6d1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhvwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhvwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.87,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://358efe8a9b0486e5087f77d3dc9b03aafae23ceeab553273d168809cc1b057b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-k4klt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k4klt webserver-deployment-7f5969cbc7- deployment-9243  fbbfaa44-3088-4cc7-b07e-3cb99f2af784 49811 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433d8a0 0xc00433d8a1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqr56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqr56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.77,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9e23a20bf48d4964d46c6970a015b9bb9ea36706995afee0906271eeedfb1b07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.831: INFO: Pod "webserver-deployment-7f5969cbc7-k89w2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k89w2 webserver-deployment-7f5969cbc7- deployment-9243  686e970c-92ae-4539-9a12-acd1b18bff04 49829 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433da80 0xc00433da81}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9t9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9t9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.250,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d187ae6ecf9c47790ac1d9952c743a14fa1c0ece9899b65c5aca0752b7d14fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.851: INFO: Pod "webserver-deployment-7f5969cbc7-k99f8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k99f8 webserver-deployment-7f5969cbc7- deployment-9243  8624f6e3-2252-4334-aa15-f6ef548ee05b 50047 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433dc60 0xc00433dc61}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rz2h5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rz2h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.852: INFO: Pod "webserver-deployment-7f5969cbc7-lp4sr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lp4sr webserver-deployment-7f5969cbc7- deployment-9243  fe8f2e3b-c94c-40b8-8a6e-b983a73aaf3a 50025 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433de10 0xc00433de11}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xrlkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xrlkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.853: INFO: Pod "webserver-deployment-7f5969cbc7-m5rpc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m5rpc webserver-deployment-7f5969cbc7- deployment-9243  538d28a9-6627-4fee-9a24-ff3a48d58938 49821 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc00433df60 0xc00433df61}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmxhh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmxhh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:10.244.0.140,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fe4f0f6bc84af809ed274c7f2c462497373ef6088dde6036edf944515cc71192,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.853: INFO: Pod "webserver-deployment-7f5969cbc7-mtzt7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtzt7 webserver-deployment-7f5969cbc7- deployment-9243  43318eac-4f25-4afb-8f4b-93d4a98b196c 50063 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b025d0 0xc003b025d1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54vjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54vjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.860: INFO: Pod "webserver-deployment-7f5969cbc7-n9dh6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n9dh6 webserver-deployment-7f5969cbc7- deployment-9243  e6194c50-4e59-42a7-a606-c70d5dd671c8 49835 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02850 0xc003b02851}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bfnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bfnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.20,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://451a47535b97f8ee6fb55478e57c933be13611ab0ca69092c3f2893fc265dd60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.876: INFO: Pod "webserver-deployment-7f5969cbc7-nmdvt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmdvt webserver-deployment-7f5969cbc7- deployment-9243  c18ea2d8-0092-4123-bc96-2cad6e13e8a2 50007 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02a90 0xc003b02a91}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn7tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn7tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-q2vz6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q2vz6 webserver-deployment-7f5969cbc7- deployment-9243  07e54c6d-3a61-4796-b3fa-10c64f7f7f56 50059 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02c50 0xc003b02c51}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxs7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxs7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-s7kz8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s7kz8 webserver-deployment-7f5969cbc7- deployment-9243  33df3369-0182-4cc0-834d-538cb2c1363b 49816 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02e00 0xc003b02e01}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbx2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbx2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.61,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c148dfbafc50bf74606949bd35411feb27d8236d03ed461535a34910c8fce2a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.877: INFO: Pod "webserver-deployment-7f5969cbc7-sfdqs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sfdqs webserver-deployment-7f5969cbc7- deployment-9243  65b41474-3299-489c-b9cd-0cdbd109c730 50022 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b02ff0 0xc003b02ff1}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ddmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ddmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.932: INFO: Pod "webserver-deployment-7f5969cbc7-vt9j7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vt9j7 webserver-deployment-7f5969cbc7- deployment-9243  ef9f5e1f-5020-466a-84f2-4e3d72cabe6c 49814 0 2023-03-22 21:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b03140 0xc003b03141}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d24tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d24tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.27,StartTime:2023-03-22 21:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-22 21:37:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://94d055a38ebac49472b298f6e49faf770a79ff8bbc690a437781cb3b70c33f7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.965: INFO: Pod "webserver-deployment-7f5969cbc7-wfqsp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wfqsp webserver-deployment-7f5969cbc7- deployment-9243  2e87014f-1b5e-41a7-a10f-1439fe9223ce 50033 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 2d50ebf2-b169-406e-80bb-aaa4669ff070 0xc003b03310 0xc003b03311}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d50ebf2-b169-406e-80bb-aaa4669ff070\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn8rd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn8rd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.965: INFO: Pod "webserver-deployment-d9f79cb5-2ggf5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2ggf5 webserver-deployment-d9f79cb5- deployment-9243  1dfbaf78-8e6d-4aae-83c0-72d2ed745621 50073 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003b034af 0xc003b034c0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62lgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62lgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.17,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.966: INFO: Pod "webserver-deployment-d9f79cb5-4vvrp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4vvrp webserver-deployment-d9f79cb5- deployment-9243  dac9b594-0842-447d-8d6f-7a96f5bccfcb 49959 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003b03fff 0xc003c3e010}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qp88g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qp88g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:10.244.0.121,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.966: INFO: Pod "webserver-deployment-d9f79cb5-88cln" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-88cln webserver-deployment-d9f79cb5- deployment-9243  7c481002-b62f-4c96-be5c-ab055597a51a 50118 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e1ff 0xc003c3e210}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcj7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcj7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:10.244.1.120,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-8x7t9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8x7t9 webserver-deployment-d9f79cb5- deployment-9243  7284497a-7d7d-40dc-899a-f5c18b651e8d 50042 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e3ff 0xc003c3e410}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq9qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq9qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-9lmsj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9lmsj webserver-deployment-d9f79cb5- deployment-9243  cb633436-d5e2-4640-ab6d-4dac8e65c1fa 50052 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e5cf 0xc003c3e5e0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqxff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqxff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.967: INFO: Pod "webserver-deployment-d9f79cb5-bnrzg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bnrzg webserver-deployment-d9f79cb5- deployment-9243  bc9cc764-80e2-4001-8cdf-bd8b9f015a78 50053 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e79f 0xc003c3e7b0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bhlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bhlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.968: INFO: Pod "webserver-deployment-d9f79cb5-c576t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-c576t webserver-deployment-d9f79cb5- deployment-9243  75cac1a2-b1da-4ec3-af86-cbfd5c5062c0 50074 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3e96f 0xc003c3e980}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv4cq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv4cq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:27.968: INFO: Pod "webserver-deployment-d9f79cb5-frvqd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-frvqd webserver-deployment-d9f79cb5- deployment-9243  e1e176b3-ccfb-4c06-9dd5-5c9f8fb5dc1a 50062 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3eb3f 0xc003c3eb50}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmb7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmb7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56k5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.2,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:28.041: INFO: Pod "webserver-deployment-d9f79cb5-kbj72" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kbj72 webserver-deployment-d9f79cb5- deployment-9243  5044eaa5-fc48-472d-9c4d-ec0297912ab3 50061 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3ed0f 0xc003c3ed20}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv58w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv58w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:28.041: INFO: Pod "webserver-deployment-d9f79cb5-lkc7b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lkc7b webserver-deployment-d9f79cb5- deployment-9243  9524f733-30ef-4e7c-9313-c9c95e5ace01 50032 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3eedf 0xc003c3eef0}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqx8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqx8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-lrhd4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lrhd4 webserver-deployment-d9f79cb5- deployment-9243  9a3843bf-9547-4f01-9a7e-43b8e550e65f 50057 0 2023-03-22 21:37:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f05f 0xc003c3f070}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8r87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8r87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.3,PodIP:,StartTime:2023-03-22 21:37:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-md8bz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-md8bz webserver-deployment-d9f79cb5- deployment-9243  2ad074f4-3d11-48e4-a8a4-13fcd51dd796 49897 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f22f 0xc003c3f240}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lqbrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lqbrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 22 21:37:28.042: INFO: Pod "webserver-deployment-d9f79cb5-s2nwx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s2nwx webserver-deployment-d9f79cb5- deployment-9243  4f8f2e3b-61d4-4b7f-b129-3eb4f739e602 49922 0 2023-03-22 21:37:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 2b4baec4-4e0e-4956-9b81-d1c101efaf8d 0xc003c3f40f 0xc003c3f420}] [] [{kube-controller-manager Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b4baec4-4e0e-4956-9b81-d1c101efaf8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-22 21:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk8t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk8t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pool-v7t41yxh0-q56kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-22 21:37:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.124.0.4,PodIP:,StartTime:2023-03-22 21:37:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9243" for this suite. 03/22/23 21:37:28.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:28.468
Mar 22 21:37:28.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:37:28.469
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:28.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:28.531
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:37:28.581
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:37:30.23
STEP: Deploying the webhook pod 03/22/23 21:37:30.239
STEP: Wait for the deployment to be ready 03/22/23 21:37:30.255
Mar 22 21:37:30.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 22 21:37:32.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:37:34.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/22/23 21:37:36.291
STEP: Verifying the service has paired with the endpoint 03/22/23 21:37:36.308
Mar 22 21:37:37.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 03/22/23 21:37:37.322
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/22/23 21:37:37.382
STEP: Creating a configMap that should not be mutated 03/22/23 21:37:37.396
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/22/23 21:37:37.409
STEP: Creating a configMap that should be mutated 03/22/23 21:37:37.419
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:37.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1518" for this suite. 03/22/23 21:37:37.549
STEP: Destroying namespace "webhook-1518-markers" for this suite. 03/22/23 21:37:37.559
------------------------------
• [SLOW TEST] [9.098 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:28.468
    Mar 22 21:37:28.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:37:28.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:28.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:28.531
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:37:28.581
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:37:30.23
    STEP: Deploying the webhook pod 03/22/23 21:37:30.239
    STEP: Wait for the deployment to be ready 03/22/23 21:37:30.255
    Mar 22 21:37:30.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 22 21:37:32.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:37:34.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 37, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/22/23 21:37:36.291
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:37:36.308
    Mar 22 21:37:37.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 03/22/23 21:37:37.322
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/22/23 21:37:37.382
    STEP: Creating a configMap that should not be mutated 03/22/23 21:37:37.396
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/22/23 21:37:37.409
    STEP: Creating a configMap that should be mutated 03/22/23 21:37:37.419
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:37.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1518" for this suite. 03/22/23 21:37:37.549
    STEP: Destroying namespace "webhook-1518-markers" for this suite. 03/22/23 21:37:37.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:37.594
Mar 22 21:37:37.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 21:37:37.595
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:37.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:37.643
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 03/22/23 21:37:37.657
STEP: submitting the pod to kubernetes 03/22/23 21:37:37.657
Mar 22 21:37:37.673: INFO: Waiting up to 5m0s for pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" in namespace "pods-717" to be "running and ready"
Mar 22 21:37:37.681: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 7.333319ms
Mar 22 21:37:37.682: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:37:39.687: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013195322s
Mar 22 21:37:39.687: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:37:41.688: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014151216s
Mar 22 21:37:41.698: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:37:43.694: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Running", Reason="", readiness=true. Elapsed: 6.019846302s
Mar 22 21:37:43.694: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Running (Ready = true)
Mar 22 21:37:43.694: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/22/23 21:37:43.698
STEP: updating the pod 03/22/23 21:37:43.707
Mar 22 21:37:44.224: INFO: Successfully updated pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674"
Mar 22 21:37:44.224: INFO: Waiting up to 5m0s for pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" in namespace "pods-717" to be "running"
Mar 22 21:37:44.228: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Running", Reason="", readiness=true. Elapsed: 4.075968ms
Mar 22 21:37:44.228: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/22/23 21:37:44.228
Mar 22 21:37:44.237: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:44.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-717" for this suite. 03/22/23 21:37:44.244
------------------------------
• [SLOW TEST] [6.658 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:37.594
    Mar 22 21:37:37.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 21:37:37.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:37.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:37.643
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 03/22/23 21:37:37.657
    STEP: submitting the pod to kubernetes 03/22/23 21:37:37.657
    Mar 22 21:37:37.673: INFO: Waiting up to 5m0s for pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" in namespace "pods-717" to be "running and ready"
    Mar 22 21:37:37.681: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 7.333319ms
    Mar 22 21:37:37.682: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:37:39.687: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013195322s
    Mar 22 21:37:39.687: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:37:41.688: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014151216s
    Mar 22 21:37:41.698: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:37:43.694: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Running", Reason="", readiness=true. Elapsed: 6.019846302s
    Mar 22 21:37:43.694: INFO: The phase of Pod pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674 is Running (Ready = true)
    Mar 22 21:37:43.694: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/22/23 21:37:43.698
    STEP: updating the pod 03/22/23 21:37:43.707
    Mar 22 21:37:44.224: INFO: Successfully updated pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674"
    Mar 22 21:37:44.224: INFO: Waiting up to 5m0s for pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" in namespace "pods-717" to be "running"
    Mar 22 21:37:44.228: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674": Phase="Running", Reason="", readiness=true. Elapsed: 4.075968ms
    Mar 22 21:37:44.228: INFO: Pod "pod-update-df37083f-7d48-4c80-9c03-2d7cec6b2674" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/22/23 21:37:44.228
    Mar 22 21:37:44.237: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:44.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-717" for this suite. 03/22/23 21:37:44.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:44.253
Mar 22 21:37:44.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:37:44.255
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:44.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:44.277
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-d734dc4d-6dd0-4565-956f-4dd44c02ffb0 03/22/23 21:37:44.285
STEP: Creating a pod to test consume secrets 03/22/23 21:37:44.293
Mar 22 21:37:44.303: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259" in namespace "projected-4362" to be "Succeeded or Failed"
Mar 22 21:37:44.310: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346356ms
Mar 22 21:37:46.316: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013130806s
Mar 22 21:37:48.322: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018464893s
STEP: Saw pod success 03/22/23 21:37:48.322
Mar 22 21:37:48.323: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259" satisfied condition "Succeeded or Failed"
Mar 22 21:37:48.327: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:37:48.339
Mar 22 21:37:48.367: INFO: Waiting for pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 to disappear
Mar 22 21:37:48.379: INFO: Pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4362" for this suite. 03/22/23 21:37:48.387
------------------------------
• [4.148 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:44.253
    Mar 22 21:37:44.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:37:44.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:44.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:44.277
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-d734dc4d-6dd0-4565-956f-4dd44c02ffb0 03/22/23 21:37:44.285
    STEP: Creating a pod to test consume secrets 03/22/23 21:37:44.293
    Mar 22 21:37:44.303: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259" in namespace "projected-4362" to be "Succeeded or Failed"
    Mar 22 21:37:44.310: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346356ms
    Mar 22 21:37:46.316: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013130806s
    Mar 22 21:37:48.322: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018464893s
    STEP: Saw pod success 03/22/23 21:37:48.322
    Mar 22 21:37:48.323: INFO: Pod "pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259" satisfied condition "Succeeded or Failed"
    Mar 22 21:37:48.327: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:37:48.339
    Mar 22 21:37:48.367: INFO: Waiting for pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 to disappear
    Mar 22 21:37:48.379: INFO: Pod pod-projected-secrets-93790ae8-b5e5-4b98-b7d7-1497251c3259 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4362" for this suite. 03/22/23 21:37:48.387
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:48.405
Mar 22 21:37:48.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename proxy 03/22/23 21:37:48.407
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:48.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:48.427
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 22 21:37:48.434: INFO: Creating pod...
Mar 22 21:37:48.445: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4809" to be "running"
Mar 22 21:37:48.449: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651723ms
Mar 22 21:37:50.456: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011463635s
Mar 22 21:37:50.456: INFO: Pod "agnhost" satisfied condition "running"
Mar 22 21:37:50.456: INFO: Creating service...
Mar 22 21:37:50.480: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/DELETE
Mar 22 21:37:50.533: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 22 21:37:50.533: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/GET
Mar 22 21:37:50.542: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 22 21:37:50.543: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/HEAD
Mar 22 21:37:50.551: INFO: http.Client request:HEAD | StatusCode:200
Mar 22 21:37:50.552: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 22 21:37:50.561: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 22 21:37:50.562: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/PATCH
Mar 22 21:37:50.573: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 22 21:37:50.573: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/POST
Mar 22 21:37:50.585: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 22 21:37:50.588: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/PUT
Mar 22 21:37:50.597: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 22 21:37:50.597: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/DELETE
Mar 22 21:37:50.606: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 22 21:37:50.606: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/GET
Mar 22 21:37:50.619: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 22 21:37:50.620: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/HEAD
Mar 22 21:37:50.633: INFO: http.Client request:HEAD | StatusCode:200
Mar 22 21:37:50.633: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/OPTIONS
Mar 22 21:37:50.642: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 22 21:37:50.642: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/PATCH
Mar 22 21:37:50.656: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 22 21:37:50.657: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/POST
Mar 22 21:37:50.668: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 22 21:37:50.668: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/PUT
Mar 22 21:37:50.676: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:50.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4809" for this suite. 03/22/23 21:37:50.684
------------------------------
• [2.289 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:48.405
    Mar 22 21:37:48.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename proxy 03/22/23 21:37:48.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:48.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:48.427
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 22 21:37:48.434: INFO: Creating pod...
    Mar 22 21:37:48.445: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4809" to be "running"
    Mar 22 21:37:48.449: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651723ms
    Mar 22 21:37:50.456: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011463635s
    Mar 22 21:37:50.456: INFO: Pod "agnhost" satisfied condition "running"
    Mar 22 21:37:50.456: INFO: Creating service...
    Mar 22 21:37:50.480: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/DELETE
    Mar 22 21:37:50.533: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 22 21:37:50.533: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/GET
    Mar 22 21:37:50.542: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 22 21:37:50.543: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/HEAD
    Mar 22 21:37:50.551: INFO: http.Client request:HEAD | StatusCode:200
    Mar 22 21:37:50.552: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 22 21:37:50.561: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 22 21:37:50.562: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/PATCH
    Mar 22 21:37:50.573: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 22 21:37:50.573: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/POST
    Mar 22 21:37:50.585: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 22 21:37:50.588: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/pods/agnhost/proxy/some/path/with/PUT
    Mar 22 21:37:50.597: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 22 21:37:50.597: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/DELETE
    Mar 22 21:37:50.606: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 22 21:37:50.606: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/GET
    Mar 22 21:37:50.619: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 22 21:37:50.620: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/HEAD
    Mar 22 21:37:50.633: INFO: http.Client request:HEAD | StatusCode:200
    Mar 22 21:37:50.633: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/OPTIONS
    Mar 22 21:37:50.642: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 22 21:37:50.642: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/PATCH
    Mar 22 21:37:50.656: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 22 21:37:50.657: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/POST
    Mar 22 21:37:50.668: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 22 21:37:50.668: INFO: Starting http.Client for https://10.245.0.1:443/api/v1/namespaces/proxy-4809/services/test-service/proxy/some/path/with/PUT
    Mar 22 21:37:50.676: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:50.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4809" for this suite. 03/22/23 21:37:50.684
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:50.703
Mar 22 21:37:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 21:37:50.705
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:50.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:50.725
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 03/22/23 21:37:50.739
STEP: Ensuring active pods == parallelism 03/22/23 21:37:50.746
STEP: Orphaning one of the Job's Pods 03/22/23 21:37:52.753
Mar 22 21:37:53.276: INFO: Successfully updated pod "adopt-release-gl64x"
STEP: Checking that the Job readopts the Pod 03/22/23 21:37:53.277
Mar 22 21:37:53.277: INFO: Waiting up to 15m0s for pod "adopt-release-gl64x" in namespace "job-8057" to be "adopted"
Mar 22 21:37:53.285: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 8.080059ms
Mar 22 21:37:55.292: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 2.014925737s
Mar 22 21:37:55.292: INFO: Pod "adopt-release-gl64x" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/22/23 21:37:55.292
Mar 22 21:37:55.810: INFO: Successfully updated pod "adopt-release-gl64x"
STEP: Checking that the Job releases the Pod 03/22/23 21:37:55.81
Mar 22 21:37:55.811: INFO: Waiting up to 15m0s for pod "adopt-release-gl64x" in namespace "job-8057" to be "released"
Mar 22 21:37:55.817: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 6.779195ms
Mar 22 21:37:57.824: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 2.013617045s
Mar 22 21:37:57.824: INFO: Pod "adopt-release-gl64x" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 21:37:57.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8057" for this suite. 03/22/23 21:37:57.831
------------------------------
• [SLOW TEST] [7.135 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:50.703
    Mar 22 21:37:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 21:37:50.705
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:50.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:50.725
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 03/22/23 21:37:50.739
    STEP: Ensuring active pods == parallelism 03/22/23 21:37:50.746
    STEP: Orphaning one of the Job's Pods 03/22/23 21:37:52.753
    Mar 22 21:37:53.276: INFO: Successfully updated pod "adopt-release-gl64x"
    STEP: Checking that the Job readopts the Pod 03/22/23 21:37:53.277
    Mar 22 21:37:53.277: INFO: Waiting up to 15m0s for pod "adopt-release-gl64x" in namespace "job-8057" to be "adopted"
    Mar 22 21:37:53.285: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 8.080059ms
    Mar 22 21:37:55.292: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 2.014925737s
    Mar 22 21:37:55.292: INFO: Pod "adopt-release-gl64x" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/22/23 21:37:55.292
    Mar 22 21:37:55.810: INFO: Successfully updated pod "adopt-release-gl64x"
    STEP: Checking that the Job releases the Pod 03/22/23 21:37:55.81
    Mar 22 21:37:55.811: INFO: Waiting up to 15m0s for pod "adopt-release-gl64x" in namespace "job-8057" to be "released"
    Mar 22 21:37:55.817: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 6.779195ms
    Mar 22 21:37:57.824: INFO: Pod "adopt-release-gl64x": Phase="Running", Reason="", readiness=true. Elapsed: 2.013617045s
    Mar 22 21:37:57.824: INFO: Pod "adopt-release-gl64x" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:37:57.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8057" for this suite. 03/22/23 21:37:57.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:37:57.843
Mar 22 21:37:57.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:37:57.845
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:57.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:57.863
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:37:57.879
Mar 22 21:37:57.889: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2114" to be "running and ready"
Mar 22 21:37:57.899: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.808812ms
Mar 22 21:37:57.899: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:37:59.906: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016563729s
Mar 22 21:37:59.906: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 22 21:37:59.907: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 03/22/23 21:37:59.912
Mar 22 21:37:59.921: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2114" to be "running and ready"
Mar 22 21:37:59.927: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376361ms
Mar 22 21:37:59.927: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:38:01.934: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013758343s
Mar 22 21:38:01.935: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 22 21:38:01.935: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/22/23 21:38:01.943
STEP: delete the pod with lifecycle hook 03/22/23 21:38:01.956
Mar 22 21:38:01.966: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 21:38:01.974: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 21:38:03.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 21:38:03.980: INFO: Pod pod-with-poststart-http-hook still exists
Mar 22 21:38:05.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 22 21:38:05.981: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 22 21:38:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2114" for this suite. 03/22/23 21:38:05.988
------------------------------
• [SLOW TEST] [8.153 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:37:57.843
    Mar 22 21:37:57.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:37:57.845
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:37:57.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:37:57.863
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:37:57.879
    Mar 22 21:37:57.889: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2114" to be "running and ready"
    Mar 22 21:37:57.899: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.808812ms
    Mar 22 21:37:57.899: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:37:59.906: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016563729s
    Mar 22 21:37:59.906: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 22 21:37:59.907: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 03/22/23 21:37:59.912
    Mar 22 21:37:59.921: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2114" to be "running and ready"
    Mar 22 21:37:59.927: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376361ms
    Mar 22 21:37:59.927: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:38:01.934: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013758343s
    Mar 22 21:38:01.935: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 22 21:38:01.935: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/22/23 21:38:01.943
    STEP: delete the pod with lifecycle hook 03/22/23 21:38:01.956
    Mar 22 21:38:01.966: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 22 21:38:01.974: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 22 21:38:03.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 22 21:38:03.980: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 22 21:38:05.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 22 21:38:05.981: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:38:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2114" for this suite. 03/22/23 21:38:05.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:38:05.998
Mar 22 21:38:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:38:06.003
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:06.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:06.029
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 03/22/23 21:38:06.036
Mar 22 21:38:06.045: INFO: Waiting up to 5m0s for pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d" in namespace "downward-api-721" to be "Succeeded or Failed"
Mar 22 21:38:06.053: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.600274ms
Mar 22 21:38:08.059: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013987855s
Mar 22 21:38:10.059: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013658026s
STEP: Saw pod success 03/22/23 21:38:10.06
Mar 22 21:38:10.060: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d" satisfied condition "Succeeded or Failed"
Mar 22 21:38:10.065: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d container dapi-container: <nil>
STEP: delete the pod 03/22/23 21:38:10.111
Mar 22 21:38:10.124: INFO: Waiting for pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d to disappear
Mar 22 21:38:10.133: INFO: Pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:38:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-721" for this suite. 03/22/23 21:38:10.14
------------------------------
• [4.151 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:38:05.998
    Mar 22 21:38:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:38:06.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:06.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:06.029
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 03/22/23 21:38:06.036
    Mar 22 21:38:06.045: INFO: Waiting up to 5m0s for pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d" in namespace "downward-api-721" to be "Succeeded or Failed"
    Mar 22 21:38:06.053: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.600274ms
    Mar 22 21:38:08.059: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013987855s
    Mar 22 21:38:10.059: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013658026s
    STEP: Saw pod success 03/22/23 21:38:10.06
    Mar 22 21:38:10.060: INFO: Pod "downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d" satisfied condition "Succeeded or Failed"
    Mar 22 21:38:10.065: INFO: Trying to get logs from node pool-v7t41yxh0-q56k5 pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d container dapi-container: <nil>
    STEP: delete the pod 03/22/23 21:38:10.111
    Mar 22 21:38:10.124: INFO: Waiting for pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d to disappear
    Mar 22 21:38:10.133: INFO: Pod downward-api-efe36a61-1639-4e8d-9646-8f489fef2d0d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:38:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-721" for this suite. 03/22/23 21:38:10.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:38:10.154
Mar 22 21:38:10.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename resourcequota 03/22/23 21:38:10.155
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:10.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:10.198
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 03/22/23 21:38:10.205
STEP: Creating a ResourceQuota 03/22/23 21:38:15.213
STEP: Ensuring resource quota status is calculated 03/22/23 21:38:15.222
STEP: Creating a Service 03/22/23 21:38:17.23
STEP: Creating a NodePort Service 03/22/23 21:38:17.265
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/22/23 21:38:17.288
STEP: Ensuring resource quota status captures service creation 03/22/23 21:38:17.314
STEP: Deleting Services 03/22/23 21:38:19.322
STEP: Ensuring resource quota status released usage 03/22/23 21:38:19.358
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 22 21:38:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9730" for this suite. 03/22/23 21:38:21.371
------------------------------
• [SLOW TEST] [11.227 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:38:10.154
    Mar 22 21:38:10.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename resourcequota 03/22/23 21:38:10.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:10.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:10.198
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 03/22/23 21:38:10.205
    STEP: Creating a ResourceQuota 03/22/23 21:38:15.213
    STEP: Ensuring resource quota status is calculated 03/22/23 21:38:15.222
    STEP: Creating a Service 03/22/23 21:38:17.23
    STEP: Creating a NodePort Service 03/22/23 21:38:17.265
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/22/23 21:38:17.288
    STEP: Ensuring resource quota status captures service creation 03/22/23 21:38:17.314
    STEP: Deleting Services 03/22/23 21:38:19.322
    STEP: Ensuring resource quota status released usage 03/22/23 21:38:19.358
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:38:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9730" for this suite. 03/22/23 21:38:21.371
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:38:21.385
Mar 22 21:38:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:38:21.39
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:21.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:21.412
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Mar 22 21:38:21.439: INFO: created pod
Mar 22 21:38:21.439: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2610" to be "Succeeded or Failed"
Mar 22 21:38:21.443: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077197ms
Mar 22 21:38:23.449: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010589052s
Mar 22 21:38:25.451: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803643s
STEP: Saw pod success 03/22/23 21:38:25.451
Mar 22 21:38:25.453: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 22 21:38:55.456: INFO: polling logs
Mar 22 21:38:55.471: INFO: Pod logs: 
I0322 21:38:22.373160       1 log.go:198] OK: Got token
I0322 21:38:22.373343       1 log.go:198] validating with in-cluster discovery
I0322 21:38:22.373846       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0322 21:38:22.373961       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2610:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679521701, NotBefore:1679521101, IssuedAt:1679521101, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2610", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"62bff170-d9b8-46e7-b6d8-b64a55b0a212"}}}
I0322 21:38:22.399357       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0322 21:38:22.402088       1 log.go:198] OK: Validated signature on JWT
I0322 21:38:22.402365       1 log.go:198] OK: Got valid claims from token!
I0322 21:38:22.402473       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2610:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679521701, NotBefore:1679521101, IssuedAt:1679521101, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2610", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"62bff170-d9b8-46e7-b6d8-b64a55b0a212"}}}

Mar 22 21:38:55.471: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 22 21:38:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2610" for this suite. 03/22/23 21:38:55.485
------------------------------
• [SLOW TEST] [34.108 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:38:21.385
    Mar 22 21:38:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svcaccounts 03/22/23 21:38:21.39
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:21.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:21.412
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Mar 22 21:38:21.439: INFO: created pod
    Mar 22 21:38:21.439: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2610" to be "Succeeded or Failed"
    Mar 22 21:38:21.443: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077197ms
    Mar 22 21:38:23.449: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010589052s
    Mar 22 21:38:25.451: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803643s
    STEP: Saw pod success 03/22/23 21:38:25.451
    Mar 22 21:38:25.453: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 22 21:38:55.456: INFO: polling logs
    Mar 22 21:38:55.471: INFO: Pod logs: 
    I0322 21:38:22.373160       1 log.go:198] OK: Got token
    I0322 21:38:22.373343       1 log.go:198] validating with in-cluster discovery
    I0322 21:38:22.373846       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0322 21:38:22.373961       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2610:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679521701, NotBefore:1679521101, IssuedAt:1679521101, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2610", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"62bff170-d9b8-46e7-b6d8-b64a55b0a212"}}}
    I0322 21:38:22.399357       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0322 21:38:22.402088       1 log.go:198] OK: Validated signature on JWT
    I0322 21:38:22.402365       1 log.go:198] OK: Got valid claims from token!
    I0322 21:38:22.402473       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2610:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679521701, NotBefore:1679521101, IssuedAt:1679521101, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2610", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"62bff170-d9b8-46e7-b6d8-b64a55b0a212"}}}

    Mar 22 21:38:55.471: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:38:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2610" for this suite. 03/22/23 21:38:55.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:38:55.498
Mar 22 21:38:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 21:38:55.5
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:55.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:55.531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 03/22/23 21:38:55.537
STEP: Ensuring job reaches completions 03/22/23 21:38:55.544
STEP: Ensuring pods with index for job exist 03/22/23 21:39:05.55
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:05.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1162" for this suite. 03/22/23 21:39:05.562
------------------------------
• [SLOW TEST] [10.073 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:38:55.498
    Mar 22 21:38:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 21:38:55.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:38:55.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:38:55.531
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 03/22/23 21:38:55.537
    STEP: Ensuring job reaches completions 03/22/23 21:38:55.544
    STEP: Ensuring pods with index for job exist 03/22/23 21:39:05.55
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:05.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1162" for this suite. 03/22/23 21:39:05.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:05.575
Mar 22 21:39:05.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename webhook 03/22/23 21:39:05.576
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:05.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:05.608
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/22/23 21:39:05.632
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:39:06.482
STEP: Deploying the webhook pod 03/22/23 21:39:06.503
STEP: Wait for the deployment to be ready 03/22/23 21:39:06.52
Mar 22 21:39:06.533: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/22/23 21:39:08.551
STEP: Verifying the service has paired with the endpoint 03/22/23 21:39:08.566
Mar 22 21:39:09.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Mar 22 21:39:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-326-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 21:39:10.105
STEP: Creating a custom resource that should be mutated by the webhook 03/22/23 21:39:10.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7627" for this suite. 03/22/23 21:39:12.831
STEP: Destroying namespace "webhook-7627-markers" for this suite. 03/22/23 21:39:12.84
------------------------------
• [SLOW TEST] [7.273 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:05.575
    Mar 22 21:39:05.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename webhook 03/22/23 21:39:05.576
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:05.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:05.608
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/22/23 21:39:05.632
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/22/23 21:39:06.482
    STEP: Deploying the webhook pod 03/22/23 21:39:06.503
    STEP: Wait for the deployment to be ready 03/22/23 21:39:06.52
    Mar 22 21:39:06.533: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/22/23 21:39:08.551
    STEP: Verifying the service has paired with the endpoint 03/22/23 21:39:08.566
    Mar 22 21:39:09.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Mar 22 21:39:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-326-crds.webhook.example.com via the AdmissionRegistration API 03/22/23 21:39:10.105
    STEP: Creating a custom resource that should be mutated by the webhook 03/22/23 21:39:10.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7627" for this suite. 03/22/23 21:39:12.831
    STEP: Destroying namespace "webhook-7627-markers" for this suite. 03/22/23 21:39:12.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:12.851
Mar 22 21:39:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename svc-latency 03/22/23 21:39:12.855
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:12.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:12.893
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 22 21:39:12.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: creating replication controller svc-latency-rc in namespace svc-latency-509 03/22/23 21:39:12.911
I0322 21:39:12.924385      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-509, replica count: 1
I0322 21:39:13.976548      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 21:39:14.091: INFO: Created: latency-svc-zrvdg
Mar 22 21:39:14.098: INFO: Got endpoints: latency-svc-zrvdg [20.512465ms]
Mar 22 21:39:14.111: INFO: Created: latency-svc-swzvn
Mar 22 21:39:14.119: INFO: Got endpoints: latency-svc-swzvn [21.269116ms]
Mar 22 21:39:14.124: INFO: Created: latency-svc-rcc5g
Mar 22 21:39:14.128: INFO: Created: latency-svc-gnvqn
Mar 22 21:39:14.128: INFO: Got endpoints: latency-svc-gnvqn [29.859573ms]
Mar 22 21:39:14.129: INFO: Got endpoints: latency-svc-rcc5g [29.842879ms]
Mar 22 21:39:14.130: INFO: Created: latency-svc-m2zkd
Mar 22 21:39:14.138: INFO: Got endpoints: latency-svc-m2zkd [38.813549ms]
Mar 22 21:39:14.139: INFO: Created: latency-svc-sv8tr
Mar 22 21:39:14.141: INFO: Got endpoints: latency-svc-sv8tr [41.904146ms]
Mar 22 21:39:14.147: INFO: Created: latency-svc-7b4rc
Mar 22 21:39:14.150: INFO: Got endpoints: latency-svc-7b4rc [50.472929ms]
Mar 22 21:39:14.156: INFO: Created: latency-svc-s5h9f
Mar 22 21:39:14.159: INFO: Got endpoints: latency-svc-s5h9f [58.704661ms]
Mar 22 21:39:14.161: INFO: Created: latency-svc-n2496
Mar 22 21:39:14.162: INFO: Got endpoints: latency-svc-n2496 [61.728304ms]
Mar 22 21:39:14.166: INFO: Created: latency-svc-bfnf8
Mar 22 21:39:14.173: INFO: Got endpoints: latency-svc-bfnf8 [71.581263ms]
Mar 22 21:39:14.175: INFO: Created: latency-svc-xskg4
Mar 22 21:39:14.176: INFO: Created: latency-svc-b4shm
Mar 22 21:39:14.176: INFO: Got endpoints: latency-svc-b4shm [74.527061ms]
Mar 22 21:39:14.182: INFO: Created: latency-svc-8rrdm
Mar 22 21:39:14.181: INFO: Got endpoints: latency-svc-xskg4 [79.405684ms]
Mar 22 21:39:14.191: INFO: Got endpoints: latency-svc-8rrdm [88.511471ms]
Mar 22 21:39:14.193: INFO: Created: latency-svc-sq78x
Mar 22 21:39:14.194: INFO: Got endpoints: latency-svc-sq78x [91.956674ms]
Mar 22 21:39:14.196: INFO: Created: latency-svc-cvbk9
Mar 22 21:39:14.198: INFO: Got endpoints: latency-svc-cvbk9 [96.414537ms]
Mar 22 21:39:14.202: INFO: Created: latency-svc-x4m85
Mar 22 21:39:14.202: INFO: Got endpoints: latency-svc-x4m85 [102.690974ms]
Mar 22 21:39:14.205: INFO: Created: latency-svc-2bd8c
Mar 22 21:39:14.211: INFO: Got endpoints: latency-svc-2bd8c [90.693981ms]
Mar 22 21:39:14.213: INFO: Created: latency-svc-pllpm
Mar 22 21:39:14.220: INFO: Got endpoints: latency-svc-pllpm [88.606918ms]
Mar 22 21:39:14.222: INFO: Created: latency-svc-q2549
Mar 22 21:39:14.223: INFO: Got endpoints: latency-svc-q2549 [91.844388ms]
Mar 22 21:39:14.226: INFO: Created: latency-svc-9r22g
Mar 22 21:39:14.242: INFO: Created: latency-svc-n9cbz
Mar 22 21:39:14.243: INFO: Created: latency-svc-ttr5m
Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-ttr5m [93.485441ms]
Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-9r22g [105.431711ms]
Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-n9cbz [101.788883ms]
Mar 22 21:39:14.246: INFO: Created: latency-svc-l6msp
Mar 22 21:39:14.257: INFO: Got endpoints: latency-svc-l6msp [98.129706ms]
Mar 22 21:39:14.269: INFO: Created: latency-svc-krxs4
Mar 22 21:39:14.277: INFO: Created: latency-svc-sdw4f
Mar 22 21:39:14.278: INFO: Got endpoints: latency-svc-krxs4 [114.071513ms]
Mar 22 21:39:14.281: INFO: Got endpoints: latency-svc-sdw4f [106.872136ms]
Mar 22 21:39:14.282: INFO: Created: latency-svc-vb7hf
Mar 22 21:39:14.286: INFO: Got endpoints: latency-svc-vb7hf [104.855327ms]
Mar 22 21:39:14.289: INFO: Created: latency-svc-zdt8q
Mar 22 21:39:14.293: INFO: Got endpoints: latency-svc-zdt8q [110.568716ms]
Mar 22 21:39:14.296: INFO: Created: latency-svc-r42rl
Mar 22 21:39:14.296: INFO: Got endpoints: latency-svc-r42rl [105.308212ms]
Mar 22 21:39:14.300: INFO: Created: latency-svc-xdclc
Mar 22 21:39:14.307: INFO: Got endpoints: latency-svc-xdclc [112.906074ms]
Mar 22 21:39:14.311: INFO: Created: latency-svc-hv98h
Mar 22 21:39:14.311: INFO: Got endpoints: latency-svc-hv98h [110.996246ms]
Mar 22 21:39:14.311: INFO: Created: latency-svc-8m7pr
Mar 22 21:39:14.314: INFO: Got endpoints: latency-svc-8m7pr [109.888803ms]
Mar 22 21:39:14.316: INFO: Created: latency-svc-zh4tv
Mar 22 21:39:14.324: INFO: Got endpoints: latency-svc-zh4tv [113.433262ms]
Mar 22 21:39:14.327: INFO: Created: latency-svc-mbpff
Mar 22 21:39:14.328: INFO: Got endpoints: latency-svc-mbpff [108.193524ms]
Mar 22 21:39:14.327: INFO: Created: latency-svc-xhjkw
Mar 22 21:39:14.331: INFO: Got endpoints: latency-svc-xhjkw [107.672376ms]
Mar 22 21:39:14.340: INFO: Created: latency-svc-wj6wc
Mar 22 21:39:14.341: INFO: Created: latency-svc-26b4t
Mar 22 21:39:14.348: INFO: Created: latency-svc-4kp8c
Mar 22 21:39:14.349: INFO: Got endpoints: latency-svc-26b4t [105.630195ms]
Mar 22 21:39:14.350: INFO: Created: latency-svc-zpnk2
Mar 22 21:39:14.372: INFO: Created: latency-svc-x6czk
Mar 22 21:39:14.375: INFO: Created: latency-svc-58wkh
Mar 22 21:39:14.379: INFO: Created: latency-svc-8kz9q
Mar 22 21:39:14.384: INFO: Created: latency-svc-prssg
Mar 22 21:39:14.390: INFO: Created: latency-svc-kg8s7
Mar 22 21:39:14.394: INFO: Created: latency-svc-djhct
Mar 22 21:39:14.400: INFO: Got endpoints: latency-svc-wj6wc [155.819343ms]
Mar 22 21:39:14.400: INFO: Created: latency-svc-cg4wq
Mar 22 21:39:14.403: INFO: Created: latency-svc-49pcs
Mar 22 21:39:14.409: INFO: Created: latency-svc-4w8k8
Mar 22 21:39:14.414: INFO: Created: latency-svc-jlhwq
Mar 22 21:39:14.417: INFO: Created: latency-svc-zpght
Mar 22 21:39:14.422: INFO: Created: latency-svc-gwg4n
Mar 22 21:39:14.429: INFO: Created: latency-svc-xz84d
Mar 22 21:39:14.446: INFO: Got endpoints: latency-svc-4kp8c [201.41705ms]
Mar 22 21:39:14.460: INFO: Created: latency-svc-htfcf
Mar 22 21:39:14.496: INFO: Got endpoints: latency-svc-zpnk2 [239.002493ms]
Mar 22 21:39:14.508: INFO: Created: latency-svc-zf7mb
Mar 22 21:39:14.545: INFO: Got endpoints: latency-svc-x6czk [267.370805ms]
Mar 22 21:39:14.557: INFO: Created: latency-svc-5tmxq
Mar 22 21:39:14.601: INFO: Got endpoints: latency-svc-58wkh [318.469637ms]
Mar 22 21:39:14.613: INFO: Created: latency-svc-mt76b
Mar 22 21:39:14.646: INFO: Got endpoints: latency-svc-8kz9q [359.428506ms]
Mar 22 21:39:14.662: INFO: Created: latency-svc-vs6nh
Mar 22 21:39:14.698: INFO: Got endpoints: latency-svc-prssg [404.417775ms]
Mar 22 21:39:14.724: INFO: Created: latency-svc-m2k4g
Mar 22 21:39:14.753: INFO: Got endpoints: latency-svc-kg8s7 [456.917231ms]
Mar 22 21:39:14.781: INFO: Created: latency-svc-zllt4
Mar 22 21:39:14.796: INFO: Got endpoints: latency-svc-djhct [488.713919ms]
Mar 22 21:39:14.814: INFO: Created: latency-svc-rn9np
Mar 22 21:39:14.849: INFO: Got endpoints: latency-svc-cg4wq [537.450523ms]
Mar 22 21:39:14.867: INFO: Created: latency-svc-dpr7g
Mar 22 21:39:14.898: INFO: Got endpoints: latency-svc-49pcs [582.207379ms]
Mar 22 21:39:14.914: INFO: Created: latency-svc-rrcxh
Mar 22 21:39:14.946: INFO: Got endpoints: latency-svc-4w8k8 [618.8687ms]
Mar 22 21:39:14.969: INFO: Created: latency-svc-l5nrz
Mar 22 21:39:14.996: INFO: Got endpoints: latency-svc-jlhwq [668.251121ms]
Mar 22 21:39:15.013: INFO: Created: latency-svc-b4gr2
Mar 22 21:39:15.045: INFO: Got endpoints: latency-svc-zpght [714.470082ms]
Mar 22 21:39:15.061: INFO: Created: latency-svc-rkhrh
Mar 22 21:39:15.097: INFO: Got endpoints: latency-svc-gwg4n [747.316103ms]
Mar 22 21:39:15.112: INFO: Created: latency-svc-rq7l7
Mar 22 21:39:15.147: INFO: Got endpoints: latency-svc-xz84d [746.982118ms]
Mar 22 21:39:15.161: INFO: Created: latency-svc-7rjnw
Mar 22 21:39:15.197: INFO: Got endpoints: latency-svc-htfcf [751.147723ms]
Mar 22 21:39:15.210: INFO: Created: latency-svc-sqhlh
Mar 22 21:39:15.246: INFO: Got endpoints: latency-svc-zf7mb [749.47749ms]
Mar 22 21:39:15.258: INFO: Created: latency-svc-6c2dw
Mar 22 21:39:15.295: INFO: Got endpoints: latency-svc-5tmxq [749.246063ms]
Mar 22 21:39:15.310: INFO: Created: latency-svc-qjknw
Mar 22 21:39:15.350: INFO: Got endpoints: latency-svc-mt76b [748.966302ms]
Mar 22 21:39:15.361: INFO: Created: latency-svc-vtgf2
Mar 22 21:39:15.409: INFO: Got endpoints: latency-svc-vs6nh [762.501483ms]
Mar 22 21:39:15.436: INFO: Created: latency-svc-4sk97
Mar 22 21:39:15.447: INFO: Got endpoints: latency-svc-m2k4g [749.087864ms]
Mar 22 21:39:15.460: INFO: Created: latency-svc-rps4g
Mar 22 21:39:15.498: INFO: Got endpoints: latency-svc-zllt4 [745.056861ms]
Mar 22 21:39:15.510: INFO: Created: latency-svc-prbf9
Mar 22 21:39:15.547: INFO: Got endpoints: latency-svc-rn9np [750.876136ms]
Mar 22 21:39:15.580: INFO: Created: latency-svc-lb5pw
Mar 22 21:39:15.596: INFO: Got endpoints: latency-svc-dpr7g [746.784083ms]
Mar 22 21:39:15.612: INFO: Created: latency-svc-twg97
Mar 22 21:39:15.649: INFO: Got endpoints: latency-svc-rrcxh [750.31808ms]
Mar 22 21:39:15.666: INFO: Created: latency-svc-9p2zq
Mar 22 21:39:15.714: INFO: Got endpoints: latency-svc-l5nrz [767.803843ms]
Mar 22 21:39:15.736: INFO: Created: latency-svc-jgxnl
Mar 22 21:39:15.747: INFO: Got endpoints: latency-svc-b4gr2 [750.931458ms]
Mar 22 21:39:15.760: INFO: Created: latency-svc-rfwb4
Mar 22 21:39:15.795: INFO: Got endpoints: latency-svc-rkhrh [749.39234ms]
Mar 22 21:39:15.813: INFO: Created: latency-svc-77x2t
Mar 22 21:39:15.859: INFO: Got endpoints: latency-svc-rq7l7 [761.616745ms]
Mar 22 21:39:15.873: INFO: Created: latency-svc-z5w7d
Mar 22 21:39:15.899: INFO: Got endpoints: latency-svc-7rjnw [751.449294ms]
Mar 22 21:39:15.914: INFO: Created: latency-svc-dng7k
Mar 22 21:39:15.957: INFO: Got endpoints: latency-svc-sqhlh [759.81874ms]
Mar 22 21:39:15.969: INFO: Created: latency-svc-j68qz
Mar 22 21:39:15.996: INFO: Got endpoints: latency-svc-6c2dw [750.535618ms]
Mar 22 21:39:16.010: INFO: Created: latency-svc-kvgff
Mar 22 21:39:16.046: INFO: Got endpoints: latency-svc-qjknw [751.555402ms]
Mar 22 21:39:16.059: INFO: Created: latency-svc-qpmdh
Mar 22 21:39:16.106: INFO: Got endpoints: latency-svc-vtgf2 [755.753102ms]
Mar 22 21:39:16.116: INFO: Created: latency-svc-bmqhn
Mar 22 21:39:16.148: INFO: Got endpoints: latency-svc-4sk97 [738.522315ms]
Mar 22 21:39:16.163: INFO: Created: latency-svc-xx7c4
Mar 22 21:39:16.199: INFO: Got endpoints: latency-svc-rps4g [751.660878ms]
Mar 22 21:39:16.211: INFO: Created: latency-svc-dmd4h
Mar 22 21:39:16.247: INFO: Got endpoints: latency-svc-prbf9 [748.546503ms]
Mar 22 21:39:16.268: INFO: Created: latency-svc-5k77m
Mar 22 21:39:16.297: INFO: Got endpoints: latency-svc-lb5pw [749.78215ms]
Mar 22 21:39:16.309: INFO: Created: latency-svc-qm9s9
Mar 22 21:39:16.354: INFO: Got endpoints: latency-svc-twg97 [757.563218ms]
Mar 22 21:39:16.366: INFO: Created: latency-svc-h2mdj
Mar 22 21:39:16.399: INFO: Got endpoints: latency-svc-9p2zq [748.334345ms]
Mar 22 21:39:16.423: INFO: Created: latency-svc-2h92v
Mar 22 21:39:16.447: INFO: Got endpoints: latency-svc-jgxnl [732.454139ms]
Mar 22 21:39:16.465: INFO: Created: latency-svc-mbl5d
Mar 22 21:39:16.498: INFO: Got endpoints: latency-svc-rfwb4 [750.80347ms]
Mar 22 21:39:16.514: INFO: Created: latency-svc-kwzxh
Mar 22 21:39:16.546: INFO: Got endpoints: latency-svc-77x2t [751.083765ms]
Mar 22 21:39:16.558: INFO: Created: latency-svc-dqczq
Mar 22 21:39:16.598: INFO: Got endpoints: latency-svc-z5w7d [738.936082ms]
Mar 22 21:39:16.613: INFO: Created: latency-svc-cbrxp
Mar 22 21:39:16.645: INFO: Got endpoints: latency-svc-dng7k [746.207292ms]
Mar 22 21:39:16.659: INFO: Created: latency-svc-dbfb9
Mar 22 21:39:16.696: INFO: Got endpoints: latency-svc-j68qz [738.196564ms]
Mar 22 21:39:16.710: INFO: Created: latency-svc-pdkxl
Mar 22 21:39:16.749: INFO: Got endpoints: latency-svc-kvgff [752.265291ms]
Mar 22 21:39:16.768: INFO: Created: latency-svc-fbj4c
Mar 22 21:39:16.800: INFO: Got endpoints: latency-svc-qpmdh [753.563084ms]
Mar 22 21:39:16.814: INFO: Created: latency-svc-fxzw9
Mar 22 21:39:16.854: INFO: Got endpoints: latency-svc-bmqhn [748.460844ms]
Mar 22 21:39:16.871: INFO: Created: latency-svc-mrksg
Mar 22 21:39:16.908: INFO: Got endpoints: latency-svc-xx7c4 [759.924718ms]
Mar 22 21:39:16.930: INFO: Created: latency-svc-jmtx7
Mar 22 21:39:16.945: INFO: Got endpoints: latency-svc-dmd4h [745.865396ms]
Mar 22 21:39:16.964: INFO: Created: latency-svc-gkckx
Mar 22 21:39:16.998: INFO: Got endpoints: latency-svc-5k77m [750.457587ms]
Mar 22 21:39:17.013: INFO: Created: latency-svc-mj89z
Mar 22 21:39:17.046: INFO: Got endpoints: latency-svc-qm9s9 [748.759349ms]
Mar 22 21:39:17.061: INFO: Created: latency-svc-qt887
Mar 22 21:39:17.099: INFO: Got endpoints: latency-svc-h2mdj [744.362177ms]
Mar 22 21:39:17.120: INFO: Created: latency-svc-cj6p7
Mar 22 21:39:17.159: INFO: Got endpoints: latency-svc-2h92v [760.421853ms]
Mar 22 21:39:17.173: INFO: Created: latency-svc-49gsh
Mar 22 21:39:17.196: INFO: Got endpoints: latency-svc-mbl5d [749.128575ms]
Mar 22 21:39:17.207: INFO: Created: latency-svc-f85hv
Mar 22 21:39:17.246: INFO: Got endpoints: latency-svc-kwzxh [747.860276ms]
Mar 22 21:39:17.266: INFO: Created: latency-svc-zlw76
Mar 22 21:39:17.297: INFO: Got endpoints: latency-svc-dqczq [751.018557ms]
Mar 22 21:39:17.309: INFO: Created: latency-svc-47vs6
Mar 22 21:39:17.348: INFO: Got endpoints: latency-svc-cbrxp [750.376976ms]
Mar 22 21:39:17.361: INFO: Created: latency-svc-jwzd8
Mar 22 21:39:17.399: INFO: Got endpoints: latency-svc-dbfb9 [754.006779ms]
Mar 22 21:39:17.411: INFO: Created: latency-svc-4zw5k
Mar 22 21:39:17.447: INFO: Got endpoints: latency-svc-pdkxl [751.393464ms]
Mar 22 21:39:17.461: INFO: Created: latency-svc-j85xj
Mar 22 21:39:17.496: INFO: Got endpoints: latency-svc-fbj4c [747.26489ms]
Mar 22 21:39:17.509: INFO: Created: latency-svc-qbpm2
Mar 22 21:39:17.556: INFO: Got endpoints: latency-svc-fxzw9 [755.517816ms]
Mar 22 21:39:17.569: INFO: Created: latency-svc-9fgbs
Mar 22 21:39:17.599: INFO: Got endpoints: latency-svc-mrksg [744.385113ms]
Mar 22 21:39:17.615: INFO: Created: latency-svc-b8cg6
Mar 22 21:39:17.649: INFO: Got endpoints: latency-svc-jmtx7 [741.043999ms]
Mar 22 21:39:17.665: INFO: Created: latency-svc-kcs2l
Mar 22 21:39:17.697: INFO: Got endpoints: latency-svc-gkckx [751.161729ms]
Mar 22 21:39:17.711: INFO: Created: latency-svc-xn72r
Mar 22 21:39:17.746: INFO: Got endpoints: latency-svc-mj89z [748.657423ms]
Mar 22 21:39:17.763: INFO: Created: latency-svc-xzsj5
Mar 22 21:39:17.799: INFO: Got endpoints: latency-svc-qt887 [751.194119ms]
Mar 22 21:39:17.816: INFO: Created: latency-svc-l8mm2
Mar 22 21:39:17.859: INFO: Got endpoints: latency-svc-cj6p7 [759.978781ms]
Mar 22 21:39:17.880: INFO: Created: latency-svc-8dfb8
Mar 22 21:39:17.900: INFO: Got endpoints: latency-svc-49gsh [738.592882ms]
Mar 22 21:39:17.911: INFO: Created: latency-svc-gxv9t
Mar 22 21:39:17.949: INFO: Got endpoints: latency-svc-f85hv [752.842733ms]
Mar 22 21:39:17.965: INFO: Created: latency-svc-4gq8f
Mar 22 21:39:17.997: INFO: Got endpoints: latency-svc-zlw76 [750.084192ms]
Mar 22 21:39:18.010: INFO: Created: latency-svc-dxjz7
Mar 22 21:39:18.051: INFO: Got endpoints: latency-svc-47vs6 [753.160866ms]
Mar 22 21:39:18.066: INFO: Created: latency-svc-qx92n
Mar 22 21:39:18.096: INFO: Got endpoints: latency-svc-jwzd8 [747.669038ms]
Mar 22 21:39:18.107: INFO: Created: latency-svc-n5cn9
Mar 22 21:39:18.147: INFO: Got endpoints: latency-svc-4zw5k [747.643479ms]
Mar 22 21:39:18.166: INFO: Created: latency-svc-279cm
Mar 22 21:39:18.198: INFO: Got endpoints: latency-svc-j85xj [751.169468ms]
Mar 22 21:39:18.211: INFO: Created: latency-svc-62mzl
Mar 22 21:39:18.247: INFO: Got endpoints: latency-svc-qbpm2 [750.782228ms]
Mar 22 21:39:18.261: INFO: Created: latency-svc-mdv67
Mar 22 21:39:18.297: INFO: Got endpoints: latency-svc-9fgbs [740.845674ms]
Mar 22 21:39:18.310: INFO: Created: latency-svc-m8nx6
Mar 22 21:39:18.347: INFO: Got endpoints: latency-svc-b8cg6 [748.075794ms]
Mar 22 21:39:18.359: INFO: Created: latency-svc-hd2c4
Mar 22 21:39:18.400: INFO: Got endpoints: latency-svc-kcs2l [751.111493ms]
Mar 22 21:39:18.413: INFO: Created: latency-svc-zq9rw
Mar 22 21:39:18.444: INFO: Got endpoints: latency-svc-xn72r [747.718719ms]
Mar 22 21:39:18.457: INFO: Created: latency-svc-gqb74
Mar 22 21:39:18.502: INFO: Got endpoints: latency-svc-xzsj5 [755.677933ms]
Mar 22 21:39:18.518: INFO: Created: latency-svc-d7jkw
Mar 22 21:39:18.547: INFO: Got endpoints: latency-svc-l8mm2 [747.923566ms]
Mar 22 21:39:18.562: INFO: Created: latency-svc-qt5pv
Mar 22 21:39:18.597: INFO: Got endpoints: latency-svc-8dfb8 [737.687561ms]
Mar 22 21:39:18.614: INFO: Created: latency-svc-pzz68
Mar 22 21:39:18.645: INFO: Got endpoints: latency-svc-gxv9t [745.651214ms]
Mar 22 21:39:18.664: INFO: Created: latency-svc-7m5z9
Mar 22 21:39:18.695: INFO: Got endpoints: latency-svc-4gq8f [746.349073ms]
Mar 22 21:39:18.716: INFO: Created: latency-svc-hp8b2
Mar 22 21:39:18.746: INFO: Got endpoints: latency-svc-dxjz7 [749.607822ms]
Mar 22 21:39:18.762: INFO: Created: latency-svc-tr82w
Mar 22 21:39:18.797: INFO: Got endpoints: latency-svc-qx92n [746.077583ms]
Mar 22 21:39:18.810: INFO: Created: latency-svc-cjlkv
Mar 22 21:39:18.849: INFO: Got endpoints: latency-svc-n5cn9 [752.006582ms]
Mar 22 21:39:18.866: INFO: Created: latency-svc-gwnmw
Mar 22 21:39:18.900: INFO: Got endpoints: latency-svc-279cm [752.182962ms]
Mar 22 21:39:18.921: INFO: Created: latency-svc-qh2bf
Mar 22 21:39:18.948: INFO: Got endpoints: latency-svc-62mzl [749.98288ms]
Mar 22 21:39:18.963: INFO: Created: latency-svc-cp5xl
Mar 22 21:39:18.996: INFO: Got endpoints: latency-svc-mdv67 [749.189301ms]
Mar 22 21:39:19.014: INFO: Created: latency-svc-2ml92
Mar 22 21:39:19.047: INFO: Got endpoints: latency-svc-m8nx6 [749.688939ms]
Mar 22 21:39:19.064: INFO: Created: latency-svc-gx7mp
Mar 22 21:39:19.099: INFO: Got endpoints: latency-svc-hd2c4 [751.409204ms]
Mar 22 21:39:19.113: INFO: Created: latency-svc-jkb92
Mar 22 21:39:19.150: INFO: Got endpoints: latency-svc-zq9rw [749.30865ms]
Mar 22 21:39:19.166: INFO: Created: latency-svc-4wppn
Mar 22 21:39:19.328: INFO: Got endpoints: latency-svc-gqb74 [883.253385ms]
Mar 22 21:39:19.330: INFO: Got endpoints: latency-svc-qt5pv [783.588936ms]
Mar 22 21:39:19.331: INFO: Got endpoints: latency-svc-d7jkw [828.582283ms]
Mar 22 21:39:19.345: INFO: Created: latency-svc-hclxw
Mar 22 21:39:19.348: INFO: Got endpoints: latency-svc-pzz68 [750.705572ms]
Mar 22 21:39:19.350: INFO: Created: latency-svc-8tqzh
Mar 22 21:39:19.359: INFO: Created: latency-svc-52wbz
Mar 22 21:39:19.366: INFO: Created: latency-svc-z84rl
Mar 22 21:39:19.397: INFO: Got endpoints: latency-svc-7m5z9 [751.48644ms]
Mar 22 21:39:19.413: INFO: Created: latency-svc-bkc5j
Mar 22 21:39:19.448: INFO: Got endpoints: latency-svc-hp8b2 [752.302687ms]
Mar 22 21:39:19.462: INFO: Created: latency-svc-4x6gk
Mar 22 21:39:19.503: INFO: Got endpoints: latency-svc-tr82w [756.254516ms]
Mar 22 21:39:19.517: INFO: Created: latency-svc-nqltj
Mar 22 21:39:19.551: INFO: Got endpoints: latency-svc-cjlkv [754.543634ms]
Mar 22 21:39:19.564: INFO: Created: latency-svc-m9mj4
Mar 22 21:39:19.599: INFO: Got endpoints: latency-svc-gwnmw [750.048545ms]
Mar 22 21:39:19.613: INFO: Created: latency-svc-pknfq
Mar 22 21:39:19.646: INFO: Got endpoints: latency-svc-qh2bf [745.38536ms]
Mar 22 21:39:19.659: INFO: Created: latency-svc-zt78f
Mar 22 21:39:19.697: INFO: Got endpoints: latency-svc-cp5xl [748.235098ms]
Mar 22 21:39:19.709: INFO: Created: latency-svc-tcbmm
Mar 22 21:39:19.753: INFO: Got endpoints: latency-svc-2ml92 [756.77657ms]
Mar 22 21:39:19.767: INFO: Created: latency-svc-z9l7q
Mar 22 21:39:19.800: INFO: Got endpoints: latency-svc-gx7mp [753.022473ms]
Mar 22 21:39:19.821: INFO: Created: latency-svc-8bfcs
Mar 22 21:39:19.860: INFO: Got endpoints: latency-svc-jkb92 [760.931937ms]
Mar 22 21:39:19.906: INFO: Got endpoints: latency-svc-4wppn [755.805381ms]
Mar 22 21:39:19.907: INFO: Created: latency-svc-tsk2w
Mar 22 21:39:19.919: INFO: Created: latency-svc-cx59w
Mar 22 21:39:19.960: INFO: Got endpoints: latency-svc-hclxw [632.071722ms]
Mar 22 21:39:19.973: INFO: Created: latency-svc-qhl24
Mar 22 21:39:19.996: INFO: Got endpoints: latency-svc-8tqzh [664.913303ms]
Mar 22 21:39:20.011: INFO: Created: latency-svc-jn4jv
Mar 22 21:39:20.047: INFO: Got endpoints: latency-svc-52wbz [717.078123ms]
Mar 22 21:39:20.064: INFO: Created: latency-svc-mgqsb
Mar 22 21:39:20.099: INFO: Got endpoints: latency-svc-z84rl [750.984873ms]
Mar 22 21:39:20.112: INFO: Created: latency-svc-kh59q
Mar 22 21:39:20.146: INFO: Got endpoints: latency-svc-bkc5j [749.290486ms]
Mar 22 21:39:20.160: INFO: Created: latency-svc-bx4cz
Mar 22 21:39:20.196: INFO: Got endpoints: latency-svc-4x6gk [748.12814ms]
Mar 22 21:39:20.214: INFO: Created: latency-svc-ftzkq
Mar 22 21:39:20.246: INFO: Got endpoints: latency-svc-nqltj [743.472769ms]
Mar 22 21:39:20.261: INFO: Created: latency-svc-b5826
Mar 22 21:39:20.298: INFO: Got endpoints: latency-svc-m9mj4 [746.951377ms]
Mar 22 21:39:20.312: INFO: Created: latency-svc-ggpwg
Mar 22 21:39:20.347: INFO: Got endpoints: latency-svc-pknfq [748.179074ms]
Mar 22 21:39:20.362: INFO: Created: latency-svc-vktrx
Mar 22 21:39:20.399: INFO: Got endpoints: latency-svc-zt78f [752.307497ms]
Mar 22 21:39:20.413: INFO: Created: latency-svc-ft5bk
Mar 22 21:39:20.446: INFO: Got endpoints: latency-svc-tcbmm [749.025039ms]
Mar 22 21:39:20.463: INFO: Created: latency-svc-rlqss
Mar 22 21:39:20.495: INFO: Got endpoints: latency-svc-z9l7q [741.79201ms]
Mar 22 21:39:20.510: INFO: Created: latency-svc-dk4hw
Mar 22 21:39:20.550: INFO: Got endpoints: latency-svc-8bfcs [750.332565ms]
Mar 22 21:39:20.565: INFO: Created: latency-svc-xlpc5
Mar 22 21:39:20.596: INFO: Got endpoints: latency-svc-tsk2w [726.113536ms]
Mar 22 21:39:20.613: INFO: Created: latency-svc-c95vp
Mar 22 21:39:20.648: INFO: Got endpoints: latency-svc-cx59w [741.495969ms]
Mar 22 21:39:20.662: INFO: Created: latency-svc-94gq4
Mar 22 21:39:20.703: INFO: Got endpoints: latency-svc-qhl24 [742.879845ms]
Mar 22 21:39:20.724: INFO: Created: latency-svc-ggkw8
Mar 22 21:39:20.753: INFO: Got endpoints: latency-svc-jn4jv [757.42977ms]
Mar 22 21:39:20.767: INFO: Created: latency-svc-57lzm
Mar 22 21:39:20.797: INFO: Got endpoints: latency-svc-mgqsb [749.125463ms]
Mar 22 21:39:20.811: INFO: Created: latency-svc-z896x
Mar 22 21:39:20.856: INFO: Got endpoints: latency-svc-kh59q [756.273681ms]
Mar 22 21:39:20.872: INFO: Created: latency-svc-sdnzp
Mar 22 21:39:20.897: INFO: Got endpoints: latency-svc-bx4cz [750.907975ms]
Mar 22 21:39:20.912: INFO: Created: latency-svc-f6vxs
Mar 22 21:39:20.948: INFO: Got endpoints: latency-svc-ftzkq [751.002415ms]
Mar 22 21:39:20.966: INFO: Created: latency-svc-kpnfw
Mar 22 21:39:20.996: INFO: Got endpoints: latency-svc-b5826 [749.581974ms]
Mar 22 21:39:21.010: INFO: Created: latency-svc-fv5w4
Mar 22 21:39:21.049: INFO: Got endpoints: latency-svc-ggpwg [750.583349ms]
Mar 22 21:39:21.063: INFO: Created: latency-svc-h55j6
Mar 22 21:39:21.099: INFO: Got endpoints: latency-svc-vktrx [750.836737ms]
Mar 22 21:39:21.112: INFO: Created: latency-svc-xpzpd
Mar 22 21:39:21.147: INFO: Got endpoints: latency-svc-ft5bk [748.206848ms]
Mar 22 21:39:21.161: INFO: Created: latency-svc-fvzfh
Mar 22 21:39:21.198: INFO: Got endpoints: latency-svc-rlqss [752.18992ms]
Mar 22 21:39:21.214: INFO: Created: latency-svc-sc47b
Mar 22 21:39:21.249: INFO: Got endpoints: latency-svc-dk4hw [752.666393ms]
Mar 22 21:39:21.266: INFO: Created: latency-svc-7f2br
Mar 22 21:39:21.304: INFO: Got endpoints: latency-svc-xlpc5 [753.654534ms]
Mar 22 21:39:21.323: INFO: Created: latency-svc-dhl6g
Mar 22 21:39:21.350: INFO: Got endpoints: latency-svc-c95vp [754.1329ms]
Mar 22 21:39:21.376: INFO: Created: latency-svc-grk4r
Mar 22 21:39:21.398: INFO: Got endpoints: latency-svc-94gq4 [749.394874ms]
Mar 22 21:39:21.415: INFO: Created: latency-svc-zqc58
Mar 22 21:39:21.449: INFO: Got endpoints: latency-svc-ggkw8 [746.46159ms]
Mar 22 21:39:21.466: INFO: Created: latency-svc-xwld2
Mar 22 21:39:21.496: INFO: Got endpoints: latency-svc-57lzm [742.768348ms]
Mar 22 21:39:21.510: INFO: Created: latency-svc-rjl8l
Mar 22 21:39:21.545: INFO: Got endpoints: latency-svc-z896x [747.663823ms]
Mar 22 21:39:21.558: INFO: Created: latency-svc-jtx7z
Mar 22 21:39:21.596: INFO: Got endpoints: latency-svc-sdnzp [740.012165ms]
Mar 22 21:39:21.610: INFO: Created: latency-svc-rqb8t
Mar 22 21:39:21.658: INFO: Got endpoints: latency-svc-f6vxs [759.683514ms]
Mar 22 21:39:21.676: INFO: Created: latency-svc-fnvn6
Mar 22 21:39:21.695: INFO: Got endpoints: latency-svc-kpnfw [747.291039ms]
Mar 22 21:39:21.709: INFO: Created: latency-svc-5zr8v
Mar 22 21:39:21.748: INFO: Got endpoints: latency-svc-fv5w4 [751.581136ms]
Mar 22 21:39:21.765: INFO: Created: latency-svc-7mxhb
Mar 22 21:39:21.795: INFO: Got endpoints: latency-svc-h55j6 [745.856898ms]
Mar 22 21:39:21.807: INFO: Created: latency-svc-q4gtp
Mar 22 21:39:21.852: INFO: Got endpoints: latency-svc-xpzpd [753.359867ms]
Mar 22 21:39:21.871: INFO: Created: latency-svc-br67m
Mar 22 21:39:21.900: INFO: Got endpoints: latency-svc-fvzfh [753.161037ms]
Mar 22 21:39:21.913: INFO: Created: latency-svc-mc7tp
Mar 22 21:39:21.948: INFO: Got endpoints: latency-svc-sc47b [750.255857ms]
Mar 22 21:39:21.999: INFO: Got endpoints: latency-svc-7f2br [749.921424ms]
Mar 22 21:39:22.046: INFO: Got endpoints: latency-svc-dhl6g [741.600683ms]
Mar 22 21:39:22.097: INFO: Got endpoints: latency-svc-grk4r [747.007001ms]
Mar 22 21:39:22.150: INFO: Got endpoints: latency-svc-zqc58 [751.077202ms]
Mar 22 21:39:22.199: INFO: Got endpoints: latency-svc-xwld2 [749.664664ms]
Mar 22 21:39:22.246: INFO: Got endpoints: latency-svc-rjl8l [749.617566ms]
Mar 22 21:39:22.295: INFO: Got endpoints: latency-svc-jtx7z [749.9813ms]
Mar 22 21:39:22.349: INFO: Got endpoints: latency-svc-rqb8t [753.292545ms]
Mar 22 21:39:22.399: INFO: Got endpoints: latency-svc-fnvn6 [740.488113ms]
Mar 22 21:39:22.450: INFO: Got endpoints: latency-svc-5zr8v [754.33364ms]
Mar 22 21:39:22.496: INFO: Got endpoints: latency-svc-7mxhb [747.728046ms]
Mar 22 21:39:22.547: INFO: Got endpoints: latency-svc-q4gtp [752.631504ms]
Mar 22 21:39:22.599: INFO: Got endpoints: latency-svc-br67m [746.741736ms]
Mar 22 21:39:22.649: INFO: Got endpoints: latency-svc-mc7tp [748.787206ms]
Mar 22 21:39:22.649: INFO: Latencies: [21.269116ms 29.842879ms 29.859573ms 38.813549ms 41.904146ms 50.472929ms 58.704661ms 61.728304ms 71.581263ms 74.527061ms 79.405684ms 88.511471ms 88.606918ms 90.693981ms 91.844388ms 91.956674ms 93.485441ms 96.414537ms 98.129706ms 101.788883ms 102.690974ms 104.855327ms 105.308212ms 105.431711ms 105.630195ms 106.872136ms 107.672376ms 108.193524ms 109.888803ms 110.568716ms 110.996246ms 112.906074ms 113.433262ms 114.071513ms 155.819343ms 201.41705ms 239.002493ms 267.370805ms 318.469637ms 359.428506ms 404.417775ms 456.917231ms 488.713919ms 537.450523ms 582.207379ms 618.8687ms 632.071722ms 664.913303ms 668.251121ms 714.470082ms 717.078123ms 726.113536ms 732.454139ms 737.687561ms 738.196564ms 738.522315ms 738.592882ms 738.936082ms 740.012165ms 740.488113ms 740.845674ms 741.043999ms 741.495969ms 741.600683ms 741.79201ms 742.768348ms 742.879845ms 743.472769ms 744.362177ms 744.385113ms 745.056861ms 745.38536ms 745.651214ms 745.856898ms 745.865396ms 746.077583ms 746.207292ms 746.349073ms 746.46159ms 746.741736ms 746.784083ms 746.951377ms 746.982118ms 747.007001ms 747.26489ms 747.291039ms 747.316103ms 747.643479ms 747.663823ms 747.669038ms 747.718719ms 747.728046ms 747.860276ms 747.923566ms 748.075794ms 748.12814ms 748.179074ms 748.206848ms 748.235098ms 748.334345ms 748.460844ms 748.546503ms 748.657423ms 748.759349ms 748.787206ms 748.966302ms 749.025039ms 749.087864ms 749.125463ms 749.128575ms 749.189301ms 749.246063ms 749.290486ms 749.30865ms 749.39234ms 749.394874ms 749.47749ms 749.581974ms 749.607822ms 749.617566ms 749.664664ms 749.688939ms 749.78215ms 749.921424ms 749.9813ms 749.98288ms 750.048545ms 750.084192ms 750.255857ms 750.31808ms 750.332565ms 750.376976ms 750.457587ms 750.535618ms 750.583349ms 750.705572ms 750.782228ms 750.80347ms 750.836737ms 750.876136ms 750.907975ms 750.931458ms 750.984873ms 751.002415ms 751.018557ms 751.077202ms 751.083765ms 751.111493ms 751.147723ms 751.161729ms 751.169468ms 751.194119ms 751.393464ms 751.409204ms 751.449294ms 751.48644ms 751.555402ms 751.581136ms 751.660878ms 752.006582ms 752.182962ms 752.18992ms 752.265291ms 752.302687ms 752.307497ms 752.631504ms 752.666393ms 752.842733ms 753.022473ms 753.160866ms 753.161037ms 753.292545ms 753.359867ms 753.563084ms 753.654534ms 754.006779ms 754.1329ms 754.33364ms 754.543634ms 755.517816ms 755.677933ms 755.753102ms 755.805381ms 756.254516ms 756.273681ms 756.77657ms 757.42977ms 757.563218ms 759.683514ms 759.81874ms 759.924718ms 759.978781ms 760.421853ms 760.931937ms 761.616745ms 762.501483ms 767.803843ms 783.588936ms 828.582283ms 883.253385ms]
Mar 22 21:39:22.649: INFO: 50 %ile: 748.460844ms
Mar 22 21:39:22.649: INFO: 90 %ile: 755.677933ms
Mar 22 21:39:22.649: INFO: 99 %ile: 828.582283ms
Mar 22 21:39:22.649: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-509" for this suite. 03/22/23 21:39:22.657
------------------------------
• [SLOW TEST] [9.813 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:12.851
    Mar 22 21:39:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename svc-latency 03/22/23 21:39:12.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:12.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:12.893
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 22 21:39:12.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-509 03/22/23 21:39:12.911
    I0322 21:39:12.924385      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-509, replica count: 1
    I0322 21:39:13.976548      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 21:39:14.091: INFO: Created: latency-svc-zrvdg
    Mar 22 21:39:14.098: INFO: Got endpoints: latency-svc-zrvdg [20.512465ms]
    Mar 22 21:39:14.111: INFO: Created: latency-svc-swzvn
    Mar 22 21:39:14.119: INFO: Got endpoints: latency-svc-swzvn [21.269116ms]
    Mar 22 21:39:14.124: INFO: Created: latency-svc-rcc5g
    Mar 22 21:39:14.128: INFO: Created: latency-svc-gnvqn
    Mar 22 21:39:14.128: INFO: Got endpoints: latency-svc-gnvqn [29.859573ms]
    Mar 22 21:39:14.129: INFO: Got endpoints: latency-svc-rcc5g [29.842879ms]
    Mar 22 21:39:14.130: INFO: Created: latency-svc-m2zkd
    Mar 22 21:39:14.138: INFO: Got endpoints: latency-svc-m2zkd [38.813549ms]
    Mar 22 21:39:14.139: INFO: Created: latency-svc-sv8tr
    Mar 22 21:39:14.141: INFO: Got endpoints: latency-svc-sv8tr [41.904146ms]
    Mar 22 21:39:14.147: INFO: Created: latency-svc-7b4rc
    Mar 22 21:39:14.150: INFO: Got endpoints: latency-svc-7b4rc [50.472929ms]
    Mar 22 21:39:14.156: INFO: Created: latency-svc-s5h9f
    Mar 22 21:39:14.159: INFO: Got endpoints: latency-svc-s5h9f [58.704661ms]
    Mar 22 21:39:14.161: INFO: Created: latency-svc-n2496
    Mar 22 21:39:14.162: INFO: Got endpoints: latency-svc-n2496 [61.728304ms]
    Mar 22 21:39:14.166: INFO: Created: latency-svc-bfnf8
    Mar 22 21:39:14.173: INFO: Got endpoints: latency-svc-bfnf8 [71.581263ms]
    Mar 22 21:39:14.175: INFO: Created: latency-svc-xskg4
    Mar 22 21:39:14.176: INFO: Created: latency-svc-b4shm
    Mar 22 21:39:14.176: INFO: Got endpoints: latency-svc-b4shm [74.527061ms]
    Mar 22 21:39:14.182: INFO: Created: latency-svc-8rrdm
    Mar 22 21:39:14.181: INFO: Got endpoints: latency-svc-xskg4 [79.405684ms]
    Mar 22 21:39:14.191: INFO: Got endpoints: latency-svc-8rrdm [88.511471ms]
    Mar 22 21:39:14.193: INFO: Created: latency-svc-sq78x
    Mar 22 21:39:14.194: INFO: Got endpoints: latency-svc-sq78x [91.956674ms]
    Mar 22 21:39:14.196: INFO: Created: latency-svc-cvbk9
    Mar 22 21:39:14.198: INFO: Got endpoints: latency-svc-cvbk9 [96.414537ms]
    Mar 22 21:39:14.202: INFO: Created: latency-svc-x4m85
    Mar 22 21:39:14.202: INFO: Got endpoints: latency-svc-x4m85 [102.690974ms]
    Mar 22 21:39:14.205: INFO: Created: latency-svc-2bd8c
    Mar 22 21:39:14.211: INFO: Got endpoints: latency-svc-2bd8c [90.693981ms]
    Mar 22 21:39:14.213: INFO: Created: latency-svc-pllpm
    Mar 22 21:39:14.220: INFO: Got endpoints: latency-svc-pllpm [88.606918ms]
    Mar 22 21:39:14.222: INFO: Created: latency-svc-q2549
    Mar 22 21:39:14.223: INFO: Got endpoints: latency-svc-q2549 [91.844388ms]
    Mar 22 21:39:14.226: INFO: Created: latency-svc-9r22g
    Mar 22 21:39:14.242: INFO: Created: latency-svc-n9cbz
    Mar 22 21:39:14.243: INFO: Created: latency-svc-ttr5m
    Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-ttr5m [93.485441ms]
    Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-9r22g [105.431711ms]
    Mar 22 21:39:14.243: INFO: Got endpoints: latency-svc-n9cbz [101.788883ms]
    Mar 22 21:39:14.246: INFO: Created: latency-svc-l6msp
    Mar 22 21:39:14.257: INFO: Got endpoints: latency-svc-l6msp [98.129706ms]
    Mar 22 21:39:14.269: INFO: Created: latency-svc-krxs4
    Mar 22 21:39:14.277: INFO: Created: latency-svc-sdw4f
    Mar 22 21:39:14.278: INFO: Got endpoints: latency-svc-krxs4 [114.071513ms]
    Mar 22 21:39:14.281: INFO: Got endpoints: latency-svc-sdw4f [106.872136ms]
    Mar 22 21:39:14.282: INFO: Created: latency-svc-vb7hf
    Mar 22 21:39:14.286: INFO: Got endpoints: latency-svc-vb7hf [104.855327ms]
    Mar 22 21:39:14.289: INFO: Created: latency-svc-zdt8q
    Mar 22 21:39:14.293: INFO: Got endpoints: latency-svc-zdt8q [110.568716ms]
    Mar 22 21:39:14.296: INFO: Created: latency-svc-r42rl
    Mar 22 21:39:14.296: INFO: Got endpoints: latency-svc-r42rl [105.308212ms]
    Mar 22 21:39:14.300: INFO: Created: latency-svc-xdclc
    Mar 22 21:39:14.307: INFO: Got endpoints: latency-svc-xdclc [112.906074ms]
    Mar 22 21:39:14.311: INFO: Created: latency-svc-hv98h
    Mar 22 21:39:14.311: INFO: Got endpoints: latency-svc-hv98h [110.996246ms]
    Mar 22 21:39:14.311: INFO: Created: latency-svc-8m7pr
    Mar 22 21:39:14.314: INFO: Got endpoints: latency-svc-8m7pr [109.888803ms]
    Mar 22 21:39:14.316: INFO: Created: latency-svc-zh4tv
    Mar 22 21:39:14.324: INFO: Got endpoints: latency-svc-zh4tv [113.433262ms]
    Mar 22 21:39:14.327: INFO: Created: latency-svc-mbpff
    Mar 22 21:39:14.328: INFO: Got endpoints: latency-svc-mbpff [108.193524ms]
    Mar 22 21:39:14.327: INFO: Created: latency-svc-xhjkw
    Mar 22 21:39:14.331: INFO: Got endpoints: latency-svc-xhjkw [107.672376ms]
    Mar 22 21:39:14.340: INFO: Created: latency-svc-wj6wc
    Mar 22 21:39:14.341: INFO: Created: latency-svc-26b4t
    Mar 22 21:39:14.348: INFO: Created: latency-svc-4kp8c
    Mar 22 21:39:14.349: INFO: Got endpoints: latency-svc-26b4t [105.630195ms]
    Mar 22 21:39:14.350: INFO: Created: latency-svc-zpnk2
    Mar 22 21:39:14.372: INFO: Created: latency-svc-x6czk
    Mar 22 21:39:14.375: INFO: Created: latency-svc-58wkh
    Mar 22 21:39:14.379: INFO: Created: latency-svc-8kz9q
    Mar 22 21:39:14.384: INFO: Created: latency-svc-prssg
    Mar 22 21:39:14.390: INFO: Created: latency-svc-kg8s7
    Mar 22 21:39:14.394: INFO: Created: latency-svc-djhct
    Mar 22 21:39:14.400: INFO: Got endpoints: latency-svc-wj6wc [155.819343ms]
    Mar 22 21:39:14.400: INFO: Created: latency-svc-cg4wq
    Mar 22 21:39:14.403: INFO: Created: latency-svc-49pcs
    Mar 22 21:39:14.409: INFO: Created: latency-svc-4w8k8
    Mar 22 21:39:14.414: INFO: Created: latency-svc-jlhwq
    Mar 22 21:39:14.417: INFO: Created: latency-svc-zpght
    Mar 22 21:39:14.422: INFO: Created: latency-svc-gwg4n
    Mar 22 21:39:14.429: INFO: Created: latency-svc-xz84d
    Mar 22 21:39:14.446: INFO: Got endpoints: latency-svc-4kp8c [201.41705ms]
    Mar 22 21:39:14.460: INFO: Created: latency-svc-htfcf
    Mar 22 21:39:14.496: INFO: Got endpoints: latency-svc-zpnk2 [239.002493ms]
    Mar 22 21:39:14.508: INFO: Created: latency-svc-zf7mb
    Mar 22 21:39:14.545: INFO: Got endpoints: latency-svc-x6czk [267.370805ms]
    Mar 22 21:39:14.557: INFO: Created: latency-svc-5tmxq
    Mar 22 21:39:14.601: INFO: Got endpoints: latency-svc-58wkh [318.469637ms]
    Mar 22 21:39:14.613: INFO: Created: latency-svc-mt76b
    Mar 22 21:39:14.646: INFO: Got endpoints: latency-svc-8kz9q [359.428506ms]
    Mar 22 21:39:14.662: INFO: Created: latency-svc-vs6nh
    Mar 22 21:39:14.698: INFO: Got endpoints: latency-svc-prssg [404.417775ms]
    Mar 22 21:39:14.724: INFO: Created: latency-svc-m2k4g
    Mar 22 21:39:14.753: INFO: Got endpoints: latency-svc-kg8s7 [456.917231ms]
    Mar 22 21:39:14.781: INFO: Created: latency-svc-zllt4
    Mar 22 21:39:14.796: INFO: Got endpoints: latency-svc-djhct [488.713919ms]
    Mar 22 21:39:14.814: INFO: Created: latency-svc-rn9np
    Mar 22 21:39:14.849: INFO: Got endpoints: latency-svc-cg4wq [537.450523ms]
    Mar 22 21:39:14.867: INFO: Created: latency-svc-dpr7g
    Mar 22 21:39:14.898: INFO: Got endpoints: latency-svc-49pcs [582.207379ms]
    Mar 22 21:39:14.914: INFO: Created: latency-svc-rrcxh
    Mar 22 21:39:14.946: INFO: Got endpoints: latency-svc-4w8k8 [618.8687ms]
    Mar 22 21:39:14.969: INFO: Created: latency-svc-l5nrz
    Mar 22 21:39:14.996: INFO: Got endpoints: latency-svc-jlhwq [668.251121ms]
    Mar 22 21:39:15.013: INFO: Created: latency-svc-b4gr2
    Mar 22 21:39:15.045: INFO: Got endpoints: latency-svc-zpght [714.470082ms]
    Mar 22 21:39:15.061: INFO: Created: latency-svc-rkhrh
    Mar 22 21:39:15.097: INFO: Got endpoints: latency-svc-gwg4n [747.316103ms]
    Mar 22 21:39:15.112: INFO: Created: latency-svc-rq7l7
    Mar 22 21:39:15.147: INFO: Got endpoints: latency-svc-xz84d [746.982118ms]
    Mar 22 21:39:15.161: INFO: Created: latency-svc-7rjnw
    Mar 22 21:39:15.197: INFO: Got endpoints: latency-svc-htfcf [751.147723ms]
    Mar 22 21:39:15.210: INFO: Created: latency-svc-sqhlh
    Mar 22 21:39:15.246: INFO: Got endpoints: latency-svc-zf7mb [749.47749ms]
    Mar 22 21:39:15.258: INFO: Created: latency-svc-6c2dw
    Mar 22 21:39:15.295: INFO: Got endpoints: latency-svc-5tmxq [749.246063ms]
    Mar 22 21:39:15.310: INFO: Created: latency-svc-qjknw
    Mar 22 21:39:15.350: INFO: Got endpoints: latency-svc-mt76b [748.966302ms]
    Mar 22 21:39:15.361: INFO: Created: latency-svc-vtgf2
    Mar 22 21:39:15.409: INFO: Got endpoints: latency-svc-vs6nh [762.501483ms]
    Mar 22 21:39:15.436: INFO: Created: latency-svc-4sk97
    Mar 22 21:39:15.447: INFO: Got endpoints: latency-svc-m2k4g [749.087864ms]
    Mar 22 21:39:15.460: INFO: Created: latency-svc-rps4g
    Mar 22 21:39:15.498: INFO: Got endpoints: latency-svc-zllt4 [745.056861ms]
    Mar 22 21:39:15.510: INFO: Created: latency-svc-prbf9
    Mar 22 21:39:15.547: INFO: Got endpoints: latency-svc-rn9np [750.876136ms]
    Mar 22 21:39:15.580: INFO: Created: latency-svc-lb5pw
    Mar 22 21:39:15.596: INFO: Got endpoints: latency-svc-dpr7g [746.784083ms]
    Mar 22 21:39:15.612: INFO: Created: latency-svc-twg97
    Mar 22 21:39:15.649: INFO: Got endpoints: latency-svc-rrcxh [750.31808ms]
    Mar 22 21:39:15.666: INFO: Created: latency-svc-9p2zq
    Mar 22 21:39:15.714: INFO: Got endpoints: latency-svc-l5nrz [767.803843ms]
    Mar 22 21:39:15.736: INFO: Created: latency-svc-jgxnl
    Mar 22 21:39:15.747: INFO: Got endpoints: latency-svc-b4gr2 [750.931458ms]
    Mar 22 21:39:15.760: INFO: Created: latency-svc-rfwb4
    Mar 22 21:39:15.795: INFO: Got endpoints: latency-svc-rkhrh [749.39234ms]
    Mar 22 21:39:15.813: INFO: Created: latency-svc-77x2t
    Mar 22 21:39:15.859: INFO: Got endpoints: latency-svc-rq7l7 [761.616745ms]
    Mar 22 21:39:15.873: INFO: Created: latency-svc-z5w7d
    Mar 22 21:39:15.899: INFO: Got endpoints: latency-svc-7rjnw [751.449294ms]
    Mar 22 21:39:15.914: INFO: Created: latency-svc-dng7k
    Mar 22 21:39:15.957: INFO: Got endpoints: latency-svc-sqhlh [759.81874ms]
    Mar 22 21:39:15.969: INFO: Created: latency-svc-j68qz
    Mar 22 21:39:15.996: INFO: Got endpoints: latency-svc-6c2dw [750.535618ms]
    Mar 22 21:39:16.010: INFO: Created: latency-svc-kvgff
    Mar 22 21:39:16.046: INFO: Got endpoints: latency-svc-qjknw [751.555402ms]
    Mar 22 21:39:16.059: INFO: Created: latency-svc-qpmdh
    Mar 22 21:39:16.106: INFO: Got endpoints: latency-svc-vtgf2 [755.753102ms]
    Mar 22 21:39:16.116: INFO: Created: latency-svc-bmqhn
    Mar 22 21:39:16.148: INFO: Got endpoints: latency-svc-4sk97 [738.522315ms]
    Mar 22 21:39:16.163: INFO: Created: latency-svc-xx7c4
    Mar 22 21:39:16.199: INFO: Got endpoints: latency-svc-rps4g [751.660878ms]
    Mar 22 21:39:16.211: INFO: Created: latency-svc-dmd4h
    Mar 22 21:39:16.247: INFO: Got endpoints: latency-svc-prbf9 [748.546503ms]
    Mar 22 21:39:16.268: INFO: Created: latency-svc-5k77m
    Mar 22 21:39:16.297: INFO: Got endpoints: latency-svc-lb5pw [749.78215ms]
    Mar 22 21:39:16.309: INFO: Created: latency-svc-qm9s9
    Mar 22 21:39:16.354: INFO: Got endpoints: latency-svc-twg97 [757.563218ms]
    Mar 22 21:39:16.366: INFO: Created: latency-svc-h2mdj
    Mar 22 21:39:16.399: INFO: Got endpoints: latency-svc-9p2zq [748.334345ms]
    Mar 22 21:39:16.423: INFO: Created: latency-svc-2h92v
    Mar 22 21:39:16.447: INFO: Got endpoints: latency-svc-jgxnl [732.454139ms]
    Mar 22 21:39:16.465: INFO: Created: latency-svc-mbl5d
    Mar 22 21:39:16.498: INFO: Got endpoints: latency-svc-rfwb4 [750.80347ms]
    Mar 22 21:39:16.514: INFO: Created: latency-svc-kwzxh
    Mar 22 21:39:16.546: INFO: Got endpoints: latency-svc-77x2t [751.083765ms]
    Mar 22 21:39:16.558: INFO: Created: latency-svc-dqczq
    Mar 22 21:39:16.598: INFO: Got endpoints: latency-svc-z5w7d [738.936082ms]
    Mar 22 21:39:16.613: INFO: Created: latency-svc-cbrxp
    Mar 22 21:39:16.645: INFO: Got endpoints: latency-svc-dng7k [746.207292ms]
    Mar 22 21:39:16.659: INFO: Created: latency-svc-dbfb9
    Mar 22 21:39:16.696: INFO: Got endpoints: latency-svc-j68qz [738.196564ms]
    Mar 22 21:39:16.710: INFO: Created: latency-svc-pdkxl
    Mar 22 21:39:16.749: INFO: Got endpoints: latency-svc-kvgff [752.265291ms]
    Mar 22 21:39:16.768: INFO: Created: latency-svc-fbj4c
    Mar 22 21:39:16.800: INFO: Got endpoints: latency-svc-qpmdh [753.563084ms]
    Mar 22 21:39:16.814: INFO: Created: latency-svc-fxzw9
    Mar 22 21:39:16.854: INFO: Got endpoints: latency-svc-bmqhn [748.460844ms]
    Mar 22 21:39:16.871: INFO: Created: latency-svc-mrksg
    Mar 22 21:39:16.908: INFO: Got endpoints: latency-svc-xx7c4 [759.924718ms]
    Mar 22 21:39:16.930: INFO: Created: latency-svc-jmtx7
    Mar 22 21:39:16.945: INFO: Got endpoints: latency-svc-dmd4h [745.865396ms]
    Mar 22 21:39:16.964: INFO: Created: latency-svc-gkckx
    Mar 22 21:39:16.998: INFO: Got endpoints: latency-svc-5k77m [750.457587ms]
    Mar 22 21:39:17.013: INFO: Created: latency-svc-mj89z
    Mar 22 21:39:17.046: INFO: Got endpoints: latency-svc-qm9s9 [748.759349ms]
    Mar 22 21:39:17.061: INFO: Created: latency-svc-qt887
    Mar 22 21:39:17.099: INFO: Got endpoints: latency-svc-h2mdj [744.362177ms]
    Mar 22 21:39:17.120: INFO: Created: latency-svc-cj6p7
    Mar 22 21:39:17.159: INFO: Got endpoints: latency-svc-2h92v [760.421853ms]
    Mar 22 21:39:17.173: INFO: Created: latency-svc-49gsh
    Mar 22 21:39:17.196: INFO: Got endpoints: latency-svc-mbl5d [749.128575ms]
    Mar 22 21:39:17.207: INFO: Created: latency-svc-f85hv
    Mar 22 21:39:17.246: INFO: Got endpoints: latency-svc-kwzxh [747.860276ms]
    Mar 22 21:39:17.266: INFO: Created: latency-svc-zlw76
    Mar 22 21:39:17.297: INFO: Got endpoints: latency-svc-dqczq [751.018557ms]
    Mar 22 21:39:17.309: INFO: Created: latency-svc-47vs6
    Mar 22 21:39:17.348: INFO: Got endpoints: latency-svc-cbrxp [750.376976ms]
    Mar 22 21:39:17.361: INFO: Created: latency-svc-jwzd8
    Mar 22 21:39:17.399: INFO: Got endpoints: latency-svc-dbfb9 [754.006779ms]
    Mar 22 21:39:17.411: INFO: Created: latency-svc-4zw5k
    Mar 22 21:39:17.447: INFO: Got endpoints: latency-svc-pdkxl [751.393464ms]
    Mar 22 21:39:17.461: INFO: Created: latency-svc-j85xj
    Mar 22 21:39:17.496: INFO: Got endpoints: latency-svc-fbj4c [747.26489ms]
    Mar 22 21:39:17.509: INFO: Created: latency-svc-qbpm2
    Mar 22 21:39:17.556: INFO: Got endpoints: latency-svc-fxzw9 [755.517816ms]
    Mar 22 21:39:17.569: INFO: Created: latency-svc-9fgbs
    Mar 22 21:39:17.599: INFO: Got endpoints: latency-svc-mrksg [744.385113ms]
    Mar 22 21:39:17.615: INFO: Created: latency-svc-b8cg6
    Mar 22 21:39:17.649: INFO: Got endpoints: latency-svc-jmtx7 [741.043999ms]
    Mar 22 21:39:17.665: INFO: Created: latency-svc-kcs2l
    Mar 22 21:39:17.697: INFO: Got endpoints: latency-svc-gkckx [751.161729ms]
    Mar 22 21:39:17.711: INFO: Created: latency-svc-xn72r
    Mar 22 21:39:17.746: INFO: Got endpoints: latency-svc-mj89z [748.657423ms]
    Mar 22 21:39:17.763: INFO: Created: latency-svc-xzsj5
    Mar 22 21:39:17.799: INFO: Got endpoints: latency-svc-qt887 [751.194119ms]
    Mar 22 21:39:17.816: INFO: Created: latency-svc-l8mm2
    Mar 22 21:39:17.859: INFO: Got endpoints: latency-svc-cj6p7 [759.978781ms]
    Mar 22 21:39:17.880: INFO: Created: latency-svc-8dfb8
    Mar 22 21:39:17.900: INFO: Got endpoints: latency-svc-49gsh [738.592882ms]
    Mar 22 21:39:17.911: INFO: Created: latency-svc-gxv9t
    Mar 22 21:39:17.949: INFO: Got endpoints: latency-svc-f85hv [752.842733ms]
    Mar 22 21:39:17.965: INFO: Created: latency-svc-4gq8f
    Mar 22 21:39:17.997: INFO: Got endpoints: latency-svc-zlw76 [750.084192ms]
    Mar 22 21:39:18.010: INFO: Created: latency-svc-dxjz7
    Mar 22 21:39:18.051: INFO: Got endpoints: latency-svc-47vs6 [753.160866ms]
    Mar 22 21:39:18.066: INFO: Created: latency-svc-qx92n
    Mar 22 21:39:18.096: INFO: Got endpoints: latency-svc-jwzd8 [747.669038ms]
    Mar 22 21:39:18.107: INFO: Created: latency-svc-n5cn9
    Mar 22 21:39:18.147: INFO: Got endpoints: latency-svc-4zw5k [747.643479ms]
    Mar 22 21:39:18.166: INFO: Created: latency-svc-279cm
    Mar 22 21:39:18.198: INFO: Got endpoints: latency-svc-j85xj [751.169468ms]
    Mar 22 21:39:18.211: INFO: Created: latency-svc-62mzl
    Mar 22 21:39:18.247: INFO: Got endpoints: latency-svc-qbpm2 [750.782228ms]
    Mar 22 21:39:18.261: INFO: Created: latency-svc-mdv67
    Mar 22 21:39:18.297: INFO: Got endpoints: latency-svc-9fgbs [740.845674ms]
    Mar 22 21:39:18.310: INFO: Created: latency-svc-m8nx6
    Mar 22 21:39:18.347: INFO: Got endpoints: latency-svc-b8cg6 [748.075794ms]
    Mar 22 21:39:18.359: INFO: Created: latency-svc-hd2c4
    Mar 22 21:39:18.400: INFO: Got endpoints: latency-svc-kcs2l [751.111493ms]
    Mar 22 21:39:18.413: INFO: Created: latency-svc-zq9rw
    Mar 22 21:39:18.444: INFO: Got endpoints: latency-svc-xn72r [747.718719ms]
    Mar 22 21:39:18.457: INFO: Created: latency-svc-gqb74
    Mar 22 21:39:18.502: INFO: Got endpoints: latency-svc-xzsj5 [755.677933ms]
    Mar 22 21:39:18.518: INFO: Created: latency-svc-d7jkw
    Mar 22 21:39:18.547: INFO: Got endpoints: latency-svc-l8mm2 [747.923566ms]
    Mar 22 21:39:18.562: INFO: Created: latency-svc-qt5pv
    Mar 22 21:39:18.597: INFO: Got endpoints: latency-svc-8dfb8 [737.687561ms]
    Mar 22 21:39:18.614: INFO: Created: latency-svc-pzz68
    Mar 22 21:39:18.645: INFO: Got endpoints: latency-svc-gxv9t [745.651214ms]
    Mar 22 21:39:18.664: INFO: Created: latency-svc-7m5z9
    Mar 22 21:39:18.695: INFO: Got endpoints: latency-svc-4gq8f [746.349073ms]
    Mar 22 21:39:18.716: INFO: Created: latency-svc-hp8b2
    Mar 22 21:39:18.746: INFO: Got endpoints: latency-svc-dxjz7 [749.607822ms]
    Mar 22 21:39:18.762: INFO: Created: latency-svc-tr82w
    Mar 22 21:39:18.797: INFO: Got endpoints: latency-svc-qx92n [746.077583ms]
    Mar 22 21:39:18.810: INFO: Created: latency-svc-cjlkv
    Mar 22 21:39:18.849: INFO: Got endpoints: latency-svc-n5cn9 [752.006582ms]
    Mar 22 21:39:18.866: INFO: Created: latency-svc-gwnmw
    Mar 22 21:39:18.900: INFO: Got endpoints: latency-svc-279cm [752.182962ms]
    Mar 22 21:39:18.921: INFO: Created: latency-svc-qh2bf
    Mar 22 21:39:18.948: INFO: Got endpoints: latency-svc-62mzl [749.98288ms]
    Mar 22 21:39:18.963: INFO: Created: latency-svc-cp5xl
    Mar 22 21:39:18.996: INFO: Got endpoints: latency-svc-mdv67 [749.189301ms]
    Mar 22 21:39:19.014: INFO: Created: latency-svc-2ml92
    Mar 22 21:39:19.047: INFO: Got endpoints: latency-svc-m8nx6 [749.688939ms]
    Mar 22 21:39:19.064: INFO: Created: latency-svc-gx7mp
    Mar 22 21:39:19.099: INFO: Got endpoints: latency-svc-hd2c4 [751.409204ms]
    Mar 22 21:39:19.113: INFO: Created: latency-svc-jkb92
    Mar 22 21:39:19.150: INFO: Got endpoints: latency-svc-zq9rw [749.30865ms]
    Mar 22 21:39:19.166: INFO: Created: latency-svc-4wppn
    Mar 22 21:39:19.328: INFO: Got endpoints: latency-svc-gqb74 [883.253385ms]
    Mar 22 21:39:19.330: INFO: Got endpoints: latency-svc-qt5pv [783.588936ms]
    Mar 22 21:39:19.331: INFO: Got endpoints: latency-svc-d7jkw [828.582283ms]
    Mar 22 21:39:19.345: INFO: Created: latency-svc-hclxw
    Mar 22 21:39:19.348: INFO: Got endpoints: latency-svc-pzz68 [750.705572ms]
    Mar 22 21:39:19.350: INFO: Created: latency-svc-8tqzh
    Mar 22 21:39:19.359: INFO: Created: latency-svc-52wbz
    Mar 22 21:39:19.366: INFO: Created: latency-svc-z84rl
    Mar 22 21:39:19.397: INFO: Got endpoints: latency-svc-7m5z9 [751.48644ms]
    Mar 22 21:39:19.413: INFO: Created: latency-svc-bkc5j
    Mar 22 21:39:19.448: INFO: Got endpoints: latency-svc-hp8b2 [752.302687ms]
    Mar 22 21:39:19.462: INFO: Created: latency-svc-4x6gk
    Mar 22 21:39:19.503: INFO: Got endpoints: latency-svc-tr82w [756.254516ms]
    Mar 22 21:39:19.517: INFO: Created: latency-svc-nqltj
    Mar 22 21:39:19.551: INFO: Got endpoints: latency-svc-cjlkv [754.543634ms]
    Mar 22 21:39:19.564: INFO: Created: latency-svc-m9mj4
    Mar 22 21:39:19.599: INFO: Got endpoints: latency-svc-gwnmw [750.048545ms]
    Mar 22 21:39:19.613: INFO: Created: latency-svc-pknfq
    Mar 22 21:39:19.646: INFO: Got endpoints: latency-svc-qh2bf [745.38536ms]
    Mar 22 21:39:19.659: INFO: Created: latency-svc-zt78f
    Mar 22 21:39:19.697: INFO: Got endpoints: latency-svc-cp5xl [748.235098ms]
    Mar 22 21:39:19.709: INFO: Created: latency-svc-tcbmm
    Mar 22 21:39:19.753: INFO: Got endpoints: latency-svc-2ml92 [756.77657ms]
    Mar 22 21:39:19.767: INFO: Created: latency-svc-z9l7q
    Mar 22 21:39:19.800: INFO: Got endpoints: latency-svc-gx7mp [753.022473ms]
    Mar 22 21:39:19.821: INFO: Created: latency-svc-8bfcs
    Mar 22 21:39:19.860: INFO: Got endpoints: latency-svc-jkb92 [760.931937ms]
    Mar 22 21:39:19.906: INFO: Got endpoints: latency-svc-4wppn [755.805381ms]
    Mar 22 21:39:19.907: INFO: Created: latency-svc-tsk2w
    Mar 22 21:39:19.919: INFO: Created: latency-svc-cx59w
    Mar 22 21:39:19.960: INFO: Got endpoints: latency-svc-hclxw [632.071722ms]
    Mar 22 21:39:19.973: INFO: Created: latency-svc-qhl24
    Mar 22 21:39:19.996: INFO: Got endpoints: latency-svc-8tqzh [664.913303ms]
    Mar 22 21:39:20.011: INFO: Created: latency-svc-jn4jv
    Mar 22 21:39:20.047: INFO: Got endpoints: latency-svc-52wbz [717.078123ms]
    Mar 22 21:39:20.064: INFO: Created: latency-svc-mgqsb
    Mar 22 21:39:20.099: INFO: Got endpoints: latency-svc-z84rl [750.984873ms]
    Mar 22 21:39:20.112: INFO: Created: latency-svc-kh59q
    Mar 22 21:39:20.146: INFO: Got endpoints: latency-svc-bkc5j [749.290486ms]
    Mar 22 21:39:20.160: INFO: Created: latency-svc-bx4cz
    Mar 22 21:39:20.196: INFO: Got endpoints: latency-svc-4x6gk [748.12814ms]
    Mar 22 21:39:20.214: INFO: Created: latency-svc-ftzkq
    Mar 22 21:39:20.246: INFO: Got endpoints: latency-svc-nqltj [743.472769ms]
    Mar 22 21:39:20.261: INFO: Created: latency-svc-b5826
    Mar 22 21:39:20.298: INFO: Got endpoints: latency-svc-m9mj4 [746.951377ms]
    Mar 22 21:39:20.312: INFO: Created: latency-svc-ggpwg
    Mar 22 21:39:20.347: INFO: Got endpoints: latency-svc-pknfq [748.179074ms]
    Mar 22 21:39:20.362: INFO: Created: latency-svc-vktrx
    Mar 22 21:39:20.399: INFO: Got endpoints: latency-svc-zt78f [752.307497ms]
    Mar 22 21:39:20.413: INFO: Created: latency-svc-ft5bk
    Mar 22 21:39:20.446: INFO: Got endpoints: latency-svc-tcbmm [749.025039ms]
    Mar 22 21:39:20.463: INFO: Created: latency-svc-rlqss
    Mar 22 21:39:20.495: INFO: Got endpoints: latency-svc-z9l7q [741.79201ms]
    Mar 22 21:39:20.510: INFO: Created: latency-svc-dk4hw
    Mar 22 21:39:20.550: INFO: Got endpoints: latency-svc-8bfcs [750.332565ms]
    Mar 22 21:39:20.565: INFO: Created: latency-svc-xlpc5
    Mar 22 21:39:20.596: INFO: Got endpoints: latency-svc-tsk2w [726.113536ms]
    Mar 22 21:39:20.613: INFO: Created: latency-svc-c95vp
    Mar 22 21:39:20.648: INFO: Got endpoints: latency-svc-cx59w [741.495969ms]
    Mar 22 21:39:20.662: INFO: Created: latency-svc-94gq4
    Mar 22 21:39:20.703: INFO: Got endpoints: latency-svc-qhl24 [742.879845ms]
    Mar 22 21:39:20.724: INFO: Created: latency-svc-ggkw8
    Mar 22 21:39:20.753: INFO: Got endpoints: latency-svc-jn4jv [757.42977ms]
    Mar 22 21:39:20.767: INFO: Created: latency-svc-57lzm
    Mar 22 21:39:20.797: INFO: Got endpoints: latency-svc-mgqsb [749.125463ms]
    Mar 22 21:39:20.811: INFO: Created: latency-svc-z896x
    Mar 22 21:39:20.856: INFO: Got endpoints: latency-svc-kh59q [756.273681ms]
    Mar 22 21:39:20.872: INFO: Created: latency-svc-sdnzp
    Mar 22 21:39:20.897: INFO: Got endpoints: latency-svc-bx4cz [750.907975ms]
    Mar 22 21:39:20.912: INFO: Created: latency-svc-f6vxs
    Mar 22 21:39:20.948: INFO: Got endpoints: latency-svc-ftzkq [751.002415ms]
    Mar 22 21:39:20.966: INFO: Created: latency-svc-kpnfw
    Mar 22 21:39:20.996: INFO: Got endpoints: latency-svc-b5826 [749.581974ms]
    Mar 22 21:39:21.010: INFO: Created: latency-svc-fv5w4
    Mar 22 21:39:21.049: INFO: Got endpoints: latency-svc-ggpwg [750.583349ms]
    Mar 22 21:39:21.063: INFO: Created: latency-svc-h55j6
    Mar 22 21:39:21.099: INFO: Got endpoints: latency-svc-vktrx [750.836737ms]
    Mar 22 21:39:21.112: INFO: Created: latency-svc-xpzpd
    Mar 22 21:39:21.147: INFO: Got endpoints: latency-svc-ft5bk [748.206848ms]
    Mar 22 21:39:21.161: INFO: Created: latency-svc-fvzfh
    Mar 22 21:39:21.198: INFO: Got endpoints: latency-svc-rlqss [752.18992ms]
    Mar 22 21:39:21.214: INFO: Created: latency-svc-sc47b
    Mar 22 21:39:21.249: INFO: Got endpoints: latency-svc-dk4hw [752.666393ms]
    Mar 22 21:39:21.266: INFO: Created: latency-svc-7f2br
    Mar 22 21:39:21.304: INFO: Got endpoints: latency-svc-xlpc5 [753.654534ms]
    Mar 22 21:39:21.323: INFO: Created: latency-svc-dhl6g
    Mar 22 21:39:21.350: INFO: Got endpoints: latency-svc-c95vp [754.1329ms]
    Mar 22 21:39:21.376: INFO: Created: latency-svc-grk4r
    Mar 22 21:39:21.398: INFO: Got endpoints: latency-svc-94gq4 [749.394874ms]
    Mar 22 21:39:21.415: INFO: Created: latency-svc-zqc58
    Mar 22 21:39:21.449: INFO: Got endpoints: latency-svc-ggkw8 [746.46159ms]
    Mar 22 21:39:21.466: INFO: Created: latency-svc-xwld2
    Mar 22 21:39:21.496: INFO: Got endpoints: latency-svc-57lzm [742.768348ms]
    Mar 22 21:39:21.510: INFO: Created: latency-svc-rjl8l
    Mar 22 21:39:21.545: INFO: Got endpoints: latency-svc-z896x [747.663823ms]
    Mar 22 21:39:21.558: INFO: Created: latency-svc-jtx7z
    Mar 22 21:39:21.596: INFO: Got endpoints: latency-svc-sdnzp [740.012165ms]
    Mar 22 21:39:21.610: INFO: Created: latency-svc-rqb8t
    Mar 22 21:39:21.658: INFO: Got endpoints: latency-svc-f6vxs [759.683514ms]
    Mar 22 21:39:21.676: INFO: Created: latency-svc-fnvn6
    Mar 22 21:39:21.695: INFO: Got endpoints: latency-svc-kpnfw [747.291039ms]
    Mar 22 21:39:21.709: INFO: Created: latency-svc-5zr8v
    Mar 22 21:39:21.748: INFO: Got endpoints: latency-svc-fv5w4 [751.581136ms]
    Mar 22 21:39:21.765: INFO: Created: latency-svc-7mxhb
    Mar 22 21:39:21.795: INFO: Got endpoints: latency-svc-h55j6 [745.856898ms]
    Mar 22 21:39:21.807: INFO: Created: latency-svc-q4gtp
    Mar 22 21:39:21.852: INFO: Got endpoints: latency-svc-xpzpd [753.359867ms]
    Mar 22 21:39:21.871: INFO: Created: latency-svc-br67m
    Mar 22 21:39:21.900: INFO: Got endpoints: latency-svc-fvzfh [753.161037ms]
    Mar 22 21:39:21.913: INFO: Created: latency-svc-mc7tp
    Mar 22 21:39:21.948: INFO: Got endpoints: latency-svc-sc47b [750.255857ms]
    Mar 22 21:39:21.999: INFO: Got endpoints: latency-svc-7f2br [749.921424ms]
    Mar 22 21:39:22.046: INFO: Got endpoints: latency-svc-dhl6g [741.600683ms]
    Mar 22 21:39:22.097: INFO: Got endpoints: latency-svc-grk4r [747.007001ms]
    Mar 22 21:39:22.150: INFO: Got endpoints: latency-svc-zqc58 [751.077202ms]
    Mar 22 21:39:22.199: INFO: Got endpoints: latency-svc-xwld2 [749.664664ms]
    Mar 22 21:39:22.246: INFO: Got endpoints: latency-svc-rjl8l [749.617566ms]
    Mar 22 21:39:22.295: INFO: Got endpoints: latency-svc-jtx7z [749.9813ms]
    Mar 22 21:39:22.349: INFO: Got endpoints: latency-svc-rqb8t [753.292545ms]
    Mar 22 21:39:22.399: INFO: Got endpoints: latency-svc-fnvn6 [740.488113ms]
    Mar 22 21:39:22.450: INFO: Got endpoints: latency-svc-5zr8v [754.33364ms]
    Mar 22 21:39:22.496: INFO: Got endpoints: latency-svc-7mxhb [747.728046ms]
    Mar 22 21:39:22.547: INFO: Got endpoints: latency-svc-q4gtp [752.631504ms]
    Mar 22 21:39:22.599: INFO: Got endpoints: latency-svc-br67m [746.741736ms]
    Mar 22 21:39:22.649: INFO: Got endpoints: latency-svc-mc7tp [748.787206ms]
    Mar 22 21:39:22.649: INFO: Latencies: [21.269116ms 29.842879ms 29.859573ms 38.813549ms 41.904146ms 50.472929ms 58.704661ms 61.728304ms 71.581263ms 74.527061ms 79.405684ms 88.511471ms 88.606918ms 90.693981ms 91.844388ms 91.956674ms 93.485441ms 96.414537ms 98.129706ms 101.788883ms 102.690974ms 104.855327ms 105.308212ms 105.431711ms 105.630195ms 106.872136ms 107.672376ms 108.193524ms 109.888803ms 110.568716ms 110.996246ms 112.906074ms 113.433262ms 114.071513ms 155.819343ms 201.41705ms 239.002493ms 267.370805ms 318.469637ms 359.428506ms 404.417775ms 456.917231ms 488.713919ms 537.450523ms 582.207379ms 618.8687ms 632.071722ms 664.913303ms 668.251121ms 714.470082ms 717.078123ms 726.113536ms 732.454139ms 737.687561ms 738.196564ms 738.522315ms 738.592882ms 738.936082ms 740.012165ms 740.488113ms 740.845674ms 741.043999ms 741.495969ms 741.600683ms 741.79201ms 742.768348ms 742.879845ms 743.472769ms 744.362177ms 744.385113ms 745.056861ms 745.38536ms 745.651214ms 745.856898ms 745.865396ms 746.077583ms 746.207292ms 746.349073ms 746.46159ms 746.741736ms 746.784083ms 746.951377ms 746.982118ms 747.007001ms 747.26489ms 747.291039ms 747.316103ms 747.643479ms 747.663823ms 747.669038ms 747.718719ms 747.728046ms 747.860276ms 747.923566ms 748.075794ms 748.12814ms 748.179074ms 748.206848ms 748.235098ms 748.334345ms 748.460844ms 748.546503ms 748.657423ms 748.759349ms 748.787206ms 748.966302ms 749.025039ms 749.087864ms 749.125463ms 749.128575ms 749.189301ms 749.246063ms 749.290486ms 749.30865ms 749.39234ms 749.394874ms 749.47749ms 749.581974ms 749.607822ms 749.617566ms 749.664664ms 749.688939ms 749.78215ms 749.921424ms 749.9813ms 749.98288ms 750.048545ms 750.084192ms 750.255857ms 750.31808ms 750.332565ms 750.376976ms 750.457587ms 750.535618ms 750.583349ms 750.705572ms 750.782228ms 750.80347ms 750.836737ms 750.876136ms 750.907975ms 750.931458ms 750.984873ms 751.002415ms 751.018557ms 751.077202ms 751.083765ms 751.111493ms 751.147723ms 751.161729ms 751.169468ms 751.194119ms 751.393464ms 751.409204ms 751.449294ms 751.48644ms 751.555402ms 751.581136ms 751.660878ms 752.006582ms 752.182962ms 752.18992ms 752.265291ms 752.302687ms 752.307497ms 752.631504ms 752.666393ms 752.842733ms 753.022473ms 753.160866ms 753.161037ms 753.292545ms 753.359867ms 753.563084ms 753.654534ms 754.006779ms 754.1329ms 754.33364ms 754.543634ms 755.517816ms 755.677933ms 755.753102ms 755.805381ms 756.254516ms 756.273681ms 756.77657ms 757.42977ms 757.563218ms 759.683514ms 759.81874ms 759.924718ms 759.978781ms 760.421853ms 760.931937ms 761.616745ms 762.501483ms 767.803843ms 783.588936ms 828.582283ms 883.253385ms]
    Mar 22 21:39:22.649: INFO: 50 %ile: 748.460844ms
    Mar 22 21:39:22.649: INFO: 90 %ile: 755.677933ms
    Mar 22 21:39:22.649: INFO: 99 %ile: 828.582283ms
    Mar 22 21:39:22.649: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-509" for this suite. 03/22/23 21:39:22.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:22.667
Mar 22 21:39:22.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:39:22.669
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:22.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:22.701
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-67b2db1d-3279-4321-ae27-ba008f7459d1 03/22/23 21:39:22.708
STEP: Creating a pod to test consume configMaps 03/22/23 21:39:22.715
Mar 22 21:39:22.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f" in namespace "configmap-9427" to be "Succeeded or Failed"
Mar 22 21:39:22.730: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.241643ms
Mar 22 21:39:24.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0107742s
Mar 22 21:39:26.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01106106s
STEP: Saw pod success 03/22/23 21:39:26.736
Mar 22 21:39:26.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f" satisfied condition "Succeeded or Failed"
Mar 22 21:39:26.741: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:39:26.753
Mar 22 21:39:26.767: INFO: Waiting for pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f to disappear
Mar 22 21:39:26.782: INFO: Pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:26.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9427" for this suite. 03/22/23 21:39:26.79
------------------------------
• [4.131 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:22.667
    Mar 22 21:39:22.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:39:22.669
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:22.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:22.701
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-67b2db1d-3279-4321-ae27-ba008f7459d1 03/22/23 21:39:22.708
    STEP: Creating a pod to test consume configMaps 03/22/23 21:39:22.715
    Mar 22 21:39:22.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f" in namespace "configmap-9427" to be "Succeeded or Failed"
    Mar 22 21:39:22.730: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.241643ms
    Mar 22 21:39:24.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0107742s
    Mar 22 21:39:26.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01106106s
    STEP: Saw pod success 03/22/23 21:39:26.736
    Mar 22 21:39:26.736: INFO: Pod "pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f" satisfied condition "Succeeded or Failed"
    Mar 22 21:39:26.741: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:39:26.753
    Mar 22 21:39:26.767: INFO: Waiting for pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f to disappear
    Mar 22 21:39:26.782: INFO: Pod pod-configmaps-d0758838-776a-4242-ba22-d5933a14f74f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:26.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9427" for this suite. 03/22/23 21:39:26.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:26.8
Mar 22 21:39:26.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context-test 03/22/23 21:39:26.802
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:26.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:26.823
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Mar 22 21:39:26.843: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108" in namespace "security-context-test-8872" to be "Succeeded or Failed"
Mar 22 21:39:26.850: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061938ms
Mar 22 21:39:28.859: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01546995s
Mar 22 21:39:30.860: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016864741s
Mar 22 21:39:32.856: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012847717s
Mar 22 21:39:32.856: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:32.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8872" for this suite. 03/22/23 21:39:32.877
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:26.8
    Mar 22 21:39:26.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context-test 03/22/23 21:39:26.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:26.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:26.823
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Mar 22 21:39:26.843: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108" in namespace "security-context-test-8872" to be "Succeeded or Failed"
    Mar 22 21:39:26.850: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061938ms
    Mar 22 21:39:28.859: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01546995s
    Mar 22 21:39:30.860: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016864741s
    Mar 22 21:39:32.856: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012847717s
    Mar 22 21:39:32.856: INFO: Pod "alpine-nnp-false-2b6f3e51-5d0d-40f7-bbf8-74b7cf855108" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:32.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8872" for this suite. 03/22/23 21:39:32.877
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:32.885
Mar 22 21:39:32.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:39:32.886
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:32.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:32.917
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 03/22/23 21:39:32.927
Mar 22 21:39:32.943: INFO: Waiting up to 5m0s for pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb" in namespace "emptydir-8096" to be "Succeeded or Failed"
Mar 22 21:39:32.948: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826508ms
Mar 22 21:39:34.954: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010613903s
Mar 22 21:39:36.954: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Running", Reason="", readiness=false. Elapsed: 4.011095729s
Mar 22 21:39:38.959: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016117647s
STEP: Saw pod success 03/22/23 21:39:38.959
Mar 22 21:39:38.960: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb" satisfied condition "Succeeded or Failed"
Mar 22 21:39:38.964: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-37336c95-20fa-4385-95ea-2796eb2818bb container test-container: <nil>
STEP: delete the pod 03/22/23 21:39:38.975
Mar 22 21:39:38.993: INFO: Waiting for pod pod-37336c95-20fa-4385-95ea-2796eb2818bb to disappear
Mar 22 21:39:38.996: INFO: Pod pod-37336c95-20fa-4385-95ea-2796eb2818bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:38.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8096" for this suite. 03/22/23 21:39:39.007
------------------------------
• [SLOW TEST] [6.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:32.885
    Mar 22 21:39:32.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:39:32.886
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:32.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:32.917
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/22/23 21:39:32.927
    Mar 22 21:39:32.943: INFO: Waiting up to 5m0s for pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb" in namespace "emptydir-8096" to be "Succeeded or Failed"
    Mar 22 21:39:32.948: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826508ms
    Mar 22 21:39:34.954: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010613903s
    Mar 22 21:39:36.954: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Running", Reason="", readiness=false. Elapsed: 4.011095729s
    Mar 22 21:39:38.959: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016117647s
    STEP: Saw pod success 03/22/23 21:39:38.959
    Mar 22 21:39:38.960: INFO: Pod "pod-37336c95-20fa-4385-95ea-2796eb2818bb" satisfied condition "Succeeded or Failed"
    Mar 22 21:39:38.964: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-37336c95-20fa-4385-95ea-2796eb2818bb container test-container: <nil>
    STEP: delete the pod 03/22/23 21:39:38.975
    Mar 22 21:39:38.993: INFO: Waiting for pod pod-37336c95-20fa-4385-95ea-2796eb2818bb to disappear
    Mar 22 21:39:38.996: INFO: Pod pod-37336c95-20fa-4385-95ea-2796eb2818bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:38.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8096" for this suite. 03/22/23 21:39:39.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:39.023
Mar 22 21:39:39.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:39:39.025
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:39.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:39.051
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/22/23 21:39:39.06
Mar 22 21:39:39.069: INFO: Waiting up to 5m0s for pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6" in namespace "emptydir-2249" to be "Succeeded or Failed"
Mar 22 21:39:39.075: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015925ms
Mar 22 21:39:41.081: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011671877s
Mar 22 21:39:43.085: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015879352s
STEP: Saw pod success 03/22/23 21:39:43.085
Mar 22 21:39:43.085: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6" satisfied condition "Succeeded or Failed"
Mar 22 21:39:43.089: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 container test-container: <nil>
STEP: delete the pod 03/22/23 21:39:43.102
Mar 22 21:39:43.117: INFO: Waiting for pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 to disappear
Mar 22 21:39:43.121: INFO: Pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:43.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2249" for this suite. 03/22/23 21:39:43.13
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:39.023
    Mar 22 21:39:39.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:39:39.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:39.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:39.051
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/22/23 21:39:39.06
    Mar 22 21:39:39.069: INFO: Waiting up to 5m0s for pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6" in namespace "emptydir-2249" to be "Succeeded or Failed"
    Mar 22 21:39:39.075: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015925ms
    Mar 22 21:39:41.081: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011671877s
    Mar 22 21:39:43.085: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015879352s
    STEP: Saw pod success 03/22/23 21:39:43.085
    Mar 22 21:39:43.085: INFO: Pod "pod-c696b027-869d-4e28-b9f5-29d4f62b86e6" satisfied condition "Succeeded or Failed"
    Mar 22 21:39:43.089: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 container test-container: <nil>
    STEP: delete the pod 03/22/23 21:39:43.102
    Mar 22 21:39:43.117: INFO: Waiting for pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 to disappear
    Mar 22 21:39:43.121: INFO: Pod pod-c696b027-869d-4e28-b9f5-29d4f62b86e6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:43.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2249" for this suite. 03/22/23 21:39:43.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:43.145
Mar 22 21:39:43.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:39:43.147
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:43.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:43.168
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-e34e4139-9fa7-4eae-b4fc-a24daa9892f5 03/22/23 21:39:43.174
STEP: Creating secret with name secret-projected-all-test-volume-6e4517a3-6ab7-4719-a3ad-688befef0b04 03/22/23 21:39:43.18
STEP: Creating a pod to test Check all projections for projected volume plugin 03/22/23 21:39:43.187
Mar 22 21:39:43.197: INFO: Waiting up to 5m0s for pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e" in namespace "projected-8571" to be "Succeeded or Failed"
Mar 22 21:39:43.203: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828737ms
Mar 22 21:39:45.213: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015515484s
Mar 22 21:39:47.210: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011894535s
STEP: Saw pod success 03/22/23 21:39:47.21
Mar 22 21:39:47.210: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e" satisfied condition "Succeeded or Failed"
Mar 22 21:39:47.214: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e container projected-all-volume-test: <nil>
STEP: delete the pod 03/22/23 21:39:47.225
Mar 22 21:39:47.239: INFO: Waiting for pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e to disappear
Mar 22 21:39:47.243: INFO: Pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Mar 22 21:39:47.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8571" for this suite. 03/22/23 21:39:47.25
------------------------------
• [4.112 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:43.145
    Mar 22 21:39:43.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:39:43.147
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:43.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:43.168
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-e34e4139-9fa7-4eae-b4fc-a24daa9892f5 03/22/23 21:39:43.174
    STEP: Creating secret with name secret-projected-all-test-volume-6e4517a3-6ab7-4719-a3ad-688befef0b04 03/22/23 21:39:43.18
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/22/23 21:39:43.187
    Mar 22 21:39:43.197: INFO: Waiting up to 5m0s for pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e" in namespace "projected-8571" to be "Succeeded or Failed"
    Mar 22 21:39:43.203: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828737ms
    Mar 22 21:39:45.213: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015515484s
    Mar 22 21:39:47.210: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011894535s
    STEP: Saw pod success 03/22/23 21:39:47.21
    Mar 22 21:39:47.210: INFO: Pod "projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e" satisfied condition "Succeeded or Failed"
    Mar 22 21:39:47.214: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e container projected-all-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:39:47.225
    Mar 22 21:39:47.239: INFO: Waiting for pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e to disappear
    Mar 22 21:39:47.243: INFO: Pod projected-volume-c4ce937c-b691-4a32-b5ed-fd53d6ecd51e no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:39:47.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8571" for this suite. 03/22/23 21:39:47.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:39:47.264
Mar 22 21:39:47.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir-wrapper 03/22/23 21:39:47.266
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:47.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:47.291
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/22/23 21:39:47.298
STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:39:47.631
Mar 22 21:39:47.649: INFO: Pod name wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2: Found 0 pods out of 5
Mar 22 21:39:52.658: INFO: Pod name wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/22/23 21:39:52.658
Mar 22 21:39:52.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:39:52.664: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924068ms
Mar 22 21:39:54.681: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022134968s
Mar 22 21:39:56.673: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013673787s
Mar 22 21:39:58.671: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011925899s
Mar 22 21:40:00.673: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013701685s
Mar 22 21:40:02.678: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Running", Reason="", readiness=true. Elapsed: 10.019239049s
Mar 22 21:40:02.687: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm" satisfied condition "running"
Mar 22 21:40:02.687: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:02.694: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp": Phase="Running", Reason="", readiness=true. Elapsed: 7.524247ms
Mar 22 21:40:02.694: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp" satisfied condition "running"
Mar 22 21:40:02.694: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:02.701: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw": Phase="Running", Reason="", readiness=true. Elapsed: 6.938662ms
Mar 22 21:40:02.701: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw" satisfied condition "running"
Mar 22 21:40:02.701: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:02.708: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r": Phase="Running", Reason="", readiness=true. Elapsed: 6.338551ms
Mar 22 21:40:02.708: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r" satisfied condition "running"
Mar 22 21:40:02.708: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:02.713: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf": Phase="Running", Reason="", readiness=true. Elapsed: 5.54117ms
Mar 22 21:40:02.713: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:02.713
Mar 22 21:40:02.780: INFO: Deleting ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 took: 11.430139ms
Mar 22 21:40:02.881: INFO: Terminating ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 pods took: 100.475164ms
STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:40:06.688
Mar 22 21:40:06.708: INFO: Pod name wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10: Found 0 pods out of 5
Mar 22 21:40:11.717: INFO: Pod name wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/22/23 21:40:11.717
Mar 22 21:40:11.717: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:11.723: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.339523ms
Mar 22 21:40:13.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017162004s
Mar 22 21:40:15.730: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012642688s
Mar 22 21:40:17.729: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01163801s
Mar 22 21:40:19.730: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012555305s
Mar 22 21:40:21.731: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013073575s
Mar 22 21:40:21.731: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf" satisfied condition "running"
Mar 22 21:40:21.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:21.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d": Phase="Running", Reason="", readiness=true. Elapsed: 4.16915ms
Mar 22 21:40:21.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d" satisfied condition "running"
Mar 22 21:40:21.735: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:21.740: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl": Phase="Running", Reason="", readiness=true. Elapsed: 4.828121ms
Mar 22 21:40:21.740: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl" satisfied condition "running"
Mar 22 21:40:21.740: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:21.745: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635778ms
Mar 22 21:40:23.751: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010836116s
Mar 22 21:40:23.751: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6" satisfied condition "running"
Mar 22 21:40:23.751: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:23.757: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf": Phase="Running", Reason="", readiness=true. Elapsed: 5.134373ms
Mar 22 21:40:23.757: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:23.757
Mar 22 21:40:23.823: INFO: Deleting ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 took: 9.055983ms
Mar 22 21:40:23.924: INFO: Terminating ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 pods took: 100.814283ms
STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:40:26.73
Mar 22 21:40:26.746: INFO: Pod name wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f: Found 0 pods out of 5
Mar 22 21:40:31.755: INFO: Pod name wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/22/23 21:40:31.755
Mar 22 21:40:31.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:31.760: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828928ms
Mar 22 21:40:33.769: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013451168s
Mar 22 21:40:35.767: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011361102s
Mar 22 21:40:37.767: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011368023s
Mar 22 21:40:39.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012739447s
Mar 22 21:40:41.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Running", Reason="", readiness=true. Elapsed: 10.012595001s
Mar 22 21:40:41.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658" satisfied condition "running"
Mar 22 21:40:41.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:41.774: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p": Phase="Running", Reason="", readiness=true. Elapsed: 5.740567ms
Mar 22 21:40:41.775: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p" satisfied condition "running"
Mar 22 21:40:41.775: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:41.780: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq": Phase="Running", Reason="", readiness=true. Elapsed: 5.108463ms
Mar 22 21:40:41.781: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq" satisfied condition "running"
Mar 22 21:40:41.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:41.785: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5": Phase="Running", Reason="", readiness=true. Elapsed: 4.669776ms
Mar 22 21:40:41.785: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5" satisfied condition "running"
Mar 22 21:40:41.785: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c" in namespace "emptydir-wrapper-8609" to be "running"
Mar 22 21:40:41.791: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.156942ms
Mar 22 21:40:43.798: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012600557s
Mar 22 21:40:43.798: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:43.798
Mar 22 21:40:43.864: INFO: Deleting ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f took: 8.549133ms
Mar 22 21:40:43.964: INFO: Terminating ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f pods took: 100.368723ms
STEP: Cleaning up the configMaps 03/22/23 21:40:46.765
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:40:47.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8609" for this suite. 03/22/23 21:40:47.175
------------------------------
• [SLOW TEST] [59.926 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:39:47.264
    Mar 22 21:39:47.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir-wrapper 03/22/23 21:39:47.266
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:39:47.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:39:47.291
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/22/23 21:39:47.298
    STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:39:47.631
    Mar 22 21:39:47.649: INFO: Pod name wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2: Found 0 pods out of 5
    Mar 22 21:39:52.658: INFO: Pod name wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/22/23 21:39:52.658
    Mar 22 21:39:52.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:39:52.664: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.924068ms
    Mar 22 21:39:54.681: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022134968s
    Mar 22 21:39:56.673: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013673787s
    Mar 22 21:39:58.671: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011925899s
    Mar 22 21:40:00.673: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013701685s
    Mar 22 21:40:02.678: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm": Phase="Running", Reason="", readiness=true. Elapsed: 10.019239049s
    Mar 22 21:40:02.687: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbbpm" satisfied condition "running"
    Mar 22 21:40:02.687: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:02.694: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp": Phase="Running", Reason="", readiness=true. Elapsed: 7.524247ms
    Mar 22 21:40:02.694: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-dbnxp" satisfied condition "running"
    Mar 22 21:40:02.694: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:02.701: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw": Phase="Running", Reason="", readiness=true. Elapsed: 6.938662ms
    Mar 22 21:40:02.701: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-fv8xw" satisfied condition "running"
    Mar 22 21:40:02.701: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:02.708: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r": Phase="Running", Reason="", readiness=true. Elapsed: 6.338551ms
    Mar 22 21:40:02.708: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-vk82r" satisfied condition "running"
    Mar 22 21:40:02.708: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:02.713: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf": Phase="Running", Reason="", readiness=true. Elapsed: 5.54117ms
    Mar 22 21:40:02.713: INFO: Pod "wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2-xbhrf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:02.713
    Mar 22 21:40:02.780: INFO: Deleting ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 took: 11.430139ms
    Mar 22 21:40:02.881: INFO: Terminating ReplicationController wrapped-volume-race-a20e1746-e36f-43e5-9d87-ffe31b9d90a2 pods took: 100.475164ms
    STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:40:06.688
    Mar 22 21:40:06.708: INFO: Pod name wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10: Found 0 pods out of 5
    Mar 22 21:40:11.717: INFO: Pod name wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/22/23 21:40:11.717
    Mar 22 21:40:11.717: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:11.723: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.339523ms
    Mar 22 21:40:13.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017162004s
    Mar 22 21:40:15.730: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012642688s
    Mar 22 21:40:17.729: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01163801s
    Mar 22 21:40:19.730: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012555305s
    Mar 22 21:40:21.731: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf": Phase="Running", Reason="", readiness=true. Elapsed: 10.013073575s
    Mar 22 21:40:21.731: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-b96kf" satisfied condition "running"
    Mar 22 21:40:21.731: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:21.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d": Phase="Running", Reason="", readiness=true. Elapsed: 4.16915ms
    Mar 22 21:40:21.735: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-gst4d" satisfied condition "running"
    Mar 22 21:40:21.735: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:21.740: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl": Phase="Running", Reason="", readiness=true. Elapsed: 4.828121ms
    Mar 22 21:40:21.740: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-jqqgl" satisfied condition "running"
    Mar 22 21:40:21.740: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:21.745: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635778ms
    Mar 22 21:40:23.751: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010836116s
    Mar 22 21:40:23.751: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-nhrc6" satisfied condition "running"
    Mar 22 21:40:23.751: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:23.757: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf": Phase="Running", Reason="", readiness=true. Elapsed: 5.134373ms
    Mar 22 21:40:23.757: INFO: Pod "wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10-x8zbf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:23.757
    Mar 22 21:40:23.823: INFO: Deleting ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 took: 9.055983ms
    Mar 22 21:40:23.924: INFO: Terminating ReplicationController wrapped-volume-race-ac6ac04c-e7c2-4e27-abb4-8f2e8af56f10 pods took: 100.814283ms
    STEP: Creating RC which spawns configmap-volume pods 03/22/23 21:40:26.73
    Mar 22 21:40:26.746: INFO: Pod name wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f: Found 0 pods out of 5
    Mar 22 21:40:31.755: INFO: Pod name wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/22/23 21:40:31.755
    Mar 22 21:40:31.756: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:31.760: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828928ms
    Mar 22 21:40:33.769: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013451168s
    Mar 22 21:40:35.767: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011361102s
    Mar 22 21:40:37.767: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011368023s
    Mar 22 21:40:39.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012739447s
    Mar 22 21:40:41.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658": Phase="Running", Reason="", readiness=true. Elapsed: 10.012595001s
    Mar 22 21:40:41.768: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-2m658" satisfied condition "running"
    Mar 22 21:40:41.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:41.774: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p": Phase="Running", Reason="", readiness=true. Elapsed: 5.740567ms
    Mar 22 21:40:41.775: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-dp84p" satisfied condition "running"
    Mar 22 21:40:41.775: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:41.780: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq": Phase="Running", Reason="", readiness=true. Elapsed: 5.108463ms
    Mar 22 21:40:41.781: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-lh9gq" satisfied condition "running"
    Mar 22 21:40:41.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:41.785: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5": Phase="Running", Reason="", readiness=true. Elapsed: 4.669776ms
    Mar 22 21:40:41.785: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-p89v5" satisfied condition "running"
    Mar 22 21:40:41.785: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c" in namespace "emptydir-wrapper-8609" to be "running"
    Mar 22 21:40:41.791: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.156942ms
    Mar 22 21:40:43.798: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012600557s
    Mar 22 21:40:43.798: INFO: Pod "wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f-tf76c" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f in namespace emptydir-wrapper-8609, will wait for the garbage collector to delete the pods 03/22/23 21:40:43.798
    Mar 22 21:40:43.864: INFO: Deleting ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f took: 8.549133ms
    Mar 22 21:40:43.964: INFO: Terminating ReplicationController wrapped-volume-race-9cd34811-94dd-40e7-bcd8-3cb040736b3f pods took: 100.368723ms
    STEP: Cleaning up the configMaps 03/22/23 21:40:46.765
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:40:47.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8609" for this suite. 03/22/23 21:40:47.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:40:47.196
Mar 22 21:40:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename aggregator 03/22/23 21:40:47.198
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:40:47.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:40:47.245
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 22 21:40:47.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/22/23 21:40:47.26
Mar 22 21:40:48.826: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 22 21:40:50.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:40:52.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:40:54.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:40:56.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:40:58.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:00.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:02.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:04.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:06.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:08.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:10.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 22 21:41:13.050: INFO: Waited 153.929621ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/22/23 21:41:13.216
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/22/23 21:41:13.224
STEP: List APIServices 03/22/23 21:41:13.232
Mar 22 21:41:13.246: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-1056" for this suite. 03/22/23 21:41:13.508
------------------------------
• [SLOW TEST] [26.338 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:40:47.196
    Mar 22 21:40:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename aggregator 03/22/23 21:40:47.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:40:47.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:40:47.245
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 22 21:40:47.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/22/23 21:40:47.26
    Mar 22 21:40:48.826: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 22 21:40:50.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:40:52.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:40:54.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:40:56.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:40:58.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:00.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:02.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:04.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:06.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:08.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:10.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 22, 21, 40, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 22 21:41:13.050: INFO: Waited 153.929621ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/22/23 21:41:13.216
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/22/23 21:41:13.224
    STEP: List APIServices 03/22/23 21:41:13.232
    Mar 22 21:41:13.246: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-1056" for this suite. 03/22/23 21:41:13.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:13.538
Mar 22 21:41:13.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 21:41:13.54
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:13.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:13.583
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-1ec89b4c-3ecf-479a-961c-a6cb8315c5a0 03/22/23 21:41:13.594
STEP: Creating a pod to test consume secrets 03/22/23 21:41:13.601
Mar 22 21:41:13.613: INFO: Waiting up to 5m0s for pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e" in namespace "secrets-8622" to be "Succeeded or Failed"
Mar 22 21:41:13.621: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043873ms
Mar 22 21:41:15.629: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01575808s
Mar 22 21:41:17.628: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014798175s
STEP: Saw pod success 03/22/23 21:41:17.628
Mar 22 21:41:17.629: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e" satisfied condition "Succeeded or Failed"
Mar 22 21:41:17.633: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:41:17.677
Mar 22 21:41:17.688: INFO: Waiting for pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e to disappear
Mar 22 21:41:17.706: INFO: Pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:17.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8622" for this suite. 03/22/23 21:41:17.713
------------------------------
• [4.187 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:13.538
    Mar 22 21:41:13.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 21:41:13.54
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:13.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:13.583
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-1ec89b4c-3ecf-479a-961c-a6cb8315c5a0 03/22/23 21:41:13.594
    STEP: Creating a pod to test consume secrets 03/22/23 21:41:13.601
    Mar 22 21:41:13.613: INFO: Waiting up to 5m0s for pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e" in namespace "secrets-8622" to be "Succeeded or Failed"
    Mar 22 21:41:13.621: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043873ms
    Mar 22 21:41:15.629: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01575808s
    Mar 22 21:41:17.628: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014798175s
    STEP: Saw pod success 03/22/23 21:41:17.628
    Mar 22 21:41:17.629: INFO: Pod "pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e" satisfied condition "Succeeded or Failed"
    Mar 22 21:41:17.633: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:41:17.677
    Mar 22 21:41:17.688: INFO: Waiting for pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e to disappear
    Mar 22 21:41:17.706: INFO: Pod pod-secrets-edb294e6-1c77-406c-98a8-d4c13feb678e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:17.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8622" for this suite. 03/22/23 21:41:17.713
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:17.726
Mar 22 21:41:17.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename podtemplate 03/22/23 21:41:17.728
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:17.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:17.752
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/22/23 21:41:17.759
Mar 22 21:41:17.768: INFO: created test-podtemplate-1
Mar 22 21:41:17.775: INFO: created test-podtemplate-2
Mar 22 21:41:17.785: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/22/23 21:41:17.785
STEP: delete collection of pod templates 03/22/23 21:41:17.789
Mar 22 21:41:17.789: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/22/23 21:41:17.803
Mar 22 21:41:17.803: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:17.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8881" for this suite. 03/22/23 21:41:17.821
------------------------------
• [0.103 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:17.726
    Mar 22 21:41:17.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename podtemplate 03/22/23 21:41:17.728
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:17.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:17.752
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/22/23 21:41:17.759
    Mar 22 21:41:17.768: INFO: created test-podtemplate-1
    Mar 22 21:41:17.775: INFO: created test-podtemplate-2
    Mar 22 21:41:17.785: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/22/23 21:41:17.785
    STEP: delete collection of pod templates 03/22/23 21:41:17.789
    Mar 22 21:41:17.789: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/22/23 21:41:17.803
    Mar 22 21:41:17.803: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:17.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8881" for this suite. 03/22/23 21:41:17.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:17.833
Mar 22 21:41:17.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename pods 03/22/23 21:41:17.835
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:17.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:17.857
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 03/22/23 21:41:17.865
Mar 22 21:41:17.878: INFO: created test-pod-1
Mar 22 21:41:17.899: INFO: created test-pod-2
Mar 22 21:41:17.929: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/22/23 21:41:17.93
Mar 22 21:41:17.930: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7989' to be running and ready
Mar 22 21:41:17.945: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 22 21:41:17.945: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 22 21:41:17.945: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 22 21:41:17.945: INFO: 0 / 3 pods in namespace 'pods-7989' are running and ready (0 seconds elapsed)
Mar 22 21:41:17.946: INFO: expected 0 pod replicas in namespace 'pods-7989', 0 are Running and Ready.
Mar 22 21:41:17.946: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
Mar 22 21:41:17.946: INFO: test-pod-1  pool-v7t41yxh0-q56kk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
Mar 22 21:41:17.946: INFO: test-pod-2  pool-v7t41yxh0-q56kk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
Mar 22 21:41:17.946: INFO: test-pod-3  pool-v7t41yxh0-q56kk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
Mar 22 21:41:17.946: INFO: 
Mar 22 21:41:19.962: INFO: 3 / 3 pods in namespace 'pods-7989' are running and ready (2 seconds elapsed)
Mar 22 21:41:19.962: INFO: expected 0 pod replicas in namespace 'pods-7989', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/22/23 21:41:19.98
Mar 22 21:41:19.987: INFO: Pod quantity 3 is different from expected quantity 0
Mar 22 21:41:20.993: INFO: Pod quantity 3 is different from expected quantity 0
Mar 22 21:41:21.994: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:22.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7989" for this suite. 03/22/23 21:41:23
------------------------------
• [SLOW TEST] [5.174 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:17.833
    Mar 22 21:41:17.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename pods 03/22/23 21:41:17.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:17.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:17.857
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 03/22/23 21:41:17.865
    Mar 22 21:41:17.878: INFO: created test-pod-1
    Mar 22 21:41:17.899: INFO: created test-pod-2
    Mar 22 21:41:17.929: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/22/23 21:41:17.93
    Mar 22 21:41:17.930: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7989' to be running and ready
    Mar 22 21:41:17.945: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 22 21:41:17.945: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 22 21:41:17.945: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 22 21:41:17.945: INFO: 0 / 3 pods in namespace 'pods-7989' are running and ready (0 seconds elapsed)
    Mar 22 21:41:17.946: INFO: expected 0 pod replicas in namespace 'pods-7989', 0 are Running and Ready.
    Mar 22 21:41:17.946: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
    Mar 22 21:41:17.946: INFO: test-pod-1  pool-v7t41yxh0-q56kk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
    Mar 22 21:41:17.946: INFO: test-pod-2  pool-v7t41yxh0-q56kk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
    Mar 22 21:41:17.946: INFO: test-pod-3  pool-v7t41yxh0-q56kk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-22 21:41:17 +0000 UTC  }]
    Mar 22 21:41:17.946: INFO: 
    Mar 22 21:41:19.962: INFO: 3 / 3 pods in namespace 'pods-7989' are running and ready (2 seconds elapsed)
    Mar 22 21:41:19.962: INFO: expected 0 pod replicas in namespace 'pods-7989', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/22/23 21:41:19.98
    Mar 22 21:41:19.987: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 22 21:41:20.993: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 22 21:41:21.994: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:22.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7989" for this suite. 03/22/23 21:41:23
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:23.009
Mar 22 21:41:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:41:23.01
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:23.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:23.036
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-3eca3924-18a4-4ec0-8a91-3883e178c2fd 03/22/23 21:41:23.05
STEP: Creating the pod 03/22/23 21:41:23.058
Mar 22 21:41:23.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8" in namespace "configmap-3375" to be "running"
Mar 22 21:41:23.076: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321865ms
Mar 22 21:41:25.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01033091s
Mar 22 21:41:27.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.009902853s
Mar 22 21:41:27.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8" satisfied condition "running"
STEP: Waiting for pod with text data 03/22/23 21:41:27.082
STEP: Waiting for pod with binary data 03/22/23 21:41:27.093
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:27.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3375" for this suite. 03/22/23 21:41:27.123
------------------------------
• [4.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:23.009
    Mar 22 21:41:23.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:41:23.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:23.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:23.036
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-3eca3924-18a4-4ec0-8a91-3883e178c2fd 03/22/23 21:41:23.05
    STEP: Creating the pod 03/22/23 21:41:23.058
    Mar 22 21:41:23.072: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8" in namespace "configmap-3375" to be "running"
    Mar 22 21:41:23.076: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321865ms
    Mar 22 21:41:25.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01033091s
    Mar 22 21:41:27.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.009902853s
    Mar 22 21:41:27.082: INFO: Pod "pod-configmaps-0bb427b8-9245-4a41-8199-da5d2e73e2a8" satisfied condition "running"
    STEP: Waiting for pod with text data 03/22/23 21:41:27.082
    STEP: Waiting for pod with binary data 03/22/23 21:41:27.093
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:27.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3375" for this suite. 03/22/23 21:41:27.123
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:27.134
Mar 22 21:41:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename var-expansion 03/22/23 21:41:27.135
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:27.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:27.163
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 03/22/23 21:41:27.171
Mar 22 21:41:27.183: INFO: Waiting up to 5m0s for pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a" in namespace "var-expansion-302" to be "Succeeded or Failed"
Mar 22 21:41:27.189: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.602741ms
Mar 22 21:41:29.205: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021879218s
Mar 22 21:41:31.195: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011812775s
STEP: Saw pod success 03/22/23 21:41:31.195
Mar 22 21:41:31.196: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a" satisfied condition "Succeeded or Failed"
Mar 22 21:41:31.201: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a container dapi-container: <nil>
STEP: delete the pod 03/22/23 21:41:31.216
Mar 22 21:41:31.231: INFO: Waiting for pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a to disappear
Mar 22 21:41:31.234: INFO: Pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:31.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-302" for this suite. 03/22/23 21:41:31.239
------------------------------
• [4.113 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:27.134
    Mar 22 21:41:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename var-expansion 03/22/23 21:41:27.135
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:27.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:27.163
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 03/22/23 21:41:27.171
    Mar 22 21:41:27.183: INFO: Waiting up to 5m0s for pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a" in namespace "var-expansion-302" to be "Succeeded or Failed"
    Mar 22 21:41:27.189: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.602741ms
    Mar 22 21:41:29.205: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021879218s
    Mar 22 21:41:31.195: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011812775s
    STEP: Saw pod success 03/22/23 21:41:31.195
    Mar 22 21:41:31.196: INFO: Pod "var-expansion-043e4a11-16ce-455e-a948-ed933c58098a" satisfied condition "Succeeded or Failed"
    Mar 22 21:41:31.201: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a container dapi-container: <nil>
    STEP: delete the pod 03/22/23 21:41:31.216
    Mar 22 21:41:31.231: INFO: Waiting for pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a to disappear
    Mar 22 21:41:31.234: INFO: Pod var-expansion-043e4a11-16ce-455e-a948-ed933c58098a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:31.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-302" for this suite. 03/22/23 21:41:31.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:31.257
Mar 22 21:41:31.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename emptydir 03/22/23 21:41:31.259
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:31.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:31.291
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 03/22/23 21:41:31.308
Mar 22 21:41:31.318: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5" in namespace "emptydir-8769" to be "running"
Mar 22 21:41:31.323: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.037064ms
Mar 22 21:41:33.330: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.011360349s
Mar 22 21:41:33.330: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/22/23 21:41:33.33
Mar 22 21:41:33.330: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8769 PodName:pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 22 21:41:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
Mar 22 21:41:33.331: INFO: ExecWithOptions: Clientset creation
Mar 22 21:41:33.331: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/emptydir-8769/pods/pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 22 21:41:33.482: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 22 21:41:33.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8769" for this suite. 03/22/23 21:41:33.491
------------------------------
• [2.250 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:31.257
    Mar 22 21:41:31.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename emptydir 03/22/23 21:41:31.259
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:31.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:31.291
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 03/22/23 21:41:31.308
    Mar 22 21:41:31.318: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5" in namespace "emptydir-8769" to be "running"
    Mar 22 21:41:31.323: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.037064ms
    Mar 22 21:41:33.330: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.011360349s
    Mar 22 21:41:33.330: INFO: Pod "pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/22/23 21:41:33.33
    Mar 22 21:41:33.330: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8769 PodName:pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 22 21:41:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    Mar 22 21:41:33.331: INFO: ExecWithOptions: Clientset creation
    Mar 22 21:41:33.331: INFO: ExecWithOptions: execute(POST https://10.245.0.1:443/api/v1/namespaces/emptydir-8769/pods/pod-sharedvolume-a5eb81cc-db89-4114-a3fb-b8f991f682b5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 22 21:41:33.482: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:41:33.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8769" for this suite. 03/22/23 21:41:33.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:41:33.507
Mar 22 21:41:33.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename crd-watch 03/22/23 21:41:33.508
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:33.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:33.528
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 22 21:41:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Creating first CR  03/22/23 21:41:36.106
Mar 22 21:41:36.113: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:36Z]] name:name1 resourceVersion:54864 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/22/23 21:41:46.115
Mar 22 21:41:46.127: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:46Z]] name:name2 resourceVersion:54928 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/22/23 21:41:56.128
Mar 22 21:41:56.137: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:56Z]] name:name1 resourceVersion:54970 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/22/23 21:42:06.138
Mar 22 21:42:06.153: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:42:06Z]] name:name2 resourceVersion:55011 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/22/23 21:42:16.157
Mar 22 21:42:16.177: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:56Z]] name:name1 resourceVersion:55052 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/22/23 21:42:26.178
Mar 22 21:42:26.188: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:42:06Z]] name:name2 resourceVersion:55093 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2216" for this suite. 03/22/23 21:42:36.713
------------------------------
• [SLOW TEST] [63.214 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:41:33.507
    Mar 22 21:41:33.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename crd-watch 03/22/23 21:41:33.508
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:41:33.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:41:33.528
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 22 21:41:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Creating first CR  03/22/23 21:41:36.106
    Mar 22 21:41:36.113: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:36Z]] name:name1 resourceVersion:54864 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/22/23 21:41:46.115
    Mar 22 21:41:46.127: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:46Z]] name:name2 resourceVersion:54928 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/22/23 21:41:56.128
    Mar 22 21:41:56.137: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:56Z]] name:name1 resourceVersion:54970 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/22/23 21:42:06.138
    Mar 22 21:42:06.153: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:42:06Z]] name:name2 resourceVersion:55011 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/22/23 21:42:16.157
    Mar 22 21:42:16.177: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:41:56Z]] name:name1 resourceVersion:55052 uid:59375108-456d-43bc-b9dd-01cb91107811] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/22/23 21:42:26.178
    Mar 22 21:42:26.188: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-22T21:41:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-22T21:42:06Z]] name:name2 resourceVersion:55093 uid:242edda1-7063-4b12-9ed5-c52bff978c19] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2216" for this suite. 03/22/23 21:42:36.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:36.722
Mar 22 21:42:36.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename ingress 03/22/23 21:42:36.723
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:36.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:36.748
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/22/23 21:42:36.757
STEP: getting /apis/networking.k8s.io 03/22/23 21:42:36.764
STEP: getting /apis/networking.k8s.iov1 03/22/23 21:42:36.766
STEP: creating 03/22/23 21:42:36.773
STEP: getting 03/22/23 21:42:36.795
STEP: listing 03/22/23 21:42:36.799
STEP: watching 03/22/23 21:42:36.826
Mar 22 21:42:36.826: INFO: starting watch
STEP: cluster-wide listing 03/22/23 21:42:36.829
STEP: cluster-wide watching 03/22/23 21:42:36.834
Mar 22 21:42:36.834: INFO: starting watch
STEP: patching 03/22/23 21:42:36.838
STEP: updating 03/22/23 21:42:36.845
Mar 22 21:42:36.857: INFO: waiting for watch events with expected annotations
Mar 22 21:42:36.857: INFO: saw patched and updated annotations
STEP: patching /status 03/22/23 21:42:36.857
STEP: updating /status 03/22/23 21:42:36.868
STEP: get /status 03/22/23 21:42:36.878
STEP: deleting 03/22/23 21:42:36.882
STEP: deleting a collection 03/22/23 21:42:36.902
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:36.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8160" for this suite. 03/22/23 21:42:36.938
------------------------------
• [0.224 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:36.722
    Mar 22 21:42:36.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename ingress 03/22/23 21:42:36.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:36.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:36.748
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/22/23 21:42:36.757
    STEP: getting /apis/networking.k8s.io 03/22/23 21:42:36.764
    STEP: getting /apis/networking.k8s.iov1 03/22/23 21:42:36.766
    STEP: creating 03/22/23 21:42:36.773
    STEP: getting 03/22/23 21:42:36.795
    STEP: listing 03/22/23 21:42:36.799
    STEP: watching 03/22/23 21:42:36.826
    Mar 22 21:42:36.826: INFO: starting watch
    STEP: cluster-wide listing 03/22/23 21:42:36.829
    STEP: cluster-wide watching 03/22/23 21:42:36.834
    Mar 22 21:42:36.834: INFO: starting watch
    STEP: patching 03/22/23 21:42:36.838
    STEP: updating 03/22/23 21:42:36.845
    Mar 22 21:42:36.857: INFO: waiting for watch events with expected annotations
    Mar 22 21:42:36.857: INFO: saw patched and updated annotations
    STEP: patching /status 03/22/23 21:42:36.857
    STEP: updating /status 03/22/23 21:42:36.868
    STEP: get /status 03/22/23 21:42:36.878
    STEP: deleting 03/22/23 21:42:36.882
    STEP: deleting a collection 03/22/23 21:42:36.902
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:36.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8160" for this suite. 03/22/23 21:42:36.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:36.951
Mar 22 21:42:36.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:42:36.952
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:36.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:36.974
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:42:36.994
Mar 22 21:42:37.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-409" to be "running and ready"
Mar 22 21:42:37.010: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.777921ms
Mar 22 21:42:37.010: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:42:39.023: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018841004s
Mar 22 21:42:39.023: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 22 21:42:39.023: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 03/22/23 21:42:39.05
Mar 22 21:42:39.058: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-409" to be "running and ready"
Mar 22 21:42:39.065: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.936197ms
Mar 22 21:42:39.065: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:42:41.081: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023090642s
Mar 22 21:42:41.081: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 22 21:42:41.081: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/22/23 21:42:41.086
STEP: delete the pod with lifecycle hook 03/22/23 21:42:41.13
Mar 22 21:42:41.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 21:42:41.147: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 21:42:43.148: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 21:42:43.154: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 22 21:42:45.148: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 22 21:42:45.160: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-409" for this suite. 03/22/23 21:42:45.168
------------------------------
• [SLOW TEST] [8.229 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:36.951
    Mar 22 21:42:36.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:42:36.952
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:36.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:36.974
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:42:36.994
    Mar 22 21:42:37.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-409" to be "running and ready"
    Mar 22 21:42:37.010: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.777921ms
    Mar 22 21:42:37.010: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:42:39.023: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018841004s
    Mar 22 21:42:39.023: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 22 21:42:39.023: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 03/22/23 21:42:39.05
    Mar 22 21:42:39.058: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-409" to be "running and ready"
    Mar 22 21:42:39.065: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.936197ms
    Mar 22 21:42:39.065: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:42:41.081: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023090642s
    Mar 22 21:42:41.081: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 22 21:42:41.081: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/22/23 21:42:41.086
    STEP: delete the pod with lifecycle hook 03/22/23 21:42:41.13
    Mar 22 21:42:41.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 22 21:42:41.147: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 22 21:42:43.148: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 22 21:42:43.154: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 22 21:42:45.148: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 22 21:42:45.160: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-409" for this suite. 03/22/23 21:42:45.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:45.18
Mar 22 21:42:45.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename replication-controller 03/22/23 21:42:45.183
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:45.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:45.21
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 03/22/23 21:42:45.223
STEP: When the matched label of one of its pods change 03/22/23 21:42:45.238
Mar 22 21:42:45.250: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/22/23 21:42:46.285
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:46.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6909" for this suite. 03/22/23 21:42:46.303
------------------------------
• [1.131 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:45.18
    Mar 22 21:42:45.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename replication-controller 03/22/23 21:42:45.183
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:45.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:45.21
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 03/22/23 21:42:45.223
    STEP: When the matched label of one of its pods change 03/22/23 21:42:45.238
    Mar 22 21:42:45.250: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/22/23 21:42:46.285
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:46.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6909" for this suite. 03/22/23 21:42:46.303
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:46.312
Mar 22 21:42:46.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename csistoragecapacity 03/22/23 21:42:46.317
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:46.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:46.346
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/22/23 21:42:46.354
STEP: getting /apis/storage.k8s.io 03/22/23 21:42:46.36
STEP: getting /apis/storage.k8s.io/v1 03/22/23 21:42:46.363
STEP: creating 03/22/23 21:42:46.366
STEP: watching 03/22/23 21:42:46.386
Mar 22 21:42:46.386: INFO: starting watch
STEP: getting 03/22/23 21:42:46.42
STEP: listing in namespace 03/22/23 21:42:46.434
STEP: listing across namespaces 03/22/23 21:42:46.439
STEP: patching 03/22/23 21:42:46.443
STEP: updating 03/22/23 21:42:46.45
Mar 22 21:42:46.457: INFO: waiting for watch events with expected annotations in namespace
Mar 22 21:42:46.457: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/22/23 21:42:46.457
STEP: deleting a collection 03/22/23 21:42:46.47
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:46.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-9203" for this suite. 03/22/23 21:42:46.496
------------------------------
• [0.197 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:46.312
    Mar 22 21:42:46.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename csistoragecapacity 03/22/23 21:42:46.317
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:46.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:46.346
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/22/23 21:42:46.354
    STEP: getting /apis/storage.k8s.io 03/22/23 21:42:46.36
    STEP: getting /apis/storage.k8s.io/v1 03/22/23 21:42:46.363
    STEP: creating 03/22/23 21:42:46.366
    STEP: watching 03/22/23 21:42:46.386
    Mar 22 21:42:46.386: INFO: starting watch
    STEP: getting 03/22/23 21:42:46.42
    STEP: listing in namespace 03/22/23 21:42:46.434
    STEP: listing across namespaces 03/22/23 21:42:46.439
    STEP: patching 03/22/23 21:42:46.443
    STEP: updating 03/22/23 21:42:46.45
    Mar 22 21:42:46.457: INFO: waiting for watch events with expected annotations in namespace
    Mar 22 21:42:46.457: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/22/23 21:42:46.457
    STEP: deleting a collection 03/22/23 21:42:46.47
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:46.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-9203" for this suite. 03/22/23 21:42:46.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:46.511
Mar 22 21:42:46.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename watch 03/22/23 21:42:46.513
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:46.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:46.544
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/22/23 21:42:46.55
STEP: starting a background goroutine to produce watch events 03/22/23 21:42:46.554
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/22/23 21:42:46.559
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:49.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5331" for this suite. 03/22/23 21:42:49.37
------------------------------
• [2.918 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:46.511
    Mar 22 21:42:46.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename watch 03/22/23 21:42:46.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:46.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:46.544
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/22/23 21:42:46.55
    STEP: starting a background goroutine to produce watch events 03/22/23 21:42:46.554
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/22/23 21:42:46.559
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:49.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5331" for this suite. 03/22/23 21:42:49.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:49.429
Mar 22 21:42:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:42:49.436
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:49.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:49.461
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-e83a43ef-9d05-423a-b3f3-c6f15cf59241 03/22/23 21:42:49.478
STEP: Creating a pod to test consume secrets 03/22/23 21:42:49.485
Mar 22 21:42:49.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837" in namespace "projected-7507" to be "Succeeded or Failed"
Mar 22 21:42:49.499: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886017ms
Mar 22 21:42:51.519: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023943505s
Mar 22 21:42:53.506: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010360165s
STEP: Saw pod success 03/22/23 21:42:53.506
Mar 22 21:42:53.506: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837" satisfied condition "Succeeded or Failed"
Mar 22 21:42:53.510: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:42:53.523
Mar 22 21:42:53.538: INFO: Waiting for pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 to disappear
Mar 22 21:42:53.542: INFO: Pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:53.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7507" for this suite. 03/22/23 21:42:53.548
------------------------------
• [4.128 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:49.429
    Mar 22 21:42:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:42:49.436
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:49.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:49.461
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-e83a43ef-9d05-423a-b3f3-c6f15cf59241 03/22/23 21:42:49.478
    STEP: Creating a pod to test consume secrets 03/22/23 21:42:49.485
    Mar 22 21:42:49.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837" in namespace "projected-7507" to be "Succeeded or Failed"
    Mar 22 21:42:49.499: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886017ms
    Mar 22 21:42:51.519: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023943505s
    Mar 22 21:42:53.506: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010360165s
    STEP: Saw pod success 03/22/23 21:42:53.506
    Mar 22 21:42:53.506: INFO: Pod "pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837" satisfied condition "Succeeded or Failed"
    Mar 22 21:42:53.510: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:42:53.523
    Mar 22 21:42:53.538: INFO: Waiting for pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 to disappear
    Mar 22 21:42:53.542: INFO: Pod pod-projected-secrets-058c717c-2b04-4a8a-a3e1-6d434ee5e837 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:53.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7507" for this suite. 03/22/23 21:42:53.548
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:53.557
Mar 22 21:42:53.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename projected 03/22/23 21:42:53.559
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:53.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:53.597
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-6547b33b-cb18-42b9-b5ce-363f695946ff 03/22/23 21:42:53.603
STEP: Creating a pod to test consume secrets 03/22/23 21:42:53.61
Mar 22 21:42:53.619: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e" in namespace "projected-8853" to be "Succeeded or Failed"
Mar 22 21:42:53.627: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083351ms
Mar 22 21:42:55.634: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014680531s
Mar 22 21:42:57.641: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02158682s
STEP: Saw pod success 03/22/23 21:42:57.641
Mar 22 21:42:57.641: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e" satisfied condition "Succeeded or Failed"
Mar 22 21:42:57.646: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:42:57.659
Mar 22 21:42:57.685: INFO: Waiting for pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e to disappear
Mar 22 21:42:57.688: INFO: Pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 22 21:42:57.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8853" for this suite. 03/22/23 21:42:57.695
------------------------------
• [4.148 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:53.557
    Mar 22 21:42:53.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename projected 03/22/23 21:42:53.559
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:53.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:53.597
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-6547b33b-cb18-42b9-b5ce-363f695946ff 03/22/23 21:42:53.603
    STEP: Creating a pod to test consume secrets 03/22/23 21:42:53.61
    Mar 22 21:42:53.619: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e" in namespace "projected-8853" to be "Succeeded or Failed"
    Mar 22 21:42:53.627: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083351ms
    Mar 22 21:42:55.634: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014680531s
    Mar 22 21:42:57.641: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02158682s
    STEP: Saw pod success 03/22/23 21:42:57.641
    Mar 22 21:42:57.641: INFO: Pod "pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e" satisfied condition "Succeeded or Failed"
    Mar 22 21:42:57.646: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:42:57.659
    Mar 22 21:42:57.685: INFO: Waiting for pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e to disappear
    Mar 22 21:42:57.688: INFO: Pod pod-projected-secrets-fcee49f5-7384-49bb-b41d-31424857374e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:42:57.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8853" for this suite. 03/22/23 21:42:57.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:42:57.706
Mar 22 21:42:57.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:42:57.707
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:57.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:57.741
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:42:57.756
Mar 22 21:42:57.770: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-273" to be "running and ready"
Mar 22 21:42:57.777: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456095ms
Mar 22 21:42:57.777: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:42:59.796: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026299052s
Mar 22 21:42:59.796: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:43:01.798: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.02761513s
Mar 22 21:43:01.798: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 22 21:43:01.798: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 03/22/23 21:43:01.802
Mar 22 21:43:01.819: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-273" to be "running and ready"
Mar 22 21:43:01.832: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.885787ms
Mar 22 21:43:01.832: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 22 21:43:03.841: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022281299s
Mar 22 21:43:03.841: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 22 21:43:03.841: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/22/23 21:43:03.846
Mar 22 21:43:03.861: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 21:43:03.866: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 21:43:05.866: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 21:43:05.872: INFO: Pod pod-with-prestop-http-hook still exists
Mar 22 21:43:07.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 22 21:43:07.873: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/22/23 21:43:07.873
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:07.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-273" for this suite. 03/22/23 21:43:07.896
------------------------------
• [SLOW TEST] [10.199 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:42:57.706
    Mar 22 21:42:57.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/22/23 21:42:57.707
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:42:57.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:42:57.741
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/22/23 21:42:57.756
    Mar 22 21:42:57.770: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-273" to be "running and ready"
    Mar 22 21:42:57.777: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456095ms
    Mar 22 21:42:57.777: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:42:59.796: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026299052s
    Mar 22 21:42:59.796: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:43:01.798: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.02761513s
    Mar 22 21:43:01.798: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 22 21:43:01.798: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 03/22/23 21:43:01.802
    Mar 22 21:43:01.819: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-273" to be "running and ready"
    Mar 22 21:43:01.832: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.885787ms
    Mar 22 21:43:01.832: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 22 21:43:03.841: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022281299s
    Mar 22 21:43:03.841: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 22 21:43:03.841: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/22/23 21:43:03.846
    Mar 22 21:43:03.861: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 22 21:43:03.866: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 22 21:43:05.866: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 22 21:43:05.872: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 22 21:43:07.867: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 22 21:43:07.873: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/22/23 21:43:07.873
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:07.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-273" for this suite. 03/22/23 21:43:07.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:07.907
Mar 22 21:43:07.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 21:43:07.908
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:07.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:07.927
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3425 03/22/23 21:43:07.935
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/22/23 21:43:07.953
STEP: creating service externalsvc in namespace services-3425 03/22/23 21:43:07.953
STEP: creating replication controller externalsvc in namespace services-3425 03/22/23 21:43:07.97
I0322 21:43:07.976916      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3425, replica count: 2
I0322 21:43:11.028250      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/22/23 21:43:11.033
Mar 22 21:43:11.053: INFO: Creating new exec pod
Mar 22 21:43:11.061: INFO: Waiting up to 5m0s for pod "execpodx7mrw" in namespace "services-3425" to be "running"
Mar 22 21:43:11.066: INFO: Pod "execpodx7mrw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982868ms
Mar 22 21:43:13.070: INFO: Pod "execpodx7mrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008874862s
Mar 22 21:43:13.071: INFO: Pod "execpodx7mrw" satisfied condition "running"
Mar 22 21:43:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-3425 exec execpodx7mrw -- /bin/sh -x -c nslookup nodeport-service.services-3425.svc.cluster.local'
Mar 22 21:43:13.386: INFO: stderr: "+ nslookup nodeport-service.services-3425.svc.cluster.local\n"
Mar 22 21:43:13.386: INFO: stdout: "Server:\t\t10.245.0.10\nAddress:\t10.245.0.10#53\n\nnodeport-service.services-3425.svc.cluster.local\tcanonical name = externalsvc.services-3425.svc.cluster.local.\nName:\texternalsvc.services-3425.svc.cluster.local\nAddress: 10.245.18.242\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3425, will wait for the garbage collector to delete the pods 03/22/23 21:43:13.386
Mar 22 21:43:13.463: INFO: Deleting ReplicationController externalsvc took: 7.637169ms
Mar 22 21:43:13.563: INFO: Terminating ReplicationController externalsvc pods took: 100.645718ms
Mar 22 21:43:15.985: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:15.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3425" for this suite. 03/22/23 21:43:16.005
------------------------------
• [SLOW TEST] [8.107 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:07.907
    Mar 22 21:43:07.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 21:43:07.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:07.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:07.927
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3425 03/22/23 21:43:07.935
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/22/23 21:43:07.953
    STEP: creating service externalsvc in namespace services-3425 03/22/23 21:43:07.953
    STEP: creating replication controller externalsvc in namespace services-3425 03/22/23 21:43:07.97
    I0322 21:43:07.976916      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3425, replica count: 2
    I0322 21:43:11.028250      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/22/23 21:43:11.033
    Mar 22 21:43:11.053: INFO: Creating new exec pod
    Mar 22 21:43:11.061: INFO: Waiting up to 5m0s for pod "execpodx7mrw" in namespace "services-3425" to be "running"
    Mar 22 21:43:11.066: INFO: Pod "execpodx7mrw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982868ms
    Mar 22 21:43:13.070: INFO: Pod "execpodx7mrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008874862s
    Mar 22 21:43:13.071: INFO: Pod "execpodx7mrw" satisfied condition "running"
    Mar 22 21:43:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-3425 exec execpodx7mrw -- /bin/sh -x -c nslookup nodeport-service.services-3425.svc.cluster.local'
    Mar 22 21:43:13.386: INFO: stderr: "+ nslookup nodeport-service.services-3425.svc.cluster.local\n"
    Mar 22 21:43:13.386: INFO: stdout: "Server:\t\t10.245.0.10\nAddress:\t10.245.0.10#53\n\nnodeport-service.services-3425.svc.cluster.local\tcanonical name = externalsvc.services-3425.svc.cluster.local.\nName:\texternalsvc.services-3425.svc.cluster.local\nAddress: 10.245.18.242\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3425, will wait for the garbage collector to delete the pods 03/22/23 21:43:13.386
    Mar 22 21:43:13.463: INFO: Deleting ReplicationController externalsvc took: 7.637169ms
    Mar 22 21:43:13.563: INFO: Terminating ReplicationController externalsvc pods took: 100.645718ms
    Mar 22 21:43:15.985: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:15.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3425" for this suite. 03/22/23 21:43:16.005
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:16.016
Mar 22 21:43:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename kubectl 03/22/23 21:43:16.019
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:16.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:16.044
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 03/22/23 21:43:16.068
Mar 22 21:43:16.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 create -f -'
Mar 22 21:43:16.995: INFO: stderr: ""
Mar 22 21:43:16.995: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:16.995
Mar 22 21:43:16.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:43:17.165: INFO: stderr: ""
Mar 22 21:43:17.165: INFO: stdout: "update-demo-nautilus-98hzn update-demo-nautilus-c2ljz "
Mar 22 21:43:17.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:17.299: INFO: stderr: ""
Mar 22 21:43:17.300: INFO: stdout: ""
Mar 22 21:43:17.300: INFO: update-demo-nautilus-98hzn is created but not running
Mar 22 21:43:22.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:43:22.428: INFO: stderr: ""
Mar 22 21:43:22.428: INFO: stdout: "update-demo-nautilus-98hzn update-demo-nautilus-c2ljz "
Mar 22 21:43:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:22.585: INFO: stderr: ""
Mar 22 21:43:22.585: INFO: stdout: "true"
Mar 22 21:43:22.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:43:22.749: INFO: stderr: ""
Mar 22 21:43:22.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:43:22.749: INFO: validating pod update-demo-nautilus-98hzn
Mar 22 21:43:22.784: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:43:22.784: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:43:22.784: INFO: update-demo-nautilus-98hzn is verified up and running
Mar 22 21:43:22.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-c2ljz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:22.965: INFO: stderr: ""
Mar 22 21:43:22.965: INFO: stdout: "true"
Mar 22 21:43:22.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-c2ljz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:43:23.109: INFO: stderr: ""
Mar 22 21:43:23.109: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:43:23.109: INFO: validating pod update-demo-nautilus-c2ljz
Mar 22 21:43:23.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:43:23.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:43:23.135: INFO: update-demo-nautilus-c2ljz is verified up and running
STEP: scaling down the replication controller 03/22/23 21:43:23.135
Mar 22 21:43:23.138: INFO: scanned /root for discovery docs: <nil>
Mar 22 21:43:23.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 22 21:43:24.338: INFO: stderr: ""
Mar 22 21:43:24.338: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:24.338
Mar 22 21:43:24.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:43:24.464: INFO: stderr: ""
Mar 22 21:43:24.464: INFO: stdout: "update-demo-nautilus-98hzn "
Mar 22 21:43:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:24.581: INFO: stderr: ""
Mar 22 21:43:24.581: INFO: stdout: "true"
Mar 22 21:43:24.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:43:24.707: INFO: stderr: ""
Mar 22 21:43:24.707: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:43:24.707: INFO: validating pod update-demo-nautilus-98hzn
Mar 22 21:43:24.721: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:43:24.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:43:24.721: INFO: update-demo-nautilus-98hzn is verified up and running
STEP: scaling up the replication controller 03/22/23 21:43:24.721
Mar 22 21:43:24.723: INFO: scanned /root for discovery docs: <nil>
Mar 22 21:43:24.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 22 21:43:25.873: INFO: stderr: ""
Mar 22 21:43:25.873: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:25.873
Mar 22 21:43:25.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:43:26.005: INFO: stderr: ""
Mar 22 21:43:26.005: INFO: stdout: "update-demo-nautilus-44pjz update-demo-nautilus-98hzn "
Mar 22 21:43:26.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:26.179: INFO: stderr: ""
Mar 22 21:43:26.179: INFO: stdout: ""
Mar 22 21:43:26.179: INFO: update-demo-nautilus-44pjz is created but not running
Mar 22 21:43:31.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 22 21:43:31.356: INFO: stderr: ""
Mar 22 21:43:31.356: INFO: stdout: "update-demo-nautilus-44pjz update-demo-nautilus-98hzn "
Mar 22 21:43:31.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:31.494: INFO: stderr: ""
Mar 22 21:43:31.494: INFO: stdout: "true"
Mar 22 21:43:31.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:43:31.611: INFO: stderr: ""
Mar 22 21:43:31.611: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:43:31.611: INFO: validating pod update-demo-nautilus-44pjz
Mar 22 21:43:31.648: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:43:31.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:43:31.648: INFO: update-demo-nautilus-44pjz is verified up and running
Mar 22 21:43:31.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 22 21:43:31.754: INFO: stderr: ""
Mar 22 21:43:31.754: INFO: stdout: "true"
Mar 22 21:43:31.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 22 21:43:31.873: INFO: stderr: ""
Mar 22 21:43:31.873: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 22 21:43:31.873: INFO: validating pod update-demo-nautilus-98hzn
Mar 22 21:43:31.884: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 22 21:43:31.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 22 21:43:31.884: INFO: update-demo-nautilus-98hzn is verified up and running
STEP: using delete to clean up resources 03/22/23 21:43:31.885
Mar 22 21:43:31.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 delete --grace-period=0 --force -f -'
Mar 22 21:43:32.003: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 22 21:43:32.003: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 22 21:43:32.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get rc,svc -l name=update-demo --no-headers'
Mar 22 21:43:32.233: INFO: stderr: "No resources found in kubectl-7696 namespace.\n"
Mar 22 21:43:32.233: INFO: stdout: ""
Mar 22 21:43:32.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 22 21:43:32.380: INFO: stderr: ""
Mar 22 21:43:32.380: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:32.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7696" for this suite. 03/22/23 21:43:32.389
------------------------------
• [SLOW TEST] [16.381 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:16.016
    Mar 22 21:43:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename kubectl 03/22/23 21:43:16.019
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:16.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:16.044
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 03/22/23 21:43:16.068
    Mar 22 21:43:16.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 create -f -'
    Mar 22 21:43:16.995: INFO: stderr: ""
    Mar 22 21:43:16.995: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:16.995
    Mar 22 21:43:16.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:43:17.165: INFO: stderr: ""
    Mar 22 21:43:17.165: INFO: stdout: "update-demo-nautilus-98hzn update-demo-nautilus-c2ljz "
    Mar 22 21:43:17.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:17.299: INFO: stderr: ""
    Mar 22 21:43:17.300: INFO: stdout: ""
    Mar 22 21:43:17.300: INFO: update-demo-nautilus-98hzn is created but not running
    Mar 22 21:43:22.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:43:22.428: INFO: stderr: ""
    Mar 22 21:43:22.428: INFO: stdout: "update-demo-nautilus-98hzn update-demo-nautilus-c2ljz "
    Mar 22 21:43:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:22.585: INFO: stderr: ""
    Mar 22 21:43:22.585: INFO: stdout: "true"
    Mar 22 21:43:22.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:43:22.749: INFO: stderr: ""
    Mar 22 21:43:22.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:43:22.749: INFO: validating pod update-demo-nautilus-98hzn
    Mar 22 21:43:22.784: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:43:22.784: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:43:22.784: INFO: update-demo-nautilus-98hzn is verified up and running
    Mar 22 21:43:22.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-c2ljz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:22.965: INFO: stderr: ""
    Mar 22 21:43:22.965: INFO: stdout: "true"
    Mar 22 21:43:22.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-c2ljz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:43:23.109: INFO: stderr: ""
    Mar 22 21:43:23.109: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:43:23.109: INFO: validating pod update-demo-nautilus-c2ljz
    Mar 22 21:43:23.135: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:43:23.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:43:23.135: INFO: update-demo-nautilus-c2ljz is verified up and running
    STEP: scaling down the replication controller 03/22/23 21:43:23.135
    Mar 22 21:43:23.138: INFO: scanned /root for discovery docs: <nil>
    Mar 22 21:43:23.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 22 21:43:24.338: INFO: stderr: ""
    Mar 22 21:43:24.338: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:24.338
    Mar 22 21:43:24.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:43:24.464: INFO: stderr: ""
    Mar 22 21:43:24.464: INFO: stdout: "update-demo-nautilus-98hzn "
    Mar 22 21:43:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:24.581: INFO: stderr: ""
    Mar 22 21:43:24.581: INFO: stdout: "true"
    Mar 22 21:43:24.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:43:24.707: INFO: stderr: ""
    Mar 22 21:43:24.707: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:43:24.707: INFO: validating pod update-demo-nautilus-98hzn
    Mar 22 21:43:24.721: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:43:24.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:43:24.721: INFO: update-demo-nautilus-98hzn is verified up and running
    STEP: scaling up the replication controller 03/22/23 21:43:24.721
    Mar 22 21:43:24.723: INFO: scanned /root for discovery docs: <nil>
    Mar 22 21:43:24.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 22 21:43:25.873: INFO: stderr: ""
    Mar 22 21:43:25.873: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/22/23 21:43:25.873
    Mar 22 21:43:25.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:43:26.005: INFO: stderr: ""
    Mar 22 21:43:26.005: INFO: stdout: "update-demo-nautilus-44pjz update-demo-nautilus-98hzn "
    Mar 22 21:43:26.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:26.179: INFO: stderr: ""
    Mar 22 21:43:26.179: INFO: stdout: ""
    Mar 22 21:43:26.179: INFO: update-demo-nautilus-44pjz is created but not running
    Mar 22 21:43:31.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 22 21:43:31.356: INFO: stderr: ""
    Mar 22 21:43:31.356: INFO: stdout: "update-demo-nautilus-44pjz update-demo-nautilus-98hzn "
    Mar 22 21:43:31.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:31.494: INFO: stderr: ""
    Mar 22 21:43:31.494: INFO: stdout: "true"
    Mar 22 21:43:31.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-44pjz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:43:31.611: INFO: stderr: ""
    Mar 22 21:43:31.611: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:43:31.611: INFO: validating pod update-demo-nautilus-44pjz
    Mar 22 21:43:31.648: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:43:31.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:43:31.648: INFO: update-demo-nautilus-44pjz is verified up and running
    Mar 22 21:43:31.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 22 21:43:31.754: INFO: stderr: ""
    Mar 22 21:43:31.754: INFO: stdout: "true"
    Mar 22 21:43:31.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods update-demo-nautilus-98hzn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 22 21:43:31.873: INFO: stderr: ""
    Mar 22 21:43:31.873: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 22 21:43:31.873: INFO: validating pod update-demo-nautilus-98hzn
    Mar 22 21:43:31.884: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 22 21:43:31.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 22 21:43:31.884: INFO: update-demo-nautilus-98hzn is verified up and running
    STEP: using delete to clean up resources 03/22/23 21:43:31.885
    Mar 22 21:43:31.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 delete --grace-period=0 --force -f -'
    Mar 22 21:43:32.003: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 22 21:43:32.003: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 22 21:43:32.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get rc,svc -l name=update-demo --no-headers'
    Mar 22 21:43:32.233: INFO: stderr: "No resources found in kubectl-7696 namespace.\n"
    Mar 22 21:43:32.233: INFO: stdout: ""
    Mar 22 21:43:32.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=kubectl-7696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 22 21:43:32.380: INFO: stderr: ""
    Mar 22 21:43:32.380: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:32.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7696" for this suite. 03/22/23 21:43:32.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:32.398
Mar 22 21:43:32.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context-test 03/22/23 21:43:32.399
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:32.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:32.433
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Mar 22 21:43:32.449: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1" in namespace "security-context-test-4513" to be "Succeeded or Failed"
Mar 22 21:43:32.454: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.674367ms
Mar 22 21:43:34.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009741081s
Mar 22 21:43:36.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009389846s
Mar 22 21:43:36.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1" satisfied condition "Succeeded or Failed"
Mar 22 21:43:36.469: INFO: Got logs for pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:36.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4513" for this suite. 03/22/23 21:43:36.475
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:32.398
    Mar 22 21:43:32.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context-test 03/22/23 21:43:32.399
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:32.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:32.433
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Mar 22 21:43:32.449: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1" in namespace "security-context-test-4513" to be "Succeeded or Failed"
    Mar 22 21:43:32.454: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.674367ms
    Mar 22 21:43:34.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009741081s
    Mar 22 21:43:36.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009389846s
    Mar 22 21:43:36.459: INFO: Pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1" satisfied condition "Succeeded or Failed"
    Mar 22 21:43:36.469: INFO: Got logs for pod "busybox-privileged-false-1ea4fe75-bba3-40e2-94df-a012ae336cc1": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:36.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4513" for this suite. 03/22/23 21:43:36.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:36.487
Mar 22 21:43:36.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename lease-test 03/22/23 21:43:36.488
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:36.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:36.519
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:36.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7863" for this suite. 03/22/23 21:43:36.614
------------------------------
• [0.135 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:36.487
    Mar 22 21:43:36.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename lease-test 03/22/23 21:43:36.488
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:36.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:36.519
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:36.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7863" for this suite. 03/22/23 21:43:36.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:36.624
Mar 22 21:43:36.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename container-runtime 03/22/23 21:43:36.626
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:36.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:36.651
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 03/22/23 21:43:36.661
STEP: wait for the container to reach Succeeded 03/22/23 21:43:36.669
STEP: get the container status 03/22/23 21:43:40.713
STEP: the container should be terminated 03/22/23 21:43:40.721
STEP: the termination message should be set 03/22/23 21:43:40.722
Mar 22 21:43:40.722: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/22/23 21:43:40.722
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:40.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6343" for this suite. 03/22/23 21:43:40.762
------------------------------
• [4.147 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:36.624
    Mar 22 21:43:36.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename container-runtime 03/22/23 21:43:36.626
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:36.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:36.651
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 03/22/23 21:43:36.661
    STEP: wait for the container to reach Succeeded 03/22/23 21:43:36.669
    STEP: get the container status 03/22/23 21:43:40.713
    STEP: the container should be terminated 03/22/23 21:43:40.721
    STEP: the termination message should be set 03/22/23 21:43:40.722
    Mar 22 21:43:40.722: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/22/23 21:43:40.722
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:40.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6343" for this suite. 03/22/23 21:43:40.762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:40.779
Mar 22 21:43:40.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename configmap 03/22/23 21:43:40.781
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:40.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:40.801
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-ea2bb8b9-211a-47f1-ac0c-a232f733b4e1 03/22/23 21:43:40.808
STEP: Creating a pod to test consume configMaps 03/22/23 21:43:40.817
Mar 22 21:43:40.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382" in namespace "configmap-1732" to be "Succeeded or Failed"
Mar 22 21:43:40.833: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Pending", Reason="", readiness=false. Elapsed: 7.039803ms
Mar 22 21:43:42.840: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01323522s
Mar 22 21:43:44.841: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015100073s
STEP: Saw pod success 03/22/23 21:43:44.842
Mar 22 21:43:44.842: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382" satisfied condition "Succeeded or Failed"
Mar 22 21:43:44.846: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 container agnhost-container: <nil>
STEP: delete the pod 03/22/23 21:43:44.856
Mar 22 21:43:44.869: INFO: Waiting for pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 to disappear
Mar 22 21:43:44.872: INFO: Pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1732" for this suite. 03/22/23 21:43:44.879
------------------------------
• [4.114 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:40.779
    Mar 22 21:43:40.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename configmap 03/22/23 21:43:40.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:40.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:40.801
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-ea2bb8b9-211a-47f1-ac0c-a232f733b4e1 03/22/23 21:43:40.808
    STEP: Creating a pod to test consume configMaps 03/22/23 21:43:40.817
    Mar 22 21:43:40.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382" in namespace "configmap-1732" to be "Succeeded or Failed"
    Mar 22 21:43:40.833: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Pending", Reason="", readiness=false. Elapsed: 7.039803ms
    Mar 22 21:43:42.840: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01323522s
    Mar 22 21:43:44.841: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015100073s
    STEP: Saw pod success 03/22/23 21:43:44.842
    Mar 22 21:43:44.842: INFO: Pod "pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382" satisfied condition "Succeeded or Failed"
    Mar 22 21:43:44.846: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 container agnhost-container: <nil>
    STEP: delete the pod 03/22/23 21:43:44.856
    Mar 22 21:43:44.869: INFO: Waiting for pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 to disappear
    Mar 22 21:43:44.872: INFO: Pod pod-configmaps-c0713755-651f-4945-ba0b-cb2cade51382 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1732" for this suite. 03/22/23 21:43:44.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:44.893
Mar 22 21:43:44.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename services 03/22/23 21:43:44.894
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:44.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:44.917
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1521 03/22/23 21:43:44.927
STEP: creating service affinity-clusterip in namespace services-1521 03/22/23 21:43:44.928
STEP: creating replication controller affinity-clusterip in namespace services-1521 03/22/23 21:43:44.94
I0322 21:43:44.952491      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1521, replica count: 3
I0322 21:43:48.003464      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 22 21:43:48.015: INFO: Creating new exec pod
Mar 22 21:43:48.025: INFO: Waiting up to 5m0s for pod "execpod-affinityfngh2" in namespace "services-1521" to be "running"
Mar 22 21:43:48.029: INFO: Pod "execpod-affinityfngh2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902502ms
Mar 22 21:43:50.033: INFO: Pod "execpod-affinityfngh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00796104s
Mar 22 21:43:50.033: INFO: Pod "execpod-affinityfngh2" satisfied condition "running"
Mar 22 21:43:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Mar 22 21:43:51.412: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 22 21:43:51.412: INFO: stdout: ""
Mar 22 21:43:51.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c nc -v -z -w 2 10.245.57.135 80'
Mar 22 21:43:51.763: INFO: stderr: "+ nc -v -z -w 2 10.245.57.135 80\nConnection to 10.245.57.135 80 port [tcp/http] succeeded!\n"
Mar 22 21:43:51.763: INFO: stdout: ""
Mar 22 21:43:51.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.57.135:80/ ; done'
Mar 22 21:43:52.226: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n"
Mar 22 21:43:52.226: INFO: stdout: "\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh"
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
Mar 22 21:43:52.226: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1521, will wait for the garbage collector to delete the pods 03/22/23 21:43:52.242
Mar 22 21:43:52.306: INFO: Deleting ReplicationController affinity-clusterip took: 8.668046ms
Mar 22 21:43:52.407: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.154088ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:54.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1521" for this suite. 03/22/23 21:43:54.841
------------------------------
• [SLOW TEST] [9.955 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:44.893
    Mar 22 21:43:44.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename services 03/22/23 21:43:44.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:44.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:44.917
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1521 03/22/23 21:43:44.927
    STEP: creating service affinity-clusterip in namespace services-1521 03/22/23 21:43:44.928
    STEP: creating replication controller affinity-clusterip in namespace services-1521 03/22/23 21:43:44.94
    I0322 21:43:44.952491      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1521, replica count: 3
    I0322 21:43:48.003464      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 22 21:43:48.015: INFO: Creating new exec pod
    Mar 22 21:43:48.025: INFO: Waiting up to 5m0s for pod "execpod-affinityfngh2" in namespace "services-1521" to be "running"
    Mar 22 21:43:48.029: INFO: Pod "execpod-affinityfngh2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902502ms
    Mar 22 21:43:50.033: INFO: Pod "execpod-affinityfngh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00796104s
    Mar 22 21:43:50.033: INFO: Pod "execpod-affinityfngh2" satisfied condition "running"
    Mar 22 21:43:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Mar 22 21:43:51.412: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 22 21:43:51.412: INFO: stdout: ""
    Mar 22 21:43:51.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c nc -v -z -w 2 10.245.57.135 80'
    Mar 22 21:43:51.763: INFO: stderr: "+ nc -v -z -w 2 10.245.57.135 80\nConnection to 10.245.57.135 80 port [tcp/http] succeeded!\n"
    Mar 22 21:43:51.763: INFO: stdout: ""
    Mar 22 21:43:51.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3196172556 --namespace=services-1521 exec execpod-affinityfngh2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.57.135:80/ ; done'
    Mar 22 21:43:52.226: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.57.135:80/\n"
    Mar 22 21:43:52.226: INFO: stdout: "\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh\naffinity-clusterip-nrdkh"
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Received response from host: affinity-clusterip-nrdkh
    Mar 22 21:43:52.226: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1521, will wait for the garbage collector to delete the pods 03/22/23 21:43:52.242
    Mar 22 21:43:52.306: INFO: Deleting ReplicationController affinity-clusterip took: 8.668046ms
    Mar 22 21:43:52.407: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.154088ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:54.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1521" for this suite. 03/22/23 21:43:54.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:54.849
Mar 22 21:43:54.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 21:43:54.85
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:54.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:54.877
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-1da7247a-7088-49e9-b082-a174892299a9 03/22/23 21:43:54.884
STEP: Creating a pod to test consume secrets 03/22/23 21:43:54.891
Mar 22 21:43:54.901: INFO: Waiting up to 5m0s for pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38" in namespace "secrets-249" to be "Succeeded or Failed"
Mar 22 21:43:54.906: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131576ms
Mar 22 21:43:56.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010077908s
Mar 22 21:43:58.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010062257s
STEP: Saw pod success 03/22/23 21:43:58.912
Mar 22 21:43:58.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38" satisfied condition "Succeeded or Failed"
Mar 22 21:43:58.917: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:43:58.938
Mar 22 21:43:58.951: INFO: Waiting for pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 to disappear
Mar 22 21:43:58.955: INFO: Pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 21:43:58.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-249" for this suite. 03/22/23 21:43:58.966
------------------------------
• [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:54.849
    Mar 22 21:43:54.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 21:43:54.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:54.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:54.877
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-1da7247a-7088-49e9-b082-a174892299a9 03/22/23 21:43:54.884
    STEP: Creating a pod to test consume secrets 03/22/23 21:43:54.891
    Mar 22 21:43:54.901: INFO: Waiting up to 5m0s for pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38" in namespace "secrets-249" to be "Succeeded or Failed"
    Mar 22 21:43:54.906: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131576ms
    Mar 22 21:43:56.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010077908s
    Mar 22 21:43:58.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010062257s
    STEP: Saw pod success 03/22/23 21:43:58.912
    Mar 22 21:43:58.912: INFO: Pod "pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38" satisfied condition "Succeeded or Failed"
    Mar 22 21:43:58.917: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:43:58.938
    Mar 22 21:43:58.951: INFO: Waiting for pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 to disappear
    Mar 22 21:43:58.955: INFO: Pod pod-secrets-be6f7015-b336-41fb-8aa4-7a7a1bea4d38 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:43:58.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-249" for this suite. 03/22/23 21:43:58.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:43:58.975
Mar 22 21:43:58.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename downward-api 03/22/23 21:43:58.977
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:58.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:59.007
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 03/22/23 21:43:59.019
Mar 22 21:43:59.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d" in namespace "downward-api-6340" to be "Succeeded or Failed"
Mar 22 21:43:59.035: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.317121ms
Mar 22 21:44:01.045: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014351069s
Mar 22 21:44:03.041: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010294214s
STEP: Saw pod success 03/22/23 21:44:03.041
Mar 22 21:44:03.042: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d" satisfied condition "Succeeded or Failed"
Mar 22 21:44:03.048: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d container client-container: <nil>
STEP: delete the pod 03/22/23 21:44:03.067
Mar 22 21:44:03.082: INFO: Waiting for pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d to disappear
Mar 22 21:44:03.086: INFO: Pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 22 21:44:03.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6340" for this suite. 03/22/23 21:44:03.104
------------------------------
• [4.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:43:58.975
    Mar 22 21:43:58.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename downward-api 03/22/23 21:43:58.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:43:58.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:43:59.007
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 03/22/23 21:43:59.019
    Mar 22 21:43:59.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d" in namespace "downward-api-6340" to be "Succeeded or Failed"
    Mar 22 21:43:59.035: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.317121ms
    Mar 22 21:44:01.045: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014351069s
    Mar 22 21:44:03.041: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010294214s
    STEP: Saw pod success 03/22/23 21:44:03.041
    Mar 22 21:44:03.042: INFO: Pod "downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d" satisfied condition "Succeeded or Failed"
    Mar 22 21:44:03.048: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d container client-container: <nil>
    STEP: delete the pod 03/22/23 21:44:03.067
    Mar 22 21:44:03.082: INFO: Waiting for pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d to disappear
    Mar 22 21:44:03.086: INFO: Pod downwardapi-volume-f4594c0b-9ecf-4888-8953-4c70c972b91d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:44:03.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6340" for this suite. 03/22/23 21:44:03.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:44:03.12
Mar 22 21:44:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename secrets 03/22/23 21:44:03.121
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:03.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:03.142
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-a7c5779f-460c-4871-88c4-b0026d2dab01 03/22/23 21:44:03.149
STEP: Creating a pod to test consume secrets 03/22/23 21:44:03.155
Mar 22 21:44:03.166: INFO: Waiting up to 5m0s for pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058" in namespace "secrets-3179" to be "Succeeded or Failed"
Mar 22 21:44:03.170: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908922ms
Mar 22 21:44:05.177: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977062s
Mar 22 21:44:07.176: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009387517s
STEP: Saw pod success 03/22/23 21:44:07.176
Mar 22 21:44:07.177: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058" satisfied condition "Succeeded or Failed"
Mar 22 21:44:07.181: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 container secret-volume-test: <nil>
STEP: delete the pod 03/22/23 21:44:07.192
Mar 22 21:44:07.206: INFO: Waiting for pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 to disappear
Mar 22 21:44:07.213: INFO: Pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 22 21:44:07.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3179" for this suite. 03/22/23 21:44:07.22
------------------------------
• [4.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:44:03.12
    Mar 22 21:44:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename secrets 03/22/23 21:44:03.121
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:03.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:03.142
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-a7c5779f-460c-4871-88c4-b0026d2dab01 03/22/23 21:44:03.149
    STEP: Creating a pod to test consume secrets 03/22/23 21:44:03.155
    Mar 22 21:44:03.166: INFO: Waiting up to 5m0s for pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058" in namespace "secrets-3179" to be "Succeeded or Failed"
    Mar 22 21:44:03.170: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908922ms
    Mar 22 21:44:05.177: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977062s
    Mar 22 21:44:07.176: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009387517s
    STEP: Saw pod success 03/22/23 21:44:07.176
    Mar 22 21:44:07.177: INFO: Pod "pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058" satisfied condition "Succeeded or Failed"
    Mar 22 21:44:07.181: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 container secret-volume-test: <nil>
    STEP: delete the pod 03/22/23 21:44:07.192
    Mar 22 21:44:07.206: INFO: Waiting for pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 to disappear
    Mar 22 21:44:07.213: INFO: Pod pod-secrets-3f3d5766-d042-4c5f-9e54-dfdb21f31058 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:44:07.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3179" for this suite. 03/22/23 21:44:07.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:44:07.25
Mar 22 21:44:07.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename security-context 03/22/23 21:44:07.252
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:07.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:07.273
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/22/23 21:44:07.279
Mar 22 21:44:07.293: INFO: Waiting up to 5m0s for pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905" in namespace "security-context-8484" to be "Succeeded or Failed"
Mar 22 21:44:07.297: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Pending", Reason="", readiness=false. Elapsed: 3.783719ms
Mar 22 21:44:09.303: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010130447s
Mar 22 21:44:11.302: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009295128s
STEP: Saw pod success 03/22/23 21:44:11.303
Mar 22 21:44:11.303: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905" satisfied condition "Succeeded or Failed"
Mar 22 21:44:11.308: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 container test-container: <nil>
STEP: delete the pod 03/22/23 21:44:11.319
Mar 22 21:44:11.334: INFO: Waiting for pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 to disappear
Mar 22 21:44:11.338: INFO: Pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 22 21:44:11.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8484" for this suite. 03/22/23 21:44:11.344
------------------------------
• [4.102 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:44:07.25
    Mar 22 21:44:07.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename security-context 03/22/23 21:44:07.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:07.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:07.273
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/22/23 21:44:07.279
    Mar 22 21:44:07.293: INFO: Waiting up to 5m0s for pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905" in namespace "security-context-8484" to be "Succeeded or Failed"
    Mar 22 21:44:07.297: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Pending", Reason="", readiness=false. Elapsed: 3.783719ms
    Mar 22 21:44:09.303: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010130447s
    Mar 22 21:44:11.302: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009295128s
    STEP: Saw pod success 03/22/23 21:44:11.303
    Mar 22 21:44:11.303: INFO: Pod "security-context-bfde3909-d320-4c7e-9f1b-c053d951c905" satisfied condition "Succeeded or Failed"
    Mar 22 21:44:11.308: INFO: Trying to get logs from node pool-v7t41yxh0-q56kk pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 container test-container: <nil>
    STEP: delete the pod 03/22/23 21:44:11.319
    Mar 22 21:44:11.334: INFO: Waiting for pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 to disappear
    Mar 22 21:44:11.338: INFO: Pod security-context-bfde3909-d320-4c7e-9f1b-c053d951c905 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:44:11.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8484" for this suite. 03/22/23 21:44:11.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:44:11.354
Mar 22 21:44:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename job 03/22/23 21:44:11.356
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:11.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:11.38
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 03/22/23 21:44:11.398
STEP: Patching the Job 03/22/23 21:44:11.405
STEP: Watching for Job to be patched 03/22/23 21:44:11.422
Mar 22 21:44:11.426: INFO: Event ADDED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 22 21:44:11.426: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 22 21:44:11.426: INFO: Event MODIFIED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/22/23 21:44:11.426
STEP: Watching for Job to be updated 03/22/23 21:44:11.438
Mar 22 21:44:11.441: INFO: Event MODIFIED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:11.442: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/22/23 21:44:11.442
Mar 22 21:44:11.447: INFO: Job: e2e-6bgdm as labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm]
STEP: Waiting for job to complete 03/22/23 21:44:11.448
STEP: Delete a job collection with a labelselector 03/22/23 21:44:21.453
STEP: Watching for Job to be deleted 03/22/23 21:44:21.461
Mar 22 21:44:21.475: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:21.475: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 22 21:44:21.476: INFO: Event DELETED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/22/23 21:44:21.476
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 22 21:44:21.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1839" for this suite. 03/22/23 21:44:21.487
------------------------------
• [SLOW TEST] [10.143 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:44:11.354
    Mar 22 21:44:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename job 03/22/23 21:44:11.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:11.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:11.38
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 03/22/23 21:44:11.398
    STEP: Patching the Job 03/22/23 21:44:11.405
    STEP: Watching for Job to be patched 03/22/23 21:44:11.422
    Mar 22 21:44:11.426: INFO: Event ADDED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 22 21:44:11.426: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 22 21:44:11.426: INFO: Event MODIFIED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/22/23 21:44:11.426
    STEP: Watching for Job to be updated 03/22/23 21:44:11.438
    Mar 22 21:44:11.441: INFO: Event MODIFIED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:11.442: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/22/23 21:44:11.442
    Mar 22 21:44:11.447: INFO: Job: e2e-6bgdm as labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm]
    STEP: Waiting for job to complete 03/22/23 21:44:11.448
    STEP: Delete a job collection with a labelselector 03/22/23 21:44:21.453
    STEP: Watching for Job to be deleted 03/22/23 21:44:21.461
    Mar 22 21:44:21.475: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:21.475: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:21.476: INFO: Event MODIFIED observed for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 22 21:44:21.476: INFO: Event DELETED found for Job e2e-6bgdm in namespace job-1839 with labels: map[e2e-6bgdm:patched e2e-job-label:e2e-6bgdm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/22/23 21:44:21.476
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:44:21.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1839" for this suite. 03/22/23 21:44:21.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/22/23 21:44:21.505
Mar 22 21:44:21.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 21:44:21.507
STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:21.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:21.528
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 22 21:44:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 22 21:44:24.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7236" for this suite. 03/22/23 21:44:24.695
------------------------------
• [3.199 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/22/23 21:44:21.505
    Mar 22 21:44:21.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    STEP: Building a namespace api object, basename custom-resource-definition 03/22/23 21:44:21.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/22/23 21:44:21.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/22/23 21:44:21.528
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 22 21:44:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3196172556
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 22 21:44:24.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7236" for this suite. 03/22/23 21:44:24.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Mar 22 21:44:24.721: INFO: Running AfterSuite actions on node 1
Mar 22 21:44:24.721: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.003 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Mar 22 21:44:24.721: INFO: Running AfterSuite actions on node 1
    Mar 22 21:44:24.721: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.204 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5997.126 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h39m58.075103214s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

